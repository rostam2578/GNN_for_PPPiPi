0: gpu012.ihep.ac.cn
GPU 0: Tesla V100-SXM2-32GB (UUID: GPU-c11fe911-4722-b3eb-2a08-290ee5f39555)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1127.8.2.el7.x86_64/extra/nvidia.ko.xz
alias:          char-major-195-*
version:        450.36.06
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.8
srcversion:     BB5CB243542347D4EB0C79C
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        
vermagic:       3.10.0-1127.8.2.el7.x86_64 SMP mod_unload modversions 
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_MapRegistersEarly:int
parm:           NVreg_RegisterForACPIEvents:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_EnableBacklightHandler:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_AssignGpus:charp

nvidia-smi:
Fri Aug 12 16:29:04 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:B5:00.0 Off |                    0 |
| N/A   41C    P0    44W / 300W |      0MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: Tesla V100-SXM2-32GB

 CUDA Device Total Memory [GB]: 34.089730048

 Device capability: (7, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2b73919f08e0> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m53.170s
user	0m3.668s
sys	0m2.656s
[16:30:00] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch
/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/cupy/_environment.py:438: UserWarning: 
--------------------------------------------------------------------------------

  CuPy may not function correctly because multiple CuPy packages are installed
  in your environment:

    cupy, cupy-cuda110

  Follow these steps to resolve this issue:

    1. For all packages listed above, run the following command to remove all
       existing CuPy installations:

         $ pip uninstall <package_name>

      If you previously installed CuPy via conda, also run the following:

         $ conda uninstall cupy

    2. Install the appropriate CuPy package.
       Refer to the Installation Guide for detailed instructions.

         https://docs.cupy.dev/en/stable/install.html

--------------------------------------------------------------------------------

  warnings.warn(f'''




 Training ... 






 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[ 0.6711],
        [-1.1375],
        [-0.2463],
        ...,
        [-1.2806],
        [ 1.3041],
        [ 0.9459]], device='cuda:0', requires_grad=True) 
node features sum: tensor(-81.2134, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14





 Loading data ... 


shape (80000, 6796) (80000, 6796)
sum 5574226 8401300
shape torch.Size([80000, 6796]) torch.Size([80000, 6796])
Model name: DGLpppipiGcnReNewestweight7N2
net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[-2.1004e-02,  1.2044e-01,  2.5330e-02, -2.4821e-02,  9.8705e-02,
          5.4332e-02,  2.0597e-03, -1.5032e-01, -9.0077e-02,  1.3204e-01,
          1.5274e-02, -9.4076e-02,  1.6862e-02,  3.1423e-02,  1.4440e-01,
         -4.3474e-02, -1.4760e-01,  2.7377e-02,  1.0761e-02, -8.6932e-02,
         -1.5361e-02,  1.1415e-01,  7.3809e-02, -1.2927e-01, -1.1216e-01,
         -9.3190e-02,  1.0549e-01, -7.1140e-02,  3.2799e-03, -1.3406e-01,
          6.3437e-02,  1.1540e-01, -7.9154e-02,  1.0155e-01, -2.9607e-02,
         -9.2042e-02,  1.0525e-01,  3.7936e-02, -8.4811e-02, -7.8127e-03,
          5.2822e-02, -1.2044e-01, -1.3931e-01,  8.6994e-02, -1.2198e-01,
          8.9737e-02,  7.9744e-03, -1.0638e-01,  1.4705e-01, -1.3305e-01,
         -1.2885e-01,  1.3716e-01, -1.2054e-02,  4.4413e-02,  8.5395e-02,
         -5.9343e-02,  4.1253e-02, -8.7163e-02,  1.0995e-01,  3.4665e-02,
         -1.9094e-02, -1.1788e-02, -1.2655e-01,  1.4897e-01,  1.3017e-01,
          1.1085e-01, -6.1627e-02, -1.4097e-01,  7.5420e-02,  3.3987e-02,
         -1.0531e-01, -1.2209e-01,  1.2971e-01, -1.8830e-02, -4.2342e-03,
         -8.4574e-02,  2.8348e-02,  1.0313e-01, -8.9082e-02,  9.3513e-02,
         -2.9024e-02,  1.3044e-01, -1.3881e-01,  7.5442e-02, -6.3353e-02,
         -3.8877e-02, -1.0304e-01,  7.9264e-02, -2.9727e-02,  1.1721e-01,
          1.2325e-01,  5.4419e-02,  8.6889e-02,  8.4650e-02, -1.3691e-01,
          8.5662e-02, -9.3203e-02,  8.2302e-02, -1.2541e-01, -3.5301e-02,
         -1.0130e-01,  8.7774e-02,  1.4735e-01,  8.0914e-02,  1.5352e-02,
         -1.5188e-01, -1.3192e-01,  5.8079e-02, -1.2842e-01,  3.5675e-02,
          2.4695e-03, -3.1997e-02, -8.0545e-02,  5.4693e-03, -8.0777e-02,
         -1.0864e-01,  1.2600e-01,  7.7000e-02,  1.1459e-01, -1.4055e-01,
         -6.3787e-02,  1.3192e-01, -8.2684e-02,  4.8597e-02,  1.2335e-01,
         -2.7768e-02, -1.0339e-01, -2.0759e-02,  1.3641e-01,  1.0573e-01,
          3.7062e-03,  1.7589e-02,  5.8841e-02, -9.8623e-02, -1.2877e-01,
          5.0146e-02, -3.2291e-02, -6.0959e-03,  1.5210e-01,  1.2552e-01,
          5.1316e-02,  1.2796e-01, -1.4197e-01, -1.1743e-01, -1.0345e-01,
          1.1357e-01,  2.6008e-02, -1.5038e-02,  1.1049e-01,  1.1109e-01,
          1.4419e-01,  1.1139e-01, -5.1171e-02,  1.2583e-01, -1.4712e-01,
          1.7206e-02,  8.1060e-02,  9.1348e-02,  1.4222e-01, -5.3313e-02,
          6.3811e-02,  6.0936e-02, -1.0254e-01, -1.9057e-02, -3.8188e-02,
         -2.8159e-02, -3.5608e-02, -5.7654e-02,  1.2343e-01, -2.8486e-02,
         -9.7631e-02, -4.4865e-03,  1.0282e-01, -3.7695e-02, -7.1636e-02,
          1.1716e-01,  1.0484e-02, -1.3146e-01,  8.5447e-02, -5.3925e-02,
          7.3053e-02,  1.2911e-01,  1.2177e-01,  5.4446e-02, -5.6340e-03,
          1.6437e-02,  2.3151e-02, -2.8674e-02,  1.0554e-01, -9.3926e-02,
          4.1622e-02, -1.0581e-01,  1.1737e-01,  1.3727e-01, -9.7404e-03,
          1.4779e-02, -8.5850e-03, -3.4828e-02, -6.7642e-02,  7.7228e-02,
          1.6605e-02, -5.1031e-02, -1.0003e-01,  2.5845e-02, -1.1773e-01,
         -1.6907e-02, -7.8835e-02,  1.0085e-01,  8.7355e-02, -7.2859e-02,
         -7.0230e-02, -6.7300e-02, -3.5715e-02, -1.8447e-02,  1.4208e-01,
         -1.4950e-02,  1.2696e-01,  8.0412e-02,  5.7866e-03,  4.7326e-02,
          2.2043e-02,  1.1178e-01, -6.7992e-02,  9.7294e-03,  2.9425e-02,
          1.2161e-02, -4.8782e-02,  8.0539e-02,  9.1922e-02, -3.8272e-02,
          1.4148e-01,  3.5902e-02, -7.2498e-02, -4.0691e-05,  3.2631e-03,
         -4.8080e-02, -3.9778e-02, -9.2767e-02,  1.0477e-01, -9.6692e-02,
          1.0080e-01,  8.3771e-03, -8.6576e-03,  1.4674e-01, -1.2558e-01,
         -1.3477e-01,  9.2769e-02,  1.0250e-01, -1.2942e-01,  1.5235e-01,
          1.6993e-02, -8.9159e-02,  4.5046e-02, -1.2072e-01,  7.2594e-02,
          5.1591e-02]], device='cuda:0') 
 Parameter containing:
tensor([[-2.1004e-02,  1.2044e-01,  2.5330e-02, -2.4821e-02,  9.8705e-02,
          5.4332e-02,  2.0597e-03, -1.5032e-01, -9.0077e-02,  1.3204e-01,
          1.5274e-02, -9.4076e-02,  1.6862e-02,  3.1423e-02,  1.4440e-01,
         -4.3474e-02, -1.4760e-01,  2.7377e-02,  1.0761e-02, -8.6932e-02,
         -1.5361e-02,  1.1415e-01,  7.3809e-02, -1.2927e-01, -1.1216e-01,
         -9.3190e-02,  1.0549e-01, -7.1140e-02,  3.2799e-03, -1.3406e-01,
          6.3437e-02,  1.1540e-01, -7.9154e-02,  1.0155e-01, -2.9607e-02,
         -9.2042e-02,  1.0525e-01,  3.7936e-02, -8.4811e-02, -7.8127e-03,
          5.2822e-02, -1.2044e-01, -1.3931e-01,  8.6994e-02, -1.2198e-01,
          8.9737e-02,  7.9744e-03, -1.0638e-01,  1.4705e-01, -1.3305e-01,
         -1.2885e-01,  1.3716e-01, -1.2054e-02,  4.4413e-02,  8.5395e-02,
         -5.9343e-02,  4.1253e-02, -8.7163e-02,  1.0995e-01,  3.4665e-02,
         -1.9094e-02, -1.1788e-02, -1.2655e-01,  1.4897e-01,  1.3017e-01,
          1.1085e-01, -6.1627e-02, -1.4097e-01,  7.5420e-02,  3.3987e-02,
         -1.0531e-01, -1.2209e-01,  1.2971e-01, -1.8830e-02, -4.2342e-03,
         -8.4574e-02,  2.8348e-02,  1.0313e-01, -8.9082e-02,  9.3513e-02,
         -2.9024e-02,  1.3044e-01, -1.3881e-01,  7.5442e-02, -6.3353e-02,
         -3.8877e-02, -1.0304e-01,  7.9264e-02, -2.9727e-02,  1.1721e-01,
          1.2325e-01,  5.4419e-02,  8.6889e-02,  8.4650e-02, -1.3691e-01,
          8.5662e-02, -9.3203e-02,  8.2302e-02, -1.2541e-01, -3.5301e-02,
         -1.0130e-01,  8.7774e-02,  1.4735e-01,  8.0914e-02,  1.5352e-02,
         -1.5188e-01, -1.3192e-01,  5.8079e-02, -1.2842e-01,  3.5675e-02,
          2.4695e-03, -3.1997e-02, -8.0545e-02,  5.4693e-03, -8.0777e-02,
         -1.0864e-01,  1.2600e-01,  7.7000e-02,  1.1459e-01, -1.4055e-01,
         -6.3787e-02,  1.3192e-01, -8.2684e-02,  4.8597e-02,  1.2335e-01,
         -2.7768e-02, -1.0339e-01, -2.0759e-02,  1.3641e-01,  1.0573e-01,
          3.7062e-03,  1.7589e-02,  5.8841e-02, -9.8623e-02, -1.2877e-01,
          5.0146e-02, -3.2291e-02, -6.0959e-03,  1.5210e-01,  1.2552e-01,
          5.1316e-02,  1.2796e-01, -1.4197e-01, -1.1743e-01, -1.0345e-01,
          1.1357e-01,  2.6008e-02, -1.5038e-02,  1.1049e-01,  1.1109e-01,
          1.4419e-01,  1.1139e-01, -5.1171e-02,  1.2583e-01, -1.4712e-01,
          1.7206e-02,  8.1060e-02,  9.1348e-02,  1.4222e-01, -5.3313e-02,
          6.3811e-02,  6.0936e-02, -1.0254e-01, -1.9057e-02, -3.8188e-02,
         -2.8159e-02, -3.5608e-02, -5.7654e-02,  1.2343e-01, -2.8486e-02,
         -9.7631e-02, -4.4865e-03,  1.0282e-01, -3.7695e-02, -7.1636e-02,
          1.1716e-01,  1.0484e-02, -1.3146e-01,  8.5447e-02, -5.3925e-02,
          7.3053e-02,  1.2911e-01,  1.2177e-01,  5.4446e-02, -5.6340e-03,
          1.6437e-02,  2.3151e-02, -2.8674e-02,  1.0554e-01, -9.3926e-02,
          4.1622e-02, -1.0581e-01,  1.1737e-01,  1.3727e-01, -9.7404e-03,
          1.4779e-02, -8.5850e-03, -3.4828e-02, -6.7642e-02,  7.7228e-02,
          1.6605e-02, -5.1031e-02, -1.0003e-01,  2.5845e-02, -1.1773e-01,
         -1.6907e-02, -7.8835e-02,  1.0085e-01,  8.7355e-02, -7.2859e-02,
         -7.0230e-02, -6.7300e-02, -3.5715e-02, -1.8447e-02,  1.4208e-01,
         -1.4950e-02,  1.2696e-01,  8.0412e-02,  5.7866e-03,  4.7326e-02,
          2.2043e-02,  1.1178e-01, -6.7992e-02,  9.7294e-03,  2.9425e-02,
          1.2161e-02, -4.8782e-02,  8.0539e-02,  9.1922e-02, -3.8272e-02,
          1.4148e-01,  3.5902e-02, -7.2498e-02, -4.0691e-05,  3.2631e-03,
         -4.8080e-02, -3.9778e-02, -9.2767e-02,  1.0477e-01, -9.6692e-02,
          1.0080e-01,  8.3771e-03, -8.6576e-03,  1.4674e-01, -1.2558e-01,
         -1.3477e-01,  9.2769e-02,  1.0250e-01, -1.2942e-01,  1.5235e-01,
          1.6993e-02, -8.9159e-02,  4.5046e-02, -1.2072e-01,  7.2594e-02,
          5.1591e-02]], device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[-0.0133, -0.1186,  0.0869,  ...,  0.0061,  0.1028,  0.1078],
        [ 0.0757, -0.0903, -0.0798,  ..., -0.0983,  0.0475,  0.0698],
        [ 0.0414, -0.1037, -0.0062,  ..., -0.0361,  0.1143, -0.0765],
        ...,
        [-0.0332, -0.0780,  0.0281,  ..., -0.0542,  0.1227, -0.0916],
        [-0.0731,  0.0051, -0.0762,  ..., -0.0297, -0.0812,  0.0634],
        [ 0.0714,  0.1052, -0.0801,  ..., -0.1135, -0.0324, -0.0451]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0133, -0.1186,  0.0869,  ...,  0.0061,  0.1028,  0.1078],
        [ 0.0757, -0.0903, -0.0798,  ..., -0.0983,  0.0475,  0.0698],
        [ 0.0414, -0.1037, -0.0062,  ..., -0.0361,  0.1143, -0.0765],
        ...,
        [-0.0332, -0.0780,  0.0281,  ..., -0.0542,  0.1227, -0.0916],
        [-0.0731,  0.0051, -0.0762,  ..., -0.0297, -0.0812,  0.0634],
        [ 0.0714,  0.1052, -0.0801,  ..., -0.1135, -0.0324, -0.0451]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[-0.0991,  0.0353, -0.0010,  ..., -0.1003,  0.0703,  0.0350],
        [-0.1500, -0.0956,  0.0710,  ...,  0.0952, -0.0992,  0.1691],
        [ 0.1029,  0.1394,  0.0819,  ..., -0.1562, -0.1618,  0.0674],
        ...,
        [ 0.0903, -0.0286,  0.1175,  ...,  0.1219, -0.0014,  0.0693],
        [ 0.0453,  0.0015, -0.0249,  ..., -0.1701, -0.0395,  0.0265],
        [ 0.0566, -0.0948,  0.1314,  ...,  0.0874, -0.1410,  0.0576]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0991,  0.0353, -0.0010,  ..., -0.1003,  0.0703,  0.0350],
        [-0.1500, -0.0956,  0.0710,  ...,  0.0952, -0.0992,  0.1691],
        [ 0.1029,  0.1394,  0.0819,  ..., -0.1562, -0.1618,  0.0674],
        ...,
        [ 0.0903, -0.0286,  0.1175,  ...,  0.1219, -0.0014,  0.0693],
        [ 0.0453,  0.0015, -0.0249,  ..., -0.1701, -0.0395,  0.0265],
        [ 0.0566, -0.0948,  0.1314,  ...,  0.0874, -0.1410,  0.0576]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[ 0.0257,  0.0797,  0.2106,  ...,  0.2012, -0.1071,  0.0248],
        [ 0.0878,  0.0747, -0.2067,  ...,  0.1125,  0.1609,  0.1752],
        [ 0.1689, -0.1416,  0.2366,  ..., -0.0208,  0.2249,  0.0857],
        ...,
        [-0.1987, -0.0707, -0.1749,  ..., -0.1221, -0.0788,  0.1124],
        [-0.1219, -0.1463, -0.2077,  ...,  0.0466,  0.1601, -0.0531],
        [-0.0321,  0.2405, -0.0368,  ...,  0.2446,  0.1787,  0.2486]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0257,  0.0797,  0.2106,  ...,  0.2012, -0.1071,  0.0248],
        [ 0.0878,  0.0747, -0.2067,  ...,  0.1125,  0.1609,  0.1752],
        [ 0.1689, -0.1416,  0.2366,  ..., -0.0208,  0.2249,  0.0857],
        ...,
        [-0.1987, -0.0707, -0.1749,  ..., -0.1221, -0.0788,  0.1124],
        [-0.1219, -0.1463, -0.2077,  ...,  0.0466,  0.1601, -0.0531],
        [-0.0321,  0.2405, -0.0368,  ...,  0.2446,  0.1787,  0.2486]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[-0.3809],
        [ 0.3477],
        [ 0.4105],
        [-0.2972],
        [-0.0123],
        [-0.0112],
        [ 0.4129],
        [ 0.2507],
        [-0.0577],
        [-0.3946],
        [ 0.2261],
        [-0.2661],
        [ 0.1259],
        [ 0.0401],
        [ 0.3581],
        [-0.1724],
        [ 0.2235],
        [ 0.0079],
        [-0.2322],
        [-0.1587],
        [-0.3154],
        [ 0.4011],
        [-0.4018],
        [-0.3837],
        [ 0.3993],
        [ 0.0897],
        [ 0.3433],
        [ 0.1404],
        [ 0.3097],
        [-0.2615],
        [-0.0440],
        [ 0.3786]], device='cuda:0') 
 Parameter containing:
tensor([[-0.3809],
        [ 0.3477],
        [ 0.4105],
        [-0.2972],
        [-0.0123],
        [-0.0112],
        [ 0.4129],
        [ 0.2507],
        [-0.0577],
        [-0.3946],
        [ 0.2261],
        [-0.2661],
        [ 0.1259],
        [ 0.0401],
        [ 0.3581],
        [-0.1724],
        [ 0.2235],
        [ 0.0079],
        [-0.2322],
        [-0.1587],
        [-0.3154],
        [ 0.4011],
        [-0.4018],
        [-0.3837],
        [ 0.3993],
        [ 0.0897],
        [ 0.3433],
        [ 0.1404],
        [ 0.3097],
        [-0.2615],
        [-0.0440],
        [ 0.3786]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[-0.0592, -0.0105,  0.1092,  0.0182, -0.0026,  0.0563,  0.1378, -0.0487,
         -0.0248, -0.0757, -0.0581, -0.0749,  0.0348,  0.1367,  0.1282, -0.0654,
         -0.1235, -0.0565, -0.0796, -0.0842, -0.0962, -0.1066, -0.0405, -0.1389,
         -0.1277, -0.0029,  0.0365,  0.0100,  0.0942, -0.0957, -0.0209,  0.0116,
         -0.1145,  0.1357,  0.0125, -0.1506, -0.0864,  0.1132, -0.0233, -0.1116,
         -0.1112,  0.0906, -0.1262,  0.0973, -0.1002,  0.0183,  0.0943, -0.0671,
          0.1495, -0.0811,  0.0841, -0.0123, -0.1348, -0.0194,  0.1050, -0.1069,
         -0.1216,  0.0323,  0.0847,  0.0067, -0.0627, -0.0854, -0.0262,  0.1259,
          0.0366,  0.0482,  0.1215, -0.0566, -0.0450,  0.0318,  0.0971, -0.0220,
          0.0097,  0.1478, -0.0764, -0.0356,  0.0712, -0.0237, -0.1257,  0.0443,
          0.1300,  0.0844, -0.1514, -0.0724, -0.0131, -0.0353,  0.0240,  0.0296,
         -0.0285,  0.1374, -0.0350, -0.0120,  0.0999,  0.1347, -0.0502, -0.0902,
          0.0778,  0.1098,  0.1372,  0.0047,  0.0780,  0.0542, -0.0231,  0.1354,
         -0.1328,  0.0026, -0.0908,  0.1236,  0.0670,  0.0906,  0.0461, -0.0215,
         -0.0199, -0.0941, -0.0817, -0.0549,  0.0561,  0.0289,  0.1166,  0.0264,
         -0.0365,  0.0718, -0.1266,  0.0529,  0.0513,  0.1445,  0.1463,  0.0094,
         -0.1488, -0.1216,  0.0744, -0.1479,  0.1104,  0.0944,  0.1284, -0.0693,
          0.0525, -0.0287, -0.0844, -0.0241, -0.1264,  0.1430,  0.0202,  0.0647,
         -0.1363, -0.0316, -0.0760,  0.0672,  0.0987,  0.0464,  0.1027,  0.0355,
          0.0791, -0.1406, -0.1159, -0.0782, -0.0227, -0.0809,  0.0217, -0.1415,
         -0.0380,  0.1132,  0.1048,  0.0959,  0.0127, -0.1101, -0.1293,  0.1447,
          0.1248, -0.1085, -0.1187, -0.0312, -0.0260, -0.0028, -0.1039,  0.0593,
         -0.0242, -0.0647, -0.1380,  0.0251, -0.1189, -0.0074, -0.1477, -0.1190,
          0.0683, -0.0870,  0.0161,  0.0020, -0.0466,  0.0325,  0.1471, -0.1495,
         -0.0279,  0.0502,  0.0298,  0.1118,  0.0798,  0.0380,  0.0251, -0.1055,
         -0.1435, -0.0479, -0.0116,  0.0610,  0.1036, -0.1398,  0.0795, -0.1082,
         -0.0969, -0.1290, -0.0815, -0.0604, -0.0920, -0.1407, -0.1523,  0.0900,
         -0.0829,  0.1311, -0.0269,  0.0213,  0.0243,  0.1375, -0.1204, -0.0643,
         -0.0051,  0.1298, -0.1298, -0.1119, -0.1184, -0.1472,  0.0699,  0.0208,
          0.0375,  0.0559, -0.0984,  0.0587,  0.1283, -0.0227,  0.0834,  0.0531,
         -0.0839, -0.0806, -0.0431,  0.0653, -0.0258, -0.0244,  0.1018, -0.0303,
         -0.1381,  0.0611, -0.0565,  0.0343, -0.0550, -0.0957,  0.0291,  0.0786]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0592, -0.0105,  0.1092,  0.0182, -0.0026,  0.0563,  0.1378, -0.0487,
         -0.0248, -0.0757, -0.0581, -0.0749,  0.0348,  0.1367,  0.1282, -0.0654,
         -0.1235, -0.0565, -0.0796, -0.0842, -0.0962, -0.1066, -0.0405, -0.1389,
         -0.1277, -0.0029,  0.0365,  0.0100,  0.0942, -0.0957, -0.0209,  0.0116,
         -0.1145,  0.1357,  0.0125, -0.1506, -0.0864,  0.1132, -0.0233, -0.1116,
         -0.1112,  0.0906, -0.1262,  0.0973, -0.1002,  0.0183,  0.0943, -0.0671,
          0.1495, -0.0811,  0.0841, -0.0123, -0.1348, -0.0194,  0.1050, -0.1069,
         -0.1216,  0.0323,  0.0847,  0.0067, -0.0627, -0.0854, -0.0262,  0.1259,
          0.0366,  0.0482,  0.1215, -0.0566, -0.0450,  0.0318,  0.0971, -0.0220,
          0.0097,  0.1478, -0.0764, -0.0356,  0.0712, -0.0237, -0.1257,  0.0443,
          0.1300,  0.0844, -0.1514, -0.0724, -0.0131, -0.0353,  0.0240,  0.0296,
         -0.0285,  0.1374, -0.0350, -0.0120,  0.0999,  0.1347, -0.0502, -0.0902,
          0.0778,  0.1098,  0.1372,  0.0047,  0.0780,  0.0542, -0.0231,  0.1354,
         -0.1328,  0.0026, -0.0908,  0.1236,  0.0670,  0.0906,  0.0461, -0.0215,
         -0.0199, -0.0941, -0.0817, -0.0549,  0.0561,  0.0289,  0.1166,  0.0264,
         -0.0365,  0.0718, -0.1266,  0.0529,  0.0513,  0.1445,  0.1463,  0.0094,
         -0.1488, -0.1216,  0.0744, -0.1479,  0.1104,  0.0944,  0.1284, -0.0693,
          0.0525, -0.0287, -0.0844, -0.0241, -0.1264,  0.1430,  0.0202,  0.0647,
         -0.1363, -0.0316, -0.0760,  0.0672,  0.0987,  0.0464,  0.1027,  0.0355,
          0.0791, -0.1406, -0.1159, -0.0782, -0.0227, -0.0809,  0.0217, -0.1415,
         -0.0380,  0.1132,  0.1048,  0.0959,  0.0127, -0.1101, -0.1293,  0.1447,
          0.1248, -0.1085, -0.1187, -0.0312, -0.0260, -0.0028, -0.1039,  0.0593,
         -0.0242, -0.0647, -0.1380,  0.0251, -0.1189, -0.0074, -0.1477, -0.1190,
          0.0683, -0.0870,  0.0161,  0.0020, -0.0466,  0.0325,  0.1471, -0.1495,
         -0.0279,  0.0502,  0.0298,  0.1118,  0.0798,  0.0380,  0.0251, -0.1055,
         -0.1435, -0.0479, -0.0116,  0.0610,  0.1036, -0.1398,  0.0795, -0.1082,
         -0.0969, -0.1290, -0.0815, -0.0604, -0.0920, -0.1407, -0.1523,  0.0900,
         -0.0829,  0.1311, -0.0269,  0.0213,  0.0243,  0.1375, -0.1204, -0.0643,
         -0.0051,  0.1298, -0.1298, -0.1119, -0.1184, -0.1472,  0.0699,  0.0208,
          0.0375,  0.0559, -0.0984,  0.0587,  0.1283, -0.0227,  0.0834,  0.0531,
         -0.0839, -0.0806, -0.0431,  0.0653, -0.0258, -0.0244,  0.1018, -0.0303,
         -0.1381,  0.0611, -0.0565,  0.0343, -0.0550, -0.0957,  0.0291,  0.0786]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[ 0.0133,  0.0018, -0.0919,  ...,  0.0547,  0.0432, -0.0811],
        [-0.0613,  0.0382,  0.0533,  ..., -0.0551, -0.1180, -0.0871],
        [ 0.0721,  0.1147, -0.0692,  ...,  0.0962, -0.1136, -0.0920],
        ...,
        [ 0.0863, -0.0695,  0.0769,  ..., -0.0643,  0.0090,  0.0323],
        [ 0.0776,  0.0017, -0.1029,  ..., -0.1112,  0.0370, -0.0789],
        [-0.1051,  0.0710,  0.0990,  ...,  0.0555, -0.0501, -0.0307]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0133,  0.0018, -0.0919,  ...,  0.0547,  0.0432, -0.0811],
        [-0.0613,  0.0382,  0.0533,  ..., -0.0551, -0.1180, -0.0871],
        [ 0.0721,  0.1147, -0.0692,  ...,  0.0962, -0.1136, -0.0920],
        ...,
        [ 0.0863, -0.0695,  0.0769,  ..., -0.0643,  0.0090,  0.0323],
        [ 0.0776,  0.0017, -0.1029,  ..., -0.1112,  0.0370, -0.0789],
        [-0.1051,  0.0710,  0.0990,  ...,  0.0555, -0.0501, -0.0307]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[ 0.1337, -0.0043,  0.0944,  ..., -0.1672,  0.1109,  0.1024],
        [ 0.0469,  0.1636,  0.1521,  ...,  0.1586,  0.0035, -0.0485],
        [ 0.1483,  0.1169,  0.0342,  ..., -0.0037,  0.0925, -0.0994],
        ...,
        [ 0.1482,  0.1059, -0.1009,  ...,  0.0437, -0.0979, -0.0384],
        [ 0.0223, -0.0768,  0.1600,  ..., -0.0775, -0.0871, -0.1603],
        [ 0.1336, -0.1512, -0.0563,  ..., -0.0127,  0.0518, -0.1537]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.1337, -0.0043,  0.0944,  ..., -0.1672,  0.1109,  0.1024],
        [ 0.0469,  0.1636,  0.1521,  ...,  0.1586,  0.0035, -0.0485],
        [ 0.1483,  0.1169,  0.0342,  ..., -0.0037,  0.0925, -0.0994],
        ...,
        [ 0.1482,  0.1059, -0.1009,  ...,  0.0437, -0.0979, -0.0384],
        [ 0.0223, -0.0768,  0.1600,  ..., -0.0775, -0.0871, -0.1603],
        [ 0.1336, -0.1512, -0.0563,  ..., -0.0127,  0.0518, -0.1537]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[ 0.0851,  0.1351,  0.1233,  ..., -0.1753, -0.1463,  0.0768],
        [ 0.2074,  0.0716,  0.1217,  ..., -0.0363,  0.0553, -0.1746],
        [-0.2494, -0.0847,  0.0091,  ..., -0.1924, -0.1421, -0.1480],
        ...,
        [-0.0498, -0.0148,  0.1421,  ...,  0.1225,  0.1864,  0.1465],
        [ 0.0950,  0.1167, -0.0876,  ..., -0.0458,  0.1684, -0.0790],
        [-0.1015, -0.1854, -0.0248,  ...,  0.0578,  0.0424, -0.1714]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0851,  0.1351,  0.1233,  ..., -0.1753, -0.1463,  0.0768],
        [ 0.2074,  0.0716,  0.1217,  ..., -0.0363,  0.0553, -0.1746],
        [-0.2494, -0.0847,  0.0091,  ..., -0.1924, -0.1421, -0.1480],
        ...,
        [-0.0498, -0.0148,  0.1421,  ...,  0.1225,  0.1864,  0.1465],
        [ 0.0950,  0.1167, -0.0876,  ..., -0.0458,  0.1684, -0.0790],
        [-0.1015, -0.1854, -0.0248,  ...,  0.0578,  0.0424, -0.1714]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[-3.8999e-01],
        [ 9.4615e-02],
        [ 1.2296e-01],
        [-8.5484e-02],
        [-1.9729e-01],
        [-6.0608e-02],
        [ 1.5064e-01],
        [-7.3940e-02],
        [ 2.7280e-01],
        [-2.7654e-02],
        [ 5.0614e-03],
        [ 2.0651e-01],
        [-2.8393e-01],
        [ 4.8441e-02],
        [ 1.5213e-01],
        [-1.0268e-02],
        [ 2.1061e-01],
        [-1.2726e-01],
        [-3.4565e-05],
        [ 2.1864e-01],
        [ 1.8250e-01],
        [ 1.0328e-01],
        [ 3.3936e-01],
        [ 3.8919e-01],
        [-1.7725e-01],
        [ 3.9101e-01],
        [-1.5308e-01],
        [ 3.1671e-01],
        [ 6.7337e-02],
        [-3.4374e-01],
        [ 2.8720e-01],
        [-2.0171e-01]], device='cuda:0') 
 Parameter containing:
tensor([[-3.8999e-01],
        [ 9.4615e-02],
        [ 1.2296e-01],
        [-8.5484e-02],
        [-1.9729e-01],
        [-6.0608e-02],
        [ 1.5064e-01],
        [-7.3940e-02],
        [ 2.7280e-01],
        [-2.7654e-02],
        [ 5.0614e-03],
        [ 2.0651e-01],
        [-2.8393e-01],
        [ 4.8441e-02],
        [ 1.5213e-01],
        [-1.0268e-02],
        [ 2.1061e-01],
        [-1.2726e-01],
        [-3.4565e-05],
        [ 2.1864e-01],
        [ 1.8250e-01],
        [ 1.0328e-01],
        [ 3.3936e-01],
        [ 3.8919e-01],
        [-1.7725e-01],
        [ 3.9101e-01],
        [-1.5308e-01],
        [ 3.1671e-01],
        [ 6.7337e-02],
        [-3.4374e-01],
        [ 2.8720e-01],
        [-2.0171e-01]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(152.8371, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(4.0273, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(4.1333, device='cuda:0')



h[100].sum tensor(-1.9322, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.9831, device='cuda:0')



h[200].sum tensor(-1.5423, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5829, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(3816.0210, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0007, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],
        [0.0034, 0.0000, 0.0000,  ..., 0.0000, 0.0094, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(14668.0537, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(132.7516, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(10.6198, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-13.3954, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-13.7726, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0653],
        [0.0800],
        [0.1153],
        ...,
        [0.0185],
        [0.0185],
        [0.0146]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(1648.2002, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 20 from the network before training 
result1: tensor([[0.0653],
        [0.0800],
        [0.1153],
        ...,
        [0.0185],
        [0.0185],
        [0.0146]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')



input graph: 
g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(93.0393, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.4758, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.3423, device='cuda:0')



h[100].sum tensor(-19.6390, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-19.4798, device='cuda:0')



h[200].sum tensor(5.3689, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(5.3254, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(15354.2295, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0124, 0.0000, 0.0083,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([13592, 128]) 
h2.sum tensor(76255.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1005.9021, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(70.7699, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-145.9841, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1070.1697, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(75.2914, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5810],
        [-0.3563],
        [-0.2180],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([13592, 1]) 
h5.sum tensor(-34129.6055, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[0.0653],
        [0.0800],
        [0.1153],
        ...,
        [0.0185],
        [0.0185],
        [0.0146]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]



load_model False 
TraEvN 12001 
BatchSize 15 
EpochNum 5 
epoch_save 5 
LrVal 0.001 
weight_decay 5e-05 
startmesh 184 
endmesh 185 






optimizer.param_groups [{'params': [Parameter containing:
tensor([[-4.9274e-02,  1.8557e-02, -1.3394e-01, -1.1032e-03, -3.8899e-02,
         -1.4039e-01,  6.5566e-02, -1.3809e-01,  5.1142e-03,  1.3695e-01,
          6.7609e-02,  1.8175e-02, -1.5321e-03,  5.6105e-02,  8.3791e-02,
          1.1829e-01, -1.0764e-02,  1.4440e-01,  7.7639e-02, -1.1915e-01,
         -1.7598e-02,  6.3754e-02,  9.7761e-02,  3.5667e-02, -3.3684e-02,
         -1.1430e-01,  8.8775e-02,  8.2770e-02, -1.3868e-02, -8.1414e-03,
          7.5715e-02,  6.5454e-02,  7.4052e-02,  1.4456e-01, -1.4183e-01,
          1.2617e-01,  6.6056e-02, -8.4052e-02,  1.1460e-01, -2.6095e-02,
          2.6663e-02,  1.1053e-01, -5.4716e-02, -1.1829e-01, -9.5985e-02,
          1.3598e-02, -1.1292e-01,  3.8413e-02, -6.7026e-02,  1.5036e-02,
         -1.4130e-01,  4.3920e-02, -7.7894e-03,  1.4606e-01,  3.5887e-02,
         -6.8013e-02, -2.3148e-02, -1.4099e-01,  2.0597e-02,  1.1219e-02,
          8.6514e-02,  6.0557e-02, -6.3048e-02,  1.7281e-02,  9.3975e-02,
          1.3283e-01, -6.6607e-02,  8.4940e-02, -6.9003e-02,  2.8778e-02,
         -1.2660e-01, -4.6548e-02, -5.4817e-02,  1.0213e-01,  3.2037e-02,
         -9.5394e-02, -1.0785e-02, -1.2091e-01, -4.4943e-02,  5.1135e-02,
         -1.2844e-01, -3.6380e-03, -1.2939e-03,  1.0138e-01,  8.7803e-02,
         -8.2005e-02, -1.1476e-01, -1.2261e-01, -9.3716e-02, -1.4159e-01,
         -2.3842e-02,  5.0036e-02, -6.0953e-02, -9.4357e-02, -1.3643e-01,
          1.3427e-01, -1.3213e-01,  3.1540e-02,  1.4126e-01,  2.3607e-02,
          4.7503e-02,  4.1808e-02,  6.6414e-02, -1.2255e-01,  1.4927e-01,
         -4.9584e-02, -1.1537e-04, -1.2409e-01, -7.1187e-02,  8.2742e-02,
          1.3723e-01,  3.4827e-02,  1.4501e-01, -9.7700e-03,  1.4908e-01,
         -7.4896e-02, -3.8530e-03,  8.7982e-02, -1.4973e-01, -1.0971e-01,
          1.0209e-01,  5.2268e-02,  6.5039e-02,  1.4090e-01, -1.0595e-01,
         -4.5484e-02,  1.4837e-01,  1.0965e-01,  1.1984e-01, -1.0712e-01,
         -1.4750e-01, -4.7457e-02, -1.0176e-01,  6.1450e-02,  1.0316e-01,
         -1.1982e-01,  1.4265e-01, -1.2215e-01,  3.2488e-02, -9.2876e-02,
          1.2431e-01, -8.3761e-02,  2.6348e-02, -1.4024e-01,  9.9836e-02,
         -1.2867e-01,  1.5191e-01,  9.1211e-02, -7.8479e-03,  9.2177e-02,
         -2.3314e-02,  1.0291e-01, -2.4706e-02, -6.5057e-02,  1.4910e-01,
          6.9848e-02,  1.2007e-01, -6.0066e-02,  3.9213e-03, -3.4297e-02,
         -6.7147e-03,  7.4723e-02, -1.4867e-01,  7.1360e-02, -6.5115e-02,
         -1.1483e-01, -3.8391e-02,  8.1275e-02, -1.6887e-02, -4.2661e-02,
         -2.5211e-02,  1.3266e-01, -1.2066e-01,  6.6998e-02,  1.4741e-01,
         -7.2709e-02,  1.3426e-01,  9.5381e-02,  1.4126e-01, -7.1589e-02,
         -1.2486e-01,  1.4436e-01, -1.2169e-01, -9.4592e-03, -5.8151e-02,
         -7.2189e-02, -1.1137e-01, -4.3608e-02,  1.1143e-01, -4.3032e-02,
         -5.2952e-02,  7.9436e-02,  4.5827e-02, -7.1964e-03,  8.9470e-02,
          2.4816e-02, -5.1922e-02,  5.5290e-02, -8.0915e-02, -1.4657e-01,
          3.8153e-02, -1.5055e-01, -5.8998e-02,  9.7380e-02,  1.1643e-01,
         -1.1114e-01,  3.8907e-02, -1.0065e-01,  7.5578e-02, -1.3467e-01,
          1.2126e-01, -1.3953e-01,  5.4670e-02,  1.2625e-01,  4.3175e-02,
          4.2354e-02,  3.7414e-02, -5.1024e-02, -1.3736e-01,  1.0114e-02,
          2.5177e-02,  1.4233e-01, -6.1388e-02, -1.3841e-01,  1.5175e-01,
          1.4701e-01, -1.5084e-01,  1.4474e-01,  3.8116e-03,  9.2182e-02,
         -1.1005e-01,  1.2241e-01,  9.1130e-02, -1.2552e-01,  1.1591e-01,
         -1.0392e-01, -8.3314e-02,  1.2881e-01,  9.1641e-02, -1.2422e-01,
          1.1381e-01,  8.5577e-02, -1.8920e-02,  7.2439e-03,  2.3226e-02,
         -1.3539e-01,  1.3194e-01, -3.6432e-02,  7.4598e-02, -1.1238e-01,
         -1.0524e-01, -1.3304e-01, -3.6504e-02,  1.0393e-02,  2.2398e-03,
         -8.1498e-02]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0124,  0.0570,  0.0591,  ...,  0.1215,  0.0963,  0.0247],
        [-0.0540,  0.0203, -0.0887,  ...,  0.0663,  0.0004,  0.0449],
        [-0.0615,  0.0355, -0.0355,  ..., -0.0938, -0.0191,  0.1104],
        ...,
        [-0.0706,  0.0350,  0.0335,  ..., -0.0098,  0.0208, -0.0770],
        [-0.0795,  0.0868, -0.0665,  ...,  0.0472,  0.0949,  0.0402],
        [ 0.0175,  0.0017, -0.1209,  ..., -0.0450, -0.0814,  0.0639]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 1.3948e-01,  1.3782e-01, -8.8691e-03,  ...,  1.6697e-01,
         -1.6201e-01,  8.2487e-02],
        [-1.0403e-01,  3.5089e-02, -7.1656e-04,  ..., -1.2028e-01,
         -1.3011e-01, -5.6680e-03],
        [-1.0036e-01,  1.0749e-02, -1.4411e-01,  ...,  1.5129e-01,
         -1.8752e-02,  4.5986e-02],
        ...,
        [ 1.4362e-02,  7.2200e-02, -8.7166e-02,  ...,  1.6196e-01,
          4.6394e-02, -9.2032e-02],
        [-1.5176e-01, -6.9566e-03,  1.6250e-01,  ...,  1.1884e-01,
          7.3422e-03,  1.6821e-01],
        [-1.2228e-01, -4.2293e-02, -1.5912e-01,  ..., -1.1311e-01,
          1.1085e-05,  2.9841e-03]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0634, -0.0129,  0.1203,  ...,  0.0249, -0.1590, -0.1408],
        [-0.0104, -0.1865,  0.1883,  ...,  0.1122,  0.0069, -0.2292],
        [-0.1566,  0.0745,  0.1356,  ..., -0.1575,  0.1137,  0.0431],
        ...,
        [-0.0080, -0.1347,  0.1447,  ...,  0.1430,  0.0811,  0.0448],
        [ 0.1952,  0.1315, -0.1334,  ...,  0.2075, -0.1566,  0.0418],
        [-0.0089,  0.0157, -0.1037,  ...,  0.0451,  0.1255, -0.1278]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0773],
        [-0.2517],
        [ 0.2839],
        [ 0.0383],
        [-0.1747],
        [-0.3380],
        [-0.3933],
        [ 0.3871],
        [ 0.3796],
        [ 0.1906],
        [-0.4248],
        [-0.3944],
        [ 0.2187],
        [ 0.2575],
        [-0.1994],
        [-0.3695],
        [ 0.0967],
        [-0.3595],
        [-0.4259],
        [ 0.1872],
        [ 0.0256],
        [ 0.1344],
        [-0.2258],
        [-0.0895],
        [ 0.2348],
        [ 0.4085],
        [-0.1342],
        [-0.0078],
        [-0.1886],
        [ 0.2006],
        [-0.3703],
        [ 0.3800]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



optimizer.param_groups [{'params': [Parameter containing:
tensor([[-4.9274e-02,  1.8557e-02, -1.3394e-01, -1.1032e-03, -3.8899e-02,
         -1.4039e-01,  6.5566e-02, -1.3809e-01,  5.1142e-03,  1.3695e-01,
          6.7609e-02,  1.8175e-02, -1.5321e-03,  5.6105e-02,  8.3791e-02,
          1.1829e-01, -1.0764e-02,  1.4440e-01,  7.7639e-02, -1.1915e-01,
         -1.7598e-02,  6.3754e-02,  9.7761e-02,  3.5667e-02, -3.3684e-02,
         -1.1430e-01,  8.8775e-02,  8.2770e-02, -1.3868e-02, -8.1414e-03,
          7.5715e-02,  6.5454e-02,  7.4052e-02,  1.4456e-01, -1.4183e-01,
          1.2617e-01,  6.6056e-02, -8.4052e-02,  1.1460e-01, -2.6095e-02,
          2.6663e-02,  1.1053e-01, -5.4716e-02, -1.1829e-01, -9.5985e-02,
          1.3598e-02, -1.1292e-01,  3.8413e-02, -6.7026e-02,  1.5036e-02,
         -1.4130e-01,  4.3920e-02, -7.7894e-03,  1.4606e-01,  3.5887e-02,
         -6.8013e-02, -2.3148e-02, -1.4099e-01,  2.0597e-02,  1.1219e-02,
          8.6514e-02,  6.0557e-02, -6.3048e-02,  1.7281e-02,  9.3975e-02,
          1.3283e-01, -6.6607e-02,  8.4940e-02, -6.9003e-02,  2.8778e-02,
         -1.2660e-01, -4.6548e-02, -5.4817e-02,  1.0213e-01,  3.2037e-02,
         -9.5394e-02, -1.0785e-02, -1.2091e-01, -4.4943e-02,  5.1135e-02,
         -1.2844e-01, -3.6380e-03, -1.2939e-03,  1.0138e-01,  8.7803e-02,
         -8.2005e-02, -1.1476e-01, -1.2261e-01, -9.3716e-02, -1.4159e-01,
         -2.3842e-02,  5.0036e-02, -6.0953e-02, -9.4357e-02, -1.3643e-01,
          1.3427e-01, -1.3213e-01,  3.1540e-02,  1.4126e-01,  2.3607e-02,
          4.7503e-02,  4.1808e-02,  6.6414e-02, -1.2255e-01,  1.4927e-01,
         -4.9584e-02, -1.1537e-04, -1.2409e-01, -7.1187e-02,  8.2742e-02,
          1.3723e-01,  3.4827e-02,  1.4501e-01, -9.7700e-03,  1.4908e-01,
         -7.4896e-02, -3.8530e-03,  8.7982e-02, -1.4973e-01, -1.0971e-01,
          1.0209e-01,  5.2268e-02,  6.5039e-02,  1.4090e-01, -1.0595e-01,
         -4.5484e-02,  1.4837e-01,  1.0965e-01,  1.1984e-01, -1.0712e-01,
         -1.4750e-01, -4.7457e-02, -1.0176e-01,  6.1450e-02,  1.0316e-01,
         -1.1982e-01,  1.4265e-01, -1.2215e-01,  3.2488e-02, -9.2876e-02,
          1.2431e-01, -8.3761e-02,  2.6348e-02, -1.4024e-01,  9.9836e-02,
         -1.2867e-01,  1.5191e-01,  9.1211e-02, -7.8479e-03,  9.2177e-02,
         -2.3314e-02,  1.0291e-01, -2.4706e-02, -6.5057e-02,  1.4910e-01,
          6.9848e-02,  1.2007e-01, -6.0066e-02,  3.9213e-03, -3.4297e-02,
         -6.7147e-03,  7.4723e-02, -1.4867e-01,  7.1360e-02, -6.5115e-02,
         -1.1483e-01, -3.8391e-02,  8.1275e-02, -1.6887e-02, -4.2661e-02,
         -2.5211e-02,  1.3266e-01, -1.2066e-01,  6.6998e-02,  1.4741e-01,
         -7.2709e-02,  1.3426e-01,  9.5381e-02,  1.4126e-01, -7.1589e-02,
         -1.2486e-01,  1.4436e-01, -1.2169e-01, -9.4592e-03, -5.8151e-02,
         -7.2189e-02, -1.1137e-01, -4.3608e-02,  1.1143e-01, -4.3032e-02,
         -5.2952e-02,  7.9436e-02,  4.5827e-02, -7.1964e-03,  8.9470e-02,
          2.4816e-02, -5.1922e-02,  5.5290e-02, -8.0915e-02, -1.4657e-01,
          3.8153e-02, -1.5055e-01, -5.8998e-02,  9.7380e-02,  1.1643e-01,
         -1.1114e-01,  3.8907e-02, -1.0065e-01,  7.5578e-02, -1.3467e-01,
          1.2126e-01, -1.3953e-01,  5.4670e-02,  1.2625e-01,  4.3175e-02,
          4.2354e-02,  3.7414e-02, -5.1024e-02, -1.3736e-01,  1.0114e-02,
          2.5177e-02,  1.4233e-01, -6.1388e-02, -1.3841e-01,  1.5175e-01,
          1.4701e-01, -1.5084e-01,  1.4474e-01,  3.8116e-03,  9.2182e-02,
         -1.1005e-01,  1.2241e-01,  9.1130e-02, -1.2552e-01,  1.1591e-01,
         -1.0392e-01, -8.3314e-02,  1.2881e-01,  9.1641e-02, -1.2422e-01,
          1.1381e-01,  8.5577e-02, -1.8920e-02,  7.2439e-03,  2.3226e-02,
         -1.3539e-01,  1.3194e-01, -3.6432e-02,  7.4598e-02, -1.1238e-01,
         -1.0524e-01, -1.3304e-01, -3.6504e-02,  1.0393e-02,  2.2398e-03,
         -8.1498e-02]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0124,  0.0570,  0.0591,  ...,  0.1215,  0.0963,  0.0247],
        [-0.0540,  0.0203, -0.0887,  ...,  0.0663,  0.0004,  0.0449],
        [-0.0615,  0.0355, -0.0355,  ..., -0.0938, -0.0191,  0.1104],
        ...,
        [-0.0706,  0.0350,  0.0335,  ..., -0.0098,  0.0208, -0.0770],
        [-0.0795,  0.0868, -0.0665,  ...,  0.0472,  0.0949,  0.0402],
        [ 0.0175,  0.0017, -0.1209,  ..., -0.0450, -0.0814,  0.0639]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 1.3948e-01,  1.3782e-01, -8.8691e-03,  ...,  1.6697e-01,
         -1.6201e-01,  8.2487e-02],
        [-1.0403e-01,  3.5089e-02, -7.1656e-04,  ..., -1.2028e-01,
         -1.3011e-01, -5.6680e-03],
        [-1.0036e-01,  1.0749e-02, -1.4411e-01,  ...,  1.5129e-01,
         -1.8752e-02,  4.5986e-02],
        ...,
        [ 1.4362e-02,  7.2200e-02, -8.7166e-02,  ...,  1.6196e-01,
          4.6394e-02, -9.2032e-02],
        [-1.5176e-01, -6.9566e-03,  1.6250e-01,  ...,  1.1884e-01,
          7.3422e-03,  1.6821e-01],
        [-1.2228e-01, -4.2293e-02, -1.5912e-01,  ..., -1.1311e-01,
          1.1085e-05,  2.9841e-03]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0634, -0.0129,  0.1203,  ...,  0.0249, -0.1590, -0.1408],
        [-0.0104, -0.1865,  0.1883,  ...,  0.1122,  0.0069, -0.2292],
        [-0.1566,  0.0745,  0.1356,  ..., -0.1575,  0.1137,  0.0431],
        ...,
        [-0.0080, -0.1347,  0.1447,  ...,  0.1430,  0.0811,  0.0448],
        [ 0.1952,  0.1315, -0.1334,  ...,  0.2075, -0.1566,  0.0418],
        [-0.0089,  0.0157, -0.1037,  ...,  0.0451,  0.1255, -0.1278]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0773],
        [-0.2517],
        [ 0.2839],
        [ 0.0383],
        [-0.1747],
        [-0.3380],
        [-0.3933],
        [ 0.3871],
        [ 0.3796],
        [ 0.1906],
        [-0.4248],
        [-0.3944],
        [ 0.2187],
        [ 0.2575],
        [-0.1994],
        [-0.3695],
        [ 0.0967],
        [-0.3595],
        [-0.4259],
        [ 0.1872],
        [ 0.0256],
        [ 0.1344],
        [-0.2258],
        [-0.0895],
        [ 0.2348],
        [ 0.4085],
        [-0.1342],
        [-0.0078],
        [-0.1886],
        [ 0.2006],
        [-0.3703],
        [ 0.3800]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}, {'params': [tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True)], 'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(773.1461, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(773.1461, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0072,  0.0027, -0.0197,  ...,  0.0015,  0.0003, -0.0120],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(456.5007, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-37.1628, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(95.3703, device='cuda:0')



h[100].sum tensor(35.8269, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-113.6804, device='cuda:0')



h[200].sum tensor(28.7754, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.0779, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0103, 0.0000,  ..., 0.0058, 0.0012, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0047, 0.0010, 0.0000],
        [0.0000, 0.0020, 0.0000,  ..., 0.0011, 0.0002, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(86259.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1672, 0.0000, 0.1003,  ..., 0.0000, 0.0000, 0.0000],
        [0.1432, 0.0000, 0.0859,  ..., 0.0000, 0.0000, 0.0000],
        [0.1150, 0.0000, 0.0690,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(490497.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6491.1118, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(190.0475, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4616.1748, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-252.6281, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8852.6152, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(627.8320, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.0826],
        [-4.4176],
        [-4.8747],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-222282.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 0.0 event: 0 loss: tensor(98.7400, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3103],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(704.9972, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3103],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(704.9972, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.6297e-03, -4.3687e-05, -7.2414e-03,  ...,  1.6206e-03,
         -9.3247e-04, -4.3848e-03],
        [-6.3750e-03,  1.3183e-03, -1.7555e-02,  ...,  2.5044e-03,
         -8.3630e-04, -1.0630e-02],
        [-4.6646e-03,  6.9632e-04, -1.2845e-02,  ...,  2.1008e-03,
         -8.8022e-04, -7.7778e-03],
        ...,
        [ 0.0000e+00, -1.0000e-03,  0.0000e+00,  ...,  1.0000e-03,
         -1.0000e-03,  0.0000e+00],
        [ 0.0000e+00, -1.0000e-03,  0.0000e+00,  ...,  1.0000e-03,
         -1.0000e-03,  0.0000e+00],
        [ 0.0000e+00, -1.0000e-03,  0.0000e+00,  ...,  1.0000e-03,
         -1.0000e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-522.8350, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-33.3158, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(86.9639, device='cuda:0')



h[100].sum tensor(135.4108, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-103.6600, device='cuda:0')



h[200].sum tensor(-76.3013, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.3385, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0043, 0.0000,  ..., 0.0094, 0.0000, 0.0000],
        [0.0000, 0.0016, 0.0000,  ..., 0.0075, 0.0000, 0.0000],
        [0.0000, 0.0017, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(134747.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0334, 0.0000, 0.0093,  ..., 0.0000, 0.0000, 0.0000],
        [0.0256, 0.0000, 0.0052,  ..., 0.0000, 0.0000, 0.0000],
        [0.0187, 0.0000, 0.0034,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0086],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0086],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0086]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(833043., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(785.5895, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-115.4156, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(354.4741, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1206.5676, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5133.7065, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(686.8606, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0225],
        [-0.0129],
        [ 0.0059],
        ...,
        [ 0.0679],
        [ 0.0676],
        [ 0.0674]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(138207.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(622.6307, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(622.6307, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0016,  0.0000,  ...,  0.0020, -0.0017,  0.0000],
        [ 0.0000, -0.0016,  0.0000,  ...,  0.0020, -0.0017,  0.0000],
        [ 0.0000, -0.0016,  0.0000,  ...,  0.0020, -0.0017,  0.0000],
        ...,
        [ 0.0000, -0.0016,  0.0000,  ...,  0.0020, -0.0017,  0.0000],
        [ 0.0000, -0.0016,  0.0000,  ...,  0.0020, -0.0017,  0.0000],
        [ 0.0000, -0.0016,  0.0000,  ...,  0.0020, -0.0017,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-4083.1633, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.9290, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(76.8037, device='cuda:0')



h[100].sum tensor(115.5173, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-91.5492, device='cuda:0')



h[200].sum tensor(-147.8935, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.0276, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0080, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0080, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0080, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0080, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0080, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0080, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(137791.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0243],
        [0.0000, 0.0000, 0.0000,  ..., 0.0097, 0.0000, 0.0306],
        [0.0000, 0.0000, 0.0000,  ..., 0.0128, 0.0000, 0.0322],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0126, 0.0000, 0.0318],
        [0.0000, 0.0000, 0.0000,  ..., 0.0126, 0.0000, 0.0318],
        [0.0000, 0.0000, 0.0000,  ..., 0.0126, 0.0000, 0.0318]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1040730., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(300.9129, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-138.7537, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(103.9255, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2056.3652, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2582.9956, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1043.4343, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0323],
        [ 0.0023],
        [-0.0191],
        ...,
        [-0.0343],
        [-0.0341],
        [-0.0341]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-42417.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2717],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(667.8506, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2717],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(667.8506, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0021,  0.0000,  ...,  0.0027, -0.0022,  0.0000],
        [-0.0031, -0.0010, -0.0089,  ...,  0.0035, -0.0022, -0.0053],
        [ 0.0000, -0.0021,  0.0000,  ...,  0.0027, -0.0022,  0.0000],
        ...,
        [ 0.0000, -0.0021,  0.0000,  ...,  0.0027, -0.0022,  0.0000],
        [ 0.0000, -0.0021,  0.0000,  ...,  0.0027, -0.0022,  0.0000],
        [ 0.0000, -0.0021,  0.0000,  ...,  0.0027, -0.0022,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-6587.1143, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.3616, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(82.3817, device='cuda:0')



h[100].sum tensor(109.0897, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-98.1981, device='cuda:0')



h[200].sum tensor(-199.4784, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.8453, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0115, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0119, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0107, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0107, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0107, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(150748.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0032, 0.0000, 0.0374],
        [0.0000, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0326],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0245],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0164, 0.0000, 0.0464],
        [0.0000, 0.0000, 0.0000,  ..., 0.0164, 0.0000, 0.0464],
        [0.0000, 0.0000, 0.0000,  ..., 0.0164, 0.0000, 0.0464]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1298892., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(66.1454, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-263.3682, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5.1583, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2435.5864, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1037.0175, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1423.8103, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0545],
        [ 0.0846],
        [ 0.1115],
        ...,
        [-0.0593],
        [-0.0590],
        [-0.0589]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-86158.2891, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1023.0052, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1023.0052, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0025,  0.0000,  ...,  0.0033, -0.0026,  0.0000],
        [ 0.0000, -0.0025,  0.0000,  ...,  0.0033, -0.0026,  0.0000],
        [ 0.0000, -0.0025,  0.0000,  ...,  0.0033, -0.0026,  0.0000],
        ...,
        [ 0.0000, -0.0025,  0.0000,  ...,  0.0033, -0.0026,  0.0000],
        [ 0.0000, -0.0025,  0.0000,  ...,  0.0033, -0.0026,  0.0000],
        [ 0.0000, -0.0025,  0.0000,  ...,  0.0033, -0.0026,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-8123.7920, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-45.3301, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(126.1913, device='cuda:0')



h[100].sum tensor(125.1603, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-150.4186, device='cuda:0')



h[200].sum tensor(-230.9783, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(41.1213, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(199098.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0000, 0.0543],
        [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0000, 0.0514],
        [0.0000, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0433],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0154, 0.0000, 0.0547],
        [0.0000, 0.0000, 0.0000,  ..., 0.0154, 0.0000, 0.0547],
        [0.0000, 0.0000, 0.0000,  ..., 0.0154, 0.0000, 0.0547]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1695766.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(89.3448, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-407.2288, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2668.4182, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2068.1375, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1754.1909, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0184],
        [ 0.0228],
        [ 0.0730],
        ...,
        [-0.0397],
        [-0.0548],
        [-0.0602]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-60883.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3965],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(716.2418, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3965],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(716.2418, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0045, -0.0009, -0.0129,  ...,  0.0046, -0.0026, -0.0077],
        [ 0.0000, -0.0025,  0.0000,  ...,  0.0033, -0.0026,  0.0000],
        [-0.0076,  0.0002, -0.0219,  ...,  0.0054, -0.0027, -0.0131],
        ...,
        [ 0.0000, -0.0025,  0.0000,  ...,  0.0033, -0.0026,  0.0000],
        [ 0.0000, -0.0025,  0.0000,  ...,  0.0033, -0.0026,  0.0000],
        [ 0.0000, -0.0025,  0.0000,  ...,  0.0033, -0.0026,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-8445.4082, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-31.6577, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(88.3509, device='cuda:0')



h[100].sum tensor(109.9104, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-105.3134, device='cuda:0')



h[200].sum tensor(-241.7014, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.7905, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0186, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0163, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(170575.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0399],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0237],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0255],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0154, 0.0000, 0.0547],
        [0.0000, 0.0000, 0.0000,  ..., 0.0154, 0.0000, 0.0547],
        [0.0000, 0.0000, 0.0000,  ..., 0.0154, 0.0000, 0.0547]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1559806., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(31.8647, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-461.0234, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0.0736, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2701.9690, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(818.3801, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1573.2727, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0681],
        [ 0.1120],
        [ 0.1407],
        ...,
        [-0.0623],
        [-0.0620],
        [-0.0619]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-69921.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2505],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1014.7023, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2505],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1014.7023, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0030, -0.0018, -0.0086,  ...,  0.0043, -0.0030, -0.0051],
        [-0.0028, -0.0019, -0.0081,  ...,  0.0043, -0.0030, -0.0048],
        [ 0.0000, -0.0029,  0.0000,  ...,  0.0035, -0.0030,  0.0000],
        ...,
        [ 0.0000, -0.0029,  0.0000,  ...,  0.0035, -0.0030,  0.0000],
        [ 0.0000, -0.0029,  0.0000,  ...,  0.0035, -0.0030,  0.0000],
        [ 0.0000, -0.0029,  0.0000,  ...,  0.0035, -0.0030,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-9729.1514, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-44.0669, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(125.1671, device='cuda:0')



h[100].sum tensor(128.7779, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-149.1978, device='cuda:0')



h[200].sum tensor(-268.2517, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.7876, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0006, 0.0000,  ..., 0.0204, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0172, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(211724.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0148],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0315],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0288],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0127, 0.0000, 0.0628],
        [0.0000, 0.0000, 0.0000,  ..., 0.0127, 0.0000, 0.0628],
        [0.0000, 0.0000, 0.0000,  ..., 0.0127, 0.0000, 0.0628]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1893767., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12.0620, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-600.0930, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2975.7576, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2093.4006, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1920.0029, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0055],
        [ 0.0033],
        [ 0.0096],
        ...,
        [-0.0699],
        [-0.0689],
        [-0.0693]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-104418.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(734.2592, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(734.2592, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0027, -0.0022, -0.0081,  ...,  0.0044, -0.0033, -0.0048],
        [ 0.0000, -0.0032,  0.0000,  ...,  0.0036, -0.0033,  0.0000],
        [ 0.0000, -0.0032,  0.0000,  ...,  0.0036, -0.0033,  0.0000],
        ...,
        [ 0.0000, -0.0032,  0.0000,  ...,  0.0036, -0.0033,  0.0000],
        [ 0.0000, -0.0032,  0.0000,  ...,  0.0036, -0.0033,  0.0000],
        [ 0.0000, -0.0032,  0.0000,  ...,  0.0036, -0.0033,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-11283.8125, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-31.0432, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(90.5734, device='cuda:0')



h[100].sum tensor(125.7840, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-107.9626, device='cuda:0')



h[200].sum tensor(-310.0546, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.5147, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0174, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(196353.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0258],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0472],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0610],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.0000, 0.0667],
        [0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.0000, 0.0667],
        [0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.0000, 0.0667]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1895343.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0.2182, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-797.0833, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2944.4409, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1152.2327, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1906.0542, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0268],
        [-0.0243],
        [-0.0261],
        ...,
        [-0.0557],
        [-0.0553],
        [-0.0552]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-80523.8906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(731.1919, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(731.1919, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0023, -0.0027, -0.0069,  ...,  0.0043, -0.0036, -0.0041],
        [-0.0029, -0.0024, -0.0088,  ...,  0.0044, -0.0036, -0.0052],
        [ 0.0000, -0.0035,  0.0000,  ...,  0.0036, -0.0035,  0.0000],
        ...,
        [ 0.0000, -0.0035,  0.0000,  ...,  0.0036, -0.0035,  0.0000],
        [ 0.0000, -0.0035,  0.0000,  ...,  0.0036, -0.0035,  0.0000],
        [ 0.0000, -0.0035,  0.0000,  ...,  0.0036, -0.0035,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-12293.5742, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.1183, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(90.1951, device='cuda:0')



h[100].sum tensor(138.4975, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-107.5116, device='cuda:0')



h[200].sum tensor(-338.3251, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.3914, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0206, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0169, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0153, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(209793.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0107],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0311],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0505],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0090, 0.0000, 0.0677],
        [0.0000, 0.0000, 0.0000,  ..., 0.0090, 0.0000, 0.0677],
        [0.0000, 0.0000, 0.0000,  ..., 0.0090, 0.0000, 0.0677]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2053053.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-941.9722, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2838.8733, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1319.3906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2076.9551, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0857],
        [ 0.0797],
        [ 0.0679],
        ...,
        [-0.0183],
        [-0.0182],
        [-0.0181]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-7135.8682, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3613],
        [0.3167],
        [0.4028],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(717.3862, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3613],
        [0.3167],
        [0.4028],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(717.3862, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0076, -0.0010, -0.0232,  ...,  0.0059, -0.0041, -0.0136],
        [-0.0079, -0.0009, -0.0241,  ...,  0.0060, -0.0041, -0.0140],
        [-0.0067, -0.0014, -0.0203,  ...,  0.0057, -0.0040, -0.0119],
        ...,
        [ 0.0000, -0.0037,  0.0000,  ...,  0.0038, -0.0038,  0.0000],
        [ 0.0000, -0.0037,  0.0000,  ...,  0.0038, -0.0038,  0.0000],
        [ 0.0000, -0.0037,  0.0000,  ...,  0.0038, -0.0038,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-13323.1055, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.0015, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(88.4921, device='cuda:0')



h[100].sum tensor(147.5788, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-105.4816, device='cuda:0')



h[200].sum tensor(-363.5056, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.8365, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0232, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0238, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0258, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0161, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(217248.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0083, 0.0000, 0.0735],
        [0.0000, 0.0000, 0.0000,  ..., 0.0050, 0.0000, 0.0660],
        [0.0000, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0503]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2194652.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1069.8383, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3017.9209, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1008.7582, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2206.3726, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0956],
        [0.0971],
        [0.0999],
        ...,
        [0.0005],
        [0.0225],
        [0.0450]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-9906.1484, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(693.4390, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(693.4390, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0039,  0.0000,  ...,  0.0041, -0.0040,  0.0000],
        [ 0.0000, -0.0039,  0.0000,  ...,  0.0041, -0.0040,  0.0000],
        [ 0.0000, -0.0039,  0.0000,  ...,  0.0041, -0.0040,  0.0000],
        ...,
        [ 0.0000, -0.0039,  0.0000,  ...,  0.0041, -0.0040,  0.0000],
        [ 0.0000, -0.0039,  0.0000,  ...,  0.0041, -0.0040,  0.0000],
        [ 0.0000, -0.0039,  0.0000,  ...,  0.0041, -0.0040,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-14368.0703, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.6124, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(85.5381, device='cuda:0')



h[100].sum tensor(149.6557, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-101.9605, device='cuda:0')



h[200].sum tensor(-386.1711, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.8739, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0162, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0162, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0162, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0162, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0162, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0162, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(225248.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0121, 0.0000, 0.0829],
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0834],
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0837],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0000, 0.0828],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0000, 0.0828],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0000, 0.0828]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2333571., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1200.3022, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3284.4402, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1065.7869, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2348.8311, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0269],
        [-0.0490],
        [-0.0664],
        ...,
        [-0.0532],
        [-0.0582],
        [-0.0597]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-81487.4297, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 10.0 event: 150 loss: tensor(539.8809, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(718.2433, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(718.2433, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0041,  0.0000,  ...,  0.0043, -0.0042,  0.0000],
        [ 0.0000, -0.0041,  0.0000,  ...,  0.0043, -0.0042,  0.0000],
        [ 0.0000, -0.0041,  0.0000,  ...,  0.0043, -0.0042,  0.0000],
        ...,
        [ 0.0000, -0.0041,  0.0000,  ...,  0.0043, -0.0042,  0.0000],
        [ 0.0000, -0.0041,  0.0000,  ...,  0.0043, -0.0042,  0.0000],
        [ 0.0000, -0.0041,  0.0000,  ...,  0.0043, -0.0042,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-15239.2715, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.9553, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(88.5978, device='cuda:0')



h[100].sum tensor(154.2598, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-105.6077, device='cuda:0')



h[200].sum tensor(-405.0307, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.8709, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0171, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0171, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0171, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0171, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0171, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0171, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(237308.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0164, 0.0000, 0.0891],
        [0.0000, 0.0000, 0.0000,  ..., 0.0164, 0.0000, 0.0891],
        [0.0000, 0.0000, 0.0000,  ..., 0.0164, 0.0000, 0.0894],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0163, 0.0000, 0.0885],
        [0.0000, 0.0000, 0.0000,  ..., 0.0163, 0.0000, 0.0885],
        [0.0000, 0.0000, 0.0000,  ..., 0.0163, 0.0000, 0.0885]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2492326.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1275.2955, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3517.9150, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1406.7500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2493.6431, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1151],
        [-0.1215],
        [-0.1268],
        ...,
        [-0.0794],
        [-0.0790],
        [-0.0789]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-111190.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(765.3921, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(765.3921, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0053, -0.0024, -0.0170,  ...,  0.0059, -0.0046, -0.0098],
        [-0.0061, -0.0021, -0.0195,  ...,  0.0061, -0.0047, -0.0112],
        [-0.0170,  0.0019, -0.0545,  ...,  0.0093, -0.0053, -0.0313],
        ...,
        [ 0.0000, -0.0043,  0.0000,  ...,  0.0043, -0.0044,  0.0000],
        [ 0.0000, -0.0043,  0.0000,  ...,  0.0043, -0.0044,  0.0000],
        [ 0.0000, -0.0043,  0.0000,  ...,  0.0043, -0.0044,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-15961.3516, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.9196, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(94.4138, device='cuda:0')



h[100].sum tensor(162.1268, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-112.5402, device='cuda:0')



h[200].sum tensor(-421.2431, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.7662, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0019, 0.0000,  ..., 0.0294, 0.0000, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0310, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0257, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0174, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0174, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0174, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(250781.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0166, 0.0000, 0.0907],
        [0.0000, 0.0000, 0.0000,  ..., 0.0166, 0.0000, 0.0907],
        [0.0000, 0.0000, 0.0000,  ..., 0.0166, 0.0000, 0.0907]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2629685.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1244.3854, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3681.3862, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1671.2595, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2604.4724, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2431],
        [ 0.2368],
        [ 0.2302],
        ...,
        [-0.0580],
        [-0.0516],
        [-0.0466]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-67009.8906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(868.3438, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(868.3438, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0045,  0.0000,  ...,  0.0044, -0.0045,  0.0000],
        [ 0.0000, -0.0045,  0.0000,  ...,  0.0044, -0.0045,  0.0000],
        [ 0.0000, -0.0045,  0.0000,  ...,  0.0044, -0.0045,  0.0000],
        ...,
        [ 0.0000, -0.0045,  0.0000,  ...,  0.0044, -0.0045,  0.0000],
        [ 0.0000, -0.0045,  0.0000,  ...,  0.0044, -0.0045,  0.0000],
        [ 0.0000, -0.0045,  0.0000,  ...,  0.0044, -0.0045,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-16555.4395, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-31.8455, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(107.1132, device='cuda:0')



h[100].sum tensor(169.9023, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-127.6778, device='cuda:0')



h[200].sum tensor(-433.6598, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.9045, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0175, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0175, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0175, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0175, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0175, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0175, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(262567.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0162, 0.0000, 0.0975],
        [0.0000, 0.0000, 0.0000,  ..., 0.0162, 0.0000, 0.0975],
        [0.0000, 0.0000, 0.0000,  ..., 0.0137, 0.0000, 0.0968],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0162, 0.0000, 0.0968],
        [0.0000, 0.0000, 0.0000,  ..., 0.0162, 0.0000, 0.0968],
        [0.0000, 0.0000, 0.0000,  ..., 0.0162, 0.0000, 0.0968]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2728514.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1179.6377, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3899.3091, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2232.8831, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2758.8311, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1042],
        [-0.0833],
        [-0.0510],
        ...,
        [-0.0860],
        [-0.0856],
        [-0.0855]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-106091.0078, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9526],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(898.2412, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9526],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(898.2412, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0046,  0.0000,  ...,  0.0044, -0.0046,  0.0000],
        [-0.0087, -0.0013, -0.0288,  ...,  0.0069, -0.0052, -0.0163],
        [ 0.0000, -0.0046,  0.0000,  ...,  0.0044, -0.0046,  0.0000],
        ...,
        [ 0.0000, -0.0046,  0.0000,  ...,  0.0044, -0.0046,  0.0000],
        [ 0.0000, -0.0046,  0.0000,  ...,  0.0044, -0.0046,  0.0000],
        [ 0.0000, -0.0046,  0.0000,  ...,  0.0044, -0.0046,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-17077.0234, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-32.1740, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(110.8012, device='cuda:0')



h[100].sum tensor(174.1141, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-132.0738, device='cuda:0')



h[200].sum tensor(-446.8316, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.1063, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0214, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0223, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0280, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0174, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0174, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0174, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(272275.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0430],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0224],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0052],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0000, 0.0999],
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0000, 0.0999],
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0000, 0.0999]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2833835.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1132.5334, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4181.1777, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2955.5894, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2873.3455, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0025],
        [-0.0067],
        [-0.0124],
        ...,
        [-0.0879],
        [-0.0875],
        [-0.0874]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-93058.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(756.6324, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(756.6324, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0047,  0.0000,  ...,  0.0043, -0.0048,  0.0000],
        [ 0.0000, -0.0047,  0.0000,  ...,  0.0043, -0.0048,  0.0000],
        [-0.0037, -0.0033, -0.0125,  ...,  0.0054, -0.0050, -0.0071],
        ...,
        [ 0.0000, -0.0047,  0.0000,  ...,  0.0043, -0.0048,  0.0000],
        [ 0.0000, -0.0047,  0.0000,  ...,  0.0043, -0.0048,  0.0000],
        [ 0.0000, -0.0047,  0.0000,  ...,  0.0043, -0.0048,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-17950.0918, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.4283, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(93.3332, device='cuda:0')



h[100].sum tensor(167.6472, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-111.2522, device='cuda:0')



h[200].sum tensor(-464.3640, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.4141, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0184, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0204, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0173, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0173, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0173, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(264146.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0580],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0617],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0436],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0160, 0.0000, 0.1021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0160, 0.0000, 0.1021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0160, 0.0000, 0.1021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2815055., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1077.8044, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4433.2656, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2302.8364, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2920.2339, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0799],
        [ 0.0783],
        [ 0.0658],
        ...,
        [-0.0834],
        [-0.0832],
        [-0.0832]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-72687.6719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(702.3387, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(702.3387, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0048,  0.0000,  ...,  0.0044, -0.0049,  0.0000],
        [ 0.0000, -0.0048,  0.0000,  ...,  0.0044, -0.0049,  0.0000],
        [-0.0025, -0.0039, -0.0087,  ...,  0.0051, -0.0051, -0.0049],
        ...,
        [ 0.0000, -0.0048,  0.0000,  ...,  0.0044, -0.0049,  0.0000],
        [ 0.0000, -0.0048,  0.0000,  ...,  0.0044, -0.0049,  0.0000],
        [ 0.0000, -0.0048,  0.0000,  ...,  0.0044, -0.0049,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-18698.0664, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.6522, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(86.6359, device='cuda:0')



h[100].sum tensor(162.2301, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-103.2691, device='cuda:0')



h[200].sum tensor(-478.1219, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.2316, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0182, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0175, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0175, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0175, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(261972.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0850],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0862],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0769],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0183, 0.0000, 0.1062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0183, 0.0000, 0.1062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0183, 0.0000, 0.1062]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2846051., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-977.4802, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4715.6035, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2127.2883, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2978.0864, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1407],
        [ 0.1532],
        [ 0.1655],
        ...,
        [-0.0971],
        [-0.0967],
        [-0.0966]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-77211.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1061.4436, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1061.4436, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0049,  0.0000,  ...,  0.0045, -0.0050,  0.0000],
        [ 0.0000, -0.0049,  0.0000,  ...,  0.0045, -0.0050,  0.0000],
        [-0.0030, -0.0037, -0.0107,  ...,  0.0054, -0.0052, -0.0059],
        ...,
        [ 0.0000, -0.0049,  0.0000,  ...,  0.0045, -0.0050,  0.0000],
        [ 0.0000, -0.0049,  0.0000,  ...,  0.0045, -0.0050,  0.0000],
        [ 0.0000, -0.0049,  0.0000,  ...,  0.0045, -0.0050,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-18512.2559, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-35.2652, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(130.9328, device='cuda:0')



h[100].sum tensor(176.5369, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-156.0705, device='cuda:0')



h[200].sum tensor(-476.7884, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(42.6664, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0180, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0212, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0180, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0180, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0180, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(305361.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0083, 0.0000, 0.1033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0854],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0592],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0231, 0.0000, 0.1123],
        [0.0000, 0.0000, 0.0000,  ..., 0.0231, 0.0000, 0.1123],
        [0.0000, 0.0000, 0.0000,  ..., 0.0231, 0.0000, 0.1123]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3169060., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-826.0288, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5035.7651, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6776.8462, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3348.9016, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0719],
        [ 0.0768],
        [ 0.0791],
        ...,
        [-0.1351],
        [-0.1348],
        [-0.1350]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-144071.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(731.2302, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(731.2302, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0061, -0.0026, -0.0219,  ...,  0.0064, -0.0056, -0.0121],
        [-0.0042, -0.0033, -0.0152,  ...,  0.0059, -0.0054, -0.0084],
        [-0.0060, -0.0026, -0.0214,  ...,  0.0064, -0.0056, -0.0118],
        ...,
        [ 0.0000, -0.0050,  0.0000,  ...,  0.0046, -0.0051,  0.0000],
        [ 0.0000, -0.0050,  0.0000,  ...,  0.0046, -0.0051,  0.0000],
        [ 0.0000, -0.0050,  0.0000,  ...,  0.0046, -0.0051,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-19906.4180, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.3545, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(90.1998, device='cuda:0')



h[100].sum tensor(153.9374, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-107.5172, device='cuda:0')



h[200].sum tensor(-497.6299, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.3930, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0230, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0236, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0217, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0185, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0185, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0185, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(270894.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0448],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0457],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0598],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0272, 0.0000, 0.1179],
        [0.0000, 0.0000, 0.0000,  ..., 0.0272, 0.0000, 0.1179],
        [0.0000, 0.0000, 0.0000,  ..., 0.0272, 0.0000, 0.1179]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2971505., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-830.9531, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5214.9658, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3489.5793, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3306.0327, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0625],
        [ 0.0744],
        [ 0.0811],
        ...,
        [-0.1692],
        [-0.1685],
        [-0.1683]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-214920.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1144.1659, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1144.1659, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0051,  0.0000,  ...,  0.0047, -0.0052,  0.0000],
        [ 0.0000, -0.0051,  0.0000,  ...,  0.0047, -0.0052,  0.0000],
        [ 0.0000, -0.0051,  0.0000,  ...,  0.0047, -0.0052,  0.0000],
        ...,
        [ 0.0000, -0.0051,  0.0000,  ...,  0.0047, -0.0052,  0.0000],
        [ 0.0000, -0.0051,  0.0000,  ...,  0.0047, -0.0052,  0.0000],
        [ 0.0000, -0.0051,  0.0000,  ...,  0.0047, -0.0052,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-19098.5352, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-35.8350, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(141.1368, device='cuda:0')



h[100].sum tensor(182.9759, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-168.2336, device='cuda:0')



h[200].sum tensor(-493.0269, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(45.9916, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(317797.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.1107],
        [0.0000, 0.0000, 0.0000,  ..., 0.0120, 0.0000, 0.1134],
        [0.0000, 0.0000, 0.0000,  ..., 0.0169, 0.0000, 0.1154],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0266, 0.0000, 0.1171],
        [0.0000, 0.0000, 0.0000,  ..., 0.0266, 0.0000, 0.1171],
        [0.0000, 0.0000, 0.0000,  ..., 0.0266, 0.0000, 0.1171]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3292776.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-799.7521, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5413.5640, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7769.9199, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3422.9961, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1597],
        [ 0.1272],
        [ 0.0883],
        ...,
        [-0.1424],
        [-0.1418],
        [-0.1416]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-111239.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(702.8621, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(702.8621, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0052,  0.0000,  ...,  0.0047, -0.0052,  0.0000],
        [ 0.0000, -0.0052,  0.0000,  ...,  0.0047, -0.0052,  0.0000],
        [ 0.0000, -0.0052,  0.0000,  ...,  0.0047, -0.0052,  0.0000],
        ...,
        [ 0.0000, -0.0052,  0.0000,  ...,  0.0047, -0.0052,  0.0000],
        [ 0.0000, -0.0052,  0.0000,  ...,  0.0047, -0.0052,  0.0000],
        [ 0.0000, -0.0052,  0.0000,  ...,  0.0047, -0.0052,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-20689.8477, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.2396, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(86.7005, device='cuda:0')



h[100].sum tensor(156.3553, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-103.3461, device='cuda:0')



h[200].sum tensor(-515.5131, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.2527, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(276456.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0271, 0.0000, 0.1230],
        [0.0000, 0.0000, 0.0000,  ..., 0.0271, 0.0000, 0.1230],
        [0.0000, 0.0000, 0.0000,  ..., 0.0271, 0.0000, 0.1234],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0270, 0.0000, 0.1222],
        [0.0000, 0.0000, 0.0000,  ..., 0.0270, 0.0000, 0.1222],
        [0.0000, 0.0000, 0.0000,  ..., 0.0270, 0.0000, 0.1222]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3049328., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-805.8876, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5450.3750, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4560.8438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3273.6790, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2400],
        [-0.2297],
        [-0.2099],
        ...,
        [-0.1713],
        [-0.1706],
        [-0.1705]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-184344.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 20.0 event: 300 loss: tensor(594.6884, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3135],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.4685],
        [0.3289]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(859.1549, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3135],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.4685],
        [0.3289]], device='cuda:0') 
g.ndata[nfet].sum tensor(859.1549, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0016, -0.0046, -0.0062,  ...,  0.0053, -0.0055, -0.0033],
        [-0.0040, -0.0035, -0.0151,  ...,  0.0060, -0.0057, -0.0082],
        [ 0.0000, -0.0053,  0.0000,  ...,  0.0048, -0.0053,  0.0000],
        ...,
        [-0.0035, -0.0037, -0.0134,  ...,  0.0058, -0.0057, -0.0072],
        [-0.0025, -0.0042, -0.0094,  ...,  0.0055, -0.0056, -0.0051],
        [-0.0035, -0.0037, -0.0134,  ...,  0.0058, -0.0057, -0.0072]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-20511.0566, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.0098, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(105.9797, device='cuda:0')



h[100].sum tensor(169.9467, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-126.3267, device='cuda:0')



h[200].sum tensor(-518.1326, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.5351, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0234, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0210, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0207, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0207, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0236, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0227, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(290582.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0802],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0917],
        [0.0000, 0.0000, 0.0000,  ..., 0.0011, 0.0000, 0.0993],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0991],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0817],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0779]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3113555.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-802.5961, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5404.8169, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5286.8047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3280.4258, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0895],
        [ 0.0882],
        [ 0.0717],
        ...,
        [ 0.0037],
        [ 0.0124],
        [-0.0015]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-153336.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(626.8560, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(626.8560, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0053,  0.0000,  ...,  0.0049, -0.0054,  0.0000],
        [ 0.0000, -0.0053,  0.0000,  ...,  0.0049, -0.0054,  0.0000],
        [ 0.0000, -0.0053,  0.0000,  ...,  0.0049, -0.0054,  0.0000],
        ...,
        [ 0.0000, -0.0053,  0.0000,  ...,  0.0049, -0.0054,  0.0000],
        [ 0.0000, -0.0053,  0.0000,  ...,  0.0049, -0.0054,  0.0000],
        [ 0.0000, -0.0053,  0.0000,  ...,  0.0049, -0.0054,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-21364.5234, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.8447, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(77.3249, device='cuda:0')



h[100].sum tensor(163.9255, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-92.1704, device='cuda:0')



h[200].sum tensor(-532.5189, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.1975, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(276154.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.1077],
        [0.0000, 0.0000, 0.0000,  ..., 0.0120, 0.0000, 0.1171],
        [0.0000, 0.0000, 0.0000,  ..., 0.0203, 0.0000, 0.1202],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0251, 0.0000, 0.1207],
        [0.0000, 0.0000, 0.0000,  ..., 0.0251, 0.0000, 0.1207],
        [0.0000, 0.0000, 0.0000,  ..., 0.0251, 0.0000, 0.1207]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3066755.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-909.7734, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5459.9585, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3597.5093, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3157.8621, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0737],
        [ 0.0734],
        [ 0.0787],
        ...,
        [-0.1448],
        [-0.1443],
        [-0.1441]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-102310.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(809.2412, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(809.2412, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0036, -0.0037, -0.0143,  ...,  0.0063, -0.0058, -0.0077],
        [-0.0015, -0.0047, -0.0059,  ...,  0.0057, -0.0056, -0.0032],
        [-0.0021, -0.0044, -0.0084,  ...,  0.0059, -0.0057, -0.0045],
        ...,
        [ 0.0000, -0.0054,  0.0000,  ...,  0.0052, -0.0054,  0.0000],
        [ 0.0000, -0.0054,  0.0000,  ...,  0.0052, -0.0054,  0.0000],
        [ 0.0000, -0.0054,  0.0000,  ...,  0.0052, -0.0054,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-21225.3594, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.3449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(99.8227, device='cuda:0')



h[100].sum tensor(173.8306, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-118.9876, device='cuda:0')



h[200].sum tensor(-533.2990, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.5288, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0227, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0250, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0227, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0208, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0208, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0208, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(293160.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0904],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0788],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0869],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0289, 0.0000, 0.1225],
        [0.0000, 0.0000, 0.0000,  ..., 0.0289, 0.0000, 0.1225],
        [0.0000, 0.0000, 0.0000,  ..., 0.0289, 0.0000, 0.1225]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3184784., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-947.6890, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5586.6274, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4735.3379, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3220.2405, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1677],
        [ 0.1690],
        [ 0.1615],
        ...,
        [-0.1919],
        [-0.1912],
        [-0.1910]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-154805.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(783.1926, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(783.1926, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0054,  0.0000,  ...,  0.0054, -0.0055,  0.0000],
        [ 0.0000, -0.0054,  0.0000,  ...,  0.0054, -0.0055,  0.0000],
        [ 0.0000, -0.0054,  0.0000,  ...,  0.0054, -0.0055,  0.0000],
        ...,
        [ 0.0000, -0.0054,  0.0000,  ...,  0.0054, -0.0055,  0.0000],
        [ 0.0000, -0.0054,  0.0000,  ...,  0.0054, -0.0055,  0.0000],
        [ 0.0000, -0.0054,  0.0000,  ...,  0.0054, -0.0055,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-21736.4629, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.8140, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(96.6095, device='cuda:0')



h[100].sum tensor(168.4884, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-115.1575, device='cuda:0')



h[200].sum tensor(-540.3346, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.4817, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0230, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0216, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0216, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0216, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0216, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0216, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(292512.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.1178],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0000, 0.1231],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0326, 0.0000, 0.1255],
        [0.0000, 0.0000, 0.0000,  ..., 0.0326, 0.0000, 0.1255],
        [0.0000, 0.0000, 0.0000,  ..., 0.0326, 0.0000, 0.1255]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3209757.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1064.6257, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5775.5713, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4692.3301, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3350.2278, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0856],
        [ 0.0214],
        [-0.0537],
        ...,
        [-0.2529],
        [-0.2519],
        [-0.2516]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-239305.2344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(676.4901, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(676.4901, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0055,  0.0000,  ...,  0.0054, -0.0055,  0.0000],
        [ 0.0000, -0.0055,  0.0000,  ...,  0.0054, -0.0055,  0.0000],
        [ 0.0000, -0.0055,  0.0000,  ...,  0.0054, -0.0055,  0.0000],
        ...,
        [ 0.0000, -0.0055,  0.0000,  ...,  0.0054, -0.0055,  0.0000],
        [ 0.0000, -0.0055,  0.0000,  ...,  0.0054, -0.0055,  0.0000],
        [ 0.0000, -0.0055,  0.0000,  ...,  0.0054, -0.0055,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-22301.1055, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.4599, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(83.4474, device='cuda:0')



h[100].sum tensor(165.2944, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-99.4684, device='cuda:0')



h[200].sum tensor(-549.1463, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.1926, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0216, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0216, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0216, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0216, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0216, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0216, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(289644.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0325, 0.0000, 0.1252],
        [0.0000, 0.0000, 0.0000,  ..., 0.0325, 0.0000, 0.1252],
        [0.0000, 0.0000, 0.0000,  ..., 0.0325, 0.0000, 0.1256],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0324, 0.0000, 0.1243],
        [0.0000, 0.0000, 0.0000,  ..., 0.0324, 0.0000, 0.1243],
        [0.0000, 0.0000, 0.0000,  ..., 0.0324, 0.0000, 0.1243]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3210183., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1113.2489, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5713.9609, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3790.2078, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3282.4133, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2943],
        [-0.2883],
        [-0.2759],
        ...,
        [-0.2216],
        [-0.2208],
        [-0.2206]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-180496.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(595.5216, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(595.5216, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0055,  0.0000,  ...,  0.0053, -0.0056,  0.0000],
        [ 0.0000, -0.0055,  0.0000,  ...,  0.0053, -0.0056,  0.0000],
        [ 0.0000, -0.0055,  0.0000,  ...,  0.0053, -0.0056,  0.0000],
        ...,
        [ 0.0000, -0.0055,  0.0000,  ...,  0.0053, -0.0056,  0.0000],
        [ 0.0000, -0.0055,  0.0000,  ...,  0.0053, -0.0056,  0.0000],
        [ 0.0000, -0.0055,  0.0000,  ...,  0.0053, -0.0056,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-22630.2715, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.8644, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(73.4597, device='cuda:0')



h[100].sum tensor(158.6667, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-87.5631, device='cuda:0')



h[200].sum tensor(-553.4009, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.9379, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0214, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0214, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0232, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0214, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0214, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0214, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(282672.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.3942e-02, 0.0000e+00,
         1.2455e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.5349e-02, 0.0000e+00,
         1.1604e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.1587e-05, 0.0000e+00,
         9.5521e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.2980e-02, 0.0000e+00,
         1.2578e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.2980e-02, 0.0000e+00,
         1.2578e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.2980e-02, 0.0000e+00,
         1.2578e-01]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3141525.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1176.2388, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5815.2622, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2818.3767, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3266.2449, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0779],
        [-0.0331],
        [-0.0115],
        ...,
        [-0.2302],
        [-0.2295],
        [-0.2293]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-177946., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2673],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(890.1510, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2673],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(890.1510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0056,  0.0000,  ...,  0.0053, -0.0056,  0.0000],
        [-0.0033, -0.0039, -0.0147,  ...,  0.0064, -0.0061, -0.0076],
        [-0.0017, -0.0048, -0.0074,  ...,  0.0059, -0.0058, -0.0038],
        ...,
        [ 0.0000, -0.0056,  0.0000,  ...,  0.0053, -0.0056,  0.0000],
        [ 0.0000, -0.0056,  0.0000,  ...,  0.0053, -0.0056,  0.0000],
        [ 0.0000, -0.0056,  0.0000,  ...,  0.0053, -0.0056,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-21711.3965, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.5994, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(109.8032, device='cuda:0')



h[100].sum tensor(171.8971, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-130.8843, device='cuda:0')



h[200].sum tensor(-545.5317, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.7811, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0228, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0230, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0276, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0213, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0213, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0213, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(309653.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.1021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0820],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0355],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0346, 0.0000, 0.1266],
        [0.0000, 0.0000, 0.0000,  ..., 0.0346, 0.0000, 0.1266],
        [0.0000, 0.0000, 0.0000,  ..., 0.0346, 0.0000, 0.1266]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3288738.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1189.0144, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5975.0225, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4919.0244, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3406.8452, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0264],
        [-0.0682],
        [-0.1346],
        ...,
        [-0.2377],
        [-0.2370],
        [-0.2368]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-169015.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(684.0228, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(684.0228, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0056,  0.0000,  ...,  0.0054, -0.0056,  0.0000],
        [ 0.0000, -0.0056,  0.0000,  ...,  0.0054, -0.0056,  0.0000],
        [ 0.0000, -0.0056,  0.0000,  ...,  0.0054, -0.0056,  0.0000],
        ...,
        [ 0.0000, -0.0056,  0.0000,  ...,  0.0054, -0.0056,  0.0000],
        [ 0.0000, -0.0056,  0.0000,  ...,  0.0054, -0.0056,  0.0000],
        [ 0.0000, -0.0056,  0.0000,  ...,  0.0054, -0.0056,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-22478.4023, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.9583, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(84.3766, device='cuda:0')



h[100].sum tensor(157.7803, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-100.5760, device='cuda:0')



h[200].sum tensor(-553.8907, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.4954, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0216, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0216, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0216, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0216, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0216, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0216, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(293284.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.1083],
        [0.0000, 0.0000, 0.0000,  ..., 0.0090, 0.0000, 0.1162],
        [0.0000, 0.0000, 0.0000,  ..., 0.0129, 0.0000, 0.1190],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0375, 0.0000, 0.1260],
        [0.0000, 0.0000, 0.0000,  ..., 0.0375, 0.0000, 0.1260],
        [0.0000, 0.0000, 0.0000,  ..., 0.0375, 0.0000, 0.1260]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3187035.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1283.7981, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6062.8589, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2799.1714, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3374.1399, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2959],
        [-0.3128],
        [-0.3313],
        ...,
        [-0.2478],
        [-0.2471],
        [-0.2469]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-164463.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1057.5356, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1057.5356, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0057,  0.0000,  ...,  0.0056, -0.0057,  0.0000],
        [ 0.0000, -0.0057,  0.0000,  ...,  0.0056, -0.0057,  0.0000],
        [ 0.0000, -0.0057,  0.0000,  ...,  0.0056, -0.0057,  0.0000],
        ...,
        [ 0.0000, -0.0057,  0.0000,  ...,  0.0056, -0.0057,  0.0000],
        [ 0.0000, -0.0057,  0.0000,  ...,  0.0056, -0.0057,  0.0000],
        [ 0.0000, -0.0057,  0.0000,  ...,  0.0056, -0.0057,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-21328.6328, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.8226, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(130.4507, device='cuda:0')



h[100].sum tensor(175.6301, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-155.4959, device='cuda:0')



h[200].sum tensor(-543.5176, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(42.5093, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0224, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0224, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0224, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0224, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0224, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0224, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(327375.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0420, 0.0000, 0.1257],
        [0.0000, 0.0000, 0.0000,  ..., 0.0420, 0.0000, 0.1257],
        [0.0000, 0.0000, 0.0000,  ..., 0.0421, 0.0000, 0.1261],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0419, 0.0000, 0.1248],
        [0.0000, 0.0000, 0.0000,  ..., 0.0419, 0.0000, 0.1248],
        [0.0000, 0.0000, 0.0000,  ..., 0.0419, 0.0000, 0.1248]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3395498.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1293.1407, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6258.6426, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5271.7124, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3583.8879, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3007],
        [-0.3161],
        [-0.3208],
        ...,
        [-0.2787],
        [-0.2780],
        [-0.2777]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-166471.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(842.9164, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(842.9164, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0057,  0.0000,  ...,  0.0058, -0.0057,  0.0000],
        [ 0.0000, -0.0057,  0.0000,  ...,  0.0058, -0.0057,  0.0000],
        [-0.0014, -0.0050, -0.0068,  ...,  0.0063, -0.0059, -0.0035],
        ...,
        [ 0.0000, -0.0057,  0.0000,  ...,  0.0058, -0.0057,  0.0000],
        [ 0.0000, -0.0057,  0.0000,  ...,  0.0058, -0.0057,  0.0000],
        [ 0.0000, -0.0057,  0.0000,  ...,  0.0058, -0.0057,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-22246.0586, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.4501, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(103.9767, device='cuda:0')



h[100].sum tensor(159.1585, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-123.9391, device='cuda:0')



h[200].sum tensor(-551.7910, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.8824, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0234, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0239, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0250, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0234, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0234, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0234, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(313563.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0274, 0.0000, 0.1203],
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.1122],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1005],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0477, 0.0000, 0.1244],
        [0.0000, 0.0000, 0.0000,  ..., 0.0477, 0.0000, 0.1244],
        [0.0000, 0.0000, 0.0000,  ..., 0.0477, 0.0000, 0.1244]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3342258., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1404.3967, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6365.8477, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3849.7449, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3621.4731, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0544],
        [ 0.0388],
        [ 0.1006],
        ...,
        [-0.3262],
        [-0.3244],
        [-0.3175]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-230676.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 30.0 event: 450 loss: tensor(581.2607, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2817],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(799.4336, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2817],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(799.4336, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0050, -0.0074,  ...,  0.0065, -0.0060, -0.0037],
        [-0.0012, -0.0051, -0.0059,  ...,  0.0064, -0.0059, -0.0030],
        [-0.0027, -0.0043, -0.0132,  ...,  0.0070, -0.0062, -0.0067],
        ...,
        [ 0.0000, -0.0058,  0.0000,  ...,  0.0060, -0.0057,  0.0000],
        [ 0.0000, -0.0058,  0.0000,  ...,  0.0060, -0.0057,  0.0000],
        [ 0.0000, -0.0058,  0.0000,  ...,  0.0060, -0.0057,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-22526.9219, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.6888, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(98.6129, device='cuda:0')



h[100].sum tensor(151.9753, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-117.5456, device='cuda:0')



h[200].sum tensor(-555.1068, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.1345, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0257, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0275, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0257, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0241, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0241, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0241, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(309311.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0981],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0872],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0971],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0525, 0.0000, 0.1236],
        [0.0000, 0.0000, 0.0000,  ..., 0.0525, 0.0000, 0.1236],
        [0.0000, 0.0000, 0.0000,  ..., 0.0525, 0.0000, 0.1236]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3311490., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1485.0963, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6394.7349, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3022.8936, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3631.7866, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1303],
        [ 0.1414],
        [ 0.1205],
        ...,
        [-0.3543],
        [-0.3532],
        [-0.3529]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-258661.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(898.5753, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(898.5753, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0058,  0.0000,  ...,  0.0062, -0.0058,  0.0000],
        [ 0.0000, -0.0058,  0.0000,  ...,  0.0062, -0.0058,  0.0000],
        [ 0.0000, -0.0058,  0.0000,  ...,  0.0062, -0.0058,  0.0000],
        ...,
        [ 0.0000, -0.0058,  0.0000,  ...,  0.0062, -0.0058,  0.0000],
        [ 0.0000, -0.0058,  0.0000,  ...,  0.0062, -0.0058,  0.0000],
        [ 0.0000, -0.0058,  0.0000,  ...,  0.0062, -0.0058,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-22124.8340, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.3135, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(110.8424, device='cuda:0')



h[100].sum tensor(153.9656, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-132.1230, device='cuda:0')



h[200].sum tensor(-553.1071, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.1197, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0246, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0246, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0246, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0246, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0246, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0246, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(320004.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0563, 0.0000, 0.1234],
        [0.0000, 0.0000, 0.0000,  ..., 0.0563, 0.0000, 0.1234],
        [0.0000, 0.0000, 0.0000,  ..., 0.0564, 0.0000, 0.1238],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0562, 0.0000, 0.1225],
        [0.0000, 0.0000, 0.0000,  ..., 0.0562, 0.0000, 0.1225],
        [0.0000, 0.0000, 0.0000,  ..., 0.0562, 0.0000, 0.1225]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3356839.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1579.9512, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6394.7690, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3600.4390, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3684.6372, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4703],
        [-0.4649],
        [-0.4481],
        ...,
        [-0.3772],
        [-0.3761],
        [-0.3758]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-269310.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(695.2383, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(695.2383, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0059,  0.0000,  ...,  0.0062, -0.0058,  0.0000],
        [ 0.0000, -0.0059,  0.0000,  ...,  0.0062, -0.0058,  0.0000],
        [-0.0022, -0.0046, -0.0111,  ...,  0.0070, -0.0062, -0.0055],
        ...,
        [ 0.0000, -0.0059,  0.0000,  ...,  0.0062, -0.0058,  0.0000],
        [ 0.0000, -0.0059,  0.0000,  ...,  0.0062, -0.0058,  0.0000],
        [ 0.0000, -0.0059,  0.0000,  ...,  0.0062, -0.0058,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-22866.2969, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.6530, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(85.7601, device='cuda:0')



h[100].sum tensor(141.3864, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-102.2251, device='cuda:0')



h[200].sum tensor(-561.2449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.9462, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0248, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0255, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0275, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0248, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0248, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0248, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(307188.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0228, 0.0000, 0.1087],
        [0.0000, 0.0000, 0.0000,  ..., 0.0102, 0.0000, 0.0809],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0394],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0571, 0.0000, 0.1199],
        [0.0000, 0.0000, 0.0000,  ..., 0.0571, 0.0000, 0.1199],
        [0.0000, 0.0000, 0.0000,  ..., 0.0566, 0.0000, 0.1197]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3271749.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1693.1694, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6299.9438, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2365.1313, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3540.0659, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0788],
        [-0.0954],
        [-0.1509],
        ...,
        [-0.3679],
        [-0.3517],
        [-0.3307]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-244547.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2849],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(827.3617, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2849],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(827.3617, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0051, -0.0073,  ...,  0.0067, -0.0061, -0.0036],
        [ 0.0000, -0.0059,  0.0000,  ...,  0.0062, -0.0058,  0.0000],
        [-0.0014, -0.0051, -0.0073,  ...,  0.0067, -0.0061, -0.0036],
        ...,
        [ 0.0000, -0.0059,  0.0000,  ...,  0.0062, -0.0058,  0.0000],
        [ 0.0000, -0.0059,  0.0000,  ...,  0.0062, -0.0058,  0.0000],
        [ 0.0000, -0.0059,  0.0000,  ...,  0.0062, -0.0058,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-22240.6445, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.4674, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(102.0579, device='cuda:0')



h[100].sum tensor(146.9402, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-121.6520, device='cuda:0')



h[200].sum tensor(-558.6713, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.2571, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0262, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0273, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0255, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0247, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0247, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0247, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(319821.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0866],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0884],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.1005],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0571, 0.0000, 0.1182],
        [0.0000, 0.0000, 0.0000,  ..., 0.0571, 0.0000, 0.1182],
        [0.0000, 0.0000, 0.0000,  ..., 0.0571, 0.0000, 0.1182]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3317775.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1748.1884, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6226.9307, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2970.5181, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3492.8384, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1671],
        [ 0.1519],
        [ 0.1112],
        ...,
        [-0.3764],
        [-0.3746],
        [-0.3693]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-182939.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5122],
        [0.4766],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(667.8674, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5122],
        [0.4766],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(667.8674, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0024, -0.0045, -0.0129,  ...,  0.0071, -0.0063, -0.0063],
        [-0.0040, -0.0035, -0.0219,  ...,  0.0077, -0.0066, -0.0107],
        [-0.0042, -0.0034, -0.0228,  ...,  0.0078, -0.0066, -0.0111],
        ...,
        [ 0.0000, -0.0059,  0.0000,  ...,  0.0062, -0.0058,  0.0000],
        [ 0.0000, -0.0059,  0.0000,  ...,  0.0062, -0.0058,  0.0000],
        [ 0.0000, -0.0059,  0.0000,  ...,  0.0062, -0.0058,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-22958.4258, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.0052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(82.3838, device='cuda:0')



h[100].sum tensor(129.9473, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-98.2006, device='cuda:0')



h[200].sum tensor(-565.1977, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.8460, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0294, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0310, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0303, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0248, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0248, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0248, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(307042.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0137],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0013],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0202],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0599, 0.0000, 0.1202],
        [0.0000, 0.0000, 0.0000,  ..., 0.0599, 0.0000, 0.1202],
        [0.0000, 0.0000, 0.0000,  ..., 0.0599, 0.0000, 0.1202]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3234931.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1957.0918, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6258.3179, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1869.2261, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3335.7839, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1020],
        [ 0.0918],
        [ 0.0751],
        ...,
        [-0.4306],
        [-0.4295],
        [-0.4293]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-274563.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1044.4760, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1044.4760, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010, -0.0053, -0.0054,  ...,  0.0066, -0.0060, -0.0027],
        [-0.0010, -0.0053, -0.0054,  ...,  0.0066, -0.0060, -0.0027],
        [ 0.0000, -0.0059,  0.0000,  ...,  0.0062, -0.0058,  0.0000],
        ...,
        [-0.0027, -0.0043, -0.0147,  ...,  0.0072, -0.0063, -0.0072],
        [ 0.0000, -0.0059,  0.0000,  ...,  0.0062, -0.0058,  0.0000],
        [ 0.0000, -0.0059,  0.0000,  ...,  0.0062, -0.0058,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-21433.2578, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.8934, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(128.8397, device='cuda:0')



h[100].sum tensor(151.3083, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-153.5756, device='cuda:0')



h[200].sum tensor(-552.7722, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(41.9844, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0271, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0262, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0276, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0264, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0262, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0248, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(345412.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0784],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0828],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0742],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0821],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0910],
        [0.0000, 0.0000, 0.0000,  ..., 0.0289, 0.0000, 0.1106]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3470243.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1885.0415, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6305.0322, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5648.3818, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3517.0518, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1033],
        [ 0.1041],
        [ 0.0901],
        ...,
        [ 0.0581],
        [-0.0132],
        [-0.1203]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-266741.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2766],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(871.6460, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2766],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(871.6460, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010, -0.0053, -0.0054,  ...,  0.0065, -0.0060, -0.0026],
        [-0.0012, -0.0052, -0.0069,  ...,  0.0066, -0.0061, -0.0033],
        [ 0.0000, -0.0059,  0.0000,  ...,  0.0061, -0.0058,  0.0000],
        ...,
        [ 0.0000, -0.0059,  0.0000,  ...,  0.0061, -0.0058,  0.0000],
        [ 0.0000, -0.0059,  0.0000,  ...,  0.0061, -0.0058,  0.0000],
        [ 0.0000, -0.0059,  0.0000,  ...,  0.0061, -0.0058,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-22342.4609, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.0793, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(107.5206, device='cuda:0')



h[100].sum tensor(141.5610, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-128.1634, device='cuda:0')



h[200].sum tensor(-559.7952, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.0372, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0280, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0259, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0249, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0244, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0244, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0244, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(325401.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0599],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0802],
        [0.0000, 0.0000, 0.0000,  ..., 0.0174, 0.0000, 0.1008],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0576, 0.0000, 0.1191],
        [0.0000, 0.0000, 0.0000,  ..., 0.0576, 0.0000, 0.1191],
        [0.0000, 0.0000, 0.0000,  ..., 0.0576, 0.0000, 0.1191]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3332891.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1919.6547, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6350.8564, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3556.1831, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3356.7188, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1028],
        [ 0.0659],
        [-0.0089],
        ...,
        [-0.4409],
        [-0.4399],
        [-0.4397]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-242937.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(948.4626, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(948.4626, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0060,  0.0000,  ...,  0.0059, -0.0059,  0.0000],
        [ 0.0000, -0.0060,  0.0000,  ...,  0.0059, -0.0059,  0.0000],
        [-0.0011, -0.0052, -0.0065,  ...,  0.0063, -0.0061, -0.0031],
        ...,
        [ 0.0000, -0.0060,  0.0000,  ...,  0.0059, -0.0059,  0.0000],
        [ 0.0000, -0.0060,  0.0000,  ...,  0.0059, -0.0059,  0.0000],
        [ 0.0000, -0.0060,  0.0000,  ...,  0.0059, -0.0059,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-22223.3828, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.7019, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(116.9962, device='cuda:0')



h[100].sum tensor(152.9867, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-139.4582, device='cuda:0')



h[200].sum tensor(-558.3712, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.1250, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0237, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0241, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0243, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0237, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0237, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0237, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(331723.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0366, 0.0000, 0.1131],
        [0.0000, 0.0000, 0.0000,  ..., 0.0173, 0.0000, 0.1043],
        [0.0000, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0965],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0513, 0.0000, 0.1156],
        [0.0000, 0.0000, 0.0000,  ..., 0.0513, 0.0000, 0.1156],
        [0.0000, 0.0000, 0.0000,  ..., 0.0513, 0.0000, 0.1156]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3351998.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1791.3215, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6370.7334, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4122.5566, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3358.0557, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1758],
        [-0.0545],
        [ 0.0421],
        ...,
        [-0.4109],
        [-0.4037],
        [-0.3970]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-140934.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8110],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(696.0815, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8110],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(696.0815, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0060,  0.0000,  ...,  0.0058, -0.0059,  0.0000],
        [-0.0033, -0.0038, -0.0199,  ...,  0.0070, -0.0066, -0.0095],
        [ 0.0000, -0.0060,  0.0000,  ...,  0.0058, -0.0059,  0.0000],
        ...,
        [ 0.0000, -0.0060,  0.0000,  ...,  0.0058, -0.0059,  0.0000],
        [ 0.0000, -0.0060,  0.0000,  ...,  0.0058, -0.0059,  0.0000],
        [ 0.0000, -0.0060,  0.0000,  ...,  0.0058, -0.0059,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-23503.4121, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.0185, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(85.8641, device='cuda:0')



h[100].sum tensor(138.1410, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-102.3491, device='cuda:0')



h[200].sum tensor(-566.7297, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.9801, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0244, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0241, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0277, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0231, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0231, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0231, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(307893.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0034, 0.0000, 0.0906],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0835],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0595],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0457, 0.0000, 0.1165],
        [0.0000, 0.0000, 0.0000,  ..., 0.0457, 0.0000, 0.1165],
        [0.0000, 0.0000, 0.0000,  ..., 0.0457, 0.0000, 0.1165]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3191449., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1709.8943, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6350.8096, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2877.8511, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3281.8345, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0964],
        [-0.0456],
        [-0.0093],
        ...,
        [-0.4488],
        [-0.4479],
        [-0.4476]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-204628.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(759.6832, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(759.6832, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0060,  0.0000,  ...,  0.0057, -0.0059,  0.0000],
        [ 0.0000, -0.0060,  0.0000,  ...,  0.0057, -0.0059,  0.0000],
        [-0.0010, -0.0053, -0.0063,  ...,  0.0061, -0.0061, -0.0030],
        ...,
        [ 0.0000, -0.0060,  0.0000,  ...,  0.0057, -0.0059,  0.0000],
        [ 0.0000, -0.0060,  0.0000,  ...,  0.0057, -0.0059,  0.0000],
        [ 0.0000, -0.0060,  0.0000,  ...,  0.0057, -0.0059,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-23541.3145, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.5576, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(93.7096, device='cuda:0')



h[100].sum tensor(144.7796, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-111.7008, device='cuda:0')



h[200].sum tensor(-564.6772, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.5367, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0229, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0233, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0242, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0229, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0229, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0229, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(316090.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.1061],
        [0.0000, 0.0000, 0.0000,  ..., 0.0085, 0.0000, 0.0998],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0875],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0424, 0.0000, 0.1159],
        [0.0000, 0.0000, 0.0000,  ..., 0.0424, 0.0000, 0.1159],
        [0.0000, 0.0000, 0.0000,  ..., 0.0424, 0.0000, 0.1159]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3264861.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1565.3651, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6465.9438, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5170.4521, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3390.4287, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0059],
        [ 0.0184],
        [ 0.0383],
        ...,
        [-0.4827],
        [-0.4815],
        [-0.4811]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-221365.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 40.0 event: 600 loss: tensor(499.1178, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(756.7803, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(756.7803, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0060,  0.0000,  ...,  0.0057, -0.0059,  0.0000],
        [ 0.0000, -0.0060,  0.0000,  ...,  0.0057, -0.0059,  0.0000],
        [ 0.0000, -0.0060,  0.0000,  ...,  0.0057, -0.0059,  0.0000],
        ...,
        [ 0.0000, -0.0060,  0.0000,  ...,  0.0057, -0.0059,  0.0000],
        [ 0.0000, -0.0060,  0.0000,  ...,  0.0057, -0.0059,  0.0000],
        [ 0.0000, -0.0060,  0.0000,  ...,  0.0057, -0.0059,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-23878.3789, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.0465, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(93.3515, device='cuda:0')



h[100].sum tensor(151.1837, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-111.2740, device='cuda:0')



h[200].sum tensor(-564.8309, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.4200, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0229, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0229, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0229, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0229, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0229, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0229, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(310338.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0387, 0.0000, 0.1144],
        [0.0000, 0.0000, 0.0000,  ..., 0.0377, 0.0000, 0.1140],
        [0.0000, 0.0000, 0.0000,  ..., 0.0280, 0.0000, 0.1123],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0410, 0.0000, 0.1141],
        [0.0000, 0.0000, 0.0000,  ..., 0.0410, 0.0000, 0.1141],
        [0.0000, 0.0000, 0.0000,  ..., 0.0410, 0.0000, 0.1141]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3230147.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1468.9285, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6521.9736, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4812.7505, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3445.0259, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3690],
        [-0.2955],
        [-0.1771],
        ...,
        [-0.5093],
        [-0.5083],
        [-0.5080]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-221806.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(718.3706, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(718.3706, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0060,  0.0000,  ...,  0.0057, -0.0059,  0.0000],
        [ 0.0000, -0.0060,  0.0000,  ...,  0.0057, -0.0059,  0.0000],
        [ 0.0000, -0.0060,  0.0000,  ...,  0.0057, -0.0059,  0.0000],
        ...,
        [ 0.0000, -0.0060,  0.0000,  ...,  0.0057, -0.0059,  0.0000],
        [ 0.0000, -0.0060,  0.0000,  ...,  0.0057, -0.0059,  0.0000],
        [ 0.0000, -0.0060,  0.0000,  ...,  0.0057, -0.0059,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-24362.4453, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.0544, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(88.6135, device='cuda:0')



h[100].sum tensor(152.0432, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-105.6264, device='cuda:0')



h[200].sum tensor(-566.1760, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.8761, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0228, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0228, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0228, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0228, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0228, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0228, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(305572.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.1067],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.1089],
        [0.0000, 0.0000, 0.0000,  ..., 0.0225, 0.0000, 0.1114],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0409, 0.0000, 0.1136],
        [0.0000, 0.0000, 0.0000,  ..., 0.0409, 0.0000, 0.1136],
        [0.0000, 0.0000, 0.0000,  ..., 0.0409, 0.0000, 0.1136]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3207489.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1387.6644, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6525.5684, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4949.3633, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3509.0735, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0035],
        [-0.0170],
        [-0.0372],
        ...,
        [-0.5510],
        [-0.5498],
        [-0.5495]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-288115.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(976.5939, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(976.5939, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0060,  0.0000,  ...,  0.0055, -0.0059,  0.0000],
        [ 0.0000, -0.0060,  0.0000,  ...,  0.0055, -0.0059,  0.0000],
        [ 0.0000, -0.0060,  0.0000,  ...,  0.0055, -0.0059,  0.0000],
        ...,
        [ 0.0000, -0.0060,  0.0000,  ...,  0.0055, -0.0059,  0.0000],
        [ 0.0000, -0.0060,  0.0000,  ...,  0.0055, -0.0059,  0.0000],
        [ 0.0000, -0.0060,  0.0000,  ...,  0.0055, -0.0059,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-23286.8398, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.9644, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(120.4662, device='cuda:0')



h[100].sum tensor(173.6039, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-143.5945, device='cuda:0')



h[200].sum tensor(-558.0605, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.2558, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0222, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0222, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0222, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0222, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0222, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0222, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(329998.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0380, 0.0000, 0.1129],
        [0.0000, 0.0000, 0.0000,  ..., 0.0382, 0.0000, 0.1130],
        [0.0000, 0.0000, 0.0000,  ..., 0.0338, 0.0000, 0.1126],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0408, 0.0000, 0.1132],
        [0.0000, 0.0000, 0.0000,  ..., 0.0408, 0.0000, 0.1132],
        [0.0000, 0.0000, 0.0000,  ..., 0.0408, 0.0000, 0.1132]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3356785., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1335.8959, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6475.1304, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8523.6074, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3547.1519, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4031],
        [-0.3500],
        [-0.2596],
        ...,
        [-0.5414],
        [-0.5403],
        [-0.5401]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-216020.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5503],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(893.9696, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5503],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(893.9696, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0018, -0.0046, -0.0129,  ...,  0.0062, -0.0064, -0.0058],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0055, -0.0059,  0.0000],
        [-0.0049, -0.0021, -0.0353,  ...,  0.0075, -0.0073, -0.0160],
        ...,
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0055, -0.0059,  0.0000],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0055, -0.0059,  0.0000],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0055, -0.0059,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-23866.1836, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.3898, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(110.2743, device='cuda:0')



h[100].sum tensor(165.9602, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-131.4458, device='cuda:0')



h[200].sum tensor(-560.7914, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.9345, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0225, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0263, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0260, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0219, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0219, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0219, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(320115.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0776],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0326],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0126],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0432, 0.0000, 0.1158],
        [0.0000, 0.0000, 0.0000,  ..., 0.0432, 0.0000, 0.1158],
        [0.0000, 0.0000, 0.0000,  ..., 0.0432, 0.0000, 0.1158]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3284222., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1337.2800, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6464.4507, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7873.3496, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3526.1768, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0236],
        [ 0.0063],
        [ 0.0085],
        ...,
        [-0.6045],
        [-0.6033],
        [-0.6029]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-303921.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2844],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(668.5909, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2844],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(668.5909, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0006, -0.0055, -0.0048,  ...,  0.0056, -0.0061, -0.0022],
        [-0.0009, -0.0053, -0.0066,  ...,  0.0057, -0.0062, -0.0030],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0054, -0.0059,  0.0000],
        ...,
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0054, -0.0059,  0.0000],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0054, -0.0059,  0.0000],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0054, -0.0059,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-25022.8789, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.1341, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(82.4730, device='cuda:0')



h[100].sum tensor(156.5760, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-98.3070, device='cuda:0')



h[200].sum tensor(-568.3888, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.8751, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0238, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0222, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0218, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0214, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0214, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0214, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(301278.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0598],
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0000, 0.0880],
        [0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0000, 0.1052],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0459, 0.0000, 0.1168],
        [0.0000, 0.0000, 0.0000,  ..., 0.0459, 0.0000, 0.1168],
        [0.0000, 0.0000, 0.0000,  ..., 0.0459, 0.0000, 0.1168]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3196341., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1400.6315, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6397.4858, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6185.0635, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3363.4507, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0464],
        [ 0.0050],
        [-0.0796],
        ...,
        [-0.6140],
        [-0.6170],
        [-0.6187]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-283454.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(801.7515, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(801.7515, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0048, -0.0109,  ...,  0.0059, -0.0064, -0.0049],
        [-0.0016, -0.0047, -0.0124,  ...,  0.0059, -0.0064, -0.0055],
        [-0.0016, -0.0046, -0.0126,  ...,  0.0059, -0.0064, -0.0056],
        ...,
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0053, -0.0059,  0.0000],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0053, -0.0059,  0.0000],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0053, -0.0059,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-24532.5430, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.2647, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(98.8988, device='cuda:0')



h[100].sum tensor(169.1420, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-117.8864, device='cuda:0')



h[200].sum tensor(-564.3026, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.2277, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0232, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0236, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0242, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0211, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0211, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0211, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(316514.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0136],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0234],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0355],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0497, 0.0000, 0.1167],
        [0.0000, 0.0000, 0.0000,  ..., 0.0497, 0.0000, 0.1167],
        [0.0000, 0.0000, 0.0000,  ..., 0.0497, 0.0000, 0.1167]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3305223., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1415.9119, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6328.8726, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8037.6650, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3387.4087, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1298],
        [ 0.1371],
        [ 0.1308],
        ...,
        [-0.6374],
        [-0.6359],
        [-0.6352]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-251797.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5815],
        [0.5928],
        [0.5962],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(696.7634, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5815],
        [0.5928],
        [0.5962],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(696.7634, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0030, -0.0033, -0.0244,  ...,  0.0065, -0.0069, -0.0108],
        [-0.0060, -0.0005, -0.0488,  ...,  0.0078, -0.0079, -0.0215],
        [-0.0030, -0.0033, -0.0245,  ...,  0.0066, -0.0069, -0.0108],
        ...,
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0053, -0.0059,  0.0000],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0053, -0.0059,  0.0000],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0053, -0.0059,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-25240.2559, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.6675, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(85.9482, device='cuda:0')



h[100].sum tensor(164.9660, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-102.4493, device='cuda:0')



h[200].sum tensor(-567.9331, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.0075, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0273, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0269, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0274, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0211, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0211, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0211, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(301756.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0552, 0.0000, 0.1171],
        [0.0000, 0.0000, 0.0000,  ..., 0.0552, 0.0000, 0.1171],
        [0.0000, 0.0000, 0.0000,  ..., 0.0552, 0.0000, 0.1171]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3205935.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1482.8893, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6245.4458, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5110.6582, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3330.7478, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1652],
        [ 0.1640],
        [ 0.1687],
        ...,
        [-0.6787],
        [-0.6770],
        [-0.6762]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-294628.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(567.5947, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(567.5947, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0061,  0.0000,  ...,  0.0053, -0.0059,  0.0000],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0053, -0.0059,  0.0000],
        [-0.0008, -0.0054, -0.0064,  ...,  0.0056, -0.0062, -0.0028],
        ...,
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0053, -0.0059,  0.0000],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0053, -0.0059,  0.0000],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0053, -0.0059,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-26071.4961, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.9310, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(70.0148, device='cuda:0')



h[100].sum tensor(157.6485, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-83.4569, device='cuda:0')



h[200].sum tensor(-572.2789, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.8154, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0213, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0216, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0230, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0213, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0213, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0213, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(291244.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0236, 0.0000, 0.1063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0092, 0.0000, 0.0884],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0536],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0607, 0.0000, 0.1176],
        [0.0000, 0.0000, 0.0000,  ..., 0.0607, 0.0000, 0.1176],
        [0.0000, 0.0000, 0.0000,  ..., 0.0607, 0.0000, 0.1176]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3168458., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1538.0247, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6192.2578, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3767.3464, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3301.6187, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0300],
        [ 0.1354],
        [ 0.2098],
        ...,
        [-0.7254],
        [-0.7235],
        [-0.7229]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-339894.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(730.3507, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(730.3507, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0061,  0.0000,  ...,  0.0053, -0.0059,  0.0000],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0053, -0.0059,  0.0000],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0053, -0.0059,  0.0000],
        ...,
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0053, -0.0059,  0.0000],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0053, -0.0059,  0.0000],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0053, -0.0059,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-25258.4727, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.6540, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(90.0913, device='cuda:0')



h[100].sum tensor(166.9840, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-107.3879, device='cuda:0')



h[200].sum tensor(-566.8313, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.3576, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0216, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0213, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0217, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0213, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0213, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0213, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(306439.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0242, 0.0000, 0.1100],
        [0.0000, 0.0000, 0.0000,  ..., 0.0299, 0.0000, 0.1124],
        [0.0000, 0.0000, 0.0000,  ..., 0.0163, 0.0000, 0.1041],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0607, 0.0000, 0.1176],
        [0.0000, 0.0000, 0.0000,  ..., 0.0607, 0.0000, 0.1176],
        [0.0000, 0.0000, 0.0000,  ..., 0.0607, 0.0000, 0.1176]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3267773.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1514.0724, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6249.6621, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6026.9336, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3360.7339, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3281],
        [-0.2535],
        [-0.1093],
        ...,
        [-0.7256],
        [-0.7237],
        [-0.7231]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-314322.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2610],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(672.5190, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2610],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(672.5190, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007, -0.0054, -0.0058,  ...,  0.0057, -0.0062, -0.0025],
        [-0.0006, -0.0055, -0.0057,  ...,  0.0057, -0.0062, -0.0025],
        [-0.0013, -0.0048, -0.0115,  ...,  0.0060, -0.0064, -0.0050],
        ...,
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0054, -0.0060,  0.0000],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0054, -0.0060,  0.0000],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0054, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-25692.6523, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.6440, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(82.9576, device='cuda:0')



h[100].sum tensor(159.3102, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-98.8845, device='cuda:0')



h[200].sum tensor(-568.5374, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.0330, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0225, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0235, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0225, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0215, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0215, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0215, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(300745.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0828],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0658],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0726],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0652, 0.0000, 0.1195],
        [0.0000, 0.0000, 0.0000,  ..., 0.0652, 0.0000, 0.1195],
        [0.0000, 0.0000, 0.0000,  ..., 0.0652, 0.0000, 0.1195]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3246004., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1556.0621, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6232.4043, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5374.3770, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3362.2175, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1393],
        [ 0.1575],
        [ 0.1659],
        ...,
        [-0.7885],
        [-0.7864],
        [-0.7858]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-402372.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 50.0 event: 750 loss: tensor(497.1146, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(682.6090, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(682.6090, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0061,  0.0000,  ...,  0.0054, -0.0060,  0.0000],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0054, -0.0060,  0.0000],
        [-0.0013, -0.0047, -0.0120,  ...,  0.0060, -0.0065, -0.0052],
        ...,
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0054, -0.0060,  0.0000],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0054, -0.0060,  0.0000],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0054, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-25605.7344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.4323, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(84.2022, device='cuda:0')



h[100].sum tensor(166.4046, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-100.3681, device='cuda:0')



h[200].sum tensor(-567.7075, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.4386, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0218, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0232, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0241, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0215, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0215, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0215, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(302151.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0097, 0.0000, 0.0819],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0373],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0119],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0667, 0.0000, 0.1170],
        [0.0000, 0.0000, 0.0000,  ..., 0.0667, 0.0000, 0.1170],
        [0.0000, 0.0000, 0.0000,  ..., 0.0667, 0.0000, 0.1170]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3254788.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1568.3516, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6241.7930, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5236.8809, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3338.7290, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0633],
        [ 0.1091],
        [ 0.1235],
        ...,
        [-0.7905],
        [-0.7884],
        [-0.7878]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-357275.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(949.4442, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(949.4442, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0061,  0.0000,  ...,  0.0054, -0.0060,  0.0000],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0054, -0.0060,  0.0000],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0054, -0.0060,  0.0000],
        ...,
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0054, -0.0060,  0.0000],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0054, -0.0060,  0.0000],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0054, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-24268.9570, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.5523, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(117.1172, device='cuda:0')



h[100].sum tensor(186.4729, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-139.6025, device='cuda:0')



h[200].sum tensor(-558.2753, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.1644, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0216, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0216, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0216, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0216, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0216, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0216, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(326999.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0319, 0.0000, 0.1057],
        [0.0000, 0.0000, 0.0000,  ..., 0.0566, 0.0000, 0.1123],
        [0.0000, 0.0000, 0.0000,  ..., 0.0680, 0.0000, 0.1154],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0692, 0.0000, 0.1148],
        [0.0000, 0.0000, 0.0000,  ..., 0.0692, 0.0000, 0.1148],
        [0.0000, 0.0000, 0.0000,  ..., 0.0692, 0.0000, 0.1148]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3404694.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1538.6653, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6279.8193, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7845.0288, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3449.2268, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0563],
        [-0.2002],
        [-0.3565],
        ...,
        [-0.7959],
        [-0.8002],
        [-0.8013]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-306443.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6772],
        [0.4797],
        [0.4402],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(729.1266, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6772],
        [0.4797],
        [0.4402],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(729.1266, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0018, -0.0040, -0.0183,  ...,  0.0064, -0.0068, -0.0077],
        [-0.0032, -0.0023, -0.0321,  ...,  0.0071, -0.0073, -0.0135],
        [-0.0017, -0.0041, -0.0174,  ...,  0.0064, -0.0067, -0.0073],
        ...,
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0055, -0.0060,  0.0000],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0055, -0.0060,  0.0000],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0055, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-25695.0605, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.1346, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(89.9403, device='cuda:0')



h[100].sum tensor(165.1124, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-107.2079, device='cuda:0')



h[200].sum tensor(-565.7704, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.3084, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0264, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0266, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0268, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0222, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0222, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0222, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(304930.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0091],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0018],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0749, 0.0000, 0.1167],
        [0.0000, 0.0000, 0.0000,  ..., 0.0749, 0.0000, 0.1167],
        [0.0000, 0.0000, 0.0000,  ..., 0.0749, 0.0000, 0.1167]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3294544., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1569.3676, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6274.8604, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5512.6743, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3460.8296, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0131],
        [ 0.0470],
        [ 0.0664],
        ...,
        [-0.8974],
        [-0.8960],
        [-0.8957]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-456293.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2461],
        [0.3000],
        [0.2915],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(898.3650, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2461],
        [0.3000],
        [0.2915],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(898.3650, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012, -0.0046, -0.0128,  ...,  0.0062, -0.0065, -0.0053],
        [-0.0011, -0.0048, -0.0115,  ...,  0.0061, -0.0065, -0.0048],
        [-0.0012, -0.0046, -0.0126,  ...,  0.0062, -0.0065, -0.0052],
        ...,
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0056, -0.0060,  0.0000],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0056, -0.0060,  0.0000],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0056, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-24955.0527, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.1470, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(110.8164, device='cuda:0')



h[100].sum tensor(175.4150, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-132.0920, device='cuda:0')



h[200].sum tensor(-559.9401, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.1112, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0243, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0244, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0243, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0223, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0223, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0230, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(314476.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0503],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0492],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0502],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0632, 0.0000, 0.1127],
        [0.0000, 0.0000, 0.0000,  ..., 0.0348, 0.0000, 0.1059],
        [0.0000, 0.0000, 0.0000,  ..., 0.0083, 0.0000, 0.0890]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3354021.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1571.5043, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6395.4629, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6250.8931, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3480.9595, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1556],
        [ 0.1577],
        [ 0.1572],
        ...,
        [-0.5625],
        [-0.2857],
        [-0.0576]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-411091.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.7568],
        [0.0000],
        [0.3125],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(755.4523, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.7568],
        [0.0000],
        [0.3125],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(755.4523, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009, -0.0049, -0.0103,  ...,  0.0060, -0.0064, -0.0042],
        [-0.0025, -0.0029, -0.0273,  ...,  0.0067, -0.0072, -0.0112],
        [-0.0004, -0.0056, -0.0046,  ...,  0.0057, -0.0062, -0.0019],
        ...,
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0055, -0.0060,  0.0000],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0055, -0.0060,  0.0000],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0055, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-25741.1133, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.6817, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(93.1877, device='cuda:0')



h[100].sum tensor(171.9169, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-111.0787, device='cuda:0')



h[200].sum tensor(-564.4709, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.3666, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0264, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0242, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0245, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0221, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0221, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0221, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(307829.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0164],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0363],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0813, 0.0000, 0.1148],
        [0.0000, 0.0000, 0.0000,  ..., 0.0813, 0.0000, 0.1148],
        [0.0000, 0.0000, 0.0000,  ..., 0.0813, 0.0000, 0.1148]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3328176., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1596.7443, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6362.4111, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5319.8252, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3410.4109, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1119],
        [ 0.1167],
        [ 0.1083],
        ...,
        [-0.9499],
        [-0.9473],
        [-0.9466]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-411943.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(997.9979, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(997.9979, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0061,  0.0000,  ...,  0.0055, -0.0060,  0.0000],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0055, -0.0060,  0.0000],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0055, -0.0060,  0.0000],
        ...,
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0055, -0.0060,  0.0000],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0055, -0.0060,  0.0000],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0055, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-24443.7324, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.1040, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(123.1065, device='cuda:0')



h[100].sum tensor(187.6931, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-146.7417, device='cuda:0')



h[200].sum tensor(-556.0439, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.1161, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0219, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0219, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0219, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0219, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0219, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0219, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(337694.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0831, 0.0000, 0.1153],
        [0.0000, 0.0000, 0.0000,  ..., 0.0831, 0.0000, 0.1153],
        [0.0000, 0.0000, 0.0000,  ..., 0.0832, 0.0000, 0.1157],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0831, 0.0000, 0.1146],
        [0.0000, 0.0000, 0.0000,  ..., 0.0831, 0.0000, 0.1146],
        [0.0000, 0.0000, 0.0000,  ..., 0.0831, 0.0000, 0.1146]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3584601.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1554.3177, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6453.2007, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10744.3262, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3498.5769, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0197],
        [-1.0641],
        [-1.0975],
        ...,
        [-0.9733],
        [-0.9709],
        [-0.9702]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-343708.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4001],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(821.8214, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4001],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(821.8214, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007, -0.0052, -0.0082,  ...,  0.0058, -0.0063, -0.0033],
        [-0.0007, -0.0051, -0.0083,  ...,  0.0058, -0.0064, -0.0033],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0055, -0.0060,  0.0000],
        ...,
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0055, -0.0060,  0.0000],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0055, -0.0060,  0.0000],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0055, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-25533.7305, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.4806, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(101.3745, device='cuda:0')



h[100].sum tensor(176.3108, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-120.8374, device='cuda:0')



h[200].sum tensor(-561.8062, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.0344, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0238, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0225, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0222, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0219, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0219, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0219, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(316382.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0555],
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0842],
        [0.0000, 0.0000, 0.0000,  ..., 0.0324, 0.0000, 0.1000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0853, 0.0000, 0.1147],
        [0.0000, 0.0000, 0.0000,  ..., 0.0853, 0.0000, 0.1147],
        [0.0000, 0.0000, 0.0000,  ..., 0.0853, 0.0000, 0.1147]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3413054., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1555.0498, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6423.8872, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6873.6494, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3312.4878, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0150],
        [-0.2221],
        [-0.4792],
        ...,
        [-1.0173],
        [-1.0147],
        [-1.0140]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-375609.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(685.3923, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(685.3923, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008, -0.0050, -0.0097,  ...,  0.0059, -0.0064, -0.0039],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0055, -0.0060,  0.0000],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0055, -0.0060,  0.0000],
        ...,
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0055, -0.0060,  0.0000],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0055, -0.0060,  0.0000],
        [ 0.0000, -0.0061,  0.0000,  ...,  0.0055, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-26417.4648, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.2611, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(84.5455, device='cuda:0')



h[100].sum tensor(168.8065, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-100.7774, device='cuda:0')



h[200].sum tensor(-566.2797, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.5504, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0227, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0223, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0219, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0219, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0219, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0219, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(301684.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0710],
        [0.0000, 0.0000, 0.0000,  ..., 0.0249, 0.0000, 0.0911],
        [0.0000, 0.0000, 0.0000,  ..., 0.0530, 0.0000, 0.1065],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0873, 0.0000, 0.1139],
        [0.0000, 0.0000, 0.0000,  ..., 0.0873, 0.0000, 0.1139],
        [0.0000, 0.0000, 0.0000,  ..., 0.0873, 0.0000, 0.1139]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3301634.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1524.5770, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6336.9727, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4143.7007, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3179.3389, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0350],
        [-0.1967],
        [-0.3800],
        ...,
        [-1.0366],
        [-1.0487],
        [-1.0522]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-423915.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(674.5704, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(674.5704, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004, -0.0055, -0.0050,  ...,  0.0057, -0.0062, -0.0020],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0055, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0055, -0.0060,  0.0000],
        ...,
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0055, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0055, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0055, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-26613.5469, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.9817, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(83.2106, device='cuda:0')



h[100].sum tensor(167.4095, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-99.1862, device='cuda:0')



h[200].sum tensor(-566.1501, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.1154, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0226, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0225, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0221, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0221, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0221, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0221, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(304229.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0100, 0.0000, 0.0889],
        [0.0000, 0.0000, 0.0000,  ..., 0.0266, 0.0000, 0.0957],
        [0.0000, 0.0000, 0.0000,  ..., 0.0616, 0.0000, 0.1068],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0907, 0.0000, 0.1132],
        [0.0000, 0.0000, 0.0000,  ..., 0.0907, 0.0000, 0.1132],
        [0.0000, 0.0000, 0.0000,  ..., 0.0907, 0.0000, 0.1132]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3331941., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1478.8400, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6333.6235, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4247.4663, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3188.2134, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0430],
        [-0.0941],
        [-0.3136],
        ...,
        [-1.1218],
        [-1.1189],
        [-1.1181]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-464701.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1020.5215, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1020.5215, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012, -0.0041, -0.0168,  ...,  0.0062, -0.0068, -0.0065],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0055, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0055, -0.0060,  0.0000],
        ...,
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0055, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0055, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0055, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-24674.2422, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.6368, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(125.8849, device='cuda:0')



h[100].sum tensor(185.4322, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-150.0534, device='cuda:0')



h[200].sum tensor(-554.2292, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(41.0215, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0245, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0237, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0231, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0221, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0221, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0221, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(343105.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0031],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0102],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0212],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0920, 0.0000, 0.1131],
        [0.0000, 0.0000, 0.0000,  ..., 0.0920, 0.0000, 0.1131],
        [0.0000, 0.0000, 0.0000,  ..., 0.0920, 0.0000, 0.1131]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3586380.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1342.9058, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6295.9556, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9496.1426, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3337.0574, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1924],
        [ 0.2039],
        [ 0.2088],
        ...,
        [-1.1786],
        [-1.1756],
        [-1.1748]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-498566.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 60.0 event: 900 loss: tensor(548.9476, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(752.4667, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(752.4667, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003, -0.0055, -0.0049,  ...,  0.0057, -0.0062, -0.0019],
        [-0.0003, -0.0055, -0.0049,  ...,  0.0057, -0.0062, -0.0019],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0055, -0.0060,  0.0000],
        ...,
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0055, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0055, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0055, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-26260.7656, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.8444, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(92.8194, device='cuda:0')



h[100].sum tensor(167.4150, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-110.6397, device='cuda:0')



h[200].sum tensor(-564.4683, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.2466, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0225, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0224, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0223, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0219, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0219, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0219, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(308984.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0823],
        [0.0000, 0.0000, 0.0000,  ..., 0.0079, 0.0000, 0.0870],
        [0.0000, 0.0000, 0.0000,  ..., 0.0197, 0.0000, 0.0940],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0913, 0.0000, 0.1126],
        [0.0000, 0.0000, 0.0000,  ..., 0.0913, 0.0000, 0.1126],
        [0.0000, 0.0000, 0.0000,  ..., 0.0913, 0.0000, 0.1126]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3342928.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1323.7080, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6298.2651, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4474.6836, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3056.2043, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0628],
        [-0.0052],
        [-0.1275],
        ...,
        [-1.2352],
        [-1.2321],
        [-1.2313]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-561866., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3350],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(582.9711, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3350],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(582.9711, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003, -0.0056, -0.0046,  ...,  0.0054, -0.0062, -0.0017],
        [-0.0007, -0.0047, -0.0112,  ...,  0.0057, -0.0065, -0.0043],
        [-0.0004, -0.0054, -0.0060,  ...,  0.0055, -0.0063, -0.0023],
        ...,
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0052, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0052, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0052, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-26981.5273, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.7974, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(71.9115, device='cuda:0')



h[100].sum tensor(170.3760, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-85.7178, device='cuda:0')



h[200].sum tensor(-570.8825, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.4335, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0221, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0217, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0222, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0209, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0209, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0209, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(301231.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0680],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0671],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0558],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0853, 0.0000, 0.1088],
        [0.0000, 0.0000, 0.0000,  ..., 0.0853, 0.0000, 0.1088],
        [0.0000, 0.0000, 0.0000,  ..., 0.0853, 0.0000, 0.1088]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3297573.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1209.4888, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6167.9160, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2928.9517, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2839.6008, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2285],
        [-0.0968],
        [-0.0257],
        ...,
        [-1.1930],
        [-1.1905],
        [-1.1900]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-382618.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2952],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(612.3512, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2952],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(612.3512, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0062,  0.0000,  ...,  0.0052, -0.0060,  0.0000],
        [-0.0006, -0.0048, -0.0106,  ...,  0.0056, -0.0065, -0.0040],
        [-0.0009, -0.0041, -0.0158,  ...,  0.0058, -0.0067, -0.0059],
        ...,
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0052, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0052, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0052, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-27042.2852, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.7480, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(75.5356, device='cuda:0')



h[100].sum tensor(171.6123, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-90.0377, device='cuda:0')



h[200].sum tensor(-570.7434, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.6144, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0219, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0217, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0217, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0208, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0208, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0208, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(306938.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0654],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0557],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0492],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0870, 0.0000, 0.1079],
        [0.0000, 0.0000, 0.0000,  ..., 0.0870, 0.0000, 0.1079],
        [0.0000, 0.0000, 0.0000,  ..., 0.0870, 0.0000, 0.1079]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3349285.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1149.7656, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6189.9946, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3939.8530, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2887.0840, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0087],
        [ 0.0433],
        [ 0.0554],
        ...,
        [-1.2514],
        [-1.2487],
        [-1.2480]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-435557.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3845],
        [0.4937],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(711.1062, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3845],
        [0.4937],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(711.1062, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009, -0.0040, -0.0166,  ...,  0.0059, -0.0068, -0.0062],
        [-0.0004, -0.0052, -0.0074,  ...,  0.0056, -0.0064, -0.0028],
        [-0.0005, -0.0049, -0.0096,  ...,  0.0057, -0.0065, -0.0036],
        ...,
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0053, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0053, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0053, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-26982.4688, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.9385, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(87.7174, device='cuda:0')



h[100].sum tensor(169.1444, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-104.5582, device='cuda:0')



h[200].sum tensor(-568.6775, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.5841, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0230, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0230, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0218, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0212, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0212, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0212, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(309531.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0035],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0116],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0393],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0935, 0.0000, 0.1083],
        [0.0000, 0.0000, 0.0000,  ..., 0.0935, 0.0000, 0.1083],
        [0.0000, 0.0000, 0.0000,  ..., 0.0935, 0.0000, 0.1083]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3362339.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1129.5259, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6284.1270, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3851.2002, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3021.6321, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6153],
        [-0.5328],
        [-0.4632],
        ...,
        [-1.3656],
        [-1.3623],
        [-1.3614]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-597030.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(700.8453, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(700.8453, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0062,  0.0000,  ...,  0.0053, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0053, -0.0060,  0.0000],
        [-0.0006, -0.0046, -0.0124,  ...,  0.0058, -0.0066, -0.0046],
        ...,
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0053, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0053, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0053, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-27346.7461, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.6874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(86.4517, device='cuda:0')



h[100].sum tensor(168.3741, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-103.0495, device='cuda:0')



h[200].sum tensor(-570.0505, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.1716, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0211, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0216, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0227, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0211, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0211, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0211, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(310259.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0512, 0.0000, 0.0924],
        [0.0000, 0.0000, 0.0000,  ..., 0.0234, 0.0000, 0.0582],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0250],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0968, 0.0000, 0.1075],
        [0.0000, 0.0000, 0.0000,  ..., 0.0968, 0.0000, 0.1075],
        [0.0000, 0.0000, 0.0000,  ..., 0.0968, 0.0000, 0.1075]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3385298., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1105.7417, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6297.4717, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3743.0701, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3073.8804, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3282],
        [-0.2604],
        [-0.2541],
        ...,
        [-1.4191],
        [-1.4155],
        [-1.4145]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-615082.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2603],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1010.0837, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2603],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1010.0837, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0062,  0.0000,  ...,  0.0051, -0.0060,  0.0000],
        [-0.0002, -0.0055, -0.0049,  ...,  0.0053, -0.0062, -0.0018],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0051, -0.0060,  0.0000],
        ...,
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0051, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0051, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0051, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-25555.0059, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.5631, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(124.5973, device='cuda:0')



h[100].sum tensor(192.7676, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-148.5187, device='cuda:0')



h[200].sum tensor(-561.0577, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.6019, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0217, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0206, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0217, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0205, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0205, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0205, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(336922.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0152],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0220],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0074],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0961, 0.0000, 0.1050],
        [0.0000, 0.0000, 0.0000,  ..., 0.0961, 0.0000, 0.1050],
        [0.0000, 0.0000, 0.0000,  ..., 0.0961, 0.0000, 0.1050]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3538755., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1064.8103, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6412.1562, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6368.7402, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3139.0112, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6225],
        [-0.7087],
        [-0.7960],
        ...,
        [-1.4027],
        [-1.3991],
        [-1.3981]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-446209.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(743.8275, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(743.8275, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0062,  0.0000,  ...,  0.0051, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0051, -0.0060,  0.0000],
        [-0.0003, -0.0054, -0.0058,  ...,  0.0053, -0.0063, -0.0021],
        ...,
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0051, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0051, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0051, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-27494.2812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.4247, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(91.7537, device='cuda:0')



h[100].sum tensor(169.8199, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-109.3695, device='cuda:0')



h[200].sum tensor(-570.7625, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.8993, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0204, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0206, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0209, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0204, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0204, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0204, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(313388.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0707, 0.0000, 0.0997],
        [0.0000, 0.0000, 0.0000,  ..., 0.0426, 0.0000, 0.0912],
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0000, 0.0786],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1020, 0.0000, 0.1081],
        [0.0000, 0.0000, 0.0000,  ..., 0.1020, 0.0000, 0.1081],
        [0.0000, 0.0000, 0.0000,  ..., 0.1020, 0.0000, 0.1081]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3409546.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1169.5763, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6460.2334, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3716.4429, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3146.9277, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4659],
        [-0.2595],
        [-0.1121],
        ...,
        [-1.4755],
        [-1.4715],
        [-1.4704]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-576174., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(718.2053, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(718.2053, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0062,  0.0000,  ...,  0.0051, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0051, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0051, -0.0060,  0.0000],
        ...,
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0051, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0051, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0051, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-27626.7422, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.3540, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(88.5931, device='cuda:0')



h[100].sum tensor(168.6093, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-105.6021, device='cuda:0')



h[200].sum tensor(-571.4707, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.8694, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0204, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0204, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0204, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0204, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0204, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0204, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(309057.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0997, 0.0000, 0.1081],
        [0.0000, 0.0000, 0.0000,  ..., 0.1014, 0.0000, 0.1085],
        [0.0000, 0.0000, 0.0000,  ..., 0.0996, 0.0000, 0.1080],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1020, 0.0000, 0.1081],
        [0.0000, 0.0000, 0.0000,  ..., 0.1020, 0.0000, 0.1081],
        [0.0000, 0.0000, 0.0000,  ..., 0.1020, 0.0000, 0.1081]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3368969.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1174.7535, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6431.6782, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2841.6348, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3134.5605, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1419],
        [-1.2906],
        [-1.3451],
        ...,
        [-1.4678],
        [-1.4646],
        [-1.4645]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-585954.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(787.2802, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(787.2802, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0062,  0.0000,  ...,  0.0051, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0051, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0051, -0.0060,  0.0000],
        ...,
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0051, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0051, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0051, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-27478.2148, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.3481, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(97.1137, device='cuda:0')



h[100].sum tensor(172.2732, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-115.7586, device='cuda:0')



h[200].sum tensor(-570.4806, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.6460, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(316794.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0897, 0.0000, 0.1017],
        [0.0000, 0.0000, 0.0000,  ..., 0.0917, 0.0000, 0.1026],
        [0.0000, 0.0000, 0.0000,  ..., 0.0880, 0.0000, 0.1020],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1047, 0.0000, 0.1073],
        [0.0000, 0.0000, 0.0000,  ..., 0.1047, 0.0000, 0.1073],
        [0.0000, 0.0000, 0.0000,  ..., 0.1047, 0.0000, 0.1073]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3421862.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1190.3274, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6474.0615, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3982.1445, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3231.8755, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5217],
        [-0.5500],
        [-0.5036],
        ...,
        [-1.4998],
        [-1.4957],
        [-1.4945]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-596816.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(639.0419, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(639.0419, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0062,  0.0000,  ...,  0.0050, -0.0060,  0.0000],
        [-0.0004, -0.0048, -0.0101,  ...,  0.0053, -0.0065, -0.0035],
        [-0.0003, -0.0050, -0.0088,  ...,  0.0053, -0.0064, -0.0031],
        ...,
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0050, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0050, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0050, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-28453.4492, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.7580, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(78.8280, device='cuda:0')



h[100].sum tensor(175.0530, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-93.9622, device='cuda:0')



h[200].sum tensor(-576.0977, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.6873, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0204, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0218, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(303697.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0328, 0.0000, 0.0690],
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0436],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0105],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1025, 0.0000, 0.1018],
        [0.0000, 0.0000, 0.0000,  ..., 0.1025, 0.0000, 0.1018],
        [0.0000, 0.0000, 0.0000,  ..., 0.1025, 0.0000, 0.1018]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3327025., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1194.9507, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6450.6455, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1785.9303, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3155.7532, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0393],
        [ 0.0711],
        [ 0.1127],
        ...,
        [-1.4603],
        [-1.4571],
        [-1.4563]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-434408.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 70.0 event: 1050 loss: tensor(479.4727, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(738.7397, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(738.7397, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0062,  0.0000,  ...,  0.0050, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0050, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0050, -0.0060,  0.0000],
        ...,
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0050, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0050, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0050, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-28065.6289, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.8568, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(91.1261, device='cuda:0')



h[100].sum tensor(180.2338, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-108.6214, device='cuda:0')



h[200].sum tensor(-573.7087, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.6948, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(312834.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1024, 0.0000, 0.1010],
        [0.0000, 0.0000, 0.0000,  ..., 0.1051, 0.0000, 0.1019],
        [0.0000, 0.0000, 0.0000,  ..., 0.1051, 0.0000, 0.1022],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1051, 0.0000, 0.1012],
        [0.0000, 0.0000, 0.0000,  ..., 0.1051, 0.0000, 0.1012],
        [0.0000, 0.0000, 0.0000,  ..., 0.1051, 0.0000, 0.1012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3401190., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1183.8257, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6544.9990, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2928.5312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3228.5947, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0849],
        [-1.3435],
        [-1.5046],
        ...,
        [-1.5104],
        [-1.5069],
        [-1.5059]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-440099.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5215],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1028.7395, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5215],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1028.7395, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008, -0.0028, -0.0247,  ...,  0.0061, -0.0073, -0.0084],
        [-0.0006, -0.0037, -0.0177,  ...,  0.0058, -0.0069, -0.0060],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0052, -0.0060,  0.0000],
        ...,
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0052, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0052, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0052, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-26449.6367, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.3748, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(126.8986, device='cuda:0')



h[100].sum tensor(179.5044, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-151.2618, device='cuda:0')



h[200].sum tensor(-564.7348, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(41.3518, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0243, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0224, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0216, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0207, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0207, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0207, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(339253.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0104],
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0412],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1113, 0.0000, 0.1089],
        [0.0000, 0.0000, 0.0000,  ..., 0.1113, 0.0000, 0.1089],
        [0.0000, 0.0000, 0.0000,  ..., 0.1113, 0.0000, 0.1089]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3585280.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1138.4229, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6742.1069, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6315.0957, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3428.7769, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3825],
        [-0.3028],
        [-0.3779],
        ...,
        [-1.6583],
        [-1.6536],
        [-1.6521]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-653677.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5762],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(708.4672, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5762],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(708.4672, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0002, -0.0055, -0.0052,  ...,  0.0054, -0.0063, -0.0017],
        [-0.0003, -0.0048, -0.0103,  ...,  0.0056, -0.0065, -0.0035],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0052, -0.0060,  0.0000],
        ...,
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0052, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0052, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0052, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-28469.4375, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.4809, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(87.3919, device='cuda:0')



h[100].sum tensor(158.2163, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-104.1702, device='cuda:0')



h[200].sum tensor(-575.9224, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.4780, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0217, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0212, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0222, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0207, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0207, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0207, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(308883.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0174],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0421],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0452],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1104, 0.0000, 0.1095],
        [0.0000, 0.0000, 0.0000,  ..., 0.1104, 0.0000, 0.1095],
        [0.0000, 0.0000, 0.0000,  ..., 0.1104, 0.0000, 0.1095]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3407801.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1120.9578, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6829.6709, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2804.7195, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3142.9990, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0234],
        [ 0.0096],
        [-0.0182],
        ...,
        [-1.7149],
        [-1.7101],
        [-1.7087]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-680937.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(756.4738, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(756.4738, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004, -0.0039, -0.0160,  ...,  0.0057, -0.0069, -0.0053],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0050, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0050, -0.0060,  0.0000],
        ...,
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0050, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0050, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0050, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-27883.1523, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.4245, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(93.3137, device='cuda:0')



h[100].sum tensor(168.1581, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-111.2289, device='cuda:0')



h[200].sum tensor(-575.0601, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.4077, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0224, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0216, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0208, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(315612.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0061, 0.0000,  ..., 0.1051, 0.0000, 0.1051],
        [0.0000, 0.0061, 0.0000,  ..., 0.1051, 0.0000, 0.1051],
        [0.0000, 0.0061, 0.0000,  ..., 0.1051, 0.0000, 0.1051]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3435440.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-974.5115, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6939.4712, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3298.4434, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3024.6157, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6474],
        [-0.5709],
        [-0.5138],
        ...,
        [-1.7026],
        [-1.6979],
        [-1.6964]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-552190.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6680],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(657.7744, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6680],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(657.7744, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.0383e-04, -5.0232e-03, -8.2183e-03,  ...,  5.2490e-03,
         -6.4439e-03, -2.6941e-03],
        [-3.7983e-04, -4.0191e-03, -1.5315e-02,  ...,  5.5134e-03,
         -6.8270e-03, -5.0205e-03],
        [-9.4638e-05, -5.6462e-03, -3.8157e-03,  ...,  5.0850e-03,
         -6.2063e-03, -1.2509e-03],
        ...,
        [ 0.0000e+00, -6.1862e-03,  0.0000e+00,  ...,  4.9428e-03,
         -6.0003e-03,  0.0000e+00],
        [ 0.0000e+00, -6.1862e-03,  0.0000e+00,  ...,  4.9428e-03,
         -6.0003e-03,  0.0000e+00],
        [ 0.0000e+00, -6.1862e-03,  0.0000e+00,  ...,  4.9428e-03,
         -6.0003e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-28364.5977, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.1098, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(81.1388, device='cuda:0')



h[100].sum tensor(168.2775, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-96.7165, device='cuda:0')



h[200].sum tensor(-578.9266, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.4403, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0221, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0210, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0210, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(313246., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0012, 0.0000,  ..., 0.0000, 0.0000, 0.0026],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0146],
        ...,
        [0.0000, 0.0038, 0.0000,  ..., 0.1018, 0.0000, 0.1013],
        [0.0000, 0.0038, 0.0000,  ..., 0.1018, 0.0000, 0.1013],
        [0.0000, 0.0038, 0.0000,  ..., 0.1018, 0.0000, 0.1013]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3421772.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-845.2235, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7019.2319, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2807.6042, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2856.7078, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2254],
        [ 0.2346],
        [ 0.2360],
        ...,
        [-1.7056],
        [-1.7011],
        [-1.6998]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-445406.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(736.8851, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(736.8851, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0062,  0.0000,  ...,  0.0051, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0051, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0051, -0.0060,  0.0000],
        ...,
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0051, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0051, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0051, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-28433.4609, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.1081, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(90.8973, device='cuda:0')



h[100].sum tensor(156.1347, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-108.3487, device='cuda:0')



h[200].sum tensor(-576.9996, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.6203, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0203, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0203, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0203, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0203, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0203, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0203, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(318438., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0987, 0.0000, 0.1060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0963, 0.0000, 0.1052],
        [0.0000, 0.0000, 0.0000,  ..., 0.0943, 0.0000, 0.1049],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1069, 0.0000, 0.1079],
        [0.0000, 0.0000, 0.0000,  ..., 0.1069, 0.0000, 0.1079],
        [0.0000, 0.0000, 0.0000,  ..., 0.1069, 0.0000, 0.1079]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3488274.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-840.4791, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7110.5479, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4783.4912, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3018.1704, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3445],
        [-1.2343],
        [-1.1366],
        ...,
        [-1.8451],
        [-1.8401],
        [-1.8387]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-701232.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6602],
        [0.2854],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(840.4587, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6602],
        [0.2854],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(840.4587, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.7466e-04, -4.9355e-03, -8.6718e-03,  ...,  5.3361e-03,
         -6.4798e-03, -2.7678e-03],
        [-3.0329e-04, -4.0131e-03, -1.5058e-02,  ...,  5.5684e-03,
         -6.8322e-03, -4.8061e-03],
        [-9.7959e-05, -5.4855e-03, -4.8637e-03,  ...,  5.1975e-03,
         -6.2696e-03, -1.5523e-03],
        ...,
        [ 0.0000e+00, -6.1880e-03,  0.0000e+00,  ...,  5.0206e-03,
         -6.0013e-03,  0.0000e+00],
        [ 0.0000e+00, -6.1880e-03,  0.0000e+00,  ...,  5.0206e-03,
         -6.0013e-03,  0.0000e+00],
        [ 0.0000e+00, -6.1880e-03,  0.0000e+00,  ...,  5.0206e-03,
         -6.0013e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-28041.6367, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.1308, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(103.6735, device='cuda:0')



h[100].sum tensor(157.9819, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-123.5777, device='cuda:0')



h[200].sum tensor(-574.0115, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.7836, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0226, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0218, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0210, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(323983.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0297],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1079, 0.0000, 0.1113],
        [0.0000, 0.0000, 0.0000,  ..., 0.1079, 0.0000, 0.1113],
        [0.0000, 0.0000, 0.0000,  ..., 0.1079, 0.0000, 0.1113]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3532580., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-860.3677, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7156.7065, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5626.2617, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3130.9092, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0099],
        [-0.0104],
        [-0.0672],
        ...,
        [-1.8994],
        [-1.8943],
        [-1.8929]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-662052.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(619.7634, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(619.7634, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0062,  0.0000,  ...,  0.0049, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0049, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0049, -0.0060,  0.0000],
        ...,
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0049, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0049, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0049, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-29668.0625, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.7336, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(76.4500, device='cuda:0')



h[100].sum tensor(150.6814, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-91.1276, device='cuda:0')



h[200].sum tensor(-581.6915, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.9124, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(303475.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1064, 0.0000, 0.1120],
        [0.0000, 0.0000, 0.0000,  ..., 0.1064, 0.0000, 0.1120],
        [0.0000, 0.0000, 0.0000,  ..., 0.1064, 0.0000, 0.1124],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1065, 0.0000, 0.1113],
        [0.0000, 0.0000, 0.0000,  ..., 0.1065, 0.0000, 0.1113],
        [0.0000, 0.0000, 0.0000,  ..., 0.1065, 0.0000, 0.1113]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3405332., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-959.2754, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6978.3330, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2824.7290, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3159.0845, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0781],
        [-2.0846],
        [-2.0799],
        ...,
        [-1.8897],
        [-1.8850],
        [-1.8837]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-574088.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(706.6313, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(706.6313, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.9164e-05, -5.6406e-03, -3.7219e-03,  ...,  4.7957e-03,
         -6.2120e-03, -1.1559e-03],
        [-5.9164e-05, -5.6406e-03, -3.7219e-03,  ...,  4.7957e-03,
         -6.2120e-03, -1.1559e-03],
        [ 0.0000e+00, -6.1895e-03,  0.0000e+00,  ...,  4.6693e-03,
         -6.0020e-03,  0.0000e+00],
        ...,
        [ 0.0000e+00, -6.1895e-03,  0.0000e+00,  ...,  4.6693e-03,
         -6.0020e-03,  0.0000e+00],
        [ 0.0000e+00, -6.1895e-03,  0.0000e+00,  ...,  4.6693e-03,
         -6.0020e-03,  0.0000e+00],
        [ 0.0000e+00, -6.1895e-03,  0.0000e+00,  ...,  4.6693e-03,
         -6.0020e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-29247.5664, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.7300, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(87.1654, device='cuda:0')



h[100].sum tensor(164.1148, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-103.9003, device='cuda:0')



h[200].sum tensor(-579.4189, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.4042, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(317197.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0000, 0.0633],
        [0.0000, 0.0000, 0.0000,  ..., 0.0128, 0.0000, 0.0694],
        [0.0000, 0.0000, 0.0000,  ..., 0.0323, 0.0000, 0.0806],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1046, 0.0000, 0.1085],
        [0.0000, 0.0000, 0.0000,  ..., 0.1046, 0.0000, 0.1085],
        [0.0000, 0.0000, 0.0000,  ..., 0.1046, 0.0000, 0.1085]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3503677., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-954.3831, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6936.2368, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4224.8906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3206.7029, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0699],
        [-0.1350],
        [-0.3737],
        ...,
        [-1.8607],
        [-1.8565],
        [-1.8554]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-384108.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1078.0719, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1078.0719, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0062,  0.0000,  ...,  0.0046, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0046, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0046, -0.0060,  0.0000],
        ...,
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0046, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0046, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0046, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-26969.2871, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.9770, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(132.9839, device='cuda:0')



h[100].sum tensor(180.0630, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-158.5154, device='cuda:0')



h[200].sum tensor(-567.4515, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(43.3348, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0185, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0185, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0185, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0185, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0185, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0185, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(354767.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0918, 0.0000, 0.1068],
        [0.0000, 0.0000, 0.0000,  ..., 0.1051, 0.0000, 0.1114],
        [0.0000, 0.0000, 0.0000,  ..., 0.0977, 0.0000, 0.1093],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1080, 0.0000, 0.1115],
        [0.0000, 0.0000, 0.0000,  ..., 0.1080, 0.0000, 0.1115],
        [0.0000, 0.0000, 0.0000,  ..., 0.1080, 0.0000, 0.1115]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3749242., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-854.0601, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7132.4468, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8889.7129, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3500.5049, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6945],
        [-1.0525],
        [-1.2146],
        ...,
        [-1.9239],
        [-1.9194],
        [-1.9182]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-433227.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 80.0 event: 1200 loss: tensor(520.0197, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4673],
        [0.4421],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(737.7015, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4673],
        [0.4421],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(737.7015, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.1865e-05, -5.0404e-03, -7.6189e-03,  ...,  4.9311e-03,
         -6.4422e-03, -2.3009e-03],
        [-1.6259e-04, -4.1554e-03, -1.3485e-02,  ...,  5.1198e-03,
         -6.7807e-03, -4.0725e-03],
        [-2.9355e-04, -2.5169e-03, -2.4346e-02,  ...,  5.4693e-03,
         -7.4073e-03, -7.3525e-03],
        ...,
        [ 0.0000e+00, -6.1898e-03,  0.0000e+00,  ...,  4.6859e-03,
         -6.0027e-03,  0.0000e+00],
        [ 0.0000e+00, -6.1898e-03,  0.0000e+00,  ...,  4.6859e-03,
         -6.0027e-03,  0.0000e+00],
        [ 0.0000e+00, -6.1898e-03,  0.0000e+00,  ...,  4.6859e-03,
         -6.0027e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-29737.6387, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.5653, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(90.9980, device='cuda:0')



h[100].sum tensor(148.1482, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-108.4687, device='cuda:0')



h[200].sum tensor(-579.0525, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.6531, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0206, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0210, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(316555.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0300],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1141, 0.0000, 0.1172],
        [0.0000, 0.0000, 0.0000,  ..., 0.1141, 0.0000, 0.1172],
        [0.0000, 0.0000, 0.0000,  ..., 0.1141, 0.0000, 0.1172]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3507442.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-900.5862, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7289.5918, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4290.5845, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3508.5220, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0914],
        [ 0.1353],
        [ 0.1370],
        ...,
        [-2.0447],
        [-2.0397],
        [-2.0383]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-614131.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(974.2247, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(974.2247, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0062,  0.0000,  ...,  0.0047, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0047, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0047, -0.0060,  0.0000],
        ...,
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0047, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0047, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0047, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-28437.3281, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.6329, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(120.1740, device='cuda:0')



h[100].sum tensor(156.4876, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-143.2462, device='cuda:0')



h[200].sum tensor(-571.6141, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.1605, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(335629.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1171, 0.0000, 0.1199],
        [0.0000, 0.0000, 0.0000,  ..., 0.1171, 0.0000, 0.1199],
        [0.0000, 0.0000, 0.0000,  ..., 0.1171, 0.0000, 0.1203],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1171, 0.0000, 0.1191],
        [0.0000, 0.0000, 0.0000,  ..., 0.1171, 0.0000, 0.1191],
        [0.0000, 0.0000, 0.0000,  ..., 0.1094, 0.0000, 0.1167]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3624579.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-837.0627, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7377.8501, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6290.3032, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3631.7905, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9569],
        [-2.1645],
        [-2.3010],
        ...,
        [-2.0717],
        [-1.9819],
        [-1.8133]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-652964.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(691.6102, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(691.6102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0062,  0.0000,  ...,  0.0046, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0046, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0046, -0.0060,  0.0000],
        ...,
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0046, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0046, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0046, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-30469.8984, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.3715, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(85.3125, device='cuda:0')



h[100].sum tensor(142.2735, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-101.6916, device='cuda:0')



h[200].sum tensor(-580.5110, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.8004, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0182, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0182, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0182, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0182, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0182, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0182, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(310131.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1020, 0.0000, 0.1137],
        [0.0000, 0.0000, 0.0000,  ..., 0.1135, 0.0000, 0.1177],
        [0.0000, 0.0000, 0.0000,  ..., 0.1154, 0.0000, 0.1188],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1155, 0.0000, 0.1176],
        [0.0000, 0.0000, 0.0000,  ..., 0.1155, 0.0000, 0.1176],
        [0.0000, 0.0000, 0.0000,  ..., 0.1155, 0.0000, 0.1176]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3462761., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-914.3354, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7328.5112, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3298.9495, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3356.7222, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1651],
        [-1.5668],
        [-1.8569],
        ...,
        [-2.1253],
        [-2.1201],
        [-2.1186]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-592839.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(688.0817, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(688.0817, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -6.1894e-03,  0.0000e+00,  ...,  4.4105e-03,
         -6.0034e-03,  0.0000e+00],
        [-5.2696e-05, -5.0183e-03, -7.4937e-03,  ...,  4.6260e-03,
         -6.4506e-03, -2.1677e-03],
        [-1.0077e-04, -3.9499e-03, -1.4330e-02,  ...,  4.8227e-03,
         -6.8587e-03, -4.1452e-03],
        ...,
        [ 0.0000e+00, -6.1894e-03,  0.0000e+00,  ...,  4.4105e-03,
         -6.0034e-03,  0.0000e+00],
        [ 0.0000e+00, -6.1894e-03,  0.0000e+00,  ...,  4.4105e-03,
         -6.0034e-03,  0.0000e+00],
        [ 0.0000e+00, -6.1894e-03,  0.0000e+00,  ...,  4.4105e-03,
         -6.0034e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-30458.4922, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.2979, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(84.8773, device='cuda:0')



h[100].sum tensor(147.1092, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-101.1728, device='cuda:0')



h[200].sum tensor(-580.2727, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.6586, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0179, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0184, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0176, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0176, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0176, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(312381.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0379, 0.0000, 0.0660],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0300],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1120, 0.0000, 0.1138],
        [0.0000, 0.0000, 0.0000,  ..., 0.1120, 0.0000, 0.1138],
        [0.0000, 0.0000, 0.0000,  ..., 0.1120, 0.0000, 0.1138]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3468739.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-941.5857, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7280.0229, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3396.8030, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3135.2866, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4651],
        [-0.0978],
        [ 0.0773],
        ...,
        [-2.0075],
        [-1.9166],
        [-1.8557]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-476281.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2520],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(705.7184, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2520],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(705.7184, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.1348e-05, -5.5805e-03, -3.8498e-03,  ...,  4.4708e-03,
         -6.2360e-03, -1.0974e-03],
        [-2.1777e-05, -5.5683e-03, -3.9273e-03,  ...,  4.4729e-03,
         -6.2407e-03, -1.1195e-03],
        [ 0.0000e+00, -6.1893e-03,  0.0000e+00,  ...,  4.3667e-03,
         -6.0036e-03,  0.0000e+00],
        ...,
        [ 0.0000e+00, -6.1893e-03,  0.0000e+00,  ...,  4.3667e-03,
         -6.0036e-03,  0.0000e+00],
        [ 0.0000e+00, -6.1893e-03,  0.0000e+00,  ...,  4.3667e-03,
         -6.0036e-03,  0.0000e+00],
        [ 0.0000e+00, -6.1893e-03,  0.0000e+00,  ...,  4.3667e-03,
         -6.0036e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-30571.8848, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.2393, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(87.0528, device='cuda:0')



h[100].sum tensor(140.1182, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-103.7660, device='cuda:0')



h[200].sum tensor(-579.1213, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.3675, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0182, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0178, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0176, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0175, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0175, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0175, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(314067.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0412],
        [0.0000, 0.0000, 0.0000,  ..., 0.0094, 0.0000, 0.0619],
        [0.0000, 0.0000, 0.0000,  ..., 0.0339, 0.0000, 0.0858],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1136, 0.0000, 0.1140],
        [0.0000, 0.0000, 0.0000,  ..., 0.1136, 0.0000, 0.1140],
        [0.0000, 0.0000, 0.0000,  ..., 0.1136, 0.0000, 0.1140]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3498372.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-861.2798, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7277.5093, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4444.0454, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3227.8752, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0452],
        [ 0.0146],
        [-0.0466],
        ...,
        [-2.1851],
        [-2.1815],
        [-2.1807]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-588182.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(682.1505, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(682.1505, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0062,  0.0000,  ...,  0.0043, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0043, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0043, -0.0060,  0.0000],
        ...,
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0043, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0043, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0043, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-31131.2344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.1700, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(84.1456, device='cuda:0')



h[100].sum tensor(130.8916, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-100.3007, device='cuda:0')



h[200].sum tensor(-579.5491, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.4201, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0175, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0174, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0174, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0174, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0174, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0174, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(308944.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0474, 0.0000, 0.0832],
        [0.0000, 0.0000, 0.0000,  ..., 0.0815, 0.0000, 0.1038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0921, 0.0000, 0.1076],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1158, 0.0000, 0.1144],
        [0.0000, 0.0000, 0.0000,  ..., 0.1158, 0.0000, 0.1144],
        [0.0000, 0.0000, 0.0000,  ..., 0.1158, 0.0000, 0.1144]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3472379.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-805.3158, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7228.7163, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4151.8740, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3304.0435, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5615],
        [-0.9018],
        [-1.0708],
        ...,
        [-2.2708],
        [-2.2658],
        [-2.2644]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-722568.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2754],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(839.8027, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2754],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(839.8027, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.7685e-06, -5.6325e-03, -3.4381e-03,  ...,  4.3761e-03,
         -6.2163e-03, -9.5128e-04],
        [-1.1922e-05, -5.5098e-03, -4.1960e-03,  ...,  4.3950e-03,
         -6.2631e-03, -1.1610e-03],
        [ 0.0000e+00, -6.1891e-03,  0.0000e+00,  ...,  4.2907e-03,
         -6.0039e-03,  0.0000e+00],
        ...,
        [ 0.0000e+00, -6.1891e-03,  0.0000e+00,  ...,  4.2907e-03,
         -6.0039e-03,  0.0000e+00],
        [ 0.0000e+00, -6.1891e-03,  0.0000e+00,  ...,  4.2907e-03,
         -6.0039e-03,  0.0000e+00],
        [ 0.0000e+00, -6.1891e-03,  0.0000e+00,  ...,  4.2907e-03,
         -6.0039e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-30201.0918, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.1426, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(103.5926, device='cuda:0')



h[100].sum tensor(145.4960, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-123.4813, device='cuda:0')



h[200].sum tensor(-573.9384, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.7572, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0178, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0175, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0174, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0172, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0172, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0172, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(330990.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0365],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0573],
        [0.0000, 0.0000, 0.0000,  ..., 0.0089, 0.0000, 0.0729],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1148, 0.0000, 0.1136],
        [0.0000, 0.0000, 0.0000,  ..., 0.1148, 0.0000, 0.1136],
        [0.0000, 0.0000, 0.0000,  ..., 0.1148, 0.0000, 0.1136]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3649152., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-767.7139, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7153.8711, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7466.5068, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3358.2866, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0362],
        [ 0.0085],
        [-0.0576],
        ...,
        [-2.2845],
        [-2.2796],
        [-2.2783]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-656942.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(795.9210, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(795.9210, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0062,  0.0000,  ...,  0.0042, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0042, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0042, -0.0060,  0.0000],
        ...,
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0042, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0042, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0042, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-30702.5723, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.0760, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(98.1796, device='cuda:0')



h[100].sum tensor(147.9576, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-117.0291, device='cuda:0')



h[200].sum tensor(-575.1968, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.9933, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0169, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0169, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0169, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0169, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0169, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0169, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(322057.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1139, 0.0000, 0.1136],
        [0.0000, 0.0000, 0.0000,  ..., 0.1139, 0.0000, 0.1136],
        [0.0000, 0.0000, 0.0000,  ..., 0.1138, 0.0000, 0.1140],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1140, 0.0000, 0.1129],
        [0.0000, 0.0000, 0.0000,  ..., 0.1140, 0.0000, 0.1129],
        [0.0000, 0.0000, 0.0000,  ..., 0.1140, 0.0000, 0.1129]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3582892.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-802.5663, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7099.5112, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5883.1348, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3255.0422, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.3792],
        [-2.4373],
        [-2.4584],
        ...,
        [-2.2984],
        [-2.2936],
        [-2.2924]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-542601.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(792.6016, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(792.6016, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0062,  0.0000,  ...,  0.0042, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0042, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0042, -0.0060,  0.0000],
        ...,
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0042, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0042, -0.0060,  0.0000],
        [ 0.0000, -0.0062,  0.0000,  ...,  0.0042, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-31161.1094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.0222, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(97.7702, device='cuda:0')



h[100].sum tensor(129.6744, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-116.5410, device='cuda:0')



h[200].sum tensor(-575.3443, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.8599, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0168, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0168, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0168, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0168, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0168, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0168, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(316170.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1189, 0.0000, 0.1154],
        [0.0000, 0.0000, 0.0000,  ..., 0.1189, 0.0000, 0.1154],
        [0.0000, 0.0000, 0.0000,  ..., 0.1188, 0.0000, 0.1158],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1189, 0.0000, 0.1146],
        [0.0000, 0.0000, 0.0000,  ..., 0.1189, 0.0000, 0.1146],
        [0.0000, 0.0000, 0.0000,  ..., 0.1189, 0.0000, 0.1146]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3550203., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-701.0253, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7165.4961, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6122.7959, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3404.1924, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.5007],
        [-2.3755],
        [-2.1745],
        ...,
        [-2.4077],
        [-2.4019],
        [-2.3999]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-807677.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4746],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(642.5922, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4746],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(642.5922, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.8483e-06, -4.1815e-03, -1.1968e-02,  ...,  4.3142e-03,
         -6.7696e-03, -3.1628e-03],
        [ 1.0742e-05, -3.0403e-03, -1.8772e-02,  ...,  4.4581e-03,
         -7.2046e-03, -4.9609e-03],
        [ 1.2208e-05, -2.6103e-03, -2.1336e-02,  ...,  4.5122e-03,
         -7.3686e-03, -5.6384e-03],
        ...,
        [ 0.0000e+00, -6.1890e-03,  0.0000e+00,  ...,  4.0612e-03,
         -6.0043e-03,  0.0000e+00],
        [ 0.0000e+00, -6.1890e-03,  0.0000e+00,  ...,  4.0612e-03,
         -6.0043e-03,  0.0000e+00],
        [ 0.0000e+00, -6.1890e-03,  0.0000e+00,  ...,  4.0612e-03,
         -6.0043e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-32256.6152, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0.0212, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(79.2660, device='cuda:0')



h[100].sum tensor(125.1130, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-94.4842, device='cuda:0')



h[200].sum tensor(-580.1457, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.8300, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[3.2658e-05, 0.0000e+00, 0.0000e+00,  ..., 1.7451e-02, 0.0000e+00,
         0.0000e+00],
        [3.3767e-05, 0.0000e+00, 0.0000e+00,  ..., 1.7492e-02, 0.0000e+00,
         0.0000e+00],
        [3.8139e-05, 0.0000e+00, 0.0000e+00,  ..., 1.7654e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.6245e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.6245e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.6245e-02, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(305786.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1178, 0.0000, 0.1142],
        [0.0000, 0.0000, 0.0000,  ..., 0.1178, 0.0000, 0.1142],
        [0.0000, 0.0000, 0.0000,  ..., 0.1178, 0.0000, 0.1142]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3488986., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-764.6306, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7130.5723, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4490.3120, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3287.6167, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0169],
        [ 0.0359],
        [ 0.0284],
        ...,
        [-2.4230],
        [-2.4175],
        [-2.4160]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-731100.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 90.0 event: 1350 loss: tensor(512.1675, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(868.9813, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(868.9813, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009, -0.0062,  0.0000,  ...,  0.0039, -0.0060,  0.0000],
        [-0.0009, -0.0062,  0.0000,  ...,  0.0039, -0.0060,  0.0000],
        [-0.0009, -0.0062,  0.0000,  ...,  0.0039, -0.0060,  0.0000],
        ...,
        [-0.0009, -0.0062,  0.0000,  ...,  0.0039, -0.0060,  0.0000],
        [-0.0009, -0.0062,  0.0000,  ...,  0.0039, -0.0060,  0.0000],
        [-0.0009, -0.0062,  0.0000,  ...,  0.0039, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-30751.5156, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-93.8262, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(107.1919, device='cuda:0')



h[100].sum tensor(148.1815, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-127.7716, device='cuda:0')



h[200].sum tensor(-573.2659, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.9301, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(325882.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1141, 0.0000, 0.1133],
        [0.0000, 0.0000, 0.0000,  ..., 0.1127, 0.0000, 0.1128],
        [0.0000, 0.0000, 0.0000,  ..., 0.1042, 0.0000, 0.1106],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1158, 0.0000, 0.1132],
        [0.0000, 0.0000, 0.0000,  ..., 0.1158, 0.0000, 0.1132],
        [0.0000, 0.0000, 0.0000,  ..., 0.1158, 0.0000, 0.1132]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3619916., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-795.9288, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7211.3154, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6470.0459, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3227.1265, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9990],
        [-1.7925],
        [-1.5207],
        ...,
        [-2.4040],
        [-2.3991],
        [-2.3978]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-482385., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3420],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(850.4086, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3420],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(850.4086, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0021, -0.0039, -0.0135,  ...,  0.0041, -0.0069, -0.0035],
        [-0.0020, -0.0047, -0.0089,  ...,  0.0040, -0.0066, -0.0023],
        [-0.0020, -0.0044, -0.0104,  ...,  0.0040, -0.0067, -0.0027],
        ...,
        [-0.0017, -0.0062,  0.0000,  ...,  0.0039, -0.0060,  0.0000],
        [-0.0017, -0.0062,  0.0000,  ...,  0.0039, -0.0060,  0.0000],
        [-0.0017, -0.0062,  0.0000,  ...,  0.0039, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-31145.5684, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-178.7651, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(104.9008, device='cuda:0')



h[100].sum tensor(135.2560, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-125.0407, device='cuda:0')



h[200].sum tensor(-574.2003, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.1835, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0161, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0162, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0161, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0154, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0154, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0154, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(327328.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1202, 0.0000, 0.1143],
        [0.0000, 0.0000, 0.0000,  ..., 0.1202, 0.0000, 0.1143],
        [0.0000, 0.0000, 0.0000,  ..., 0.1202, 0.0000, 0.1143]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3623160.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-674.9049, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7290.6367, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6534.5059, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3374.4412, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2749],
        [ 0.2824],
        [ 0.2921],
        ...,
        [-2.4829],
        [-2.4770],
        [-2.4753]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-622523., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3867],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(666.9403, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3867],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(666.9403, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0027, -0.0055, -0.0042,  ...,  0.0039, -0.0063, -0.0011],
        [-0.0029, -0.0045, -0.0097,  ...,  0.0040, -0.0066, -0.0024],
        [-0.0025, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        ...,
        [-0.0025, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        [-0.0025, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        [-0.0025, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-32882.9648, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-255.2509, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(82.2694, device='cuda:0')



h[100].sum tensor(116.4527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-98.0643, device='cuda:0')



h[200].sum tensor(-580.0645, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.8087, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0161, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0158, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0153, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0153, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0153, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(308102.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0084],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0273],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1250, 0.0000, 0.1154],
        [0.0000, 0.0000, 0.0000,  ..., 0.1250, 0.0000, 0.1154],
        [0.0000, 0.0000, 0.0000,  ..., 0.1250, 0.0000, 0.1154]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3512665.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-808.5699, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7286.9839, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4330.0913, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3307.7161, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2425],
        [ 0.1698],
        [-0.0492],
        ...,
        [-2.5189],
        [-2.5303],
        [-2.5381]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-760869.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(671.6265, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(671.6265, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0032, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        [-0.0032, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        [-0.0032, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        ...,
        [-0.0032, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        [-0.0032, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        [-0.0032, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-33133.4609, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-324.7877, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(82.8475, device='cuda:0')



h[100].sum tensor(120.0623, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-98.7533, device='cuda:0')



h[200].sum tensor(-579.6898, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.9971, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(308619.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0480, 0.0000, 0.0900],
        [0.0000, 0.0000, 0.0000,  ..., 0.0375, 0.0000, 0.0866],
        [0.0000, 0.0000, 0.0000,  ..., 0.0379, 0.0000, 0.0871],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1255, 0.0000, 0.1150],
        [0.0000, 0.0000, 0.0000,  ..., 0.1255, 0.0000, 0.1150],
        [0.0000, 0.0000, 0.0000,  ..., 0.1255, 0.0000, 0.1150]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3520791., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-876.9524, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7249.3525, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4052.2446, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3291.7375, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1524],
        [ 0.0367],
        [ 0.1421],
        ...,
        [-2.5905],
        [-2.5844],
        [-2.5827]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-733001.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5649],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(814.8634, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5649],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(814.8634, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0042, -0.0052, -0.0058,  ...,  0.0038, -0.0064, -0.0014],
        [-0.0043, -0.0048, -0.0078,  ...,  0.0039, -0.0065, -0.0019],
        [-0.0043, -0.0048, -0.0077,  ...,  0.0039, -0.0065, -0.0019],
        ...,
        [-0.0038, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0038, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0038, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-32281.8535, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-388.2280, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(100.5162, device='cuda:0')



h[100].sum tensor(136.7179, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-119.8143, device='cuda:0')



h[200].sum tensor(-574.4953, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.7547, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0153, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(322369.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0353],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0140],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1233, 0.0000, 0.1137],
        [0.0000, 0.0000, 0.0000,  ..., 0.1233, 0.0000, 0.1137],
        [0.0000, 0.0000, 0.0000,  ..., 0.1233, 0.0000, 0.1137]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3610696.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-884.7247, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7219.9097, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5256.9951, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3325.6150, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2839],
        [ 0.3529],
        [ 0.3672],
        ...,
        [-2.5986],
        [-2.5927],
        [-2.5912]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-642904.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3130],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(660.7474, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3130],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(660.7474, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0049, -0.0048, -0.0074,  ...,  0.0038, -0.0065, -0.0018],
        [-0.0047, -0.0054, -0.0043,  ...,  0.0037, -0.0063, -0.0010],
        [-0.0043, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        ...,
        [-0.0043, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0043, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0043, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-33685.9062, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-444.6072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(81.5055, device='cuda:0')



h[100].sum tensor(127.1652, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-97.1537, device='cuda:0')



h[200].sum tensor(-577.7285, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.5598, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(308428.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0307],
        [0.0000, 0.0000, 0.0000,  ..., 0.0137, 0.0000, 0.0549],
        [0.0000, 0.0000, 0.0000,  ..., 0.0452, 0.0000, 0.0852],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1166, 0.0000, 0.1120],
        [0.0000, 0.0000, 0.0000,  ..., 0.1065, 0.0000, 0.1089],
        [0.0000, 0.0000, 0.0000,  ..., 0.1014, 0.0000, 0.1074]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3537750.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-923.7999, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7231.5244, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4051.2710, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3241.8574, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0343],
        [-0.1612],
        [-0.4990],
        ...,
        [-2.4519],
        [-2.3113],
        [-2.2189]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-612627.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(634.9830, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(634.9830, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0048, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0048, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0053, -0.0052, -0.0053,  ...,  0.0037, -0.0064, -0.0012],
        ...,
        [-0.0048, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0048, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0048, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-34178.5703, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-496.0441, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(78.3274, device='cuda:0')



h[100].sum tensor(122.8707, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-93.3654, device='cuda:0')



h[200].sum tensor(-576.8684, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.5242, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(307157.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0660, 0.0000, 0.0961],
        [0.0000, 0.0000, 0.0000,  ..., 0.0422, 0.0000, 0.0709],
        [0.0000, 0.0000, 0.0000,  ..., 0.0081, 0.0000, 0.0363],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1222, 0.0000, 0.1141],
        [0.0000, 0.0000, 0.0000,  ..., 0.1222, 0.0000, 0.1141],
        [0.0000, 0.0000, 0.0000,  ..., 0.1222, 0.0000, 0.1141]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3553693.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-927.3181, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7231.3062, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4715.1484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3254.7900, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4824],
        [-0.3258],
        [-0.1386],
        ...,
        [-2.6869],
        [-2.6809],
        [-2.6793]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-713072.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3762],
        [0.4150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(703.0192, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3762],
        [0.4150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(703.0192, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0064, -0.0041, -0.0112,  ...,  0.0038, -0.0068, -0.0026],
        [-0.0058, -0.0053, -0.0050,  ...,  0.0037, -0.0064, -0.0012],
        [-0.0058, -0.0052, -0.0056,  ...,  0.0037, -0.0064, -0.0013],
        ...,
        [-0.0053, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0053, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0053, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-34019.8594, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-543.0178, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(86.7199, device='cuda:0')



h[100].sum tensor(129.1338, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-103.3692, device='cuda:0')



h[200].sum tensor(-573.4267, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.2590, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(311321.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0121],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0158],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0308],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1228, 0.0000, 0.1140],
        [0.0000, 0.0000, 0.0000,  ..., 0.1228, 0.0000, 0.1140],
        [0.0000, 0.0000, 0.0000,  ..., 0.1228, 0.0000, 0.1140]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3578743.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-935.1986, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7207.6909, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5008.0684, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3268.3423, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1341],
        [ 0.0985],
        [-0.0314],
        ...,
        [-2.7324],
        [-2.7258],
        [-2.7231]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-740010.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(860.3190, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(860.3190, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0057, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0057, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0057, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        ...,
        [-0.0057, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0057, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0057, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-33177.3672, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-586.1350, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(106.1233, device='cuda:0')



h[100].sum tensor(145.7606, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-126.4979, device='cuda:0')



h[200].sum tensor(-567.0851, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.5819, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(327166.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0936, 0.0000, 0.1041],
        [0.0000, 0.0000, 0.0000,  ..., 0.0665, 0.0000, 0.0942],
        [0.0000, 0.0000, 0.0000,  ..., 0.0280, 0.0000, 0.0578],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1223, 0.0000, 0.1128],
        [0.0000, 0.0000, 0.0000,  ..., 0.1223, 0.0000, 0.1128],
        [0.0000, 0.0000, 0.0000,  ..., 0.1223, 0.0000, 0.1128]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3699008., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-928.1770, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7215.6841, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6930.8779, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3275.9893, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3822],
        [-1.1458],
        [-0.6523],
        ...,
        [-2.7599],
        [-2.7538],
        [-2.7522]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-632175.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(768.4202, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(768.4202, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0061, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0068, -0.0050, -0.0063,  ...,  0.0038, -0.0065, -0.0014],
        [-0.0061, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        ...,
        [-0.0061, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0061, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0061, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-34319.6250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-623.8647, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(94.7873, device='cuda:0')



h[100].sum tensor(136.4465, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-112.9855, device='cuda:0')



h[200].sum tensor(-569.1889, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.8879, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(316549.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0484],
        [0.0000, 0.0000, 0.0000,  ..., 0.0206, 0.0000, 0.0747],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0606],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1243, 0.0000, 0.1133],
        [0.0000, 0.0000, 0.0000,  ..., 0.1243, 0.0000, 0.1133],
        [0.0000, 0.0000, 0.0000,  ..., 0.1243, 0.0000, 0.1133]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3623394.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-951.1986, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7177.8555, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5580.5381, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3259.0171, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0196],
        [-0.0061],
        [ 0.0129],
        ...,
        [-2.8284],
        [-2.8221],
        [-2.8204]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-794659.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 100.0 event: 1500 loss: tensor(499.5040, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(761.2621, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(761.2621, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0075, -0.0045, -0.0091,  ...,  0.0038, -0.0067, -0.0020],
        [-0.0076, -0.0044, -0.0095,  ...,  0.0038, -0.0067, -0.0021],
        [-0.0080, -0.0037, -0.0130,  ...,  0.0038, -0.0070, -0.0029],
        ...,
        [-0.0064, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0064, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0064, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-34621.9844, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-658.4434, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(93.9043, device='cuda:0')



h[100].sum tensor(137.2108, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-111.9330, device='cuda:0')



h[200].sum tensor(-568.7501, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.6002, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(315835.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0077],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1240, 0.0000, 0.1126],
        [0.0000, 0.0000, 0.0000,  ..., 0.1240, 0.0000, 0.1126],
        [0.0000, 0.0000, 0.0000,  ..., 0.1240, 0.0000, 0.1126]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3626514.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-961.8800, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7226.1309, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5592.9067, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3229.5273, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0871],
        [-0.0217],
        [ 0.0355],
        ...,
        [-2.8625],
        [-2.8561],
        [-2.8544]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-753966.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.2448],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(793.1372, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.2448],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(793.1372, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0077, -0.0047, -0.0076,  ...,  0.0037, -0.0066, -0.0016],
        [-0.0076, -0.0049, -0.0068,  ...,  0.0037, -0.0065, -0.0015],
        [-0.0076, -0.0049, -0.0068,  ...,  0.0037, -0.0065, -0.0015],
        ...,
        [-0.0067, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0067, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0067, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-34526.1367, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-689.9695, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(97.8362, device='cuda:0')



h[100].sum tensor(142.9433, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-116.6198, device='cuda:0')



h[200].sum tensor(-567.3389, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.8814, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(323809.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0039],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0156],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0259],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1222, 0.0000, 0.1110],
        [0.0000, 0.0000, 0.0000,  ..., 0.1222, 0.0000, 0.1110],
        [0.0000, 0.0000, 0.0000,  ..., 0.1222, 0.0000, 0.1110]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3688225., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-957.3840, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7237.8320, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6844.6084, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3233.5957, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0909],
        [-0.1248],
        [-0.2329],
        ...,
        [-2.8748],
        [-2.8684],
        [-2.8667]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-722821.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(719.9349, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(719.9349, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0070, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0070, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0070, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        ...,
        [-0.0070, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0070, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0070, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-35400.1367, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-717.8050, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(88.8065, device='cuda:0')



h[100].sum tensor(137.5655, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-105.8564, device='cuda:0')



h[200].sum tensor(-569.7972, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.9389, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(315263.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1194, 0.0000, 0.1103],
        [0.0000, 0.0000, 0.0000,  ..., 0.1038, 0.0000, 0.1053],
        [0.0000, 0.0000, 0.0000,  ..., 0.0656, 0.0000, 0.0931],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1220, 0.0000, 0.1104],
        [0.0000, 0.0000, 0.0000,  ..., 0.1220, 0.0000, 0.1104],
        [0.0000, 0.0000, 0.0000,  ..., 0.1220, 0.0000, 0.1104]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3634182.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-976.6956, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7331.7402, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5863.9624, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3177.0244, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7853],
        [-2.3341],
        [-1.6971],
        ...,
        [-2.9101],
        [-2.9038],
        [-2.9021]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-674753.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(820.8998, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(820.8998, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0072, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0072, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0072, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        ...,
        [-0.0072, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0072, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0072, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-35018.6914, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-744.0817, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(101.2608, device='cuda:0')



h[100].sum tensor(139.3671, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-120.7019, device='cuda:0')



h[200].sum tensor(-566.9026, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.9974, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(324289.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1230, 0.0000, 0.1110],
        [0.0000, 0.0000, 0.0000,  ..., 0.1230, 0.0000, 0.1110],
        [0.0000, 0.0000, 0.0000,  ..., 0.1229, 0.0000, 0.1114],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1232, 0.0000, 0.1103],
        [0.0000, 0.0000, 0.0000,  ..., 0.1232, 0.0000, 0.1103],
        [0.0000, 0.0000, 0.0000,  ..., 0.1232, 0.0000, 0.1103]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3675962., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-950.7889, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7321.0181, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6736.0107, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3260.2495, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.0009],
        [-3.0758],
        [-3.0725],
        ...,
        [-2.9648],
        [-2.9585],
        [-2.9569]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-768918.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(714.6083, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(714.6083, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0075, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0075, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0075, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        ...,
        [-0.0075, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0075, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0075, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-36046.6523, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-766.6028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(88.1494, device='cuda:0')



h[100].sum tensor(132.4237, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-105.0732, device='cuda:0')



h[200].sum tensor(-570.2775, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.7248, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(315038.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0989, 0.0000, 0.1026],
        [0.0000, 0.0000, 0.0000,  ..., 0.0540, 0.0000, 0.0878],
        [0.0000, 0.0000, 0.0000,  ..., 0.0217, 0.0000, 0.0598],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1242, 0.0000, 0.1099],
        [0.0000, 0.0000, 0.0000,  ..., 0.1242, 0.0000, 0.1099],
        [0.0000, 0.0000, 0.0000,  ..., 0.1242, 0.0000, 0.1099]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3627572.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-969.3242, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7389.8325, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5699.7446, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3214.7100, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2539],
        [-0.7118],
        [-0.2872],
        ...,
        [-3.0148],
        [-3.0084],
        [-3.0067]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-753718.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(801.6138, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(801.6138, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0077, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0077, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0077, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        ...,
        [-0.0077, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0077, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0088, -0.0047, -0.0074,  ...,  0.0036, -0.0066, -0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-35616.7344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-788.1893, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(98.8818, device='cuda:0')



h[100].sum tensor(139.6617, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-117.8661, device='cuda:0')



h[200].sum tensor(-568.0259, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.2222, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(325160.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0786, 0.0000, 0.0942],
        [0.0000, 0.0000, 0.0000,  ..., 0.1155, 0.0000, 0.1067],
        [0.0000, 0.0000, 0.0000,  ..., 0.1234, 0.0000, 0.1096],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0876, 0.0000, 0.0957],
        [0.0000, 0.0000, 0.0000,  ..., 0.0412, 0.0000, 0.0637],
        [0.0000, 0.0000, 0.0000,  ..., 0.0093, 0.0000, 0.0334]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3706127.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-947.2623, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7468.2778, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7147.9648, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3246.1021, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1246],
        [-1.8197],
        [-2.4194],
        ...,
        [-2.2475],
        [-1.5149],
        [-0.7468]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-705221.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(886.3563, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(886.3563, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0079, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0079, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0079, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        ...,
        [-0.0079, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0079, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0079, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-35150.3398, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-807.8505, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(109.3351, device='cuda:0')



h[100].sum tensor(144.5122, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-130.3263, device='cuda:0')



h[200].sum tensor(-565.5292, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.6285, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(333458.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0648, 0.0000, 0.0880],
        [0.0000, 0.0000, 0.0000,  ..., 0.0408, 0.0000, 0.0727],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0321],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1242, 0.0000, 0.1078],
        [0.0000, 0.0000, 0.0000,  ..., 0.1242, 0.0000, 0.1078],
        [0.0000, 0.0000, 0.0000,  ..., 0.1242, 0.0000, 0.1078]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3760969., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-920.4071, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7486.4722, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8017.7891, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3301.2495, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3899],
        [-0.7048],
        [-0.1535],
        ...,
        [-3.0886],
        [-3.0820],
        [-3.0802]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-724079.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(824.5493, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(824.5493, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0090, -0.0050, -0.0058,  ...,  0.0036, -0.0065, -0.0011],
        [-0.0080, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0080, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        ...,
        [-0.0080, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0080, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0080, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-35858.1094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-824.5535, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(101.7110, device='cuda:0')



h[100].sum tensor(138.0508, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-121.2385, device='cuda:0')



h[200].sum tensor(-567.7428, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.1441, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(328277.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0138],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0222],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1250, 0.0000, 0.1075],
        [0.0000, 0.0000, 0.0000,  ..., 0.1250, 0.0000, 0.1075],
        [0.0000, 0.0000, 0.0000,  ..., 0.1250, 0.0000, 0.1075]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3750784.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-916.5513, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7589.1875, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8179.1309, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3274.7446, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3724],
        [ 0.3793],
        [ 0.3822],
        ...,
        [-3.1420],
        [-3.1425],
        [-3.1422]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-736455.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3311],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(781.0431, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3311],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(781.0431, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0082, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0089, -0.0054, -0.0039,  ...,  0.0035, -0.0063, -0.0007],
        [-0.0082, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        ...,
        [-0.0082, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0082, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0082, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-36199.5117, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-839.7063, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(96.3444, device='cuda:0')



h[100].sum tensor(131.7207, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-114.8415, device='cuda:0')



h[200].sum tensor(-568.9462, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.3953, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(322099.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0158, 0.0000, 0.0720],
        [0.0000, 0.0000, 0.0000,  ..., 0.0213, 0.0000, 0.0623],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0318],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1251, 0.0000, 0.1070],
        [0.0000, 0.0000, 0.0000,  ..., 0.1251, 0.0000, 0.1070],
        [0.0000, 0.0000, 0.0000,  ..., 0.1251, 0.0000, 0.1070]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3672028.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-923.4231, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7710.7607, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7015.6904, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3161.4639, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5014],
        [-0.9176],
        [-0.3338],
        ...,
        [-3.2014],
        [-3.1947],
        [-3.1929]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-764142.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(817.5811, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(817.5811, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0083, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0083, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0088, -0.0056, -0.0026,  ...,  0.0035, -0.0062, -0.0005],
        ...,
        [-0.0083, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0083, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0083, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-35692.2422, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-854.0845, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(100.8515, device='cuda:0')



h[100].sum tensor(137.4334, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-120.2139, device='cuda:0')



h[200].sum tensor(-566.9025, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.8640, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(329389.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.0000, 0.0628],
        [0.0000, 0.0000, 0.0000,  ..., 0.0089, 0.0000, 0.0601],
        [0.0000, 0.0000, 0.0000,  ..., 0.0104, 0.0000, 0.0477],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1229, 0.0000, 0.1048],
        [0.0000, 0.0000, 0.0000,  ..., 0.1229, 0.0000, 0.1048],
        [0.0000, 0.0000, 0.0000,  ..., 0.1229, 0.0000, 0.1048]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3713237.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-931.5752, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7872.1118, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8035.2031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3046.8235, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2024],
        [ 0.1914],
        [ 0.2194],
        ...,
        [-3.2174],
        [-3.2109],
        [-3.2092]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-695219.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 110.0 event: 1650 loss: tensor(504.3009, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(748.4974, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(748.4974, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0084, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0084, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0084, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        ...,
        [-0.0084, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0084, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0084, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-35958.3477, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-866.2672, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(92.3298, device='cuda:0')



h[100].sum tensor(139.6941, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-110.0561, device='cuda:0')



h[200].sum tensor(-567.9430, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.0871, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(329845.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0902, 0.0000, 0.0927],
        [0.0000, 0.0000, 0.0000,  ..., 0.1175, 0.0000, 0.1022],
        [0.0000, 0.0000, 0.0000,  ..., 0.1198, 0.0000, 0.1033],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1202, 0.0000, 0.1023],
        [0.0000, 0.0000, 0.0000,  ..., 0.1202, 0.0000, 0.1023],
        [0.0000, 0.0000, 0.0000,  ..., 0.1202, 0.0000, 0.1023]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3710040., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-963.0282, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7983.3003, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7667.9673, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2879.7871, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6883],
        [-2.2441],
        [-2.5665],
        ...,
        [-3.2202],
        [-3.2137],
        [-3.2119]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-558115.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(821.4076, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(821.4076, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0085, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0085, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0102, -0.0042, -0.0090,  ...,  0.0035, -0.0068, -0.0016],
        ...,
        [-0.0100, -0.0045, -0.0077,  ...,  0.0035, -0.0066, -0.0014],
        [-0.0085, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0085, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-35703.0234, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-878.3748, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(101.3235, device='cuda:0')



h[100].sum tensor(138.6306, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-120.7765, device='cuda:0')



h[200].sum tensor(-564.5571, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.0178, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(331610.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0814, 0.0000, 0.0893],
        [0.0000, 0.0000, 0.0000,  ..., 0.0324, 0.0000, 0.0595],
        [0.0000, 0.0000, 0.0000,  ..., 0.0101, 0.0000, 0.0351],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0182, 0.0000, 0.0540],
        [0.0000, 0.0000, 0.0000,  ..., 0.0418, 0.0000, 0.0644],
        [0.0000, 0.0000, 0.0000,  ..., 0.0884, 0.0000, 0.0912]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3712307., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-897.6246, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7871.9961, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7839.9556, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2987.4597, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.5819],
        [-1.9064],
        [-1.2484],
        ...,
        [-1.5820],
        [-2.0660],
        [-2.6142]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-727493.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(927.7004, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(927.7004, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0087, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0087, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0087, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        ...,
        [-0.0094, -0.0053, -0.0039,  ...,  0.0036, -0.0063, -0.0007],
        [-0.0087, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0087, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-35179.5078, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-889.6400, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(114.4351, device='cuda:0')



h[100].sum tensor(141.0086, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-136.4054, device='cuda:0')



h[200].sum tensor(-560.4335, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.2904, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(342495.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1235, 0.0000, 0.1047],
        [0.0000, 0.0000, 0.0000,  ..., 0.1235, 0.0000, 0.1047],
        [0.0000, 0.0000, 0.0000,  ..., 0.1234, 0.0000, 0.1050],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0236],
        [0.0000, 0.0000, 0.0000,  ..., 0.0400, 0.0000, 0.0543],
        [0.0000, 0.0000, 0.0000,  ..., 0.0783, 0.0000, 0.0882]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3828556.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-816.7021, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7863.1504, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10804.5889, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3094.7100, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.7935],
        [-3.8006],
        [-3.7816],
        ...,
        [-0.2667],
        [-1.0738],
        [-2.0723]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-813481.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(686.4061, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(686.4061, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0087, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0087, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0087, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        ...,
        [-0.0087, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0087, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0087, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-37170.3672, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-887.5641, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(84.6706, device='cuda:0')



h[100].sum tensor(126.9152, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-100.9264, device='cuda:0')



h[200].sum tensor(-568.4160, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.5912, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(317493.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0587, 0.0000, 0.0823],
        [0.0000, 0.0000, 0.0000,  ..., 0.1038, 0.0000, 0.0982],
        [0.0000, 0.0000, 0.0000,  ..., 0.1173, 0.0000, 0.1031],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1238, 0.0000, 0.1040],
        [0.0000, 0.0000, 0.0000,  ..., 0.1238, 0.0000, 0.1040],
        [0.0000, 0.0000, 0.0000,  ..., 0.1238, 0.0000, 0.1040]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3652527.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-876.4741, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7792.5518, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7014.3535, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2988.9478, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4504],
        [-2.0211],
        [-2.2944],
        ...,
        [-3.3938],
        [-3.3865],
        [-3.3850]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-842042.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(740.0468, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(740.0468, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0087, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0087, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0087, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        ...,
        [-0.0087, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0087, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0087, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-36696.2266, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-897.4372, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(91.2873, device='cuda:0')



h[100].sum tensor(136.9044, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-108.8135, device='cuda:0')



h[200].sum tensor(-565.7242, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.7474, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(327128.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1023, 0.0000, 0.0964],
        [0.0000, 0.0000, 0.0000,  ..., 0.0575, 0.0000, 0.0770],
        [0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0000, 0.0355],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1223, 0.0000, 0.1027],
        [0.0000, 0.0000, 0.0000,  ..., 0.1223, 0.0000, 0.1027],
        [0.0000, 0.0000, 0.0000,  ..., 0.1223, 0.0000, 0.1027]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3720939.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-831.9592, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7736.1680, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7902.1611, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2985.8535, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7907],
        [-0.9660],
        [-0.3003],
        ...,
        [-3.4186],
        [-3.4113],
        [-3.4095]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-782063.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(646.9058, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(646.9058, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0095, -0.0054, -0.0033,  ...,  0.0036, -0.0063, -0.0006],
        [-0.0088, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0109, -0.0039, -0.0102,  ...,  0.0036, -0.0069, -0.0017],
        ...,
        [-0.0088, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0088, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0088, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-37477.3008, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-905.1486, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(79.7981, device='cuda:0')



h[100].sum tensor(134.6117, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-95.1185, device='cuda:0')



h[200].sum tensor(-567.9073, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.0034, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(316889.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0581],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0367],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0281],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1212, 0.0000, 0.1019],
        [0.0000, 0.0000, 0.0000,  ..., 0.1212, 0.0000, 0.1019],
        [0.0000, 0.0000, 0.0000,  ..., 0.1212, 0.0000, 0.1019]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3656290., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-827.2168, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7703.0542, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6679.5557, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2922.0464, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1598],
        [-0.0594],
        [-0.1583],
        ...,
        [-3.4557],
        [-3.4487],
        [-3.4471]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-725429.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6763],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(739.4287, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6763],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(739.4287, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0113, -0.0035, -0.0117,  ...,  0.0036, -0.0070, -0.0019],
        [-0.0089, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0104, -0.0045, -0.0072,  ...,  0.0036, -0.0066, -0.0012],
        ...,
        [-0.0089, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0089, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0089, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-37043.4219, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-913.5792, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(91.2111, device='cuda:0')



h[100].sum tensor(131.0779, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-108.7227, device='cuda:0')



h[200].sum tensor(-564.3572, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.7225, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(321550.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0122],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1222, 0.0000, 0.1034],
        [0.0000, 0.0000, 0.0000,  ..., 0.1222, 0.0000, 0.1034],
        [0.0000, 0.0000, 0.0000,  ..., 0.1222, 0.0000, 0.1034]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3697445.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-752.4075, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7702.1514, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8573.3115, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3029.0088, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0510],
        [ 0.0388],
        [ 0.1138],
        ...,
        [-3.5554],
        [-3.5479],
        [-3.5459]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-894744.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(879.5100, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(879.5100, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0090, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0090, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0090, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        ...,
        [-0.0090, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0090, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0090, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-35787.7969, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-921.7318, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(108.4906, device='cuda:0')



h[100].sum tensor(144.8510, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-129.3197, device='cuda:0')



h[200].sum tensor(-559.1821, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.3533, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(334259.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0049, 0.0000, 0.0573],
        [0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.0000, 0.0414],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0203],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1131, 0.0000, 0.1008],
        [0.0000, 0.0000, 0.0000,  ..., 0.1099, 0.0000, 0.0998],
        [0.0000, 0.0000, 0.0000,  ..., 0.1131, 0.0000, 0.1008]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3766200., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-689.4294, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7697.3677, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10149.6797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2996.9534, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.4591e-01],
        [ 7.5221e-02],
        [-1.0746e-03],
        ...,
        [-3.2554e+00],
        [-3.1928e+00],
        [-3.1862e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-763807.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4211],
        [0.5288],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(836.2919, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4211],
        [0.5288],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(836.2919, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0121, -0.0028, -0.0142,  ...,  0.0034, -0.0073, -0.0023],
        [-0.0102, -0.0049, -0.0055,  ...,  0.0035, -0.0065, -0.0009],
        [-0.0107, -0.0044, -0.0076,  ...,  0.0034, -0.0067, -0.0012],
        ...,
        [-0.0090, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0090, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0090, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-36020.1953, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-927.6375, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(103.1595, device='cuda:0')



h[100].sum tensor(151.2920, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-122.9650, device='cuda:0')



h[200].sum tensor(-560.0058, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.6161, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0137, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(337772.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1174, 0.0000, 0.1020],
        [0.0000, 0.0000, 0.0000,  ..., 0.1085, 0.0000, 0.0987],
        [0.0000, 0.0000, 0.0000,  ..., 0.0793, 0.0000, 0.0882]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3803247., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-668.1771, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7611.6294, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10193.8418, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2922.1855, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2474],
        [ 0.2683],
        [ 0.2796],
        ...,
        [-3.4247],
        [-3.1725],
        [-2.6922]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-582693.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(698.2069, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(698.2069, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0091, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0091, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0091, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        ...,
        [-0.0091, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0091, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0091, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-37418.7500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-932.1426, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(86.1263, device='cuda:0')



h[100].sum tensor(135.9766, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-102.6616, device='cuda:0')



h[200].sum tensor(-563.9770, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.0655, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(320605.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1199, 0.0000, 0.1041],
        [0.0000, 0.0000, 0.0000,  ..., 0.1199, 0.0000, 0.1041],
        [0.0000, 0.0000, 0.0000,  ..., 0.1198, 0.0000, 0.1045],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1202, 0.0000, 0.1035],
        [0.0000, 0.0000, 0.0000,  ..., 0.1202, 0.0000, 0.1035],
        [0.0000, 0.0000, 0.0000,  ..., 0.1202, 0.0000, 0.1035]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3702569., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-672.7262, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7577.3809, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8536.9502, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2924.1384, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.9815],
        [-4.0113],
        [-4.0143],
        ...,
        [-3.3209],
        [-3.2919],
        [-3.1843]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-798122.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 120.0 event: 1800 loss: tensor(438.3108, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3020],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(735.3322, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3020],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(735.3322, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057, -0.0021,  ...,  0.0037, -0.0062, -0.0003],
        [-0.0098, -0.0054, -0.0031,  ...,  0.0037, -0.0063, -0.0005],
        [-0.0091, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        ...,
        [-0.0091, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0091, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0091, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-37314.6484, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-937.6584, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(90.7058, device='cuda:0')



h[100].sum tensor(133.5118, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-108.1203, device='cuda:0')



h[200].sum tensor(-562.2667, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.5579, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(322778.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0177],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0000, 0.0495],
        [0.0000, 0.0000, 0.0000,  ..., 0.0509, 0.0000, 0.0784],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1230, 0.0000, 0.1047],
        [0.0000, 0.0000, 0.0000,  ..., 0.1230, 0.0000, 0.1047],
        [0.0000, 0.0000, 0.0000,  ..., 0.1230, 0.0000, 0.1047]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3715725.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-644.2385, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7538.1377, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8545.7715, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3003.4673, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0418],
        [-0.4412],
        [-1.1910],
        ...,
        [-3.7808],
        [-3.7729],
        [-3.7709]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-993555.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(640.5535, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(640.5535, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0092, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0101, -0.0052, -0.0039,  ...,  0.0036, -0.0064, -0.0006],
        [-0.0098, -0.0055, -0.0026,  ...,  0.0037, -0.0063, -0.0004],
        ...,
        [-0.0092, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0092, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0092, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-38054.2344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-941.3998, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(79.0145, device='cuda:0')



h[100].sum tensor(131.1282, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-94.1844, device='cuda:0')



h[200].sum tensor(-565.2319, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.7481, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(314720.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0409, 0.0000, 0.0682],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0000, 0.0371],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0052],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1225, 0.0000, 0.1049],
        [0.0000, 0.0000, 0.0000,  ..., 0.1225, 0.0000, 0.1049],
        [0.0000, 0.0000, 0.0000,  ..., 0.1225, 0.0000, 0.1049]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3673642., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-780.4170, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7463.1387, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7191.5010, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3061.4106, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1633],
        [ 0.0830],
        [ 0.2764],
        ...,
        [-3.8206],
        [-3.8126],
        [-3.8106]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-907379.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(603.2856, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(603.2856, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0092, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0092, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0092, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        ...,
        [-0.0092, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0092, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0092, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-38121.7109, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-945.3049, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(74.4174, device='cuda:0')



h[100].sum tensor(136.4635, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-88.7047, device='cuda:0')



h[200].sum tensor(-565.8810, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.2500, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(316148.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1194, 0.0000, 0.1047],
        [0.0000, 0.0000, 0.0000,  ..., 0.1194, 0.0000, 0.1047],
        [0.0000, 0.0000, 0.0000,  ..., 0.1192, 0.0000, 0.1051],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1197, 0.0000, 0.1040],
        [0.0000, 0.0000, 0.0000,  ..., 0.1197, 0.0000, 0.1040],
        [0.0000, 0.0000, 0.0000,  ..., 0.1197, 0.0000, 0.1040]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3690728., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-816.5328, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7445.7070, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7494.6260, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3028.7939, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.6638],
        [-3.8446],
        [-3.9934],
        ...,
        [-3.8087],
        [-3.8011],
        [-3.7992]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-720256.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(860.9531, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(860.9531, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0099, -0.0055, -0.0027,  ...,  0.0035, -0.0063, -0.0004],
        [-0.0093, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0093, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        ...,
        [-0.0093, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0093, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0093, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-36112.2656, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-951.4053, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(106.2016, device='cuda:0')



h[100].sum tensor(146.0470, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-126.5911, device='cuda:0')



h[200].sum tensor(-557.1285, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.6074, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(334120.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0166],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0018],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0044],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1197, 0.0000, 0.1046],
        [0.0000, 0.0000, 0.0000,  ..., 0.1197, 0.0000, 0.1046],
        [0.0000, 0.0000, 0.0000,  ..., 0.1197, 0.0000, 0.1046]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3804540., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-631.0724, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7645.4531, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11688.9922, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2961.7295, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3963],
        [ 0.3850],
        [ 0.3709],
        ...,
        [-3.8624],
        [-3.8550],
        [-3.8532]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-706306.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(606.1854, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(606.1854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0093, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0093, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0093, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        ...,
        [-0.0093, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0093, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0093, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-38285.5156, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-949.0701, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(74.7751, device='cuda:0')



h[100].sum tensor(131.1792, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-89.1311, device='cuda:0')



h[200].sum tensor(-565.5649, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.3666, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(311666.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1159, 0.0000, 0.1042],
        [0.0000, 0.0000, 0.0000,  ..., 0.0950, 0.0000, 0.0972],
        [0.0000, 0.0000, 0.0000,  ..., 0.0471, 0.0000, 0.0771],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1197, 0.0000, 0.1046],
        [0.0000, 0.0000, 0.0000,  ..., 0.1197, 0.0000, 0.1046],
        [0.0000, 0.0000, 0.0000,  ..., 0.1197, 0.0000, 0.1046]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3649552.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-688.4518, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7527.3354, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7944.4863, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2881.0667, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.1993],
        [-2.4659],
        [-1.5635],
        ...,
        [-3.8624],
        [-3.8550],
        [-3.8532]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-806521.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4924],
        [0.5430],
        [0.6982],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(716.1239, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4924],
        [0.5430],
        [0.6982],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(716.1239, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0118, -0.0035, -0.0104,  ...,  0.0034, -0.0070, -0.0015],
        [-0.0127, -0.0026, -0.0141,  ...,  0.0033, -0.0074, -0.0020],
        [-0.0119, -0.0034, -0.0108,  ...,  0.0033, -0.0071, -0.0015],
        ...,
        [-0.0093, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0093, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0093, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-37452.5000, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-953.4998, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(88.3364, device='cuda:0')



h[100].sum tensor(136.0279, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-105.2960, device='cuda:0')



h[200].sum tensor(-561.2805, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.7858, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(320691.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1202, 0.0000, 0.1050],
        [0.0000, 0.0000, 0.0000,  ..., 0.1202, 0.0000, 0.1050],
        [0.0000, 0.0000, 0.0000,  ..., 0.1202, 0.0000, 0.1050]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3721524., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-604.9133, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7623.5049, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10203.8789, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2839.6204, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 2.7827e-02],
        [-2.8398e-03],
        [-3.3066e-02],
        ...,
        [-3.9185e+00],
        [-3.9110e+00],
        [-3.9091e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-781352.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(819.5873, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(819.5873, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0093, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0093, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0093, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        ...,
        [-0.0093, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0093, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0093, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-36695.6719, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-957.5894, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(101.0989, device='cuda:0')



h[100].sum tensor(147.8742, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-120.5089, device='cuda:0')



h[200].sum tensor(-557.0786, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.9446, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(328764.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1203, 0.0000, 0.1052],
        [0.0000, 0.0000, 0.0000,  ..., 0.1203, 0.0000, 0.1052],
        [0.0000, 0.0000, 0.0000,  ..., 0.1164, 0.0000, 0.1043],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1207, 0.0000, 0.1045],
        [0.0000, 0.0000, 0.0000,  ..., 0.1207, 0.0000, 0.1045],
        [0.0000, 0.0000, 0.0000,  ..., 0.1207, 0.0000, 0.1045]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3764894.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-583.9628, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7586.6338, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10004.3672, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2840.8599, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.1016],
        [-3.8769],
        [-3.4811],
        ...,
        [-3.9397],
        [-3.9220],
        [-3.9156]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-745951.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(596.8394, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(596.8394, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0099, -0.0056, -0.0024,  ...,  0.0036, -0.0062, -0.0003],
        [-0.0093, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0093, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        ...,
        [-0.0093, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0093, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0093, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-38847.8672, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-958.3157, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(73.6222, device='cuda:0')



h[100].sum tensor(138.4205, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-87.7569, device='cuda:0')



h[200].sum tensor(-564.1711, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.9909, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(310585.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0115, 0.0000, 0.0289],
        [0.0000, 0.0000, 0.0000,  ..., 0.0385, 0.0000, 0.0617],
        [0.0000, 0.0000, 0.0000,  ..., 0.0747, 0.0000, 0.0893],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1168, 0.0000, 0.1022],
        [0.0000, 0.0000, 0.0000,  ..., 0.1227, 0.0000, 0.1044],
        [0.0000, 0.0000, 0.0000,  ..., 0.1227, 0.0000, 0.1044]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3653698.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-639.5082, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7412.6543, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6082.8726, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2776.2300, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2085],
        [-0.2088],
        [-0.8245],
        ...,
        [-3.7512],
        [-3.9324],
        [-4.0041]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-841564.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(714.4186, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(714.4186, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0094, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0094, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0094, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        ...,
        [-0.0094, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0094, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0094, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-38179.2344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-961.9175, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(88.1260, device='cuda:0')



h[100].sum tensor(139.2188, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-105.0453, device='cuda:0')



h[200].sum tensor(-560.2511, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.7172, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(317249.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1220, 0.0000, 0.1049],
        [0.0000, 0.0000, 0.0000,  ..., 0.1247, 0.0000, 0.1058],
        [0.0000, 0.0000, 0.0000,  ..., 0.1259, 0.0000, 0.1066],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1264, 0.0000, 0.1056],
        [0.0000, 0.0000, 0.0000,  ..., 0.1264, 0.0000, 0.1056],
        [0.0000, 0.0000, 0.0000,  ..., 0.1264, 0.0000, 0.1056]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3706985.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-618.9734, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7449.8340, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7227.5059, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2815.3816, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.8552],
        [-3.4600],
        [-3.8642],
        ...,
        [-4.1453],
        [-4.1368],
        [-4.1347]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1019331., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(800.6929, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(800.6929, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0094, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0094, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0094, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        ...,
        [-0.0094, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0094, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0094, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-37481.8594, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-965.0093, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(98.7682, device='cuda:0')



h[100].sum tensor(144.1316, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-117.7307, device='cuda:0')



h[200].sum tensor(-556.9166, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.1851, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(328932., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1252, 0.0000, 0.1061],
        [0.0000, 0.0000, 0.0000,  ..., 0.1222, 0.0000, 0.1051],
        [0.0000, 0.0000, 0.0000,  ..., 0.1093, 0.0000, 0.1014],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1255, 0.0000, 0.1054],
        [0.0000, 0.0000, 0.0000,  ..., 0.1255, 0.0000, 0.1054],
        [0.0000, 0.0000, 0.0000,  ..., 0.1255, 0.0000, 0.1054]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3802569.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-570.1151, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7514.2334, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9852.9717, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2804.5806, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.9860],
        [-2.7562],
        [-2.2027],
        ...,
        [-4.1740],
        [-4.1657],
        [-4.1636]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-949801.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 130.0 event: 1950 loss: tensor(426.3960, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3306],
        [0.5620],
        [0.3574],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(561.5115, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3306],
        [0.5620],
        [0.3574],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(561.5115, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0107, -0.0048, -0.0051,  ...,  0.0035, -0.0065, -0.0006],
        [-0.0118, -0.0036, -0.0093,  ...,  0.0033, -0.0070, -0.0012],
        [-0.0123, -0.0031, -0.0112,  ...,  0.0033, -0.0072, -0.0014],
        ...,
        [-0.0094, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0094, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0094, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-39532.6328, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-964.8759, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(69.2644, device='cuda:0')



h[100].sum tensor(137.3861, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-82.5624, device='cuda:0')



h[200].sum tensor(-564.2675, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.5709, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(307803.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1223, 0.0000, 0.1040],
        [0.0000, 0.0000, 0.0000,  ..., 0.1223, 0.0000, 0.1040],
        [0.0000, 0.0000, 0.0000,  ..., 0.1223, 0.0000, 0.1040]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3653490.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-598.7136, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7410.2837, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6951.4404, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2639.7100, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2583],
        [ 0.2269],
        [ 0.2168],
        ...,
        [-4.1454],
        [-4.1375],
        [-4.1356]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-845193.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(889.4646, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(889.4646, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0094, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0103, -0.0053, -0.0031,  ...,  0.0034, -0.0063, -0.0004],
        [-0.0125, -0.0030, -0.0116,  ...,  0.0032, -0.0072, -0.0014],
        ...,
        [-0.0094, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0094, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0094, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-36797.0117, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-969.6873, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(109.7186, device='cuda:0')



h[100].sum tensor(163.2656, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-130.7834, device='cuda:0')



h[200].sum tensor(-553.2512, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.7535, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0139, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(340339.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.0000, 0.0217],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1199, 0.0000, 0.1027],
        [0.0000, 0.0000, 0.0000,  ..., 0.1199, 0.0000, 0.1027],
        [0.0000, 0.0000, 0.0000,  ..., 0.1199, 0.0000, 0.1027]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3855683.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-497.6895, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7442.1191, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11006.9248, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2716.2271, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1222],
        [-0.1653],
        [-0.2803],
        ...,
        [-4.1268],
        [-4.1193],
        [-4.1176]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-698945.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4541],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(895.5227, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4541],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(895.5227, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0127, -0.0028, -0.0119,  ...,  0.0032, -0.0073, -0.0014],
        [-0.0116, -0.0040, -0.0079,  ...,  0.0033, -0.0069, -0.0009],
        [-0.0106, -0.0050, -0.0041,  ...,  0.0034, -0.0064, -0.0005],
        ...,
        [-0.0094, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0094, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0094, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-36817.3477, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-971.5370, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(110.4658, device='cuda:0')



h[100].sum tensor(166.2661, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-131.6741, device='cuda:0')



h[200].sum tensor(-552.4950, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.9970, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0131, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0137, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(338785.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1206, 0.0000, 0.1025],
        [0.0000, 0.0000, 0.0000,  ..., 0.1206, 0.0000, 0.1025],
        [0.0000, 0.0000, 0.0000,  ..., 0.1206, 0.0000, 0.1025]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3856903., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-493.7112, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7487.7607, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11258.6836, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2678.2061, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2846],
        [ 0.3124],
        [ 0.3490],
        ...,
        [-4.1702],
        [-4.1626],
        [-4.1608]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-638046.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3276],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(829.1929, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3276],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(829.1929, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0102, -0.0054, -0.0028,  ...,  0.0036, -0.0063, -0.0003],
        [-0.0101, -0.0055, -0.0024,  ...,  0.0036, -0.0063, -0.0003],
        [-0.0109, -0.0047, -0.0052,  ...,  0.0035, -0.0066, -0.0006],
        ...,
        [-0.0095, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0095, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0095, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-37831.6797, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-972.4785, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(102.2838, device='cuda:0')



h[100].sum tensor(156.2188, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-121.9212, device='cuda:0')



h[200].sum tensor(-554.3695, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.3307, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(329557.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0088, 0.0000, 0.0343],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0082],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0038],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1245, 0.0000, 0.1038],
        [0.0000, 0.0000, 0.0000,  ..., 0.1245, 0.0000, 0.1038],
        [0.0000, 0.0000, 0.0000,  ..., 0.1245, 0.0000, 0.1038]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3796641.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-518.4652, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7453.3770, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9679.6621, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2684.3418, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2973],
        [ 0.3678],
        [ 0.3823],
        ...,
        [-4.2913],
        [-4.2830],
        [-4.2809]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-833437.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3127],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(812.6552, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3127],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(812.6552, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0095, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        [-0.0102, -0.0054, -0.0027,  ...,  0.0037, -0.0063, -0.0003],
        [-0.0095, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        ...,
        [-0.0095, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        [-0.0095, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        [-0.0095, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-38383.4609, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-973.6908, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(100.2438, device='cuda:0')



h[100].sum tensor(151.1781, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-119.4896, device='cuda:0')



h[200].sum tensor(-554.9121, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.6660, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(328218.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0783, 0.0000, 0.0907],
        [0.0000, 0.0000, 0.0000,  ..., 0.0683, 0.0000, 0.0878],
        [0.0000, 0.0000, 0.0000,  ..., 0.0256, 0.0000, 0.0754],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1279, 0.0000, 0.1047],
        [0.0000, 0.0000, 0.0000,  ..., 0.1279, 0.0000, 0.1047],
        [0.0000, 0.0000, 0.0000,  ..., 0.1279, 0.0000, 0.1047]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3795412.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-526.2378, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7417.3784, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9032.7812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2712.5137, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.2447],
        [-3.4205],
        [-3.5419],
        ...,
        [-4.3920],
        [-4.3831],
        [-4.3808]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1020067.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(683.2072, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(683.2072, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0095, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0095, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0095, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        ...,
        [-0.0095, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0095, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0095, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-39516.2188, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-973.7360, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(84.2760, device='cuda:0')



h[100].sum tensor(149.9499, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-100.4561, device='cuda:0')



h[200].sum tensor(-558.9772, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.4626, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(319816.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1207, 0.0000, 0.1027],
        [0.0000, 0.0000, 0.0000,  ..., 0.1002, 0.0000, 0.0961],
        [0.0000, 0.0000, 0.0000,  ..., 0.0498, 0.0000, 0.0739],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1274, 0.0000, 0.1042],
        [0.0000, 0.0000, 0.0000,  ..., 0.1274, 0.0000, 0.1042],
        [0.0000, 0.0000, 0.0000,  ..., 0.1274, 0.0000, 0.1042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3743021.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-667.5574, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7247.7070, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7492.7793, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2617.9189, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.9615],
        [-2.1421],
        [-1.2228],
        ...,
        [-4.3948],
        [-4.3859],
        [-4.3837]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-917760.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(734.4700, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(734.4700, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0109, -0.0047, -0.0051,  ...,  0.0035, -0.0066, -0.0006],
        [-0.0095, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0095, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        ...,
        [-0.0095, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0095, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0095, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-39069.1758, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-974.2125, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(90.5994, device='cuda:0')



h[100].sum tensor(152.9216, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-107.9936, device='cuda:0')



h[200].sum tensor(-557.2918, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.5232, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(322847.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0051],
        [0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0000, 0.0351],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1274, 0.0000, 0.1042],
        [0.0000, 0.0000, 0.0000,  ..., 0.1274, 0.0000, 0.1042],
        [0.0000, 0.0000, 0.0000,  ..., 0.1274, 0.0000, 0.1042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3760648.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-658.8778, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7275.1753, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7992.5449, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2625.9824, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2852],
        [ 0.1825],
        [-0.2778],
        ...,
        [-4.3944],
        [-4.3853],
        [-4.3825]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-905469.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4324],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(818.9298, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4324],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(818.9298, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0105, -0.0051, -0.0036,  ...,  0.0035, -0.0064, -0.0004],
        [-0.0095, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0105, -0.0051, -0.0036,  ...,  0.0035, -0.0064, -0.0004],
        ...,
        [-0.0095, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0095, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0095, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-38464.2109, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-976.1447, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(101.0178, device='cuda:0')



h[100].sum tensor(156.7395, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-120.4122, device='cuda:0')



h[200].sum tensor(-554.0338, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.9182, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(332512.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0469, 0.0000, 0.0780],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0583],
        [0.0000, 0.0000, 0.0000,  ..., 0.0467, 0.0000, 0.0783],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0204, 0.0000, 0.0508],
        [0.0000, 0.0000, 0.0000,  ..., 0.0520, 0.0000, 0.0793],
        [0.0000, 0.0000, 0.0000,  ..., 0.0916, 0.0000, 0.0929]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3842773.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-515.4053, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7417.4053, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10981.5986, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2630.6262, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.9184],
        [-2.8290],
        [-3.1605],
        ...,
        [-0.3690],
        [-1.2760],
        [-2.3687]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-817025.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.8413],
        [0.5146],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(866.4119, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.8413],
        [0.5146],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(866.4119, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0131, -0.0025, -0.0122,  ...,  0.0031, -0.0074, -0.0013],
        [-0.0134, -0.0021, -0.0134,  ...,  0.0031, -0.0076, -0.0014],
        [-0.0122, -0.0034, -0.0091,  ...,  0.0033, -0.0071, -0.0010],
        ...,
        [-0.0095, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0095, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0095, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-38250.4531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-977.5859, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(106.8749, device='cuda:0')



h[100].sum tensor(159.5975, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-127.3938, device='cuda:0')



h[200].sum tensor(-552.2723, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.8268, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0124, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0130, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(333901.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1257, 0.0000, 0.1038],
        [0.0000, 0.0000, 0.0000,  ..., 0.1257, 0.0000, 0.1038],
        [0.0000, 0.0000, 0.0000,  ..., 0.1257, 0.0000, 0.1038]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3830165., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-464.0999, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7429.7148, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11321.8057, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2608.4980, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5140],
        [-0.4668],
        [-0.4210],
        ...,
        [-4.4226],
        [-4.4145],
        [-4.4126]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-828985.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3062],
        [0.0000],
        [0.6787],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(684.8071, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3062],
        [0.0000],
        [0.6787],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(684.8071, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0100, -0.0057, -0.0017,  ...,  0.0035, -0.0062, -0.0002],
        [-0.0131, -0.0025, -0.0122,  ...,  0.0031, -0.0074, -0.0013],
        [-0.0122, -0.0034, -0.0092,  ...,  0.0032, -0.0071, -0.0010],
        ...,
        [-0.0095, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0095, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0095, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-40022.4609, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-976.8171, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(84.4733, device='cuda:0')



h[100].sum tensor(154.8454, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-100.6913, device='cuda:0')



h[200].sum tensor(-558.0518, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.5269, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0126, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0126, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(320023.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1258, 0.0000, 0.1029],
        [0.0000, 0.0000, 0.0000,  ..., 0.1258, 0.0000, 0.1029],
        [0.0000, 0.0000, 0.0000,  ..., 0.1258, 0.0000, 0.1029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3769557.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-502.0466, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7404.0815, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9828.7148, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2509.8440, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 3.2789e-04],
        [-1.8533e-02],
        [-3.5008e-03],
        ...,
        [-4.3697e+00],
        [-4.3442e+00],
        [-4.3117e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-690745.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 140.0 event: 2100 loss: tensor(450.0197, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5986],
        [0.6084],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(830.1096, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5986],
        [0.6084],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(830.1096, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0109, -0.0047, -0.0048,  ...,  0.0035, -0.0066, -0.0005],
        [-0.0110, -0.0047, -0.0048,  ...,  0.0035, -0.0066, -0.0005],
        [-0.0124, -0.0032, -0.0096,  ...,  0.0032, -0.0071, -0.0010],
        ...,
        [-0.0095, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0095, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0095, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-39162.6094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-978.9579, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(102.3969, device='cuda:0')



h[100].sum tensor(160.8102, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-122.0560, device='cuda:0')



h[200].sum tensor(-553.2478, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.3676, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(332988.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0107, 0.0000, 0.0269],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1295, 0.0000, 0.1033],
        [0.0000, 0.0000, 0.0000,  ..., 0.1295, 0.0000, 0.1033],
        [0.0000, 0.0000, 0.0000,  ..., 0.1295, 0.0000, 0.1033]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3844358., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-476.8022, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7336.9561, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10011.2432, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2605.0344, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2214],
        [ 0.3447],
        [ 0.3606],
        ...,
        [-4.5201],
        [-4.5115],
        [-4.5094]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-849630.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(668.7966, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(668.7966, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0095, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        [-0.0095, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        [-0.0095, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        ...,
        [-0.0095, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        [-0.0095, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        [-0.0102, -0.0055, -0.0023,  ...,  0.0037, -0.0063, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-40917.7461, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-978.2911, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(82.4984, device='cuda:0')



h[100].sum tensor(142.4078, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-98.3372, device='cuda:0')



h[200].sum tensor(-558.0054, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.8834, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(315376.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1337, 0.0000, 0.1055],
        [0.0000, 0.0000, 0.0000,  ..., 0.1333, 0.0000, 0.1053],
        [0.0000, 0.0000, 0.0000,  ..., 0.1320, 0.0000, 0.1052],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1090, 0.0000, 0.0974],
        [0.0000, 0.0000, 0.0000,  ..., 0.0588, 0.0000, 0.0827],
        [0.0000, 0.0000, 0.0000,  ..., 0.0428, 0.0000, 0.0780]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3733904.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-509.9032, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7311.8413, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7982.9336, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2550.2476, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.9309],
        [-4.6827],
        [-4.2653],
        ...,
        [-4.0149],
        [-3.5746],
        [-3.3081]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1140641.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(950.9102, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(950.9102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0101, -0.0056, -0.0019,  ...,  0.0036, -0.0062, -0.0002],
        [-0.0095, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0095, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        ...,
        [-0.0095, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0095, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0095, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-38426.4688, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-981.5894, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(117.2981, device='cuda:0')



h[100].sum tensor(167.2838, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-139.8181, device='cuda:0')



h[200].sum tensor(-548.2956, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.2234, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(344716.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0536],
        [0.0000, 0.0000, 0.0000,  ..., 0.0295, 0.0000, 0.0665],
        [0.0000, 0.0000, 0.0000,  ..., 0.0733, 0.0000, 0.0870],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1319, 0.0000, 0.1032],
        [0.0000, 0.0000, 0.0000,  ..., 0.1319, 0.0000, 0.1032],
        [0.0000, 0.0000, 0.0000,  ..., 0.1319, 0.0000, 0.1032]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3944659.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-419.4629, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7425.5288, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12641.4707, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2605.6663, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1264],
        [-0.0500],
        [-0.1756],
        ...,
        [-4.6060],
        [-4.5973],
        [-4.5952]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-822149.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.9517],
        [0.2546],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(778.1165, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.9517],
        [0.2546],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(778.1165, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0137, -0.0019, -0.0132,  ...,  0.0029, -0.0076, -0.0012],
        [-0.0125, -0.0031, -0.0094,  ...,  0.0031, -0.0072, -0.0009],
        [-0.0130, -0.0026, -0.0109,  ...,  0.0031, -0.0074, -0.0010],
        ...,
        [-0.0095, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0095, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0095, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-40089.5078, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-980.5642, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(95.9834, device='cuda:0')



h[100].sum tensor(161.5081, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-114.4112, device='cuda:0')



h[200].sum tensor(-553.0611, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.2776, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0122, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0129, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(326970., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1299, 0.0000, 0.1023],
        [0.0000, 0.0000, 0.0000,  ..., 0.1299, 0.0000, 0.1023],
        [0.0000, 0.0000, 0.0000,  ..., 0.1299, 0.0000, 0.1023]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3808359., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-442.2153, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7350.4287, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10283.3887, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2472.4619, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3642],
        [ 0.3242],
        [ 0.2906],
        ...,
        [-4.5818],
        [-4.5737],
        [-4.5718]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-726280.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(709.1974, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(709.1974, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-40948.8789, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-980.4753, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(87.4820, device='cuda:0')



h[100].sum tensor(154.0231, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-104.2776, device='cuda:0')



h[200].sum tensor(-554.3979, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.5073, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(322975.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0339, 0.0000, 0.0512],
        [0.0000, 0.0000, 0.0000,  ..., 0.0687, 0.0000, 0.0846],
        [0.0000, 0.0000, 0.0000,  ..., 0.0887, 0.0000, 0.0915],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1307, 0.0000, 0.1027],
        [0.0000, 0.0000, 0.0000,  ..., 0.1307, 0.0000, 0.1027],
        [0.0000, 0.0000, 0.0000,  ..., 0.1307, 0.0000, 0.1027]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3790938.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-427.2737, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7314.6704, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10712.5420, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2436.4551, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2427],
        [-0.6303],
        [-0.8720],
        ...,
        [-4.6277],
        [-4.6195],
        [-4.6175]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-828238.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(592.2180, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(592.2180, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0118, -0.0039, -0.0070,  ...,  0.0033, -0.0069, -0.0006],
        [-0.0118, -0.0039, -0.0070,  ...,  0.0033, -0.0069, -0.0006],
        [-0.0119, -0.0038, -0.0073,  ...,  0.0033, -0.0069, -0.0007],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-42253.7656, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-979.8801, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(73.0521, device='cuda:0')



h[100].sum tensor(147.0912, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-87.0774, device='cuda:0')



h[200].sum tensor(-557.4159, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.8052, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0137, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(311572.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1320, 0.0000, 0.1025],
        [0.0000, 0.0000, 0.0000,  ..., 0.1320, 0.0000, 0.1025],
        [0.0000, 0.0000, 0.0000,  ..., 0.1320, 0.0000, 0.1025]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3717388., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-449.7585, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7257.9629, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9096.5078, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2375.9863, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2953],
        [ 0.2801],
        [ 0.1565],
        ...,
        [-4.6729],
        [-4.6645],
        [-4.6625]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-856408.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4397],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(736.9994, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4397],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(736.9994, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0120, -0.0037, -0.0074,  ...,  0.0033, -0.0070, -0.0006],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        [-0.0106, -0.0051, -0.0032,  ...,  0.0036, -0.0064, -0.0003],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-41344.6016, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-981.6550, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(90.9114, device='cuda:0')



h[100].sum tensor(155.4539, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-108.3655, device='cuda:0')



h[200].sum tensor(-552.0192, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.6249, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(320373.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0024],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0057],
        [0.0000, 0.0000, 0.0000,  ..., 0.0231, 0.0000, 0.0354],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1352, 0.0000, 0.1021],
        [0.0000, 0.0000, 0.0000,  ..., 0.1352, 0.0000, 0.1021],
        [0.0000, 0.0000, 0.0000,  ..., 0.1352, 0.0000, 0.1021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3758306.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-441.1473, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7232.2935, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8769.0508, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2431.4097, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4234],
        [ 0.3820],
        [ 0.2564],
        ...,
        [-4.7432],
        [-4.7351],
        [-4.7336]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-921327.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4617],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(596.1998, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4617],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(596.1998, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0123, -0.0033, -0.0084,  ...,  0.0033, -0.0071, -0.0007],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        [-0.0107, -0.0051, -0.0033,  ...,  0.0036, -0.0064, -0.0003],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-42871.4844, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-980.7625, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(73.5433, device='cuda:0')



h[100].sum tensor(144.3785, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-87.6629, device='cuda:0')



h[200].sum tensor(-555.7375, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.9652, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(308672.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         4.7399e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         1.1991e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.3776e-01, 0.0000e+00,
         1.0227e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.3776e-01, 0.0000e+00,
         1.0227e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.3776e-01, 0.0000e+00,
         1.0227e-01]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3675229., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-472.9137, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7120.6846, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6391.7041, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2404.2261, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0952],
        [ 0.3471],
        [ 0.4315],
        ...,
        [-4.8217],
        [-4.8125],
        [-4.8102]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1139451., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(629.3951, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(629.3951, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-42645.8516, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-981.4478, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(77.6381, device='cuda:0')



h[100].sum tensor(147.4355, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-92.5438, device='cuda:0')



h[200].sum tensor(-553.6742, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.2996, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(313313.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1369, 0.0000, 0.1024],
        [0.0000, 0.0000, 0.0000,  ..., 0.1369, 0.0000, 0.1024],
        [0.0000, 0.0000, 0.0000,  ..., 0.1367, 0.0000, 0.1027],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1372, 0.0000, 0.1017],
        [0.0000, 0.0000, 0.0000,  ..., 0.1372, 0.0000, 0.1017],
        [0.0000, 0.0000, 0.0000,  ..., 0.1372, 0.0000, 0.1017]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3723601.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-452.9215, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7175.7432, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8188.3867, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2372.4373, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.1750],
        [-5.2466],
        [-5.2946],
        ...,
        [-4.8292],
        [-4.8205],
        [-4.8184]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-986083.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(751.1064, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(751.1064, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-41567.1172, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-982.5740, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(92.6516, device='cuda:0')



h[100].sum tensor(154.4355, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-110.4397, device='cuda:0')



h[200].sum tensor(-549.6990, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.1919, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(322679.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0128, 0.0000, 0.0278],
        [0.0000, 0.0000, 0.0000,  ..., 0.0462, 0.0000, 0.0653],
        [0.0000, 0.0000, 0.0000,  ..., 0.0921, 0.0000, 0.0885],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1372, 0.0000, 0.1017],
        [0.0000, 0.0000, 0.0000,  ..., 0.1372, 0.0000, 0.1017],
        [0.0000, 0.0000, 0.0000,  ..., 0.1372, 0.0000, 0.1017]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3776070.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-430.1394, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7214.8027, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9371.7734, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2407.6084, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0352],
        [-0.5884],
        [-1.4152],
        ...,
        [-4.8276],
        [-4.8119],
        [-4.7721]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-947409.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 150.0 event: 2250 loss: tensor(433.1915, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2620],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(744.5989, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2620],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(744.5989, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0102, -0.0056, -0.0017,  ...,  0.0036, -0.0062, -0.0001],
        [-0.0102, -0.0055, -0.0018,  ...,  0.0036, -0.0063, -0.0001],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-41649.9297, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-982.8369, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(91.8489, device='cuda:0')



h[100].sum tensor(154.0697, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-109.4829, device='cuda:0')



h[200].sum tensor(-548.9946, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.9303, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(321723.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0061],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0247],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0361],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1361, 0.0000, 0.1013],
        [0.0000, 0.0000, 0.0000,  ..., 0.1361, 0.0000, 0.1013],
        [0.0000, 0.0000, 0.0000,  ..., 0.1361, 0.0000, 0.1013]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3762604.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-412.4288, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7224.5439, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9851.1533, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2358.2273, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3495],
        [ 0.3022],
        [ 0.1836],
        ...,
        [-4.8291],
        [-4.8205],
        [-4.8186]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-912149.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(837.0669, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(837.0669, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0107, -0.0050, -0.0032,  ...,  0.0034, -0.0064, -0.0003],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0108, -0.0049, -0.0035,  ...,  0.0034, -0.0065, -0.0003],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-40724.2344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.0336, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(103.2551, device='cuda:0')



h[100].sum tensor(166.8098, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-123.0790, device='cuda:0')



h[200].sum tensor(-544.8306, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.6473, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(331883.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0084],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1334, 0.0000, 0.0995],
        [0.0000, 0.0000, 0.0000,  ..., 0.1334, 0.0000, 0.0995],
        [0.0000, 0.0000, 0.0000,  ..., 0.1334, 0.0000, 0.0995]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3839956., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-373.5115, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7263.0503, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11776., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2334.1729, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1197],
        [-0.0296],
        [-0.1815],
        ...,
        [-4.7844],
        [-4.7762],
        [-4.7743]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-722004.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(833.3936, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(833.3936, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0124, -0.0033, -0.0081,  ...,  0.0031, -0.0071, -0.0006],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-40852.5000, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.2615, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(102.8020, device='cuda:0')



h[100].sum tensor(169.0974, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-122.5389, device='cuda:0')



h[200].sum tensor(-544.2456, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.4996, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(335186., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0149],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1337, 0.0000, 0.0988],
        [0.0000, 0.0000, 0.0000,  ..., 0.1337, 0.0000, 0.0988],
        [0.0000, 0.0000, 0.0000,  ..., 0.1337, 0.0000, 0.0988]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3861569.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-373.9252, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7200.2915, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11577.5430, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2342.6133, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4952],
        [ 0.4535],
        [ 0.4139],
        ...,
        [-4.8061],
        [-4.7982],
        [-4.7964]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-714355.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3877],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(667.8540, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3877],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(667.8540, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0115, -0.0042, -0.0054,  ...,  0.0033, -0.0068, -0.0004],
        [-0.0106, -0.0052, -0.0028,  ...,  0.0035, -0.0064, -0.0002],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-42535.0781, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-982.9824, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(82.3821, device='cuda:0')



h[100].sum tensor(153.3019, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-98.1986, device='cuda:0')



h[200].sum tensor(-548.9558, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.8455, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(320177.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0152],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0205],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0246],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1366, 0.0000, 0.0998],
        [0.0000, 0.0000, 0.0000,  ..., 0.1366, 0.0000, 0.0998],
        [0.0000, 0.0000, 0.0000,  ..., 0.1366, 0.0000, 0.0998]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3762758.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-314.2104, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7286.9106, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9916.7490, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2429.1899, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4012],
        [ 0.4170],
        [ 0.4428],
        ...,
        [-4.9081],
        [-4.8996],
        [-4.8976]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-904579.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2700],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(795.8710, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2700],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(795.8710, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0102, -0.0055, -0.0018,  ...,  0.0035, -0.0063, -0.0001],
        [-0.0102, -0.0056, -0.0016,  ...,  0.0035, -0.0062, -0.0001],
        [-0.0108, -0.0049, -0.0034,  ...,  0.0034, -0.0065, -0.0002],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-41807.5430, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.2992, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(98.1734, device='cuda:0')



h[100].sum tensor(160.3547, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-117.0217, device='cuda:0')



h[200].sum tensor(-544.5925, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.9913, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(325819.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0215],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0125],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0342],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1377, 0.0000, 0.0994],
        [0.0000, 0.0000, 0.0000,  ..., 0.1377, 0.0000, 0.0994],
        [0.0000, 0.0000, 0.0000,  ..., 0.1377, 0.0000, 0.0994]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3782761., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-382.5433, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7181.0962, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9945.5566, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2343.0913, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3238],
        [ 0.3211],
        [ 0.2100],
        ...,
        [-4.9559],
        [-4.9473],
        [-4.9453]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-916790.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(775.7432, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(775.7432, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-42215.6211, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.3971, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(95.6906, device='cuda:0')



h[100].sum tensor(165.2510, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-114.0622, device='cuda:0')



h[200].sum tensor(-544.3912, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.1822, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0139, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(326914.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0334, 0.0000, 0.0563],
        [0.0000, 0.0000, 0.0000,  ..., 0.0273, 0.0000, 0.0482],
        [0.0000, 0.0000, 0.0000,  ..., 0.0107, 0.0000, 0.0198],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1380, 0.0000, 0.0977],
        [0.0000, 0.0000, 0.0000,  ..., 0.1380, 0.0000, 0.0977],
        [0.0000, 0.0000, 0.0000,  ..., 0.1380, 0.0000, 0.0977]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3803053.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-395.4971, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7159.4771, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9526.1084, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2321.1663, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2807],
        [ 0.3888],
        [ 0.4645],
        ...,
        [-4.9777],
        [-4.9691],
        [-4.9671]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-817550.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6108],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(892.3154, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6108],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(892.3154, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0110, -0.0047, -0.0039,  ...,  0.0033, -0.0066, -0.0003],
        [-0.0118, -0.0039, -0.0059,  ...,  0.0032, -0.0069, -0.0004],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-41529.5078, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.6024, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(110.0702, device='cuda:0')



h[100].sum tensor(164.9360, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-131.2025, device='cuda:0')



h[200].sum tensor(-540.2098, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.8681, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0137, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0123, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(335872.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0019],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1392, 0.0000, 0.0983],
        [0.0000, 0.0000, 0.0000,  ..., 0.1392, 0.0000, 0.0983],
        [0.0000, 0.0000, 0.0000,  ..., 0.1392, 0.0000, 0.0983]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3864159., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-331.3340, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7233.8765, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12077.5801, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2368.5288, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2661],
        [ 0.2266],
        [ 0.1751],
        ...,
        [-5.0503],
        [-5.0415],
        [-5.0394]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-924994.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(772.9529, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(772.9529, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-42721.1992, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.7808, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(95.3464, device='cuda:0')



h[100].sum tensor(158.5185, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-113.6519, device='cuda:0')



h[200].sum tensor(-542.9121, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.0701, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(327915.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1337, 0.0000, 0.0969],
        [0.0000, 0.0000, 0.0000,  ..., 0.1080, 0.0000, 0.0890],
        [0.0000, 0.0000, 0.0000,  ..., 0.0680, 0.0000, 0.0763],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1393, 0.0000, 0.0978],
        [0.0000, 0.0000, 0.0000,  ..., 0.1393, 0.0000, 0.0978],
        [0.0000, 0.0000, 0.0000,  ..., 0.1393, 0.0000, 0.0978]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3811019.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-325.9585, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7175.5576, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11156.5547, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2352.9116, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.0656],
        [-2.1803],
        [-1.2049],
        ...,
        [-5.0867],
        [-5.0777],
        [-5.0757]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-994612.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(685.4463, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(685.4463, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-43659.5391, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.0657, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(84.5522, device='cuda:0')



h[100].sum tensor(160.9433, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-100.7853, device='cuda:0')



h[200].sum tensor(-545.2179, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.5526, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(318356.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0177, 0.0000, 0.0207],
        [0.0000, 0.0000, 0.0000,  ..., 0.0515, 0.0000, 0.0569],
        [0.0000, 0.0000, 0.0000,  ..., 0.0911, 0.0000, 0.0812],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1370, 0.0000, 0.0955],
        [0.0000, 0.0000, 0.0000,  ..., 0.1370, 0.0000, 0.0955],
        [0.0000, 0.0000, 0.0000,  ..., 0.1370, 0.0000, 0.0955]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3733042., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-321.4221, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7150.2700, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9112.1367, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2273.1030, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1348],
        [-0.1781],
        [-0.5828],
        ...,
        [-5.0496],
        [-5.0409],
        [-5.0390]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-796785.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(668.5398, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(668.5398, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-44026.6055, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.0630, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(82.4667, device='cuda:0')



h[100].sum tensor(154.9285, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-98.2994, device='cuda:0')



h[200].sum tensor(-544.9491, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.8730, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(317211.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1347, 0.0000, 0.0958],
        [0.0000, 0.0000, 0.0000,  ..., 0.1165, 0.0000, 0.0898],
        [0.0000, 0.0000, 0.0000,  ..., 0.0640, 0.0000, 0.0718],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1381, 0.0000, 0.0961],
        [0.0000, 0.0000, 0.0000,  ..., 0.1381, 0.0000, 0.0961],
        [0.0000, 0.0000, 0.0000,  ..., 0.1381, 0.0000, 0.0961]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3731688.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-297.2886, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7176.3428, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9935.3281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2280.6250, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.2869],
        [-3.4344],
        [-2.3243],
        ...,
        [-5.1142],
        [-5.1053],
        [-5.1033]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-907706.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 160.0 event: 2400 loss: tensor(479.2907, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2483],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(721.0981, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2483],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(721.0981, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0107, -0.0050, -0.0029,  ...,  0.0033, -0.0065, -0.0002],
        [-0.0112, -0.0045, -0.0041,  ...,  0.0032, -0.0066, -0.0002],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-43620.5703, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.6715, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(88.9500, device='cuda:0')



h[100].sum tensor(157.7630, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-106.0274, device='cuda:0')



h[200].sum tensor(-542.4125, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.9857, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0127, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(321664.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0436, 0.0000, 0.0471],
        [0.0000, 0.0000, 0.0000,  ..., 0.0116, 0.0000, 0.0208],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1386, 0.0000, 0.0956],
        [0.0000, 0.0000, 0.0000,  ..., 0.1386, 0.0000, 0.0956],
        [0.0000, 0.0000, 0.0000,  ..., 0.1386, 0.0000, 0.0956]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3745631., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-256.1871, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7191.7798, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10379.4795, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2422.6306, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3804],
        [ 0.0654],
        [ 0.3268],
        ...,
        [-5.1510],
        [-5.1419],
        [-5.1397]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-901394.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5166],
        [0.5562],
        [0.3340],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(919.1187, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5166],
        [0.5562],
        [0.3340],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(919.1187, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0122, -0.0035, -0.0066,  ...,  0.0030, -0.0070, -0.0004],
        [-0.0122, -0.0035, -0.0065,  ...,  0.0030, -0.0070, -0.0004],
        [-0.0124, -0.0033, -0.0068,  ...,  0.0029, -0.0071, -0.0004],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-41955.3750, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.5267, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(113.3765, device='cuda:0')



h[100].sum tensor(171.5162, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-135.1436, device='cuda:0')



h[200].sum tensor(-535.4036, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.9455, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0117, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0122, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(339750.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1391, 0.0000, 0.0945],
        [0.0000, 0.0000, 0.0000,  ..., 0.1391, 0.0000, 0.0945],
        [0.0000, 0.0000, 0.0000,  ..., 0.1391, 0.0000, 0.0945]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3845040.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-191.5396, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7173.0371, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11508.9854, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2641.1226, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7377e-02],
        [ 1.2297e-03],
        [ 2.6985e-02],
        ...,
        [-5.1812e+00],
        [-5.1723e+00],
        [-5.1703e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-905039.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2778],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(592.2856, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2778],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(592.2856, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0809e-02, -4.9384e-03, -2.9634e-03,  ...,  3.2549e-03,
         -6.4836e-03, -1.6837e-04],
        [-1.0145e-02, -5.6235e-03, -1.3423e-03,  ...,  3.3947e-03,
         -6.2221e-03, -7.6264e-05],
        [-1.0259e-02, -5.5057e-03, -1.6211e-03,  ...,  3.3707e-03,
         -6.2671e-03, -9.2109e-05],
        ...,
        [-9.5948e-03, -6.1908e-03,  0.0000e+00,  ...,  3.5105e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.5948e-03, -6.1908e-03,  0.0000e+00,  ...,  3.5105e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.5948e-03, -6.1908e-03,  0.0000e+00,  ...,  3.5105e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-45042.1016, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-983.5996, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(73.0605, device='cuda:0')



h[100].sum tensor(150.7074, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-87.0873, device='cuda:0')



h[200].sum tensor(-545.7323, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.8079, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0131, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(314210.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0000, 0.0242],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0058],
        [0.0000, 0.0000, 0.0000,  ..., 0.0174, 0.0000, 0.0307],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1403, 0.0000, 0.0947],
        [0.0000, 0.0000, 0.0000,  ..., 0.1403, 0.0000, 0.0947],
        [0.0000, 0.0000, 0.0000,  ..., 0.1403, 0.0000, 0.0947]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3691444.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-222.7195, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7272.1030, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8560.2881, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2398.1421, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1908],
        [ 0.2705],
        [-0.0720],
        ...,
        [-5.2313],
        [-5.2223],
        [-5.2202]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1006209.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4675],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(855.4351, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4675],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(855.4351, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0107, -0.0050, -0.0027,  ...,  0.0033, -0.0064, -0.0002],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0107, -0.0050, -0.0027,  ...,  0.0033, -0.0064, -0.0002],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-42639.1328, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.0498, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(105.5209, device='cuda:0')



h[100].sum tensor(165.9637, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-125.7798, device='cuda:0')



h[200].sum tensor(-537.0128, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.3856, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(334911., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0540, 0.0000, 0.0640],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0411],
        [0.0000, 0.0000, 0.0000,  ..., 0.0538, 0.0000, 0.0643],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1403, 0.0000, 0.0947],
        [0.0000, 0.0000, 0.0000,  ..., 0.1403, 0.0000, 0.0947],
        [0.0000, 0.0000, 0.0000,  ..., 0.1403, 0.0000, 0.0947]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3821271., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-170.1252, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7356.1313, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11483.5166, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2475.8618, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.1349],
        [-2.9397],
        [-3.2250],
        ...,
        [-5.2313],
        [-5.2223],
        [-5.2202]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-953758.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(765.2953, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(765.2953, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-43327.9414, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.4366, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(94.4018, device='cuda:0')



h[100].sum tensor(159.8681, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-112.5260, device='cuda:0')



h[200].sum tensor(-539.2909, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.7623, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0139, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0139, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0139, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0139, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0139, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0139, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(334546.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1168, 0.0000, 0.0875],
        [0.0000, 0.0000, 0.0000,  ..., 0.1344, 0.0000, 0.0933],
        [0.0000, 0.0000, 0.0000,  ..., 0.1386, 0.0000, 0.0952],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1407, 0.0000, 0.0949],
        [0.0000, 0.0000, 0.0000,  ..., 0.1407, 0.0000, 0.0949],
        [0.0000, 0.0000, 0.0000,  ..., 0.1407, 0.0000, 0.0949]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3829555.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-125.1776, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7556.2212, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12171.2441, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2311.7339, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.5268],
        [-3.0676],
        [-3.2620],
        ...,
        [-5.2504],
        [-5.2413],
        [-5.2393]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-975643.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(748.3770, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(748.3770, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-43764.8594, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.2316, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(92.3149, device='cuda:0')



h[100].sum tensor(159.1440, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-110.0384, device='cuda:0')



h[200].sum tensor(-539.9910, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.0822, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0137, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0137, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0137, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0137, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0137, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0137, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(327073.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1173, 0.0000, 0.0878],
        [0.0000, 0.0000, 0.0000,  ..., 0.1360, 0.0000, 0.0940],
        [0.0000, 0.0000, 0.0000,  ..., 0.1390, 0.0000, 0.0954],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1396, 0.0000, 0.0944],
        [0.0000, 0.0000, 0.0000,  ..., 0.1396, 0.0000, 0.0944],
        [0.0000, 0.0000, 0.0000,  ..., 0.1396, 0.0000, 0.0944]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3785285.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-138.7358, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7493.7764, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11735.2676, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2250.2456, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.3408],
        [-4.0216],
        [-4.2194],
        ...,
        [-5.2357],
        [-5.2268],
        [-5.2249]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-831735.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(717.0449, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(717.0449, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-44325.8203, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.0458, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(88.4500, device='cuda:0')



h[100].sum tensor(156.4799, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-105.4314, device='cuda:0')



h[200].sum tensor(-540.5890, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.8228, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0129, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(322487.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1121, 0.0000, 0.0850],
        [0.0000, 0.0000, 0.0000,  ..., 0.0648, 0.0000, 0.0578],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0251],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1397, 0.0000, 0.0945],
        [0.0000, 0.0000, 0.0000,  ..., 0.1397, 0.0000, 0.0945],
        [0.0000, 0.0000, 0.0000,  ..., 0.1337, 0.0000, 0.0923]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3756172.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-166.7040, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7273.2783, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10888.6240, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2251.0203, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.1076],
        [-1.6176],
        [-0.4508],
        ...,
        [-5.2183],
        [-5.0581],
        [-4.6072]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-866129.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(772.0468, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(772.0468, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0201e-02, -5.5691e-03, -1.3835e-03,  ...,  3.2809e-03,
         -6.2429e-03, -6.9336e-05],
        [-1.1460e-02, -4.2701e-03, -4.2734e-03,  ...,  3.0041e-03,
         -6.7389e-03, -2.1417e-04],
        [-1.1460e-02, -4.2701e-03, -4.2734e-03,  ...,  3.0041e-03,
         -6.7389e-03, -2.1417e-04],
        ...,
        [-9.5978e-03, -6.1909e-03,  0.0000e+00,  ...,  3.4134e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.5978e-03, -6.1909e-03,  0.0000e+00,  ...,  3.4134e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.5978e-03, -6.1909e-03,  0.0000e+00,  ...,  3.4134e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-44310.2188, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.6199, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(95.2346, device='cuda:0')



h[100].sum tensor(156.2944, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-113.5187, device='cuda:0')



h[200].sum tensor(-538.3168, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.0337, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0128, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0127, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0137, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0137, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0137, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(322998.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1406, 0.0000, 0.0951],
        [0.0000, 0.0000, 0.0000,  ..., 0.1406, 0.0000, 0.0951],
        [0.0000, 0.0000, 0.0000,  ..., 0.1406, 0.0000, 0.0951]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3765396.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-163.6550, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7258.2207, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11499.3955, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2255.2515, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.6109],
        [ 0.6108],
        [ 0.6074],
        ...,
        [-5.3120],
        [-5.3030],
        [-5.3009]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-914247.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(600.6669, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(600.6669, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-46250.6094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.0908, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(74.0943, device='cuda:0')



h[100].sum tensor(148.3069, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-88.3197, device='cuda:0')



h[200].sum tensor(-543.3472, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.1448, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(312081.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1004, 0.0000, 0.0810],
        [0.0000, 0.0000, 0.0000,  ..., 0.0639, 0.0000, 0.0676],
        [0.0000, 0.0000, 0.0000,  ..., 0.0278, 0.0000, 0.0435],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1408, 0.0000, 0.0946],
        [0.0000, 0.0000, 0.0000,  ..., 0.1408, 0.0000, 0.0946],
        [0.0000, 0.0000, 0.0000,  ..., 0.1408, 0.0000, 0.0946]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3699100., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-193.6128, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7181.3809, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9472.3184, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2195.9150, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2730],
        [-0.9293],
        [-0.5606],
        ...,
        [-5.3226],
        [-5.3136],
        [-5.3116]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-932355.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(679.1522, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(679.1522, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1328e-02, -4.4080e-03, -3.8448e-03,  ...,  2.9791e-03,
         -6.6863e-03, -1.8061e-04],
        [-1.0084e-02, -5.6906e-03, -1.0790e-03,  ...,  3.2548e-03,
         -6.1966e-03, -5.0684e-05],
        [-9.5989e-03, -6.1910e-03,  0.0000e+00,  ...,  3.3623e-03,
         -6.0055e-03,  0.0000e+00],
        ...,
        [-9.5989e-03, -6.1910e-03,  0.0000e+00,  ...,  3.3623e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.5989e-03, -6.1910e-03,  0.0000e+00,  ...,  3.3623e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.5989e-03, -6.1910e-03,  0.0000e+00,  ...,  3.3623e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-45838.8438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.8647, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(83.7758, device='cuda:0')



h[100].sum tensor(151.6464, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-99.8599, device='cuda:0')



h[200].sum tensor(-540.0254, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.2996, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0128, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0129, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0129, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(317306.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0134],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0158],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0136],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1408, 0.0000, 0.0948],
        [0.0000, 0.0000, 0.0000,  ..., 0.1408, 0.0000, 0.0948],
        [0.0000, 0.0000, 0.0000,  ..., 0.1408, 0.0000, 0.0948]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3728901., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-171.2183, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7226.1387, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10544.3545, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2186.5056, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.6180],
        [ 0.6058],
        [ 0.5806],
        ...,
        [-5.3136],
        [-5.3039],
        [-5.3013]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-916919.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 170.0 event: 2550 loss: tensor(425.1780, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3137],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(676.3348, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3137],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(676.3348, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.5994e-03, -6.1910e-03,  0.0000e+00,  ...,  3.3384e-03,
         -6.0055e-03,  0.0000e+00],
        [-1.0350e-02, -5.4175e-03, -1.6421e-03,  ...,  3.1716e-03,
         -6.3009e-03, -7.4637e-05],
        [-1.0103e-02, -5.6720e-03, -1.1018e-03,  ...,  3.2265e-03,
         -6.2037e-03, -5.0080e-05],
        ...,
        [-9.5994e-03, -6.1910e-03,  0.0000e+00,  ...,  3.3384e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.5994e-03, -6.1910e-03,  0.0000e+00,  ...,  3.3384e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.5994e-03, -6.1910e-03,  0.0000e+00,  ...,  3.3384e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-46110.3828, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.9125, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(83.4283, device='cuda:0')



h[100].sum tensor(151.0140, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-99.4456, device='cuda:0')



h[200].sum tensor(-539.1750, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.1864, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0130, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0129, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0124, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(320507.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0117, 0.0000, 0.0139],
        [0.0000, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0104],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0007],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1411, 0.0000, 0.0950],
        [0.0000, 0.0000, 0.0000,  ..., 0.1411, 0.0000, 0.0950],
        [0.0000, 0.0000, 0.0000,  ..., 0.1411, 0.0000, 0.0950]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3753992.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-159.1064, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7221.7490, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11049.0303, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2176.3682, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3493],
        [ 0.4044],
        [ 0.3936],
        ...,
        [-5.3524],
        [-5.3436],
        [-5.3417]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-954405.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2839],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(665.7146, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2839],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(665.7146, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.5998e-03, -6.1910e-03,  0.0000e+00,  ...,  3.3079e-03,
         -6.0055e-03,  0.0000e+00],
        [-1.0279e-02, -5.4909e-03, -1.4629e-03,  ...,  3.1574e-03,
         -6.2728e-03, -6.4318e-05],
        [-1.0909e-02, -4.8409e-03, -2.8214e-03,  ...,  3.0177e-03,
         -6.5210e-03, -1.2404e-04],
        ...,
        [-9.5998e-03, -6.1910e-03,  0.0000e+00,  ...,  3.3079e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.5998e-03, -6.1910e-03,  0.0000e+00,  ...,  3.3079e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.5998e-03, -6.1910e-03,  0.0000e+00,  ...,  3.3079e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-46455.6016, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.8141, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(82.1182, device='cuda:0')



h[100].sum tensor(152.3435, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-97.8840, device='cuda:0')



h[200].sum tensor(-538.8845, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.7595, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0131, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0127, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0122, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(317423.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0591, 0.0000, 0.0635],
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0000, 0.0313],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0060],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1409, 0.0000, 0.0947],
        [0.0000, 0.0000, 0.0000,  ..., 0.1409, 0.0000, 0.0947],
        [0.0000, 0.0000, 0.0000,  ..., 0.1409, 0.0000, 0.0947]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3734931.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-168.4219, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7248.0737, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10798.2383, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2113.3752, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9519],
        [-0.6978],
        [ 0.0637],
        ...,
        [-5.3477],
        [-5.3389],
        [-5.3370]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-820510., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3022],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(743.8781, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3022],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(743.8781, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6002e-03, -6.1910e-03,  0.0000e+00,  ...,  3.3181e-03,
         -6.0055e-03,  0.0000e+00],
        [-1.0323e-02, -5.4458e-03, -1.5329e-03,  ...,  3.1586e-03,
         -6.2901e-03, -6.5161e-05],
        [-1.0126e-02, -5.6494e-03, -1.1141e-03,  ...,  3.2022e-03,
         -6.2123e-03, -4.7359e-05],
        ...,
        [-9.6002e-03, -6.1910e-03,  0.0000e+00,  ...,  3.3181e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6002e-03, -6.1910e-03,  0.0000e+00,  ...,  3.3181e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6002e-03, -6.1910e-03,  0.0000e+00,  ...,  3.3181e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-46161.9727, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.5690, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(91.7599, device='cuda:0')



h[100].sum tensor(155.5521, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-109.3769, device='cuda:0')



h[200].sum tensor(-535.5776, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.9014, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0131, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0129, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0124, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(323601.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0632, 0.0000, 0.0679],
        [0.0000, 0.0000, 0.0000,  ..., 0.0293, 0.0000, 0.0433],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0156],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1425, 0.0000, 0.0949],
        [0.0000, 0.0000, 0.0000,  ..., 0.1425, 0.0000, 0.0949],
        [0.0000, 0.0000, 0.0000,  ..., 0.1425, 0.0000, 0.0949]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3799274.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-161.7151, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7313.0249, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12354.0020, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2114.9033, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1167],
        [-0.3820],
        [ 0.0313],
        ...,
        [-5.4063],
        [-5.3974],
        [-5.3955]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-826848.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(617.3267, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(617.3267, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0108, -0.0050, -0.0025,  ...,  0.0031, -0.0065, -0.0001],
        [-0.0108, -0.0050, -0.0025,  ...,  0.0031, -0.0065, -0.0001],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-47763.3281, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.4396, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(76.1494, device='cuda:0')



h[100].sum tensor(148.1622, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-90.7693, device='cuda:0')



h[200].sum tensor(-538.9634, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.8144, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0127, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0128, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0126, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(313353.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0054],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0035],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1448, 0.0000, 0.0950],
        [0.0000, 0.0000, 0.0000,  ..., 0.1448, 0.0000, 0.0950],
        [0.0000, 0.0000, 0.0000,  ..., 0.1448, 0.0000, 0.0950]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3721138.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-212.2241, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7219.4756, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9554.6113, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2084.6611, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5217],
        [ 0.5427],
        [ 0.5636],
        ...,
        [-5.4628],
        [-5.4747],
        [-5.4769]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1004466.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(752.8810, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(752.8810, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-46474.6719, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.7330, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(92.8705, device='cuda:0')



h[100].sum tensor(156.2465, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-110.7006, device='cuda:0')



h[200].sum tensor(-534.3430, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.2633, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(323327.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1444, 0.0000, 0.0956],
        [0.0000, 0.0000, 0.0000,  ..., 0.1409, 0.0000, 0.0945],
        [0.0000, 0.0000, 0.0000,  ..., 0.1275, 0.0000, 0.0905],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1448, 0.0000, 0.0950],
        [0.0000, 0.0000, 0.0000,  ..., 0.1448, 0.0000, 0.0950],
        [0.0000, 0.0000, 0.0000,  ..., 0.1448, 0.0000, 0.0950]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3770969., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-187.5889, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7238.0605, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10624.4473, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2129.2200, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.1992],
        [-4.5828],
        [-3.6496],
        ...,
        [-5.4896],
        [-5.4801],
        [-5.4780]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-977744.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2744],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(692.5428, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2744],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(692.5428, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6008e-03, -6.1910e-03,  0.0000e+00,  ...,  3.3217e-03,
         -6.0055e-03,  0.0000e+00],
        [-1.0257e-02, -5.5145e-03, -1.3482e-03,  ...,  3.1796e-03,
         -6.2639e-03, -5.3516e-05],
        [-9.6008e-03, -6.1910e-03,  0.0000e+00,  ...,  3.3217e-03,
         -6.0055e-03,  0.0000e+00],
        ...,
        [-9.6008e-03, -6.1910e-03,  0.0000e+00,  ...,  3.3217e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6008e-03, -6.1910e-03,  0.0000e+00,  ...,  3.3217e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6008e-03, -6.1910e-03,  0.0000e+00,  ...,  3.3217e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-47279.5078, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.1908, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(85.4276, device='cuda:0')



h[100].sum tensor(158.0219, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-101.8288, device='cuda:0')



h[200].sum tensor(-535.6573, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.8379, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0131, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0131, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0126, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(319840.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0791, 0.0000, 0.0728],
        [0.0000, 0.0000, 0.0000,  ..., 0.0536, 0.0000, 0.0606],
        [0.0000, 0.0000, 0.0000,  ..., 0.0090, 0.0000, 0.0424],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1451, 0.0000, 0.0941],
        [0.0000, 0.0000, 0.0000,  ..., 0.1451, 0.0000, 0.0941],
        [0.0000, 0.0000, 0.0000,  ..., 0.1451, 0.0000, 0.0941]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3752994., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-222.4360, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7230.8335, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9288.8691, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2066.7136, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2128],
        [-1.4690],
        [-1.0564],
        ...,
        [-5.4604],
        [-5.4513],
        [-5.4493]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-788945.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(844.3544, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(844.3544, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0033, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0033, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0033, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0033, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0033, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0033, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-46183.2734, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.6901, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(104.1540, device='cuda:0')



h[100].sum tensor(156.1677, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-124.1505, device='cuda:0')



h[200].sum tensor(-529.6803, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.9402, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(332642.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1312, 0.0000, 0.0916],
        [0.0000, 0.0000, 0.0000,  ..., 0.1432, 0.0000, 0.0954],
        [0.0000, 0.0000, 0.0000,  ..., 0.1467, 0.0000, 0.0969],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1473, 0.0000, 0.0960],
        [0.0000, 0.0000, 0.0000,  ..., 0.1473, 0.0000, 0.0960],
        [0.0000, 0.0000, 0.0000,  ..., 0.1473, 0.0000, 0.0960]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3849227.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-164.7024, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7374.2080, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13689.3770, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2146.2461, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.3926],
        [-5.6805],
        [-5.7227],
        ...,
        [-5.6111],
        [-5.6013],
        [-5.5990]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1154899., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(695.6584, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(695.6584, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0032, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0032, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0032, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0032, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0032, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0032, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-47649.4570, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.2726, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(85.8119, device='cuda:0')



h[100].sum tensor(163.7387, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-102.2869, device='cuda:0')



h[200].sum tensor(-534.5483, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.9631, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0129, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0129, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0129, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0129, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0129, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0129, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(323015.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1400, 0.0000, 0.0916],
        [0.0000, 0.0000, 0.0000,  ..., 0.1390, 0.0000, 0.0913],
        [0.0000, 0.0000, 0.0000,  ..., 0.1380, 0.0000, 0.0912],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1408, 0.0000, 0.0914],
        [0.0000, 0.0000, 0.0000,  ..., 0.1438, 0.0000, 0.0924],
        [0.0000, 0.0000, 0.0000,  ..., 0.1438, 0.0000, 0.0924]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3765886., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-149.1416, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7269.0371, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11314.3008, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2006.4165, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.2000],
        [-4.2964],
        [-4.2494],
        ...,
        [-4.1255],
        [-4.9262],
        [-5.2817]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-719968., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(588.2256, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(588.2256, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0032, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0032, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0032, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0032, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0032, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0032, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-48847.1094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.2773, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(72.5597, device='cuda:0')



h[100].sum tensor(169.0779, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-86.4904, device='cuda:0')



h[200].sum tensor(-538.0461, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.6447, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0130, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0130, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0130, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0130, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0130, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0130, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(312908.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1426, 0.0000, 0.0904],
        [0.0000, 0.0000, 0.0000,  ..., 0.1426, 0.0000, 0.0904],
        [0.0000, 0.0000, 0.0000,  ..., 0.1424, 0.0000, 0.0907],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1430, 0.0000, 0.0898],
        [0.0000, 0.0000, 0.0000,  ..., 0.1430, 0.0000, 0.0898],
        [0.0000, 0.0000, 0.0000,  ..., 0.1430, 0.0000, 0.0898]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3675702.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-159.0822, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7141.7505, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8009.9004, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1895.3854, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.2073],
        [-5.0064],
        [-4.5742],
        ...,
        [-5.3051],
        [-5.2979],
        [-5.2965]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-450206.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.2742]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(767.4426, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.2742]], device='cuda:0') 
g.ndata[nfet].sum tensor(767.4426, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6018e-03, -6.1912e-03,  0.0000e+00,  ...,  3.4783e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6018e-03, -6.1912e-03,  0.0000e+00,  ...,  3.4783e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6018e-03, -6.1912e-03,  0.0000e+00,  ...,  3.4783e-03,
         -6.0055e-03,  0.0000e+00],
        ...,
        [-9.6018e-03, -6.1912e-03,  0.0000e+00,  ...,  3.4783e-03,
         -6.0055e-03,  0.0000e+00],
        [-1.0963e-02, -4.7889e-03, -2.6213e-03,  ...,  3.1848e-03,
         -6.5412e-03, -9.0300e-05],
        [-1.0307e-02, -5.4646e-03, -1.3582e-03,  ...,  3.3262e-03,
         -6.2831e-03, -4.6789e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-47801.2109, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.9858, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(94.6667, device='cuda:0')



h[100].sum tensor(159.5085, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-112.8417, device='cuda:0')



h[200].sum tensor(-532.0184, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.8486, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0139, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0139, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0129, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(322775.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0884, 0.0000, 0.0738],
        [0.0000, 0.0000, 0.0000,  ..., 0.1318, 0.0000, 0.0886],
        [0.0000, 0.0000, 0.0000,  ..., 0.1394, 0.0000, 0.0913],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0340, 0.0000, 0.0360],
        [0.0000, 0.0000, 0.0000,  ..., 0.0111, 0.0000, 0.0152],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0038]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3748798., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-94.7468, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7257.8911, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13813.7578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2024.1392, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.3357],
        [-4.0902],
        [-4.1505],
        ...,
        [-1.1581],
        [-0.0185],
        [ 0.5719]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-893638.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 180.0 event: 2700 loss: tensor(453.6801, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4451],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(686.5952, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4451],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(686.5952, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0667e-02, -5.0945e-03, -2.0174e-03,  ...,  3.4157e-03,
         -6.4245e-03, -6.7005e-05],
        [-9.6020e-03, -6.1913e-03,  0.0000e+00,  ...,  3.6387e-03,
         -6.0055e-03,  0.0000e+00],
        [-1.0667e-02, -5.0945e-03, -2.0174e-03,  ...,  3.4157e-03,
         -6.4245e-03, -6.7005e-05],
        ...,
        [-1.0766e-02, -4.9917e-03, -2.2065e-03,  ...,  3.3948e-03,
         -6.4638e-03, -7.3286e-05],
        [-1.0766e-02, -4.9917e-03, -2.2065e-03,  ...,  3.3948e-03,
         -6.4638e-03, -7.3286e-05],
        [-9.6020e-03, -6.1913e-03,  0.0000e+00,  ...,  3.6387e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-49096.4453, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.2389, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(84.6939, device='cuda:0')



h[100].sum tensor(146.2180, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-100.9542, device='cuda:0')



h[200].sum tensor(-534.6743, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.5988, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(314303.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0452, 0.0000, 0.0526],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0138],
        [0.0000, 0.0000, 0.0000,  ..., 0.0011, 0.0000, 0.0094],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0181, 0.0000, 0.0383],
        [0.0000, 0.0000, 0.0000,  ..., 0.0181, 0.0000, 0.0383],
        [0.0000, 0.0000, 0.0000,  ..., 0.0423, 0.0000, 0.0482]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3680469., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-111.6082, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7151.4507, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12911.3828, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2039.5676, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3507],
        [ 0.1585],
        [ 0.3971],
        ...,
        [-2.2836],
        [-2.2762],
        [-2.9533]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1112298., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(656.0895, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(656.0895, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6022e-03, -6.1914e-03,  0.0000e+00,  ...,  3.6124e-03,
         -6.0055e-03,  0.0000e+00],
        [-1.0130e-02, -5.6482e-03, -9.8321e-04,  ...,  3.5039e-03,
         -6.2131e-03, -3.1471e-05],
        [-1.1203e-02, -4.5425e-03, -2.9844e-03,  ...,  3.2831e-03,
         -6.6355e-03, -9.5526e-05],
        ...,
        [-9.6022e-03, -6.1914e-03,  0.0000e+00,  ...,  3.6124e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6022e-03, -6.1914e-03,  0.0000e+00,  ...,  3.6124e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6022e-03, -6.1914e-03,  0.0000e+00,  ...,  3.6124e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-49624.7969, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.9707, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(80.9309, device='cuda:0')



h[100].sum tensor(152.1489, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-96.4688, device='cuda:0')



h[200].sum tensor(-535.3400, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.3726, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0139, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(309134.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0000, 0.0255],
        [0.0000, 0.0000, 0.0000,  ..., 0.0032, 0.0000, 0.0102],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0031],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1127, 0.0000, 0.0803],
        [0.0000, 0.0000, 0.0000,  ..., 0.1388, 0.0000, 0.0894],
        [0.0000, 0.0000, 0.0000,  ..., 0.1518, 0.0000, 0.0940]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3628007.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-141.5499, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6996.9595, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10067.6621, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1991.1958, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4458],
        [ 0.6355],
        [ 0.7307],
        ...,
        [-3.5773],
        [-4.4268],
        [-5.0458]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-872404.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(736.2992, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(736.2992, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-48837.8281, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.7527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(90.8251, device='cuda:0')



h[100].sum tensor(157.0739, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-108.2625, device='cuda:0')



h[200].sum tensor(-532.5406, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.5967, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(319170., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1217, 0.0000, 0.0841],
        [0.0000, 0.0000, 0.0000,  ..., 0.1350, 0.0000, 0.0886],
        [0.0000, 0.0000, 0.0000,  ..., 0.1407, 0.0000, 0.0908],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1518, 0.0000, 0.0940],
        [0.0000, 0.0000, 0.0000,  ..., 0.1518, 0.0000, 0.0940],
        [0.0000, 0.0000, 0.0000,  ..., 0.1518, 0.0000, 0.0940]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3710655.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-116.0203, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7045.3828, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12048.2568, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2023.9844, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.9255],
        [-3.3426],
        [-3.6890],
        ...,
        [-5.5667],
        [-5.5577],
        [-5.5557]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-822000.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(606.7572, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(606.7572, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-50312.0508, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.5321, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(74.8456, device='cuda:0')



h[100].sum tensor(157.5920, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-89.2152, device='cuda:0')



h[200].sum tensor(-536.2528, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.3896, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(304261.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0405, 0.0000, 0.0490],
        [0.0000, 0.0000, 0.0000,  ..., 0.0685, 0.0000, 0.0644],
        [0.0000, 0.0000, 0.0000,  ..., 0.0355, 0.0000, 0.0486],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1519, 0.0000, 0.0932],
        [0.0000, 0.0000, 0.0000,  ..., 0.1519, 0.0000, 0.0932],
        [0.0000, 0.0000, 0.0000,  ..., 0.1519, 0.0000, 0.0932]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3574519.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-183.8403, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6799.1313, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7166.0654, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1970.2891, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.8362e-01],
        [-8.5148e-04],
        [-1.7644e-02],
        ...,
        [-5.5066e+00],
        [-5.4982e+00],
        [-5.4966e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-709480.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5459],
        [0.4412],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(626.6273, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5459],
        [0.4412],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(626.6273, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2988e-02, -2.7050e-03, -6.1085e-03,  ...,  3.0639e-03,
         -7.3379e-03, -1.8135e-04],
        [-1.0658e-02, -5.1048e-03, -1.9041e-03,  ...,  3.5372e-03,
         -6.4208e-03, -5.6527e-05],
        [-1.1989e-02, -3.7341e-03, -4.3055e-03,  ...,  3.2669e-03,
         -6.9446e-03, -1.2782e-04],
        ...,
        [-9.6025e-03, -6.1915e-03,  0.0000e+00,  ...,  3.7515e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6025e-03, -6.1915e-03,  0.0000e+00,  ...,  3.7515e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6025e-03, -6.1915e-03,  0.0000e+00,  ...,  3.7515e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-50625.1875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.7081, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(77.2967, device='cuda:0')



h[100].sum tensor(141.0352, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-92.1368, device='cuda:0')



h[200].sum tensor(-534.7930, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.1883, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0131, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(303605.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1563, 0.0000, 0.0961],
        [0.0000, 0.0000, 0.0000,  ..., 0.1563, 0.0000, 0.0961],
        [0.0000, 0.0000, 0.0000,  ..., 0.1563, 0.0000, 0.0961]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3573461., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-157.4402, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6743.5029, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9050.2559, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2021.0430, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.7307],
        [ 0.7361],
        [ 0.7330],
        ...,
        [-5.6063],
        [-5.5973],
        [-5.5954]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-951241.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3396],
        [0.2639],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(658.0156, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3396],
        [0.2639],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(658.0156, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0770e-02, -4.9896e-03, -2.0717e-03,  ...,  3.5512e-03,
         -6.4649e-03, -5.9190e-05],
        [-1.0951e-02, -4.8032e-03, -2.3931e-03,  ...,  3.5133e-03,
         -6.5362e-03, -6.8371e-05],
        [-1.0890e-02, -4.8656e-03, -2.2855e-03,  ...,  3.5260e-03,
         -6.5123e-03, -6.5298e-05],
        ...,
        [-9.6026e-03, -6.1916e-03,  0.0000e+00,  ...,  3.7955e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6026e-03, -6.1916e-03,  0.0000e+00,  ...,  3.7955e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6026e-03, -6.1916e-03,  0.0000e+00,  ...,  3.7955e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-50702.8672, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.0436, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(81.1685, device='cuda:0')



h[100].sum tensor(133.0921, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-96.7520, device='cuda:0')



h[200].sum tensor(-532.2763, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.4500, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(307793.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0122],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0235],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1575, 0.0000, 0.0978],
        [0.0000, 0.0000, 0.0000,  ..., 0.1575, 0.0000, 0.0978],
        [0.0000, 0.0000, 0.0000,  ..., 0.1575, 0.0000, 0.0978]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3610428.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-109.7903, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6721.5044, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11778.1123, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2062.3489, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4027],
        [ 0.1582],
        [ 0.3299],
        ...,
        [-5.4726],
        [-5.5889],
        [-5.6219]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1009834.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(577.7657, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(577.7657, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6027e-03, -6.1916e-03,  0.0000e+00,  ...,  3.6449e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6027e-03, -6.1916e-03,  0.0000e+00,  ...,  3.6449e-03,
         -6.0055e-03,  0.0000e+00],
        [-1.0259e-02, -5.5156e-03, -1.1463e-03,  ...,  3.4993e-03,
         -6.2639e-03, -3.1502e-05],
        ...,
        [-9.6027e-03, -6.1916e-03,  0.0000e+00,  ...,  3.6449e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6027e-03, -6.1916e-03,  0.0000e+00,  ...,  3.6449e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6027e-03, -6.1916e-03,  0.0000e+00,  ...,  3.6449e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-51617.8516, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.2941, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(71.2694, device='cuda:0')



h[100].sum tensor(136.4072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-84.9524, device='cuda:0')



h[200].sum tensor(-533.5529, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.2242, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0139, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(299947.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0979, 0.0000, 0.0783],
        [0.0000, 0.0000, 0.0000,  ..., 0.0551, 0.0000, 0.0574],
        [0.0000, 0.0000, 0.0000,  ..., 0.0165, 0.0000, 0.0295],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1540, 0.0000, 0.0970],
        [0.0000, 0.0000, 0.0000,  ..., 0.1540, 0.0000, 0.0970],
        [0.0000, 0.0000, 0.0000,  ..., 0.1540, 0.0000, 0.0970]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3534308., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-112.7125, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6633.0801, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10162.2109, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2004.1420, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3055],
        [-1.0515],
        [-0.5897],
        ...,
        [-5.5470],
        [-5.5373],
        [-5.5344]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-851360.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1021.8862, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1021.8862, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-47584.1367, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-988.5923, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(126.0532, device='cuda:0')



h[100].sum tensor(172.2877, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-150.2541, device='cuda:0')



h[200].sum tensor(-516.6629, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(41.0764, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(344581., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1394, 0.0000, 0.0927],
        [0.0000, 0.0000, 0.0000,  ..., 0.1491, 0.0000, 0.0961],
        [0.0000, 0.0000, 0.0000,  ..., 0.1503, 0.0000, 0.0970],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1513, 0.0000, 0.0963],
        [0.0000, 0.0000, 0.0000,  ..., 0.1513, 0.0000, 0.0963],
        [0.0000, 0.0000, 0.0000,  ..., 0.1513, 0.0000, 0.0963]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3836189.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-31.8883, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6697.6279, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(16167.2822, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2147.1389, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.0238],
        [-3.8192],
        [-4.2140],
        ...,
        [-5.4955],
        [-5.4875],
        [-5.4859]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-630951.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(671.1691, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(671.1691, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-51596.9727, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.1697, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(82.7910, device='cuda:0')



h[100].sum tensor(147.4017, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-98.6860, device='cuda:0')



h[200].sum tensor(-528.5226, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.9787, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(304541.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1275, 0.0000, 0.0894],
        [0.0000, 0.0000, 0.0000,  ..., 0.1492, 0.0000, 0.0971],
        [0.0000, 0.0000, 0.0000,  ..., 0.1382, 0.0000, 0.0937],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1531, 0.0000, 0.0976],
        [0.0000, 0.0000, 0.0000,  ..., 0.1531, 0.0000, 0.0976],
        [0.0000, 0.0000, 0.0000,  ..., 0.1531, 0.0000, 0.0976]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3542463.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-107.9620, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6516.2939, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9539.4971, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2041.8479, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2697],
        [-1.4827],
        [-1.2406],
        ...,
        [-5.5594],
        [-5.5511],
        [-5.5495]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-753350.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3145],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(939.4324, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3145],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(939.4324, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6030e-03, -6.1918e-03,  0.0000e+00,  ...,  3.6583e-03,
         -6.0055e-03,  0.0000e+00],
        [-1.1995e-02, -3.7298e-03, -3.9719e-03,  ...,  3.0627e-03,
         -6.9467e-03, -9.6849e-05],
        [-1.1242e-02, -4.5042e-03, -2.7227e-03,  ...,  3.2500e-03,
         -6.6507e-03, -6.6388e-05],
        ...,
        [-9.6030e-03, -6.1918e-03,  0.0000e+00,  ...,  3.6583e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6030e-03, -6.1918e-03,  0.0000e+00,  ...,  3.6583e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6030e-03, -6.1918e-03,  0.0000e+00,  ...,  3.6583e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-49730.6797, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-987.6680, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(115.8822, device='cuda:0')



h[100].sum tensor(148.0380, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-138.1304, device='cuda:0')



h[200].sum tensor(-519.3346, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.7620, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0123, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0137, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0137, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(325941.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0105],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1577, 0.0000, 0.1014],
        [0.0000, 0.0000, 0.0000,  ..., 0.1577, 0.0000, 0.1014],
        [0.0000, 0.0000, 0.0000,  ..., 0.1577, 0.0000, 0.1014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3686875.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-11.8013, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6515.4683, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13370.5332, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2185.2700, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4354],
        [ 0.3361],
        [-0.1540],
        ...,
        [-5.7135],
        [-5.7044],
        [-5.7025]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1022953.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 190.0 event: 2850 loss: tensor(436.3797, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3274],
        [0.3391],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(682.7941, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3274],
        [0.3391],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(682.7941, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1213e-02, -4.5344e-03, -2.6297e-03,  ...,  3.2458e-03,
         -6.6392e-03, -6.1547e-05],
        [-1.0386e-02, -5.3857e-03, -1.2791e-03,  ...,  3.4616e-03,
         -6.3137e-03, -2.9936e-05],
        [-1.0414e-02, -5.3568e-03, -1.3248e-03,  ...,  3.4542e-03,
         -6.3248e-03, -3.1008e-05],
        ...,
        [-9.6031e-03, -6.1918e-03,  0.0000e+00,  ...,  3.6659e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6031e-03, -6.1918e-03,  0.0000e+00,  ...,  3.6659e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6031e-03, -6.1918e-03,  0.0000e+00,  ...,  3.6659e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-52455.6445, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.3262, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(84.2250, device='cuda:0')



h[100].sum tensor(128.5841, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-100.3953, device='cuda:0')



h[200].sum tensor(-527.9656, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.4460, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(300206.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0193],
        [0.0000, 0.0000, 0.0000,  ..., 0.0321, 0.0000, 0.0500],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1590, 0.0000, 0.1024],
        [0.0000, 0.0000, 0.0000,  ..., 0.1590, 0.0000, 0.1024],
        [0.0000, 0.0000, 0.0000,  ..., 0.1590, 0.0000, 0.1024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3486180., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-132.0085, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6580.1084, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8839.1143, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2087.6736, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5099],
        [ 0.3564],
        [ 0.1155],
        ...,
        [-5.7696],
        [-5.7603],
        [-5.7583]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1159827.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(806.7513, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(806.7513, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-51280.9219, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.4612, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(99.5156, device='cuda:0')



h[100].sum tensor(144.6142, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-118.6215, device='cuda:0')



h[200].sum tensor(-523.8918, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.4287, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(314465.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0745, 0.0000, 0.0658],
        [0.0000, 0.0000, 0.0000,  ..., 0.1340, 0.0000, 0.0931],
        [0.0000, 0.0000, 0.0000,  ..., 0.1514, 0.0000, 0.0995],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1547, 0.0000, 0.0994],
        [0.0000, 0.0000, 0.0000,  ..., 0.1547, 0.0000, 0.0994],
        [0.0000, 0.0000, 0.0000,  ..., 0.1547, 0.0000, 0.0994]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3567465.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-184.2341, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6891.7451, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10926.9600, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2034.2990, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3808],
        [-2.6786],
        [-3.6571],
        ...,
        [-5.6681],
        [-5.6595],
        [-5.6578]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-804834.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.7583],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(749.2865, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.7583],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(749.2865, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6032e-03, -6.1919e-03,  0.0000e+00,  ...,  3.4109e-03,
         -6.0055e-03,  0.0000e+00],
        [-1.1417e-02, -4.3250e-03, -2.8644e-03,  ...,  2.8824e-03,
         -6.7194e-03, -6.1663e-05],
        [-9.6032e-03, -6.1919e-03,  0.0000e+00,  ...,  3.4109e-03,
         -6.0055e-03,  0.0000e+00],
        ...,
        [-9.6032e-03, -6.1919e-03,  0.0000e+00,  ...,  3.4109e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6032e-03, -6.1919e-03,  0.0000e+00,  ...,  3.4109e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6032e-03, -6.1919e-03,  0.0000e+00,  ...,  3.4109e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-51931.8750, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.9705, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(92.4271, device='cuda:0')



h[100].sum tensor(141.0401, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-110.1721, device='cuda:0')



h[200].sum tensor(-525.7202, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.1188, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0124, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0111, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(311487.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0124],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0107],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1530, 0.0000, 0.0991],
        [0.0000, 0.0000, 0.0000,  ..., 0.1530, 0.0000, 0.0991],
        [0.0000, 0.0000, 0.0000,  ..., 0.1530, 0.0000, 0.0991]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3545157., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-39.1562, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6917.5693, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11626.4375, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2043.6592, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.6127],
        [ 0.6441],
        [ 0.6526],
        ...,
        [-5.6514],
        [-5.6430],
        [-5.6413]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-758269.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5645],
        [0.3079],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(864.6859, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5645],
        [0.3079],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(864.6859, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2083e-02, -3.6400e-03, -3.8499e-03,  ...,  2.6768e-03,
         -6.9814e-03, -7.9411e-05],
        [-1.1450e-02, -4.2909e-03, -2.8679e-03,  ...,  2.8686e-03,
         -6.7325e-03, -5.9155e-05],
        [-1.0340e-02, -5.4340e-03, -1.1434e-03,  ...,  3.2053e-03,
         -6.2953e-03, -2.3585e-05],
        ...,
        [-9.6032e-03, -6.1919e-03,  0.0000e+00,  ...,  3.4285e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6032e-03, -6.1919e-03,  0.0000e+00,  ...,  3.4285e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6032e-03, -6.1919e-03,  0.0000e+00,  ...,  3.4285e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-51152.6172, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-987.0671, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(106.6620, device='cuda:0')



h[100].sum tensor(137.4149, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-127.1400, device='cuda:0')



h[200].sum tensor(-521.9697, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.7574, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0106, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0114, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0127, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0137, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0137, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0137, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(323055.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0126, 0.0000, 0.0234],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1548, 0.0000, 0.1014],
        [0.0000, 0.0000, 0.0000,  ..., 0.1548, 0.0000, 0.1014],
        [0.0000, 0.0000, 0.0000,  ..., 0.1548, 0.0000, 0.1014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3625633., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(222.2566, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6888.0977, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15160.0586, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2180.6755, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4478],
        [ 0.3325],
        [-0.1167],
        ...,
        [-5.7360],
        [-5.7273],
        [-5.7254]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-881410.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(639.9918, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(639.9918, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-53834.3359, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.9716, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(78.9452, device='cuda:0')



h[100].sum tensor(119.8323, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-94.1019, device='cuda:0')



h[200].sum tensor(-529.9302, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.7255, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(299254.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1194, 0.0000, 0.0893],
        [0.0000, 0.0000, 0.0000,  ..., 0.1426, 0.0000, 0.0972],
        [0.0000, 0.0000, 0.0000,  ..., 0.1521, 0.0000, 0.1009],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1569, 0.0000, 0.1014],
        [0.0000, 0.0000, 0.0000,  ..., 0.1569, 0.0000, 0.1014],
        [0.0000, 0.0000, 0.0000,  ..., 0.1569, 0.0000, 0.1014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3456684.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(235.9169, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6646.1274, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10614.3584, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2178.6121, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0771],
        [-3.2168],
        [-4.1587],
        ...,
        [-5.8038],
        [-5.7947],
        [-5.7928]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1048475.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5654],
        [0.6240],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(734.3351, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5654],
        [0.6240],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(734.3351, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3797e-02, -1.8766e-03, -6.2931e-03,  ...,  2.0849e-03,
         -7.6559e-03, -1.1895e-04],
        [-1.3572e-02, -2.1081e-03, -5.9555e-03,  ...,  2.1564e-03,
         -7.5674e-03, -1.1257e-04],
        [-1.2357e-02, -3.3584e-03, -4.1321e-03,  ...,  2.5424e-03,
         -7.0892e-03, -7.8106e-05],
        ...,
        [-1.0907e-02, -4.8506e-03, -1.9561e-03,  ...,  3.0032e-03,
         -6.5185e-03, -3.6975e-05],
        [-9.6033e-03, -6.1920e-03,  0.0000e+00,  ...,  3.4173e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6033e-03, -6.1920e-03,  0.0000e+00,  ...,  3.4173e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-53351.7891, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.8354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(90.5828, device='cuda:0')



h[100].sum tensor(126.4941, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-107.9737, device='cuda:0')



h[200].sum tensor(-526.7808, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.5178, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0084, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0091, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0107, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0130, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0131, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0137, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(301908.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0000, 0.0213],
        [0.0000, 0.0000, 0.0000,  ..., 0.0401, 0.0000, 0.0432],
        [0.0000, 0.0000, 0.0000,  ..., 0.0999, 0.0000, 0.0802]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3453922.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(225.4244, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6589.8115, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10043.1865, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2187.7275, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4220],
        [ 0.4749],
        [ 0.5527],
        ...,
        [-0.0673],
        [-1.2557],
        [-2.9835]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1005187.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(723.8710, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(723.8710, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0033, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0033, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0033, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0033, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0033, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0033, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-53711.7500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.7249, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(89.2920, device='cuda:0')



h[100].sum tensor(130.6591, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-106.4351, device='cuda:0')



h[200].sum tensor(-526.4758, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.0972, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(300931.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1499, 0.0000, 0.0973],
        [0.0000, 0.0000, 0.0000,  ..., 0.1499, 0.0000, 0.0973],
        [0.0000, 0.0000, 0.0000,  ..., 0.1494, 0.0000, 0.0975],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1540, 0.0000, 0.0984],
        [0.0000, 0.0000, 0.0000,  ..., 0.1540, 0.0000, 0.0984],
        [0.0000, 0.0000, 0.0000,  ..., 0.1540, 0.0000, 0.0984]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3453691., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(178.5133, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6567.8198, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10227.5840, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2113.4854, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.1345],
        [-4.4289],
        [-4.7185],
        ...,
        [-5.7590],
        [-5.7504],
        [-5.7487]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-861621.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2944],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(756.7474, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2944],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(756.7474, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0843e-02, -4.9168e-03, -1.7972e-03,  ...,  2.8896e-03,
         -6.4933e-03, -3.1047e-05],
        [-1.0138e-02, -5.6415e-03, -7.7585e-04,  ...,  3.1196e-03,
         -6.2161e-03, -1.3403e-05],
        [-1.0308e-02, -5.4673e-03, -1.0214e-03,  ...,  3.0643e-03,
         -6.2827e-03, -1.7644e-05],
        ...,
        [-9.6034e-03, -6.1920e-03,  0.0000e+00,  ...,  3.2943e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6034e-03, -6.1920e-03,  0.0000e+00,  ...,  3.2943e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6034e-03, -6.1920e-03,  0.0000e+00,  ...,  3.2943e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-53589.0625, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.0752, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(93.3474, device='cuda:0')



h[100].sum tensor(132.0992, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-111.2691, device='cuda:0')



h[200].sum tensor(-524.3318, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.4187, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0124, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0120, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0127, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(307358.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0297],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0324],
        [0.0000, 0.0000, 0.0000,  ..., 0.0421, 0.0000, 0.0565],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1526, 0.0000, 0.0983],
        [0.0000, 0.0000, 0.0000,  ..., 0.1526, 0.0000, 0.0983],
        [0.0000, 0.0000, 0.0000,  ..., 0.1526, 0.0000, 0.0983]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3501867.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(232.4221, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6615.0156, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12452.8359, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2088.8364, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2604],
        [-0.3211],
        [-1.2275],
        ...,
        [-5.7435],
        [-5.7350],
        [-5.7332]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-801589.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(673.5757, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(673.5757, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0204e-02, -5.5738e-03, -8.5639e-04,  ...,  3.1057e-03,
         -6.2420e-03, -1.4128e-05],
        [-1.0204e-02, -5.5738e-03, -8.5639e-04,  ...,  3.1057e-03,
         -6.2420e-03, -1.4128e-05],
        [-9.6034e-03, -6.1921e-03,  0.0000e+00,  ...,  3.3026e-03,
         -6.0055e-03,  0.0000e+00],
        ...,
        [-9.6034e-03, -6.1921e-03,  0.0000e+00,  ...,  3.3026e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6034e-03, -6.1921e-03,  0.0000e+00,  ...,  3.3026e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6034e-03, -6.1921e-03,  0.0000e+00,  ...,  3.3026e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-54693.9805, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.2991, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(83.0879, device='cuda:0')



h[100].sum tensor(122.8861, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-99.0399, device='cuda:0')



h[200].sum tensor(-526.2927, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.0755, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0127, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0126, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0127, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(300976.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0268, 0.0000, 0.0535],
        [0.0000, 0.0000, 0.0000,  ..., 0.0215, 0.0000, 0.0479],
        [0.0000, 0.0000, 0.0000,  ..., 0.0462, 0.0000, 0.0592],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1529, 0.0000, 0.0990],
        [0.0000, 0.0000, 0.0000,  ..., 0.1529, 0.0000, 0.0990],
        [0.0000, 0.0000, 0.0000,  ..., 0.1529, 0.0000, 0.0990]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3463118.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(263.3351, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6584.6650, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12368.6250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2061.4929, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9745],
        [-0.8870],
        [-1.4821],
        ...,
        [-5.7809],
        [-5.7724],
        [-5.7707]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-875820.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(720.0071, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(720.0071, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0033, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0033, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0033, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0033, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0033, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0033, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-54536.7383, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.7441, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(88.8154, device='cuda:0')



h[100].sum tensor(125.3459, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-105.8670, device='cuda:0')



h[200].sum tensor(-524.1932, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.9418, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0130, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(303729.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1494, 0.0000, 0.0983],
        [0.0000, 0.0000, 0.0000,  ..., 0.1053, 0.0000, 0.0830],
        [0.0000, 0.0000, 0.0000,  ..., 0.0504, 0.0000, 0.0473],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1534, 0.0000, 0.0988],
        [0.0000, 0.0000, 0.0000,  ..., 0.1534, 0.0000, 0.0988],
        [0.0000, 0.0000, 0.0000,  ..., 0.1534, 0.0000, 0.0988]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3467150.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(288.8159, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6595.2139, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12286.7949, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2059.4399, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1499],
        [-1.1001],
        [-0.3683],
        ...,
        [-5.8143],
        [-5.8058],
        [-5.8041]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-915009.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 200.0 event: 3000 loss: tensor(492.0155, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(715.7893, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(715.7893, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0033, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0033, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0033, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0033, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0033, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0033, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-54947.5156, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.6646, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(88.2951, device='cuda:0')



h[100].sum tensor(125.5886, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-105.2468, device='cuda:0')



h[200].sum tensor(-524.0357, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.7723, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(302162.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1004, 0.0000, 0.0796],
        [0.0000, 0.0000, 0.0000,  ..., 0.1367, 0.0000, 0.0925],
        [0.0000, 0.0000, 0.0000,  ..., 0.1393, 0.0000, 0.0942],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1547, 0.0000, 0.0984],
        [0.0000, 0.0000, 0.0000,  ..., 0.1547, 0.0000, 0.0984],
        [0.0000, 0.0000, 0.0000,  ..., 0.1547, 0.0000, 0.0984]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3449314.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(287.3761, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6566.5791, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11043.9570, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2067.2617, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3416],
        [-2.4767],
        [-3.2714],
        ...,
        [-5.8621],
        [-5.8544],
        [-5.8536]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-965403., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(742.8186, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(742.8186, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-54995.9688, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.9130, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(91.6293, device='cuda:0')



h[100].sum tensor(128.9789, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-109.2211, device='cuda:0')



h[200].sum tensor(-522.9111, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.8588, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(300061.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1517, 0.0000, 0.0971],
        [0.0000, 0.0000, 0.0000,  ..., 0.1362, 0.0000, 0.0919],
        [0.0000, 0.0000, 0.0000,  ..., 0.0948, 0.0000, 0.0785],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1561, 0.0000, 0.0978],
        [0.0000, 0.0000, 0.0000,  ..., 0.1561, 0.0000, 0.0978],
        [0.0000, 0.0000, 0.0000,  ..., 0.1561, 0.0000, 0.0978]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3410264.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(278.2429, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6553.7681, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9390.2402, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2065.7678, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.9295],
        [-5.4389],
        [-4.6674],
        ...,
        [-5.9148],
        [-5.9059],
        [-5.9041]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-993798.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2808],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(773.8542, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2808],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(773.8542, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6035e-03, -6.1921e-03,  0.0000e+00,  ...,  3.3736e-03,
         -6.0055e-03,  0.0000e+00],
        [-1.0275e-02, -5.5011e-03, -9.0877e-04,  ...,  3.1559e-03,
         -6.2698e-03, -1.2998e-05],
        [-1.0124e-02, -5.6564e-03, -7.0458e-04,  ...,  3.2048e-03,
         -6.2104e-03, -1.0077e-05],
        ...,
        [-9.6035e-03, -6.1921e-03,  0.0000e+00,  ...,  3.3736e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6035e-03, -6.1921e-03,  0.0000e+00,  ...,  3.3736e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6035e-03, -6.1921e-03,  0.0000e+00,  ...,  3.3736e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-54666.9219, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.2332, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(95.4576, device='cuda:0')



h[100].sum tensor(131.0193, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-113.7845, device='cuda:0')



h[200].sum tensor(-521.7555, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.1063, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0124, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0129, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0121, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(305319.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0077],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0150],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0056],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1561, 0.0000, 0.0978],
        [0.0000, 0.0000, 0.0000,  ..., 0.1561, 0.0000, 0.0978],
        [0.0000, 0.0000, 0.0000,  ..., 0.1561, 0.0000, 0.0978]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3459104.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(290.0808, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6558.4414, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10379.6299, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2088.4075, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.7559],
        [ 0.7599],
        [ 0.7384],
        ...,
        [-5.9148],
        [-5.9059],
        [-5.9041]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-971869.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(698.3163, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(698.3163, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-55663.7578, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.5084, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(86.1397, device='cuda:0')



h[100].sum tensor(125.1841, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-102.6777, device='cuda:0')



h[200].sum tensor(-524.1126, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.0699, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(298644.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1572, 0.0000, 0.0987],
        [0.0000, 0.0000, 0.0000,  ..., 0.1508, 0.0000, 0.0964],
        [0.0000, 0.0000, 0.0000,  ..., 0.1363, 0.0000, 0.0916],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1576, 0.0000, 0.0981],
        [0.0000, 0.0000, 0.0000,  ..., 0.1576, 0.0000, 0.0981],
        [0.0000, 0.0000, 0.0000,  ..., 0.1576, 0.0000, 0.0981]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3401005.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(287.9640, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6517.0659, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8747.3906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2072.2168, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.3518],
        [-3.6486],
        [-2.7859],
        ...,
        [-5.9635],
        [-5.9542],
        [-5.9522]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1079405.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4607],
        [0.4805],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(725.4158, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4607],
        [0.4805],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(725.4158, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2393e-02, -3.3221e-03, -3.6449e-03,  ...,  2.4803e-03,
         -7.1034e-03, -4.7201e-05],
        [-1.1631e-02, -4.1064e-03, -2.6487e-03,  ...,  2.7285e-03,
         -6.8033e-03, -3.4301e-05],
        [-1.0706e-02, -5.0583e-03, -1.4399e-03,  ...,  3.0297e-03,
         -6.4392e-03, -1.8647e-05],
        ...,
        [-9.6036e-03, -6.1921e-03,  0.0000e+00,  ...,  3.3886e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6036e-03, -6.1921e-03,  0.0000e+00,  ...,  3.3886e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6036e-03, -6.1921e-03,  0.0000e+00,  ...,  3.3886e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-55508.2617, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.8232, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(89.4826, device='cuda:0')



h[100].sum tensor(123.6500, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-106.6623, device='cuda:0')



h[200].sum tensor(-522.9889, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.1593, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0109, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0114, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(301975.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1586, 0.0000, 0.0991],
        [0.0000, 0.0000, 0.0000,  ..., 0.1586, 0.0000, 0.0991],
        [0.0000, 0.0000, 0.0000,  ..., 0.1586, 0.0000, 0.0991]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3425807.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(327.4354, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6569.0879, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10030.7217, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2061.9932, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.6601],
        [ 0.6464],
        [ 0.6309],
        ...,
        [-5.9925],
        [-5.9834],
        [-5.9816]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1056243.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3125],
        [0.4851],
        [0.2457],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(639.6056, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3125],
        [0.4851],
        [0.2457],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(639.6056, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2214e-02, -3.5064e-03, -3.3514e-03,  ...,  2.4720e-03,
         -7.0329e-03, -4.1241e-05],
        [-1.2105e-02, -3.6188e-03, -3.2111e-03,  ...,  2.5080e-03,
         -6.9899e-03, -3.9515e-05],
        [-1.2815e-02, -2.8882e-03, -4.1228e-03,  ...,  2.2744e-03,
         -7.2694e-03, -5.0734e-05],
        ...,
        [-9.6036e-03, -6.1921e-03,  0.0000e+00,  ...,  3.3305e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6036e-03, -6.1921e-03,  0.0000e+00,  ...,  3.3305e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6036e-03, -6.1921e-03,  0.0000e+00,  ...,  3.3305e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-56300.3047, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.9388, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(78.8976, device='cuda:0')



h[100].sum tensor(119.3764, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-94.0451, device='cuda:0')



h[200].sum tensor(-525.8719, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.7100, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0104, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(296905.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1577, 0.0000, 0.0990],
        [0.0000, 0.0000, 0.0000,  ..., 0.1577, 0.0000, 0.0990],
        [0.0000, 0.0000, 0.0000,  ..., 0.1577, 0.0000, 0.0990]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3393173.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(458.7161, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6578.9409, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9861.4863, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2009.2212, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4328],
        [ 0.5555],
        [ 0.5720],
        ...,
        [-5.9611],
        [-5.9524],
        [-5.9507]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1023627.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2710],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(875.0629, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2710],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(875.0629, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0252e-02, -5.5254e-03, -8.1769e-04,  ...,  3.0425e-03,
         -6.2606e-03, -9.5525e-06],
        [-1.0218e-02, -5.5600e-03, -7.7531e-04,  ...,  3.0537e-03,
         -6.2474e-03, -9.0574e-06],
        [-1.0867e-02, -4.8931e-03, -1.5930e-03,  ...,  2.8389e-03,
         -6.5025e-03, -1.8610e-05],
        ...,
        [-9.6036e-03, -6.1922e-03,  0.0000e+00,  ...,  3.2573e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6036e-03, -6.1922e-03,  0.0000e+00,  ...,  3.2573e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6036e-03, -6.1922e-03,  0.0000e+00,  ...,  3.2573e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-54003.0742, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-987.1965, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(107.9420, device='cuda:0')



h[100].sum tensor(140.0944, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-128.6658, device='cuda:0')



h[200].sum tensor(-517.7784, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.1746, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0123, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0116, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0123, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0130, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0130, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0130, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(318696.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0283, 0.0000, 0.0403],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0179],
        [0.0000, 0.0000, 0.0000,  ..., 0.0179, 0.0000, 0.0389],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1562, 0.0000, 0.0979],
        [0.0000, 0.0000, 0.0000,  ..., 0.1562, 0.0000, 0.0979],
        [0.0000, 0.0000, 0.0000,  ..., 0.1562, 0.0000, 0.0979]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3526779.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(468.8839, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6689.6284, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12834.6953, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2015.7550, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4602],
        [-0.1691],
        [-0.6739],
        ...,
        [-5.9033],
        [-5.8950],
        [-5.8934]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-768167.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(732.1022, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(732.1022, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0032, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0032, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0032, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0032, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0032, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0032, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-55650.1406, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.8455, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(90.3073, device='cuda:0')



h[100].sum tensor(132.7867, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-107.6454, device='cuda:0')



h[200].sum tensor(-522.6584, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.4280, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0130, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0130, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0130, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0130, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0130, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0130, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(302578.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1466, 0.0000, 0.0943],
        [0.0000, 0.0000, 0.0000,  ..., 0.1445, 0.0000, 0.0934],
        [0.0000, 0.0000, 0.0000,  ..., 0.1441, 0.0000, 0.0936],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1569, 0.0000, 0.0979],
        [0.0000, 0.0000, 0.0000,  ..., 0.1569, 0.0000, 0.0979],
        [0.0000, 0.0000, 0.0000,  ..., 0.1569, 0.0000, 0.0979]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3411289.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(338.7531, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6620.7114, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9821.1504, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1922.0712, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.2313],
        [-3.9879],
        [-3.8537],
        ...,
        [-5.9179],
        [-5.9097],
        [-5.9081]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-835738., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5527],
        [0.3008],
        [0.4023],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(671.0349, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5527],
        [0.3008],
        [0.4023],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(671.0349, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0323e-02, -5.4524e-03, -8.9165e-04,  ...,  3.0046e-03,
         -6.2887e-03, -9.8788e-06],
        [-1.2403e-02, -3.3139e-03, -3.4683e-03,  ...,  2.3149e-03,
         -7.1070e-03, -3.8426e-05],
        [-1.1868e-02, -3.8641e-03, -2.8053e-03,  ...,  2.4924e-03,
         -6.8964e-03, -3.1081e-05],
        ...,
        [-9.6036e-03, -6.1924e-03,  0.0000e+00,  ...,  3.2432e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6036e-03, -6.1924e-03,  0.0000e+00,  ...,  3.2432e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6036e-03, -6.1924e-03,  0.0000e+00,  ...,  3.2432e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-56244.2070, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.2695, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(82.7745, device='cuda:0')



h[100].sum tensor(129.1078, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-98.6663, device='cuda:0')



h[200].sum tensor(-524.7352, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.9733, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0104, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0105, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0097, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0130, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0130, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0130, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(295795.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1569, 0.0000, 0.0979],
        [0.0000, 0.0000, 0.0000,  ..., 0.1569, 0.0000, 0.0979],
        [0.0000, 0.0000, 0.0000,  ..., 0.1569, 0.0000, 0.0979]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3357126., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(324.8361, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6600.8198, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8741.8350, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1898.2585, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2732],
        [ 0.2388],
        [ 0.0832],
        ...,
        [-5.9179],
        [-5.9097],
        [-5.9081]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-826191.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(639.6882, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(639.6882, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0033, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0033, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0033, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0033, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0033, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0033, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-56853.8125, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.0008, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(78.9078, device='cuda:0')



h[100].sum tensor(124.0819, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-94.0572, device='cuda:0')



h[200].sum tensor(-525.3167, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.7133, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0131, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0131, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0131, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0131, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0131, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0131, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(294008.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1588, 0.0000, 0.0997],
        [0.0000, 0.0000, 0.0000,  ..., 0.1514, 0.0000, 0.0969],
        [0.0000, 0.0000, 0.0000,  ..., 0.1401, 0.0000, 0.0932],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1593, 0.0000, 0.0990],
        [0.0000, 0.0000, 0.0000,  ..., 0.1593, 0.0000, 0.0990],
        [0.0000, 0.0000, 0.0000,  ..., 0.1593, 0.0000, 0.0990]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3352630., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(318.6299, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6585.9478, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8576.7988, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1899.4381, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.9500],
        [-4.0942],
        [-3.0109],
        ...,
        [-5.9935],
        [-5.9849],
        [-5.9833]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-919530.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 210.0 event: 3150 loss: tensor(377.1321, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4165],
        [0.3311],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(833.5092, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4165],
        [0.3311],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(833.5092, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0396e-02, -5.3783e-03, -9.4709e-04,  ...,  3.0882e-03,
         -6.3172e-03, -9.4073e-06],
        [-1.0600e-02, -5.1682e-03, -1.1915e-03,  ...,  3.0210e-03,
         -6.3976e-03, -1.1835e-05],
        [-1.1182e-02, -4.5698e-03, -1.8875e-03,  ...,  2.8296e-03,
         -6.6266e-03, -1.8748e-05],
        ...,
        [-9.6036e-03, -6.1926e-03,  0.0000e+00,  ...,  3.3486e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6036e-03, -6.1926e-03,  0.0000e+00,  ...,  3.3486e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6036e-03, -6.1926e-03,  0.0000e+00,  ...,  3.3486e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-55351.0742, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.7908, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(102.8162, device='cuda:0')



h[100].sum tensor(132.0784, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-122.5559, device='cuda:0')



h[200].sum tensor(-518.3517, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.5042, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(309029.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1619, 0.0000, 0.0999],
        [0.0000, 0.0000, 0.0000,  ..., 0.1619, 0.0000, 0.0999],
        [0.0000, 0.0000, 0.0000,  ..., 0.1619, 0.0000, 0.0999]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3450553.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(346.3474, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6637.3477, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10582.9209, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1970.9270, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2780],
        [ 0.2132],
        [ 0.1497],
        ...,
        [-6.0836],
        [-6.0746],
        [-6.0729]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-962914.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(627.4034, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(627.4034, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-57571.7031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.8802, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(77.3924, device='cuda:0')



h[100].sum tensor(123.1161, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-92.2509, device='cuda:0')



h[200].sum tensor(-524.6361, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.2195, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(291862.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1048, 0.0000, 0.0802],
        [0.0000, 0.0000, 0.0000,  ..., 0.1249, 0.0000, 0.0871],
        [0.0000, 0.0000, 0.0000,  ..., 0.0828, 0.0000, 0.0691],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1624, 0.0000, 0.0987],
        [0.0000, 0.0000, 0.0000,  ..., 0.1624, 0.0000, 0.0987],
        [0.0000, 0.0000, 0.0000,  ..., 0.1624, 0.0000, 0.0987]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3331597.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(292.6401, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6550.8057, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7328.8076, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1930.2688, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.3502],
        [-2.6364],
        [-1.9906],
        ...,
        [-6.1247],
        [-6.1155],
        [-6.1136]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1097406.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(661.1234, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(661.1234, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.1927e-03,  0.0000e+00,  ...,  3.4001e-03,
         -6.0055e-03,  0.0000e+00],
        [-1.1025e-02, -4.7320e-03, -1.6396e-03,  ...,  2.9370e-03,
         -6.5648e-03, -1.4532e-05],
        [-1.1025e-02, -4.7320e-03, -1.6396e-03,  ...,  2.9370e-03,
         -6.5648e-03, -1.4532e-05],
        ...,
        [-9.6037e-03, -6.1927e-03,  0.0000e+00,  ...,  3.4001e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1927e-03,  0.0000e+00,  ...,  3.4001e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1927e-03,  0.0000e+00,  ...,  3.4001e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-57325.8711, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.2017, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(81.5519, device='cuda:0')



h[100].sum tensor(132.4400, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-97.2090, device='cuda:0')



h[200].sum tensor(-522.5165, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.5749, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0128, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0128, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0128, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(295872.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0183],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0216],
        [0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0000, 0.0306],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1609, 0.0000, 0.0967],
        [0.0000, 0.0000, 0.0000,  ..., 0.1609, 0.0000, 0.0967],
        [0.0000, 0.0000, 0.0000,  ..., 0.1609, 0.0000, 0.0967]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3362114., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(281.8777, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6571.1060, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7756.0894, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1920.4288, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.6611],
        [ 0.4255],
        [ 0.0186],
        ...,
        [-6.1089],
        [-6.0995],
        [-6.0967]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-967086.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(690.2186, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(690.2186, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-57102.2227, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.4849, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(85.1409, device='cuda:0')



h[100].sum tensor(134.4680, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-101.4870, device='cuda:0')



h[200].sum tensor(-520.6153, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.7444, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(297810.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1379, 0.0000, 0.0885],
        [0.0000, 0.0000, 0.0000,  ..., 0.1154, 0.0000, 0.0807],
        [0.0000, 0.0000, 0.0000,  ..., 0.0982, 0.0000, 0.0752],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1604, 0.0000, 0.0963],
        [0.0000, 0.0000, 0.0000,  ..., 0.1604, 0.0000, 0.0963],
        [0.0000, 0.0000, 0.0000,  ..., 0.1604, 0.0000, 0.0963]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3369321.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(298.0718, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6627.2002, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8519.4473, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1889.2772, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.4314],
        [-2.9874],
        [-2.7872],
        ...,
        [-6.1194],
        [-6.1106],
        [-6.1089]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-933489.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4353],
        [0.0000],
        [0.2944],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(664.4467, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4353],
        [0.0000],
        [0.2944],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(664.4467, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1426e-02, -4.3203e-03, -2.0279e-03,  ...,  2.8564e-03,
         -6.7226e-03, -1.5954e-05],
        [-1.2956e-02, -2.7483e-03, -3.7303e-03,  ...,  2.3568e-03,
         -7.3246e-03, -2.9348e-05],
        [-1.0908e-02, -4.8528e-03, -1.4513e-03,  ...,  3.0256e-03,
         -6.5187e-03, -1.1418e-05],
        ...,
        [-9.6037e-03, -6.1929e-03,  0.0000e+00,  ...,  3.4514e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1929e-03,  0.0000e+00,  ...,  3.4514e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1929e-03,  0.0000e+00,  ...,  3.4514e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-57502.5586, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.2195, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(81.9618, device='cuda:0')



h[100].sum tensor(128.8689, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-97.6976, device='cuda:0')



h[200].sum tensor(-520.6245, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.7085, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0093, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0108, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(293991.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1611, 0.0000, 0.0971],
        [0.0000, 0.0000, 0.0000,  ..., 0.1611, 0.0000, 0.0971],
        [0.0000, 0.0000, 0.0000,  ..., 0.1611, 0.0000, 0.0971]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3355648.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(309.7499, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6658.1372, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9356.2070, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1851.1096, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2242],
        [ 0.2312],
        [ 0.2597],
        ...,
        [-6.1687],
        [-6.1595],
        [-6.1577]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-922793.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(754.8121, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(754.8121, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-56795.4766, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.0977, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(93.1087, device='cuda:0')



h[100].sum tensor(131.7062, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-110.9846, device='cuda:0')



h[200].sum tensor(-516.5775, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.3409, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(304807.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1438, 0.0000, 0.0918],
        [0.0000, 0.0000, 0.0000,  ..., 0.1121, 0.0000, 0.0819],
        [0.0000, 0.0000, 0.0000,  ..., 0.0471, 0.0000, 0.0531],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1627, 0.0000, 0.0974],
        [0.0000, 0.0000, 0.0000,  ..., 0.1627, 0.0000, 0.0974],
        [0.0000, 0.0000, 0.0000,  ..., 0.1627, 0.0000, 0.0974]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3431864.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(335.5605, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6650.2153, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11183.3203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1907.5193, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.3168],
        [-2.4503],
        [-1.1653],
        ...,
        [-6.2412],
        [-6.2317],
        [-6.2298]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1025870.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(713.0103, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(713.0103, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-57377.7422, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.6553, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(87.9523, device='cuda:0')



h[100].sum tensor(135.5670, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-104.8382, device='cuda:0')



h[200].sum tensor(-517.3783, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.6606, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(297561.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1565, 0.0000, 0.0941],
        [0.0000, 0.0000, 0.0000,  ..., 0.1388, 0.0000, 0.0883],
        [0.0000, 0.0000, 0.0000,  ..., 0.0852, 0.0000, 0.0713],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1623, 0.0000, 0.0956],
        [0.0000, 0.0000, 0.0000,  ..., 0.1623, 0.0000, 0.0956],
        [0.0000, 0.0000, 0.0000,  ..., 0.1585, 0.0000, 0.0944]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3389990.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(290.3489, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6677.8975, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9671.3389, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1840.9043, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.7955],
        [-2.7448],
        [-1.4905],
        ...,
        [-6.2196],
        [-6.1165],
        [-5.8482]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-953494.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(605.3301, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(605.3301, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-58510.4766, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.6832, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(74.6696, device='cuda:0')



h[100].sum tensor(136.0061, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-89.0053, device='cuda:0')



h[200].sum tensor(-520.1793, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.3322, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(289085.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1359, 0.0000, 0.0860],
        [0.0000, 0.0000, 0.0000,  ..., 0.1555, 0.0000, 0.0923],
        [0.0000, 0.0000, 0.0000,  ..., 0.1613, 0.0000, 0.0946],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1621, 0.0000, 0.0937],
        [0.0000, 0.0000, 0.0000,  ..., 0.1621, 0.0000, 0.0937],
        [0.0000, 0.0000, 0.0000,  ..., 0.1621, 0.0000, 0.0937]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3310360., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(240.3721, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6563.9321, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6771.2002, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1820.0977, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.7342],
        [-5.1153],
        [-5.9510],
        ...,
        [-6.2459],
        [-6.2385],
        [-6.2388]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-976664.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(746.4790, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(746.4790, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2049e-02, -3.6814e-03, -2.5304e-03,  ...,  2.8870e-03,
         -6.9679e-03, -1.5394e-05],
        [-1.0825e-02, -4.9393e-03, -1.2632e-03,  ...,  3.2730e-03,
         -6.4859e-03, -7.6851e-06],
        [-1.1137e-02, -4.6178e-03, -1.5870e-03,  ...,  3.1744e-03,
         -6.6091e-03, -9.6549e-06],
        ...,
        [-9.6037e-03, -6.1932e-03,  0.0000e+00,  ...,  3.6579e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1932e-03,  0.0000e+00,  ...,  3.6579e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1932e-03,  0.0000e+00,  ...,  3.6579e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-57334.9297, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.9971, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(92.0808, device='cuda:0')



h[100].sum tensor(139.0796, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-109.7593, device='cuda:0')



h[200].sum tensor(-514.7831, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.0059, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0120, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0122, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(305357., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1639, 0.0000, 0.0944],
        [0.0000, 0.0000, 0.0000,  ..., 0.1639, 0.0000, 0.0944],
        [0.0000, 0.0000, 0.0000,  ..., 0.1639, 0.0000, 0.0944]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3450550.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(295.0964, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6625.5898, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10458.8887, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1873.4326, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4821],
        [ 0.4683],
        [ 0.4829],
        ...,
        [-6.3292],
        [-6.3192],
        [-6.3171]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1044505.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6421],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(663.0317, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6421],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(663.0317, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.1933e-03,  0.0000e+00,  ...,  3.6680e-03,
         -6.0055e-03,  0.0000e+00],
        [-1.1140e-02, -4.6157e-03, -1.5605e-03,  ...,  3.1839e-03,
         -6.6100e-03, -8.8622e-06],
        [-9.6037e-03, -6.1933e-03,  0.0000e+00,  ...,  3.6680e-03,
         -6.0055e-03,  0.0000e+00],
        ...,
        [-9.6037e-03, -6.1933e-03,  0.0000e+00,  ...,  3.6680e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1933e-03,  0.0000e+00,  ...,  3.6680e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1933e-03,  0.0000e+00,  ...,  3.6680e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-58196.8203, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.2106, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(81.7873, device='cuda:0')



h[100].sum tensor(132.8649, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-97.4896, device='cuda:0')



h[200].sum tensor(-517.1686, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.6516, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0129, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(295751.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0098],
        [0.0000, 0.0000, 0.0000,  ..., 0.0397, 0.0000, 0.0454],
        [0.0000, 0.0000, 0.0000,  ..., 0.0726, 0.0000, 0.0617],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1639, 0.0000, 0.0944],
        [0.0000, 0.0000, 0.0000,  ..., 0.1639, 0.0000, 0.0944],
        [0.0000, 0.0000, 0.0000,  ..., 0.1639, 0.0000, 0.0944]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3384296.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(292.4695, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6624.5996, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9656.3574, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1798.4005, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0220],
        [-1.2317],
        [-2.9318],
        ...,
        [-6.3416],
        [-6.3317],
        [-6.3296]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1012249.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 220.0 event: 3300 loss: tensor(470.8731, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2524],
        [0.3745],
        [0.2993],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(786.6696, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2524],
        [0.3745],
        [0.2993],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(786.6696, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1128e-02, -4.6278e-03, -1.5205e-03,  ...,  3.1447e-03,
         -6.6054e-03, -8.0433e-06],
        [-1.0924e-02, -4.8378e-03, -1.3165e-03,  ...,  3.2094e-03,
         -6.5250e-03, -6.9643e-06],
        [-1.1763e-02, -3.9761e-03, -2.1534e-03,  ...,  2.9437e-03,
         -6.8552e-03, -1.1392e-05],
        ...,
        [-9.6037e-03, -6.1934e-03,  0.0000e+00,  ...,  3.6274e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1934e-03,  0.0000e+00,  ...,  3.6274e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1934e-03,  0.0000e+00,  ...,  3.6274e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-56949.7461, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.3580, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(97.0384, device='cuda:0')



h[100].sum tensor(143.7486, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-115.6688, device='cuda:0')



h[200].sum tensor(-512.6605, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.6214, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0127, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0122, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(306696.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1626, 0.0000, 0.0936],
        [0.0000, 0.0000, 0.0000,  ..., 0.1626, 0.0000, 0.0936],
        [0.0000, 0.0000, 0.0000,  ..., 0.1626, 0.0000, 0.0936]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3448968., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(315.9938, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6697.2388, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11340.8242, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1772.5864, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.7164],
        [ 0.6892],
        [ 0.6705],
        ...,
        [-6.3036],
        [-6.2943],
        [-6.2924]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-901862., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2460],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(668.4948, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2460],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(668.4948, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0958e-02, -4.8032e-03, -1.3255e-03,  ...,  3.1762e-03,
         -6.5383e-03, -6.5168e-06],
        [-1.0852e-02, -4.9119e-03, -1.2219e-03,  ...,  3.2098e-03,
         -6.4966e-03, -6.0072e-06],
        [-1.1571e-02, -4.1736e-03, -1.9259e-03,  ...,  2.9815e-03,
         -6.7796e-03, -9.4683e-06],
        ...,
        [-9.6037e-03, -6.1934e-03,  0.0000e+00,  ...,  3.6060e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1934e-03,  0.0000e+00,  ...,  3.6060e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1934e-03,  0.0000e+00,  ...,  3.6060e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-58138.5000, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.2282, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(82.4612, device='cuda:0')



h[100].sum tensor(140.2315, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-98.2928, device='cuda:0')



h[200].sum tensor(-516.6237, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.8712, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0123, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0126, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0131, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(297440.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1621, 0.0000, 0.0925],
        [0.0000, 0.0000, 0.0000,  ..., 0.1621, 0.0000, 0.0925],
        [0.0000, 0.0000, 0.0000,  ..., 0.1621, 0.0000, 0.0925]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3382358.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(282.5006, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6626.7461, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9191.4316, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1716.9056, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.6993],
        [ 0.7375],
        [ 0.7676],
        ...,
        [-6.2879],
        [-6.2789],
        [-6.2771]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-894598.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2832],
        [0.0000],
        [0.5044],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(730.8582, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2832],
        [0.0000],
        [0.5044],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(730.8582, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0855e-02, -4.9083e-03, -1.2030e-03,  ...,  3.2447e-03,
         -6.4981e-03, -5.4830e-06],
        [-1.2070e-02, -3.6615e-03, -2.3701e-03,  ...,  2.8600e-03,
         -6.9759e-03, -1.0802e-05],
        [-1.0696e-02, -5.0722e-03, -1.0496e-03,  ...,  3.2953e-03,
         -6.4353e-03, -4.7837e-06],
        ...,
        [-1.1668e-02, -4.0743e-03, -1.9837e-03,  ...,  2.9874e-03,
         -6.8177e-03, -9.0409e-06],
        [-1.0612e-02, -5.1584e-03, -9.6893e-04,  ...,  3.3218e-03,
         -6.4022e-03, -4.4160e-06],
        [-9.6037e-03, -6.1935e-03,  0.0000e+00,  ...,  3.6412e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-57590.8516, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.8430, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(90.1539, device='cuda:0')



h[100].sum tensor(142.8875, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-107.4625, device='cuda:0')



h[200].sum tensor(-514.4399, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.3780, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0123, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0128, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0130, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(298469.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0095],
        [0.0000, 0.0000, 0.0000,  ..., 0.0422, 0.0000, 0.0356]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3371828., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(274.0857, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6638.0020, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8827.8066, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1695.9572, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.6978],
        [ 0.6800],
        [ 0.6438],
        ...,
        [ 0.5009],
        [-0.1156],
        [-1.6036]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-918004.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(974.8064, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(974.8064, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-55127.8867, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-988.2037, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(120.2458, device='cuda:0')



h[100].sum tensor(158.0661, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-143.3317, device='cuda:0')



h[200].sum tensor(-505.8724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.1839, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(324026.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1376, 0.0000, 0.0849],
        [0.0000, 0.0000, 0.0000,  ..., 0.1572, 0.0000, 0.0911],
        [0.0000, 0.0000, 0.0000,  ..., 0.1618, 0.0000, 0.0931],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1630, 0.0000, 0.0924],
        [0.0000, 0.0000, 0.0000,  ..., 0.1630, 0.0000, 0.0924],
        [0.0000, 0.0000, 0.0000,  ..., 0.1630, 0.0000, 0.0924]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3547254.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(331.4796, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6689.2998, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12644.5312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1802.3018, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.3671],
        [-4.5336],
        [-5.3315],
        ...,
        [-6.3243],
        [-6.3151],
        [-6.3133]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-874273.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(749.3197, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(749.3197, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-57493.9297, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.0458, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(92.4312, device='cuda:0')



h[100].sum tensor(140.9407, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-110.1770, device='cuda:0')



h[200].sum tensor(-513.8330, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.1201, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(300386.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1492, 0.0000, 0.0890],
        [0.0000, 0.0000, 0.0000,  ..., 0.1639, 0.0000, 0.0932],
        [0.0000, 0.0000, 0.0000,  ..., 0.1626, 0.0000, 0.0930],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1647, 0.0000, 0.0928],
        [0.0000, 0.0000, 0.0000,  ..., 0.1647, 0.0000, 0.0928],
        [0.0000, 0.0000, 0.0000,  ..., 0.1647, 0.0000, 0.0928]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3387034.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(272.8683, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6625.2402, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9057.9922, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1699.3309, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.1529],
        [-5.5558],
        [-5.4445],
        ...,
        [-6.3790],
        [-6.3693],
        [-6.3673]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-989005.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6152],
        [0.6953],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(682.3420, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6152],
        [0.6953],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(682.3420, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4480e-02, -1.1880e-03, -4.5148e-03,  ...,  2.0754e-03,
         -7.9242e-03, -1.7533e-05],
        [-1.1076e-02, -4.6826e-03, -1.3629e-03,  ...,  3.1627e-03,
         -6.5847e-03, -5.2928e-06],
        [-1.1267e-02, -4.4859e-03, -1.5402e-03,  ...,  3.1015e-03,
         -6.6601e-03, -5.9817e-06],
        ...,
        [-9.6037e-03, -6.1936e-03,  0.0000e+00,  ...,  3.6329e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1936e-03,  0.0000e+00,  ...,  3.6329e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1936e-03,  0.0000e+00,  ...,  3.6329e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-58113.1875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.3812, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(84.1693, device='cuda:0')



h[100].sum tensor(140.9073, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-100.3289, device='cuda:0')



h[200].sum tensor(-516.0626, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.4278, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0100, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0109, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(293442.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0066],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1638, 0.0000, 0.0917],
        [0.0000, 0.0000, 0.0000,  ..., 0.1638, 0.0000, 0.0917],
        [0.0000, 0.0000, 0.0000,  ..., 0.1638, 0.0000, 0.0917]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3332004., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(253.8782, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6626.0288, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7733.5386, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1646.5588, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5613],
        [ 0.6766],
        [ 0.7863],
        ...,
        [-6.3481],
        [-6.3390],
        [-6.3372]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-880146.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(752.5095, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(752.5095, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-57406.5352, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.0297, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(92.8247, device='cuda:0')



h[100].sum tensor(143.8734, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-110.6460, device='cuda:0')



h[200].sum tensor(-513.3425, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.2483, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(302342.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1637, 0.0000, 0.0924],
        [0.0000, 0.0000, 0.0000,  ..., 0.1637, 0.0000, 0.0924],
        [0.0000, 0.0000, 0.0000,  ..., 0.1635, 0.0000, 0.0926],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1642, 0.0000, 0.0918],
        [0.0000, 0.0000, 0.0000,  ..., 0.1642, 0.0000, 0.0918],
        [0.0000, 0.0000, 0.0000,  ..., 0.1642, 0.0000, 0.0918]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3409592.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(279.8978, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6686.4985, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9780.3418, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1654.2024, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.9580],
        [-6.3574],
        [-6.5711],
        ...,
        [-6.3609],
        [-6.3518],
        [-6.3499]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-835689.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3611],
        [0.3528],
        [0.3372],
        ...,
        [0.0000],
        [0.0000],
        [0.5405]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(760.6605, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3611],
        [0.3528],
        [0.3372],
        ...,
        [0.0000],
        [0.0000],
        [0.5405]], device='cuda:0') 
g.ndata[nfet].sum tensor(760.6605, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2411e-02, -3.3118e-03, -2.5039e-03,  ...,  2.7430e-03,
         -7.1103e-03, -8.1753e-06],
        [-1.2715e-02, -2.9996e-03, -2.7751e-03,  ...,  2.6452e-03,
         -7.2300e-03, -9.0609e-06],
        [-1.1869e-02, -3.8687e-03, -2.0200e-03,  ...,  2.9176e-03,
         -6.8968e-03, -6.5953e-06],
        ...,
        [-9.6037e-03, -6.1936e-03,  0.0000e+00,  ...,  3.6461e-03,
         -6.0055e-03,  0.0000e+00],
        [-1.0897e-02, -4.8663e-03, -1.1533e-03,  ...,  3.2301e-03,
         -6.5144e-03, -3.7655e-06],
        [-1.0723e-02, -5.0442e-03, -9.9865e-04,  ...,  3.2859e-03,
         -6.4461e-03, -3.2606e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-57326.9023, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.1100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(93.8301, device='cuda:0')



h[100].sum tensor(139.2204, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-111.8445, device='cuda:0')



h[200].sum tensor(-512.5912, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.5760, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0111, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0115, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0124, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(303343.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0596, 0.0000, 0.0526],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0000, 0.0254],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3417753.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(298.5050, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6697.9463, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10623.2832, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1646.1018, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.6402],
        [ 0.6658],
        [ 0.6941],
        ...,
        [-2.8894],
        [-1.2487],
        [-0.3094]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-896315.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3193],
        [0.2859],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(731.9716, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3193],
        [0.2859],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(731.9716, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0368e-02, -5.4095e-03, -6.6863e-04,  ...,  3.4163e-03,
         -6.3061e-03, -1.9896e-06],
        [-1.0803e-02, -4.9625e-03, -1.0498e-03,  ...,  3.2767e-03,
         -6.4775e-03, -3.1238e-06],
        [-1.1996e-02, -3.7377e-03, -2.0942e-03,  ...,  2.8942e-03,
         -6.9471e-03, -6.2315e-06],
        ...,
        [-9.6037e-03, -6.1937e-03,  0.0000e+00,  ...,  3.6612e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1937e-03,  0.0000e+00,  ...,  3.6612e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1937e-03,  0.0000e+00,  ...,  3.6612e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-57659.5195, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.7938, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(90.2912, device='cuda:0')



h[100].sum tensor(136.2794, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-107.6262, device='cuda:0')



h[200].sum tensor(-513.0455, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.4228, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0139, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0127, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0120, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(299508.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0308, 0.0000, 0.0319],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0031],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1663, 0.0000, 0.0924],
        [0.0000, 0.0000, 0.0000,  ..., 0.1663, 0.0000, 0.0924],
        [0.0000, 0.0000, 0.0000,  ..., 0.1663, 0.0000, 0.0924]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3388175.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(295.2382, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6661.0239, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9912.8574, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1628.4082, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0715],
        [ 0.5072],
        [ 0.6213],
        ...,
        [-6.4396],
        [-6.4303],
        [-6.4285]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-937037.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5166],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(776.4883, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5166],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(776.4883, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.1937e-03,  0.0000e+00,  ...,  3.6413e-03,
         -6.0055e-03,  0.0000e+00],
        [-1.0840e-02, -4.9252e-03, -1.0614e-03,  ...,  3.2479e-03,
         -6.4919e-03, -2.8649e-06],
        [-1.0666e-02, -5.1038e-03, -9.1193e-04,  ...,  3.3033e-03,
         -6.4234e-03, -2.4614e-06],
        ...,
        [-9.6037e-03, -6.1937e-03,  0.0000e+00,  ...,  3.6413e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1937e-03,  0.0000e+00,  ...,  3.6413e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1937e-03,  0.0000e+00,  ...,  3.6413e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-57137.2500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.2519, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(95.7825, device='cuda:0')



h[100].sum tensor(143.9944, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-114.1718, device='cuda:0')



h[200].sum tensor(-510.2813, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.2122, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0139, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(305809.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0615, 0.0000, 0.0552],
        [0.0000, 0.0000, 0.0000,  ..., 0.0311, 0.0000, 0.0346],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0052],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1638, 0.0000, 0.0904],
        [0.0000, 0.0000, 0.0000,  ..., 0.1660, 0.0000, 0.0910],
        [0.0000, 0.0000, 0.0000,  ..., 0.1660, 0.0000, 0.0910]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3451242.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(306.5275, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6713.8867, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11068.2363, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1617.3751, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4761],
        [-0.0071],
        [ 0.3549],
        ...,
        [-5.8472],
        [-6.1228],
        [-6.3088]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-820612.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 230.0 event: 3450 loss: tensor(435.5915, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(704.8528, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(704.8528, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-57949.4219, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.5379, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(86.9460, device='cuda:0')



h[100].sum tensor(139.7857, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-103.6388, device='cuda:0')



h[200].sum tensor(-511.9319, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.3327, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(299500.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0542, 0.0000, 0.0507],
        [0.0000, 0.0000, 0.0000,  ..., 0.1189, 0.0000, 0.0771],
        [0.0000, 0.0000, 0.0000,  ..., 0.1445, 0.0000, 0.0844],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1673, 0.0000, 0.0903],
        [0.0000, 0.0000, 0.0000,  ..., 0.1673, 0.0000, 0.0903],
        [0.0000, 0.0000, 0.0000,  ..., 0.1673, 0.0000, 0.0903]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3408185., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(280.2419, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6628.7949, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9468.5957, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1608.9795, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1536],
        [-2.8995],
        [-3.0356],
        ...,
        [-6.4917],
        [-6.4824],
        [-6.4805]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-865245.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(846.5963, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(846.5963, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0240e-02, -5.5403e-03, -5.3649e-04,  ...,  3.4871e-03,
         -6.2560e-03, -1.3064e-06],
        [-9.6037e-03, -6.1937e-03,  0.0000e+00,  ...,  3.6867e-03,
         -6.0055e-03,  0.0000e+00],
        [-1.2152e-02, -3.5780e-03, -2.1477e-03,  ...,  2.8876e-03,
         -7.0084e-03, -5.2296e-06],
        ...,
        [-9.6037e-03, -6.1937e-03,  0.0000e+00,  ...,  3.6867e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1937e-03,  0.0000e+00,  ...,  3.6867e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1937e-03,  0.0000e+00,  ...,  3.6867e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-56436.0781, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.9806, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(104.4306, device='cuda:0')



h[100].sum tensor(149.0785, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-124.4802, device='cuda:0')



h[200].sum tensor(-506.6802, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.0303, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(315036.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0126],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0036],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0006],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1673, 0.0000, 0.0903],
        [0.0000, 0.0000, 0.0000,  ..., 0.1673, 0.0000, 0.0903],
        [0.0000, 0.0000, 0.0000,  ..., 0.1673, 0.0000, 0.0903]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3505606.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(313.8379, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6627.7061, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11341.9482, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1683.9102, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.7447],
        [ 0.6866],
        [ 0.6268],
        ...,
        [-6.4913],
        [-6.4816],
        [-6.4795]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-883313.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1173.6125, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1173.6125, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-53325.6797, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-990.0447, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(144.7692, device='cuda:0')



h[100].sum tensor(161.3661, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-172.5634, device='cuda:0')



h[200].sum tensor(-494.7444, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(47.1752, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(349183.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1696, 0.0000, 0.0916],
        [0.0000, 0.0000, 0.0000,  ..., 0.1696, 0.0000, 0.0916],
        [0.0000, 0.0000, 0.0000,  ..., 0.1694, 0.0000, 0.0919],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1701, 0.0000, 0.0910],
        [0.0000, 0.0000, 0.0000,  ..., 0.1701, 0.0000, 0.0910],
        [0.0000, 0.0000, 0.0000,  ..., 0.1701, 0.0000, 0.0910]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3799769.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(386.0291, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6715.5659, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(17868.1660, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1817.8359, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.7916],
        [-6.8716],
        [-6.8717],
        ...,
        [-6.3193],
        [-6.4808],
        [-6.5571]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1020124.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(589.2750, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(589.2750, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-59008.8633, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.5179, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(72.6891, device='cuda:0')



h[100].sum tensor(129.6008, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-86.6447, device='cuda:0')



h[200].sum tensor(-514.6317, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.6869, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(290462.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1691, 0.0000, 0.0905],
        [0.0000, 0.0000, 0.0000,  ..., 0.1611, 0.0000, 0.0883],
        [0.0000, 0.0000, 0.0000,  ..., 0.1504, 0.0000, 0.0856],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1696, 0.0000, 0.0900],
        [0.0000, 0.0000, 0.0000,  ..., 0.1696, 0.0000, 0.0900],
        [0.0000, 0.0000, 0.0000,  ..., 0.1696, 0.0000, 0.0900]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3340178.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(268.5070, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6548.0049, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7528.5879, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1568.2434, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.5906],
        [-6.2708],
        [-5.6520],
        ...,
        [-6.5645],
        [-6.5550],
        [-6.5531]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-989520.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.7803],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(700.7450, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.7803],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(700.7450, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.1939e-03,  0.0000e+00,  ...,  3.6696e-03,
         -6.0055e-03,  0.0000e+00],
        [-1.1470e-02, -4.2784e-03, -1.4855e-03,  ...,  3.1011e-03,
         -6.7401e-03, -2.5464e-06],
        [-9.6037e-03, -6.1939e-03,  0.0000e+00,  ...,  3.6696e-03,
         -6.0055e-03,  0.0000e+00],
        ...,
        [-9.6037e-03, -6.1939e-03,  0.0000e+00,  ...,  3.6696e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1939e-03,  0.0000e+00,  ...,  3.6696e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1939e-03,  0.0000e+00,  ...,  3.6696e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-57725.9219, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.5482, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(86.4393, device='cuda:0')



h[100].sum tensor(139.7711, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-103.0348, device='cuda:0')



h[200].sum tensor(-510.4485, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.1676, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0122, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(299695.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0614, 0.0000, 0.0518],
        [0.0000, 0.0000, 0.0000,  ..., 0.0290, 0.0000, 0.0318],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1683, 0.0000, 0.0889],
        [0.0000, 0.0000, 0.0000,  ..., 0.1683, 0.0000, 0.0889],
        [0.0000, 0.0000, 0.0000,  ..., 0.1683, 0.0000, 0.0889]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3408659.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(315.8229, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6653.8784, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9384.2207, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1538.6816, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6898],
        [-0.3763],
        [ 0.5276],
        ...,
        [-6.5074],
        [-6.4983],
        [-6.4967]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-768438.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1005.0828, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1005.0828, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1874e-02, -3.8647e-03, -1.7721e-03,  ...,  3.0213e-03,
         -6.8988e-03, -2.6520e-06],
        [-1.0742e-02, -5.0264e-03, -8.8828e-04,  ...,  3.3623e-03,
         -6.4533e-03, -1.3293e-06],
        [-9.6037e-03, -6.1939e-03,  0.0000e+00,  ...,  3.7051e-03,
         -6.0055e-03,  0.0000e+00],
        ...,
        [-9.6037e-03, -6.1939e-03,  0.0000e+00,  ...,  3.7051e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1939e-03,  0.0000e+00,  ...,  3.7051e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1939e-03,  0.0000e+00,  ...,  3.7051e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-54586.6367, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-988.4376, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(123.9805, device='cuda:0')



h[100].sum tensor(152.0104, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-147.7834, device='cuda:0')



h[200].sum tensor(-499.1008, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.4009, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0129, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(330193.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0143],
        [0.0000, 0.0000, 0.0000,  ..., 0.0462, 0.0000, 0.0393],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1693, 0.0000, 0.0897],
        [0.0000, 0.0000, 0.0000,  ..., 0.1693, 0.0000, 0.0897],
        [0.0000, 0.0000, 0.0000,  ..., 0.1693, 0.0000, 0.0897]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3625678., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(407.1957, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6733.1343, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14940.6299, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1637.4722, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5893],
        [-0.0495],
        [-1.1840],
        ...,
        [-6.4946],
        [-6.4282],
        [-6.3570]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-780445.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(873.2468, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(873.2468, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.1940e-03,  0.0000e+00,  ...,  3.7456e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1940e-03,  0.0000e+00,  ...,  3.7456e-03,
         -6.0055e-03,  0.0000e+00],
        [-1.1197e-02, -4.5597e-03, -1.2197e-03,  ...,  3.2708e-03,
         -6.6323e-03, -1.5731e-06],
        ...,
        [-9.6037e-03, -6.1940e-03,  0.0000e+00,  ...,  3.7456e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1940e-03,  0.0000e+00,  ...,  3.7456e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1940e-03,  0.0000e+00,  ...,  3.7456e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-55871.0117, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-987.1906, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(107.7180, device='cuda:0')



h[100].sum tensor(141.1676, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-128.3988, device='cuda:0')



h[200].sum tensor(-502.6292, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.1016, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(316523.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0031, 0.0000, 0.0337],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0188],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0177],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1705, 0.0000, 0.0896],
        [0.0000, 0.0000, 0.0000,  ..., 0.1705, 0.0000, 0.0896],
        [0.0000, 0.0000, 0.0000,  ..., 0.1705, 0.0000, 0.0896]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3542240.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(375.9678, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6717.5586, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13309.4883, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1563.8102, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.8398],
        [ 0.9492],
        [ 0.9750],
        ...,
        [-6.5816],
        [-6.5723],
        [-6.5705]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-796986.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5698],
        [0.6216],
        [0.5776],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(592.9977, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5698],
        [0.6216],
        [0.5776],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(592.9977, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3367e-02, -2.3327e-03, -2.8269e-03,  ...,  2.6607e-03,
         -7.4866e-03, -3.0910e-06],
        [-1.4624e-02, -1.0439e-03, -3.7704e-03,  ...,  2.2920e-03,
         -7.9809e-03, -4.1227e-06],
        [-1.3219e-02, -2.4853e-03, -2.7152e-03,  ...,  2.7044e-03,
         -7.4281e-03, -2.9688e-06],
        ...,
        [-9.6037e-03, -6.1940e-03,  0.0000e+00,  ...,  3.7654e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1940e-03,  0.0000e+00,  ...,  3.7654e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1940e-03,  0.0000e+00,  ...,  3.7654e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-58621.1953, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.5878, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(73.1483, device='cuda:0')



h[100].sum tensor(126.4820, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-87.1920, device='cuda:0')



h[200].sum tensor(-510.5233, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.8365, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0094, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0094, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(292878., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1714, 0.0000, 0.0889],
        [0.0000, 0.0000, 0.0000,  ..., 0.1714, 0.0000, 0.0889],
        [0.0000, 0.0000, 0.0000,  ..., 0.1714, 0.0000, 0.0889]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3373984.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(308.0804, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6561.8677, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8598.7246, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1492.1443, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5336],
        [ 0.5121],
        [ 0.5181],
        ...,
        [-5.8285],
        [-6.3719],
        [-6.5481]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-900329.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(811.9741, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(811.9741, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1487e-02, -4.2623e-03, -1.3872e-03,  ...,  3.2815e-03,
         -6.7465e-03, -1.2586e-06],
        [-9.6037e-03, -6.1941e-03,  0.0000e+00,  ...,  3.8247e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1941e-03,  0.0000e+00,  ...,  3.8247e-03,
         -6.0055e-03,  0.0000e+00],
        ...,
        [-9.6037e-03, -6.1941e-03,  0.0000e+00,  ...,  3.8247e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1941e-03,  0.0000e+00,  ...,  3.8247e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1941e-03,  0.0000e+00,  ...,  3.8247e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-56462.0391, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.6495, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(100.1598, device='cuda:0')



h[100].sum tensor(137.1244, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-119.3895, device='cuda:0')



h[200].sum tensor(-501.1708, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.6386, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0153, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0153, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0153, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0153, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(310402.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0116, 0.0000, 0.0163],
        [0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0000, 0.0300],
        [0.0000, 0.0000, 0.0000,  ..., 0.0123, 0.0000, 0.0384],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1727, 0.0000, 0.0889],
        [0.0000, 0.0000, 0.0000,  ..., 0.1727, 0.0000, 0.0889],
        [0.0000, 0.0000, 0.0000,  ..., 0.1727, 0.0000, 0.0889]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3480547.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(332.0850, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6612.6953, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10758.6680, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1557.5588, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.7731],
        [ 0.7839],
        [ 0.7903],
        ...,
        [-6.6776],
        [-6.6679],
        [-6.6659]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-934978.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2764],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(809.3580, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2764],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(809.3580, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0138e-02, -5.6464e-03, -3.8589e-04,  ...,  3.7010e-03,
         -6.2157e-03, -2.8208e-07],
        [-1.0265e-02, -5.5161e-03, -4.7768e-04,  ...,  3.6646e-03,
         -6.2657e-03, -3.4918e-07],
        [-9.6037e-03, -6.1942e-03,  0.0000e+00,  ...,  3.8537e-03,
         -6.0055e-03,  0.0000e+00],
        ...,
        [-9.6037e-03, -6.1942e-03,  0.0000e+00,  ...,  3.8537e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1942e-03,  0.0000e+00,  ...,  3.8537e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1942e-03,  0.0000e+00,  ...,  3.8537e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-56513.7305, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.5350, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(99.8371, device='cuda:0')



h[100].sum tensor(133.9620, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-119.0048, device='cuda:0')



h[200].sum tensor(-499.9211, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.5334, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0154, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0154, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0154, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(310032.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0118],
        [0.0000, 0.0000, 0.0000,  ..., 0.0331, 0.0000, 0.0376],
        [0.0000, 0.0000, 0.0000,  ..., 0.0788, 0.0000, 0.0640],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1731, 0.0000, 0.0891],
        [0.0000, 0.0000, 0.0000,  ..., 0.1731, 0.0000, 0.0891],
        [0.0000, 0.0000, 0.0000,  ..., 0.1731, 0.0000, 0.0891]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3486068.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(333.9174, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6608.1533, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11262.3281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1550.4158, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4417],
        [-0.5217],
        [-2.0525],
        ...,
        [-6.7102],
        [-6.7004],
        [-6.6984]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-977757.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 240.0 event: 3600 loss: tensor(427.9174, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2485],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(693.7061, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2485],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(693.7061, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1186e-02, -4.5716e-03, -1.1213e-03,  ...,  3.3412e-03,
         -6.6283e-03, -6.3268e-07],
        [-1.0957e-02, -4.8071e-03, -9.5852e-04,  ...,  3.4073e-03,
         -6.5379e-03, -5.4084e-07],
        [-1.0362e-02, -5.4168e-03, -5.3724e-04,  ...,  3.5783e-03,
         -6.3039e-03, -3.0314e-07],
        ...,
        [-9.6037e-03, -6.1944e-03,  0.0000e+00,  ...,  3.7964e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1944e-03,  0.0000e+00,  ...,  3.7964e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1944e-03,  0.0000e+00,  ...,  3.7964e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-57445.6523, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.4762, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(85.5711, device='cuda:0')



h[100].sum tensor(130.5855, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-101.9998, device='cuda:0')



h[200].sum tensor(-502.5391, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.8846, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0131, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(298913.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0075],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1678, 0.0000, 0.0875],
        [0.0000, 0.0000, 0.0000,  ..., 0.1325, 0.0000, 0.0783],
        [0.0000, 0.0000, 0.0000,  ..., 0.0669, 0.0000, 0.0576]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3410663.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(317.5691, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6617.1123, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10123.1699, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1480.8450, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.7958],
        [ 0.7828],
        [ 0.7808],
        ...,
        [-5.6698],
        [-4.2936],
        [-2.3601]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-842869.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(676.9705, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(676.9705, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-57401.7695, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.4354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(83.5067, device='cuda:0')



h[100].sum tensor(128.0780, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-99.5391, device='cuda:0')



h[200].sum tensor(-501.4570, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.2119, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0153, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0153, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0153, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0153, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0153, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(301528.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0494, 0.0000, 0.0540],
        [0.0000, 0.0000, 0.0000,  ..., 0.1231, 0.0000, 0.0774],
        [0.0000, 0.0000, 0.0000,  ..., 0.1532, 0.0000, 0.0851],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1716, 0.0000, 0.0885],
        [0.0000, 0.0000, 0.0000,  ..., 0.1716, 0.0000, 0.0885],
        [0.0000, 0.0000, 0.0000,  ..., 0.1716, 0.0000, 0.0885]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3437117., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(333.9249, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6608.2876, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11031.7812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1487.8628, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7673],
        [-4.4202],
        [-5.6629],
        ...,
        [-6.6877],
        [-6.6781],
        [-6.6748]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-896582.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(678.8495, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(678.8495, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.1947e-03,  0.0000e+00,  ...,  3.8493e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1947e-03,  0.0000e+00,  ...,  3.8493e-03,
         -6.0055e-03,  0.0000e+00],
        [-1.1761e-02, -3.9832e-03, -1.4699e-03,  ...,  3.2380e-03,
         -6.8545e-03, -3.7958e-07],
        ...,
        [-9.6037e-03, -6.1947e-03,  0.0000e+00,  ...,  3.8493e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1947e-03,  0.0000e+00,  ...,  3.8493e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1947e-03,  0.0000e+00,  ...,  3.8493e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-57636.7461, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.3590, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(83.7384, device='cuda:0')



h[100].sum tensor(127.1048, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-99.8153, device='cuda:0')



h[200].sum tensor(-500.6763, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.2874, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0154, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0154, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0154, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0154, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(299077.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0981, 0.0000, 0.0700],
        [0.0000, 0.0000, 0.0000,  ..., 0.0434, 0.0000, 0.0425],
        [0.0000, 0.0000, 0.0000,  ..., 0.0124, 0.0000, 0.0208],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1722, 0.0000, 0.0883],
        [0.0000, 0.0000, 0.0000,  ..., 0.1722, 0.0000, 0.0883],
        [0.0000, 0.0000, 0.0000,  ..., 0.1722, 0.0000, 0.0883]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3407521., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(318.9077, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6583.9976, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10074.5918, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1484.0338, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8032],
        [-0.1760],
        [ 0.3161],
        ...,
        [-6.7281],
        [-6.7184],
        [-6.7163]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-925993.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(656.4587, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(656.4587, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1224e-02, -4.5342e-03, -1.0825e-03,  ...,  3.4321e-03,
         -6.6432e-03, -1.2814e-07],
        [-9.6037e-03, -6.1948e-03,  0.0000e+00,  ...,  3.8827e-03,
         -6.0055e-03,  0.0000e+00],
        [-1.3343e-02, -2.3630e-03, -2.4977e-03,  ...,  2.8428e-03,
         -7.4769e-03, -2.9567e-07],
        ...,
        [-9.6037e-03, -6.1948e-03,  0.0000e+00,  ...,  3.8827e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1948e-03,  0.0000e+00,  ...,  3.8827e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1948e-03,  0.0000e+00,  ...,  3.8827e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-58032.4805, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.1452, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(80.9765, device='cuda:0')



h[100].sum tensor(126.1770, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-96.5231, device='cuda:0')



h[200].sum tensor(-500.5027, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.3874, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0155, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0155, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0155, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(296577.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1730, 0.0000, 0.0878],
        [0.0000, 0.0000, 0.0000,  ..., 0.1730, 0.0000, 0.0878],
        [0.0000, 0.0000, 0.0000,  ..., 0.1730, 0.0000, 0.0878]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3383773.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(294.8806, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6566.6143, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9112.8652, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1476.9584, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5196],
        [ 0.4905],
        [ 0.4775],
        ...,
        [-6.7474],
        [-6.7429],
        [-6.7464]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-946547.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2423],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(665.2990, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2423],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(665.2990, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0183e-02, -5.6010e-03, -3.7966e-04,  ...,  3.7713e-03,
         -6.2336e-03,  4.9751e-09],
        [-9.6037e-03, -6.1949e-03,  0.0000e+00,  ...,  3.9299e-03,
         -6.0055e-03,  0.0000e+00],
        [-1.0183e-02, -5.6010e-03, -3.7966e-04,  ...,  3.7713e-03,
         -6.2336e-03,  4.9751e-09],
        ...,
        [-9.6037e-03, -6.1949e-03,  0.0000e+00,  ...,  3.9299e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1949e-03,  0.0000e+00,  ...,  3.9299e-03,
         -6.0055e-03,  0.0000e+00],
        [-9.6037e-03, -6.1949e-03,  0.0000e+00,  ...,  3.9299e-03,
         -6.0055e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-58111.9531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.1990, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(82.0669, device='cuda:0')



h[100].sum tensor(124.0727, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-97.8229, device='cuda:0')



h[200].sum tensor(-499.3965, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.7428, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.5590e-02, 0.0000e+00,
         4.0621e-09],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.5143e-02, 0.0000e+00,
         1.8074e-08],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.5590e-02, 0.0000e+00,
         4.0621e-09],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.5719e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.5719e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.5719e-02, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(297639.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0463, 0.0000, 0.0501],
        [0.0000, 0.0000, 0.0000,  ..., 0.0093, 0.0000, 0.0288],
        [0.0000, 0.0000, 0.0000,  ..., 0.0172, 0.0000, 0.0338],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1739, 0.0000, 0.0877],
        [0.0000, 0.0000, 0.0000,  ..., 0.1739, 0.0000, 0.0877],
        [0.0000, 0.0000, 0.0000,  ..., 0.1739, 0.0000, 0.0877]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3389090.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(296.3701, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6533.2832, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8973.0039, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1482.7441, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0402],
        [ 0.4672],
        [ 0.5828],
        ...,
        [-6.7914],
        [-6.7853],
        [-6.7882]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1054822., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5176],
        [0.3240],
        [0.4539],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(806.5464, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5176],
        [0.3240],
        [0.4539],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(806.5464, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0126, -0.0031, -0.0019,  ...,  0.0031, -0.0072, -0.0019],
        [-0.0119, -0.0038, -0.0015,  ...,  0.0033, -0.0069, -0.0018],
        [-0.0113, -0.0045, -0.0011,  ...,  0.0035, -0.0067, -0.0017],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0014],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0014],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-56821.5312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.5696, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(99.4903, device='cuda:0')



h[100].sum tensor(134.5620, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-118.5914, device='cuda:0')



h[200].sum tensor(-493.3962, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.4204, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0117, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0123, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0122, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(316949.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1731, 0.0000, 0.0869],
        [0.0000, 0.0000, 0.0000,  ..., 0.1731, 0.0000, 0.0869],
        [0.0000, 0.0000, 0.0000,  ..., 0.1731, 0.0000, 0.0869]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3551077., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(328.4191, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6603.8047, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12657.2051, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1524.9943, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2776],
        [ 0.2970],
        [ 0.3498],
        ...,
        [-6.7944],
        [-6.7847],
        [-6.7828]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-945568.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6113],
        [0.6641],
        [0.5474],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(681.1772, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6113],
        [0.6641],
        [0.5474],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(681.1772, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0122, -0.0035, -0.0017,  ...,  0.0032, -0.0070, -0.0035],
        [-0.0135, -0.0022, -0.0025,  ...,  0.0029, -0.0075, -0.0038],
        [-0.0136, -0.0021, -0.0025,  ...,  0.0028, -0.0076, -0.0039],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0027],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0027],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-58184.3008, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.4005, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(84.0256, device='cuda:0')



h[100].sum tensor(127.1928, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-100.1576, device='cuda:0')



h[200].sum tensor(-497.1837, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.3810, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0116, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0115, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0121, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(301036.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1725, 0.0000, 0.0864],
        [0.0000, 0.0000, 0.0000,  ..., 0.1725, 0.0000, 0.0864],
        [0.0000, 0.0000, 0.0000,  ..., 0.1725, 0.0000, 0.0864]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3419143.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(297.5149, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6582.7217, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9944.4609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1421.1116, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3974],
        [ 0.3933],
        [ 0.4118],
        ...,
        [-6.7723],
        [-6.7627],
        [-6.7609]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-921492.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(652.0826, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(652.0826, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0027],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0027],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0027],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0027],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0027],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-58480.8828, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.1206, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(80.4367, device='cuda:0')



h[100].sum tensor(125.3842, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-95.8797, device='cuda:0')



h[200].sum tensor(-498.2179, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.2115, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(298179.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1720, 0.0000, 0.0869],
        [0.0000, 0.0000, 0.0000,  ..., 0.1720, 0.0000, 0.0869],
        [0.0000, 0.0000, 0.0000,  ..., 0.1718, 0.0000, 0.0872],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1725, 0.0000, 0.0864],
        [0.0000, 0.0000, 0.0000,  ..., 0.1725, 0.0000, 0.0864],
        [0.0000, 0.0000, 0.0000,  ..., 0.1725, 0.0000, 0.0864]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3392654.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(293.3853, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6580.9526, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9327.2344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1409.2218, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.9298],
        [-6.4290],
        [-6.7191],
        ...,
        [-6.7733],
        [-6.7637],
        [-6.7619]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-912612.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(621.3097, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(621.3097, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0039],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0039],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0039],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0039],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0039],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0039]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-58970.8086, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.8033, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(76.6407, device='cuda:0')



h[100].sum tensor(120.3358, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-91.3549, device='cuda:0')



h[200].sum tensor(-498.9550, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.9745, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(292782.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1725, 0.0000, 0.0866],
        [0.0000, 0.0000, 0.0000,  ..., 0.1725, 0.0000, 0.0866],
        [0.0000, 0.0000, 0.0000,  ..., 0.1722, 0.0000, 0.0869],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1730, 0.0000, 0.0861],
        [0.0000, 0.0000, 0.0000,  ..., 0.1730, 0.0000, 0.0861],
        [0.0000, 0.0000, 0.0000,  ..., 0.1730, 0.0000, 0.0861]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3344659.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(292.8850, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6557.8164, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8518.3105, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1362.6025, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.8216],
        [-6.7202],
        [-6.4451],
        ...,
        [-6.7931],
        [-6.7835],
        [-6.7817]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-972138., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(743.5371, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(743.5371, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0050],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0050],
        [-0.0102, -0.0055, -0.0004,  ...,  0.0037, -0.0063, -0.0053],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0050],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0050],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0050]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-58001.5703, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.9503, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(91.7179, device='cuda:0')



h[100].sum tensor(129.7229, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-109.3268, device='cuda:0')



h[200].sum tensor(-494.0768, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.8877, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(307959.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0725, 0.0000, 0.0600],
        [0.0000, 0.0000, 0.0000,  ..., 0.0284, 0.0000, 0.0275],
        [0.0000, 0.0000, 0.0000,  ..., 0.0023, 0.0000, 0.0089],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1724, 0.0000, 0.0850],
        [0.0000, 0.0000, 0.0000,  ..., 0.1724, 0.0000, 0.0850],
        [0.0000, 0.0000, 0.0000,  ..., 0.1724, 0.0000, 0.0850]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3450650.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(330.5131, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6591.9634, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10629.1816, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1407.5940, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3270],
        [ 0.2352],
        [ 0.5113],
        ...,
        [-6.7705],
        [-6.7609],
        [-6.7590]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-907408.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 250.0 event: 3750 loss: tensor(427.5748, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(623.6949, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(623.6949, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0059],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0059],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0059],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0059],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0059],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0059]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-59511.7031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.7935, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(76.9349, device='cuda:0')



h[100].sum tensor(123.4556, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-91.7056, device='cuda:0')



h[200].sum tensor(-498.3878, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.0704, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0155, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0155, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0155, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0155, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0155, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0155, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(296268.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1722, 0.0000, 0.0846],
        [0.0000, 0.0000, 0.0000,  ..., 0.1722, 0.0000, 0.0846],
        [0.0000, 0.0000, 0.0000,  ..., 0.1719, 0.0000, 0.0849],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1727, 0.0000, 0.0841],
        [0.0000, 0.0000, 0.0000,  ..., 0.1727, 0.0000, 0.0841],
        [0.0000, 0.0000, 0.0000,  ..., 0.1727, 0.0000, 0.0841]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3383336.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(305.2354, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6580.8574, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8778.4609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1350.1643, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.8024],
        [-6.6486],
        [-6.3549],
        ...,
        [-6.7773],
        [-6.7679],
        [-6.7661]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-863777.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(695.0833, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(695.0833, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0102, -0.0055, -0.0004,  ...,  0.0038, -0.0063, -0.0072],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0040, -0.0060, -0.0068],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0040, -0.0060, -0.0068],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0040, -0.0060, -0.0068],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0040, -0.0060, -0.0068],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0040, -0.0060, -0.0068]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-59165.7461, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.4916, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(85.7409, device='cuda:0')



h[100].sum tensor(120.4574, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-102.2023, device='cuda:0')



h[200].sum tensor(-496.2056, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.9400, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0154, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0159, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0159, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0159, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0159, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(301709.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0272, 0.0000, 0.0319],
        [0.0000, 0.0000, 0.0000,  ..., 0.0721, 0.0000, 0.0566],
        [0.0000, 0.0000, 0.0000,  ..., 0.1331, 0.0000, 0.0740],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1755, 0.0000, 0.0843],
        [0.0000, 0.0000, 0.0000,  ..., 0.1755, 0.0000, 0.0843],
        [0.0000, 0.0000, 0.0000,  ..., 0.1755, 0.0000, 0.0843]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3414998.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(321.0637, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6614.7373, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9604.1406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1359.0190, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3630],
        [-1.7418],
        [-3.3479],
        ...,
        [-6.8771],
        [-6.8674],
        [-6.8655]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-998050.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(654.2928, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(654.2928, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0075],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0075],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0075],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0075],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0075],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0075]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-59721.5859, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.1196, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(80.7093, device='cuda:0')



h[100].sum tensor(122.1123, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-96.2046, device='cuda:0')



h[200].sum tensor(-497.6530, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.3004, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0154, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(299354.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0273, 0.0000, 0.0306],
        [0.0000, 0.0000, 0.0000,  ..., 0.0323, 0.0000, 0.0367],
        [0.0000, 0.0000, 0.0000,  ..., 0.0412, 0.0000, 0.0452],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1747, 0.0000, 0.0829],
        [0.0000, 0.0000, 0.0000,  ..., 0.1747, 0.0000, 0.0829],
        [0.0000, 0.0000, 0.0000,  ..., 0.1747, 0.0000, 0.0829]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3391920.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(333.8706, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6586.8208, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8602.7500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1374.3789, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4587],
        [ 0.7574],
        [ 0.8329],
        ...,
        [-6.8296],
        [-6.7355],
        [-6.4417]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-906650.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(667.7500, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(667.7500, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0101, -0.0057, -0.0003,  ...,  0.0038, -0.0062, -0.0086],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0082],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0082],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0082],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0082],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0082]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-59792.3672, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.2207, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(82.3693, device='cuda:0')



h[100].sum tensor(117.8091, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-98.1833, device='cuda:0')



h[200].sum tensor(-497.2073, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.8413, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0154, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0154, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(300548.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1186, 0.0000, 0.0669],
        [0.0000, 0.0000, 0.0000,  ..., 0.1292, 0.0000, 0.0700],
        [0.0000, 0.0000, 0.0000,  ..., 0.1597, 0.0000, 0.0792],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1755, 0.0000, 0.0827],
        [0.0000, 0.0000, 0.0000,  ..., 0.1755, 0.0000, 0.0827],
        [0.0000, 0.0000, 0.0000,  ..., 0.1755, 0.0000, 0.0827]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3414742.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(379.6843, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6622.5498, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9904.5156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1377.7122, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.8842],
        [-4.6723],
        [-5.3337],
        ...,
        [-6.8900],
        [-6.8804],
        [-6.8786]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-953504.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2703],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(691.0382, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2703],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(691.0382, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0089],
        [-0.0103, -0.0055, -0.0004,  ...,  0.0037, -0.0063, -0.0095],
        [-0.0101, -0.0056, -0.0003,  ...,  0.0037, -0.0062, -0.0094],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0089],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0089],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0089]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-59662.9883, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.4509, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(85.2420, device='cuda:0')



h[100].sum tensor(122.3536, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-101.6075, device='cuda:0')



h[200].sum tensor(-495.7800, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.7774, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0154, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0154, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0154, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(304530.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0773, 0.0000, 0.0562],
        [0.0000, 0.0000, 0.0000,  ..., 0.0322, 0.0000, 0.0334],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0137],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1747, 0.0000, 0.0818],
        [0.0000, 0.0000, 0.0000,  ..., 0.1747, 0.0000, 0.0818],
        [0.0000, 0.0000, 0.0000,  ..., 0.1747, 0.0000, 0.0818]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3447355.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(411.3474, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6655.2964, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10911.1406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1401.3481, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8237],
        [-0.4984],
        [ 0.1667],
        ...,
        [-6.8689],
        [-6.8594],
        [-6.8576]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-859549.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(801.1567, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(801.1567, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0112, -0.0046, -0.0008,  ...,  0.0034, -0.0066, -0.0110],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060, -0.0094],
        [-0.0111, -0.0047, -0.0008,  ...,  0.0034, -0.0066, -0.0109],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060, -0.0094],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060, -0.0094],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060, -0.0094]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-58688.4688, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.5275, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(98.8255, device='cuda:0')



h[100].sum tensor(131.3317, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-117.7989, device='cuda:0')



h[200].sum tensor(-491.1721, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.2038, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0139, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0153, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0153, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0153, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(316440.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0060],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1747, 0.0000, 0.0809],
        [0.0000, 0.0000, 0.0000,  ..., 0.1747, 0.0000, 0.0809],
        [0.0000, 0.0000, 0.0000,  ..., 0.1747, 0.0000, 0.0809]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3527340.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(447.1118, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6661.7578, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12345.6201, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1472.8872, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.8861],
        [ 0.9387],
        [ 0.9474],
        ...,
        [-6.8793],
        [-6.8699],
        [-6.8681]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-810372.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(692.3062, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(692.3062, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0100],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0100],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0100],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0100],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0100],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0100]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-60039.0703, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.5149, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(85.3984, device='cuda:0')



h[100].sum tensor(120.0781, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-101.7940, device='cuda:0')



h[200].sum tensor(-494.3940, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.8284, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(304551.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1153, 0.0000, 0.0665],
        [0.0000, 0.0000, 0.0000,  ..., 0.1673, 0.0000, 0.0791],
        [0.0000, 0.0000, 0.0000,  ..., 0.1759, 0.0000, 0.0815],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1767, 0.0000, 0.0807],
        [0.0000, 0.0000, 0.0000,  ..., 0.1767, 0.0000, 0.0807],
        [0.0000, 0.0000, 0.0000,  ..., 0.1767, 0.0000, 0.0807]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3447730., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(432.7911, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6666.9756, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10962.1816, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1434.4515, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.3713],
        [-5.2069],
        [-6.4178],
        ...,
        [-6.9645],
        [-6.9512],
        [-6.9465]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-912370.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(771.8138, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(771.8138, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0122, -0.0035, -0.0013,  ...,  0.0034, -0.0070, -0.0133],
        [-0.0122, -0.0035, -0.0014,  ...,  0.0034, -0.0070, -0.0133],
        [-0.0115, -0.0042, -0.0010,  ...,  0.0036, -0.0068, -0.0125],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0041, -0.0060, -0.0104],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0041, -0.0060, -0.0104],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0041, -0.0060, -0.0104]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-59423.0703, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.2837, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(95.2059, device='cuda:0')



h[100].sum tensor(119.4980, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-113.4845, device='cuda:0')



h[200].sum tensor(-491.1589, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.0243, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0162, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0162, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0162, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(311790.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0015],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1799, 0.0000, 0.0805],
        [0.0000, 0.0000, 0.0000,  ..., 0.1799, 0.0000, 0.0805],
        [0.0000, 0.0000, 0.0000,  ..., 0.1799, 0.0000, 0.0805]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3475168.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(446.7445, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6549.8730, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11007.8291, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1464.3464, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.9055],
        [ 0.9381],
        [ 0.8749],
        ...,
        [-7.1070],
        [-7.0966],
        [-7.0946]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1137637.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1008.4421, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1008.4421, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0041, -0.0060, -0.0104],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0041, -0.0060, -0.0104],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0041, -0.0060, -0.0104],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0041, -0.0060, -0.0104],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0041, -0.0060, -0.0104],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0041, -0.0060, -0.0104]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-57110.8125, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-988.4584, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(124.3948, device='cuda:0')



h[100].sum tensor(133.5922, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-148.2773, device='cuda:0')



h[200].sum tensor(-483.0975, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.5360, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0162, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0162, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0161, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0162, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0162, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0162, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(342691.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1721, 0.0000, 0.0789],
        [0.0000, 0.0000, 0.0000,  ..., 0.1468, 0.0000, 0.0723],
        [0.0000, 0.0000, 0.0000,  ..., 0.1016, 0.0000, 0.0607],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1799, 0.0000, 0.0805],
        [0.0000, 0.0000, 0.0000,  ..., 0.1799, 0.0000, 0.0805],
        [0.0000, 0.0000, 0.0000,  ..., 0.1799, 0.0000, 0.0805]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3741910., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(512.1985, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6578.3530, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(16420.6875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1607.3982, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.4523],
        [-4.1802],
        [-2.6976],
        ...,
        [-7.1070],
        [-7.0966],
        [-7.0946]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1104918., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(609.0077, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(609.0077, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0102, -0.0056, -0.0003,  ...,  0.0037, -0.0063, -0.0115],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0108],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0108],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0108],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0108],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0108]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-60930.9844, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.6890, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(75.1232, device='cuda:0')



h[100].sum tensor(121.9350, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-89.5461, device='cuda:0')



h[200].sum tensor(-496.7080, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.4800, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0155, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(298922.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0277, 0.0000, 0.0247],
        [0.0000, 0.0000, 0.0000,  ..., 0.0758, 0.0000, 0.0515],
        [0.0000, 0.0000, 0.0000,  ..., 0.1419, 0.0000, 0.0713],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1774, 0.0000, 0.0795],
        [0.0000, 0.0000, 0.0000,  ..., 0.1774, 0.0000, 0.0795],
        [0.0000, 0.0000, 0.0000,  ..., 0.1774, 0.0000, 0.0795]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3373159.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(411.5122, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6334.2012, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7538.0137, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1379.4805, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3447],
        [-1.9979],
        [-3.8643],
        ...,
        [-7.0009],
        [-6.9912],
        [-6.9892]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-831509.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 260.0 event: 3900 loss: tensor(419.4032, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(663.1602, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(663.1602, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0041, -0.0060, -0.0112],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0041, -0.0060, -0.0112],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0041, -0.0060, -0.0112],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0041, -0.0060, -0.0112],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0041, -0.0060, -0.0112],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0041, -0.0060, -0.0112]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-60810.0859, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.1868, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(81.8031, device='cuda:0')



h[100].sum tensor(103.2441, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-97.5084, device='cuda:0')



h[200].sum tensor(-494.8842, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.6568, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0165, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0165, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0165, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0165, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0165, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0165, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(299037.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1771, 0.0000, 0.0808],
        [0.0000, 0.0000, 0.0000,  ..., 0.1802, 0.0000, 0.0816],
        [0.0000, 0.0000, 0.0000,  ..., 0.1800, 0.0000, 0.0819],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1807, 0.0000, 0.0811],
        [0.0000, 0.0000, 0.0000,  ..., 0.1807, 0.0000, 0.0811],
        [0.0000, 0.0000, 0.0000,  ..., 0.1807, 0.0000, 0.0811]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3392759.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(501.7322, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6565.4604, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11923.7988, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1364.1096, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.4100],
        [-6.9744],
        [-7.1734],
        ...,
        [-7.1796],
        [-7.1691],
        [-7.1670]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1276385.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(759.9287, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(759.9287, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0115],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0115],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0115],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0115],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0115],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0039, -0.0060, -0.0115]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-59549.1367, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.1083, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(93.7398, device='cuda:0')



h[100].sum tensor(120.4282, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-111.7369, device='cuda:0')



h[200].sum tensor(-490.7907, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.5466, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0155, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0155, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0155, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0155, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0155, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0155, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(315637.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1688, 0.0000, 0.0796],
        [0.0000, 0.0000, 0.0000,  ..., 0.1745, 0.0000, 0.0811],
        [0.0000, 0.0000, 0.0000,  ..., 0.1751, 0.0000, 0.0817],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1759, 0.0000, 0.0809],
        [0.0000, 0.0000, 0.0000,  ..., 0.1759, 0.0000, 0.0809],
        [0.0000, 0.0000, 0.0000,  ..., 0.1759, 0.0000, 0.0809]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3540145.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(377.8945, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6732.8970, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(16268.7822, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1460.7156, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.7350],
        [-5.7495],
        [-6.3510],
        ...,
        [-6.9948],
        [-6.9852],
        [-6.9835]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-857634.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3884],
        [0.6343],
        [0.3738],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(657.2925, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3884],
        [0.6343],
        [0.3738],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(657.2925, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0123, -0.0034, -0.0013,  ...,  0.0029, -0.0071, -0.0152],
        [-0.0114, -0.0043, -0.0009,  ...,  0.0031, -0.0067, -0.0141],
        [-0.0120, -0.0037, -0.0012,  ...,  0.0030, -0.0070, -0.0149],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060, -0.0119],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060, -0.0119],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060, -0.0119]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-60428.2422, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.1781, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(81.0793, device='cuda:0')



h[100].sum tensor(132.5215, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-96.6457, device='cuda:0')



h[200].sum tensor(-493.5227, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.4209, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0122, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0116, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0119, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(305216.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1719, 0.0000, 0.0797],
        [0.0000, 0.0000, 0.0000,  ..., 0.1719, 0.0000, 0.0797],
        [0.0000, 0.0000, 0.0000,  ..., 0.1719, 0.0000, 0.0797]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3409553.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(118.2058, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6756.6235, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11358.3252, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1515.0778, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.7892],
        [ 0.8127],
        [ 0.8608],
        ...,
        [-6.8172],
        [-6.8092],
        [-6.8094]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-555775.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(627.7366, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(627.7366, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060, -0.0121],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060, -0.0121],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060, -0.0121],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060, -0.0121],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060, -0.0121],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060, -0.0121]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-61081.6875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.8807, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(77.4335, device='cuda:0')



h[100].sum tensor(128.5991, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-92.2999, device='cuda:0')



h[200].sum tensor(-494.2999, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.2329, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(302436.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1731, 0.0000, 0.0812],
        [0.0000, 0.0000, 0.0000,  ..., 0.1722, 0.0000, 0.0809],
        [0.0000, 0.0000, 0.0000,  ..., 0.1672, 0.0000, 0.0797],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1737, 0.0000, 0.0807],
        [0.0000, 0.0000, 0.0000,  ..., 0.1737, 0.0000, 0.0807],
        [0.0000, 0.0000, 0.0000,  ..., 0.1737, 0.0000, 0.0807]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3388637., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(235.2057, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6681.5444, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10884.0508, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1306.6406, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.3083],
        [-5.9159],
        [-5.2045],
        ...,
        [-6.9211],
        [-6.9121],
        [-6.9106]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-643971.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(688.5387, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(688.5387, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060, -0.0124],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060, -0.0124],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060, -0.0124],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060, -0.0124],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060, -0.0124],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060, -0.0124]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-61016.6094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.4439, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(84.9336, device='cuda:0')



h[100].sum tensor(117.6599, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-101.2400, device='cuda:0')



h[200].sum tensor(-491.8449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.6769, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(303789.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1712, 0.0000, 0.0816],
        [0.0000, 0.0000, 0.0000,  ..., 0.1699, 0.0000, 0.0815],
        [0.0000, 0.0000, 0.0000,  ..., 0.1617, 0.0000, 0.0800],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1788, 0.0000, 0.0834],
        [0.0000, 0.0000, 0.0000,  ..., 0.1788, 0.0000, 0.0834],
        [0.0000, 0.0000, 0.0000,  ..., 0.1788, 0.0000, 0.0834]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3388671., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(432.2649, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6618.9570, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12139.3936, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1115.0248, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.2094],
        [-4.7593],
        [-3.9874],
        ...,
        [-7.1617],
        [-7.1517],
        [-7.1494]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1025954., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(719.2560, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(719.2560, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0106, -0.0052, -0.0004,  ...,  0.0035, -0.0064, -0.0139],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060, -0.0126],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060, -0.0126],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060, -0.0126],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060, -0.0126],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0038, -0.0060, -0.0126]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-61312.1914, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.7646, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(88.7227, device='cuda:0')



h[100].sum tensor(116.4944, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-105.7566, device='cuda:0')



h[200].sum tensor(-489.9167, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.9117, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(304972.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0000, 0.0250],
        [0.0000, 0.0000, 0.0000,  ..., 0.0377, 0.0000, 0.0443],
        [0.0000, 0.0000, 0.0000,  ..., 0.0347, 0.0000, 0.0459],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0790, 0.0000, 0.0591],
        [0.0000, 0.0000, 0.0000,  ..., 0.1394, 0.0000, 0.0737],
        [0.0000, 0.0000, 0.0000,  ..., 0.1813, 0.0000, 0.0838]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3413181.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(563.0560, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6584.9258, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12226.5244, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1202.7029, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.9556],
        [ 0.9487],
        [ 0.9765],
        ...,
        [-2.9054],
        [-4.8592],
        [-6.2389]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1159008.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3687],
        [0.3198],
        [0.3198],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(731.7681, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3687],
        [0.3198],
        [0.3198],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(731.7681, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0104, -0.0054, -0.0003,  ...,  0.0034, -0.0063, -0.0138],
        [-0.0121, -0.0036, -0.0011,  ...,  0.0029, -0.0070, -0.0162],
        [-0.0121, -0.0036, -0.0011,  ...,  0.0030, -0.0070, -0.0162],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060, -0.0128],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060, -0.0128],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060, -0.0128]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-61481.4023, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.8284, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(90.2661, device='cuda:0')



h[100].sum tensor(131.5166, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-107.5963, device='cuda:0')



h[200].sum tensor(-488.1204, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.4146, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0123, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0122, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0120, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(304671.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1777, 0.0000, 0.0808],
        [0.0000, 0.0000, 0.0000,  ..., 0.1777, 0.0000, 0.0808],
        [0.0000, 0.0000, 0.0000,  ..., 0.1777, 0.0000, 0.0808]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3404774., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(551.3970, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6603.1025, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10990.7832, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1354.3262, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.9506],
        [ 0.9274],
        [ 0.9130],
        ...,
        [-7.0807],
        [-7.0474],
        [-6.9547]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-735491.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3723],
        [0.3076],
        [0.3792],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(736.1877, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3723],
        [0.3076],
        [0.3792],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(736.1877, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0130, -0.0027, -0.0014,  ...,  0.0027, -0.0073, -0.0176],
        [-0.0121, -0.0036, -0.0011,  ...,  0.0029, -0.0070, -0.0164],
        [-0.0103, -0.0054, -0.0003,  ...,  0.0034, -0.0063, -0.0140],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060, -0.0130],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060, -0.0130],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0036, -0.0060, -0.0130]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-61860.7578, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.8746, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(90.8113, device='cuda:0')



h[100].sum tensor(124.4270, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-108.2461, device='cuda:0')



h[200].sum tensor(-486.2529, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.5923, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0116, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0121, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0127, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(301447., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0032],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1776, 0.0000, 0.0814],
        [0.0000, 0.0000, 0.0000,  ..., 0.1776, 0.0000, 0.0814],
        [0.0000, 0.0000, 0.0000,  ..., 0.1776, 0.0000, 0.0814]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3363933.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(603.1958, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6678.0190, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13028.6953, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1318.0096, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.9631],
        [ 1.0232],
        [ 1.0727],
        ...,
        [-7.0665],
        [-7.0573],
        [-7.0556]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-687870.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(872.1223, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(872.1223, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0114, -0.0043, -0.0008,  ...,  0.0032, -0.0067, -0.0157],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060, -0.0132],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060, -0.0132],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060, -0.0132],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060, -0.0132],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060, -0.0132]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-61178.6953, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-987.1017, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(107.5793, device='cuda:0')



h[100].sum tensor(114.2023, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-128.2334, device='cuda:0')



h[200].sum tensor(-480.2978, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.0564, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0128, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0137, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(312203.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0030],
        [0.0000, 0.0000, 0.0000,  ..., 0.0206, 0.0000, 0.0252],
        [0.0000, 0.0000, 0.0000,  ..., 0.0731, 0.0000, 0.0539],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1678, 0.0000, 0.0803],
        [0.0000, 0.0000, 0.0000,  ..., 0.1165, 0.0000, 0.0682],
        [0.0000, 0.0000, 0.0000,  ..., 0.0463, 0.0000, 0.0376]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3428157.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(703.1560, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6706.0088, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(17287.2656, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1401.3169, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4479],
        [-0.1247],
        [-0.8507],
        ...,
        [-5.3946],
        [-3.4781],
        [-1.3083]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-925501.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(674.3522, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(674.3522, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0104, -0.0054, -0.0003,  ...,  0.0034, -0.0063, -0.0144],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060, -0.0133],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060, -0.0133],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060, -0.0133],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060, -0.0133],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0037, -0.0060, -0.0133]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-63739.5508, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.2877, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(83.1837, device='cuda:0')



h[100].sum tensor(95.6964, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-99.1541, device='cuda:0')



h[200].sum tensor(-486.0450, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.1067, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(292143., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1067, 0.0000, 0.0658],
        [0.0000, 0.0000, 0.0000,  ..., 0.1258, 0.0000, 0.0707],
        [0.0000, 0.0000, 0.0000,  ..., 0.1607, 0.0000, 0.0794],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1842, 0.0000, 0.0840],
        [0.0000, 0.0000, 0.0000,  ..., 0.1842, 0.0000, 0.0840],
        [0.0000, 0.0000, 0.0000,  ..., 0.1842, 0.0000, 0.0840]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3286537.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(673.0227, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6662.6455, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14404.3867, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1377.9009, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.3832],
        [-4.6840],
        [-5.4141],
        ...,
        [-7.3027],
        [-7.2926],
        [-7.2908]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1061779.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 270.0 event: 4050 loss: tensor(484.4141, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(817.7351, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(817.7351, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0110, -0.0048, -0.0006,  ...,  0.0031, -0.0066, -0.0154],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060, -0.0134],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060, -0.0134],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060, -0.0134],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060, -0.0134],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060, -0.0134]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-62708.3086, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.6557, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(100.8705, device='cuda:0')



h[100].sum tensor(112.9362, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-120.2365, device='cuda:0')



h[200].sum tensor(-479.7728, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.8702, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0127, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(306263.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         3.4058e-03],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         3.2246e-03],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         1.8124e-04],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.8433e-01, 0.0000e+00,
         8.2406e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.8433e-01, 0.0000e+00,
         8.2406e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.8433e-01, 0.0000e+00,
         8.2406e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3356283.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(655.1178, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6639.9312, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13850.8555, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1492.6970, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.7345],
        [ 0.5775],
        [ 0.3846],
        ...,
        [-7.2911],
        [-7.2812],
        [-7.2794]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-922712.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(742.9426, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(742.9426, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060, -0.0136],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060, -0.0136],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060, -0.0136],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060, -0.0136],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060, -0.0136],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0034, -0.0060, -0.0136]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-63882.7617, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.9778, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(91.6446, device='cuda:0')



h[100].sum tensor(109.3960, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-109.2393, device='cuda:0')



h[200].sum tensor(-481.5404, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.8638, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(297272.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1851, 0.0000, 0.0822],
        [0.0000, 0.0000, 0.0000,  ..., 0.1851, 0.0000, 0.0822],
        [0.0000, 0.0000, 0.0000,  ..., 0.1848, 0.0000, 0.0825],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1856, 0.0000, 0.0817],
        [0.0000, 0.0000, 0.0000,  ..., 0.1856, 0.0000, 0.0817],
        [0.0000, 0.0000, 0.0000,  ..., 0.1856, 0.0000, 0.0817]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3282512., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(620.6709, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6601.5400, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11593.4463, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1521.3665, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.5995],
        [-7.6428],
        [-7.6214],
        ...,
        [-7.3349],
        [-7.3248],
        [-7.3230]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-938601., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(835.6107, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(835.6107, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060, -0.0137],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060, -0.0137],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060, -0.0137],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060, -0.0137],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060, -0.0137],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0035, -0.0060, -0.0137]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-63538.6719, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.8427, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(103.0755, device='cuda:0')



h[100].sum tensor(93.7075, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-122.8649, device='cuda:0')



h[200].sum tensor(-477.7805, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.5887, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(301276., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1510, 0.0000, 0.0748],
        [0.0000, 0.0000, 0.0000,  ..., 0.1853, 0.0000, 0.0832],
        [0.0000, 0.0000, 0.0000,  ..., 0.1886, 0.0000, 0.0843],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1703, 0.0000, 0.0781],
        [0.0000, 0.0000, 0.0000,  ..., 0.1824, 0.0000, 0.0815],
        [0.0000, 0.0000, 0.0000,  ..., 0.1893, 0.0000, 0.0835]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3282956., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(698.5927, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6631.6353, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14498.1787, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1610.7876, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.3876],
        [-5.9625],
        [-6.9073],
        ...,
        [-6.7062],
        [-7.0891],
        [-7.3387]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1349036.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(676.7633, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(676.7633, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0120, -0.0037, -0.0009,  ...,  0.0025, -0.0070, -0.0173],
        [-0.0107, -0.0050, -0.0004,  ...,  0.0029, -0.0065, -0.0154],
        [-0.0101, -0.0056, -0.0002,  ...,  0.0031, -0.0062, -0.0145],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0033, -0.0060, -0.0138],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0033, -0.0060, -0.0138],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0033, -0.0060, -0.0138]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-65189.9570, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.3582, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(83.4811, device='cuda:0')



h[100].sum tensor(90.5170, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-99.5086, device='cuda:0')



h[200].sum tensor(-482.8897, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.2036, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0115, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0121, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(291674.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0094],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1849, 0.0000, 0.0822],
        [0.0000, 0.0000, 0.0000,  ..., 0.1849, 0.0000, 0.0822],
        [0.0000, 0.0000, 0.0000,  ..., 0.1849, 0.0000, 0.0822]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3244436., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(690.8074, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6621.1909, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14749.1309, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1530.8838, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.9968],
        [ 1.0202],
        [ 0.9939],
        ...,
        [-7.3777],
        [-7.3678],
        [-7.3660]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1080706., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(642.4899, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(642.4899, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0103, -0.0054, -0.0003,  ...,  0.0028, -0.0063, -0.0149],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0031, -0.0060, -0.0139],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0031, -0.0060, -0.0139],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0031, -0.0060, -0.0139],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0031, -0.0060, -0.0139],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0031, -0.0060, -0.0139]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-65555.0312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.0186, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(79.2534, device='cuda:0')



h[100].sum tensor(102.7416, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-94.4692, device='cuda:0')



h[200].sum tensor(-483.7861, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.8259, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0120, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0120, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0122, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0122, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0122, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0122, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(289188.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1099, 0.0000, 0.0625],
        [0.0000, 0.0000, 0.0000,  ..., 0.1255, 0.0000, 0.0670],
        [0.0000, 0.0000, 0.0000,  ..., 0.1624, 0.0000, 0.0766],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1795, 0.0000, 0.0799],
        [0.0000, 0.0000, 0.0000,  ..., 0.1795, 0.0000, 0.0799],
        [0.0000, 0.0000, 0.0000,  ..., 0.1795, 0.0000, 0.0799]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3210297., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(659.7859, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6590.7256, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13574.7607, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1470.1025, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.4879],
        [-4.3658],
        [-5.2819],
        ...,
        [-7.2030],
        [-7.1940],
        [-7.1925]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-742330.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(629.4998, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(629.4998, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0030, -0.0060, -0.0139],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0030, -0.0060, -0.0139],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0030, -0.0060, -0.0139],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0030, -0.0060, -0.0139],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0030, -0.0060, -0.0139],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0030, -0.0060, -0.0139]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-65921.3516, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.8992, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(77.6510, device='cuda:0')



h[100].sum tensor(105.5029, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-92.5592, device='cuda:0')



h[200].sum tensor(-484.3199, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.3038, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(287378.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1253, 0.0000, 0.0674],
        [0.0000, 0.0000, 0.0000,  ..., 0.1482, 0.0000, 0.0726],
        [0.0000, 0.0000, 0.0000,  ..., 0.1173, 0.0000, 0.0652],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1780, 0.0000, 0.0790],
        [0.0000, 0.0000, 0.0000,  ..., 0.1780, 0.0000, 0.0790],
        [0.0000, 0.0000, 0.0000,  ..., 0.1780, 0.0000, 0.0790]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3174383.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(638.5411, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6528.6870, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11998.9355, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1468.3513, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.8514],
        [-2.9944],
        [-2.0893],
        ...,
        [-7.1702],
        [-7.1614],
        [-7.1600]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-694200.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.9556],
        [0.0000],
        [0.4387],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(605.6528, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.9556],
        [0.0000],
        [0.4387],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(605.6528, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0030, -0.0060, -0.0140],
        [-0.0129, -0.0028, -0.0012,  ...,  0.0019, -0.0073, -0.0189],
        [-0.0105, -0.0053, -0.0003,  ...,  0.0027, -0.0064, -0.0153],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0030, -0.0060, -0.0140],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0030, -0.0060, -0.0140],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0030, -0.0060, -0.0140]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-66613.8984, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.6713, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(74.7094, device='cuda:0')



h[100].sum tensor(91.3570, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-89.0528, device='cuda:0')



h[200].sum tensor(-485.4686, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.3452, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0088, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0108, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0097, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0120, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0120, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0120, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(283521.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1806, 0.0000, 0.0803],
        [0.0000, 0.0000, 0.0000,  ..., 0.1806, 0.0000, 0.0803],
        [0.0000, 0.0000, 0.0000,  ..., 0.1806, 0.0000, 0.0803]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3150880., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(650.5052, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6475.2744, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12089.1152, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1501.5703, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.7652],
        [ 0.9565],
        [ 0.9880],
        ...,
        [-7.3004],
        [-7.2912],
        [-7.2896]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-923013.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2661],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(934.2201, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2661],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(934.2201, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0030, -0.0060, -0.0141],
        [-0.0102, -0.0055, -0.0002,  ...,  0.0028, -0.0063, -0.0150],
        [-0.0101, -0.0057, -0.0002,  ...,  0.0029, -0.0062, -0.0148],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0030, -0.0060, -0.0141],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0030, -0.0060, -0.0141],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0030, -0.0060, -0.0141]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-63708.4492, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-987.8116, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(115.2393, device='cuda:0')



h[100].sum tensor(100.9870, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-137.3640, device='cuda:0')



h[200].sum tensor(-474.0170, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.5525, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0120, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0117, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0107, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0122, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0122, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0122, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(309238.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0872, 0.0000, 0.0581],
        [0.0000, 0.0000, 0.0000,  ..., 0.0424, 0.0000, 0.0296],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0067],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1836, 0.0000, 0.0813],
        [0.0000, 0.0000, 0.0000,  ..., 0.1836, 0.0000, 0.0813],
        [0.0000, 0.0000, 0.0000,  ..., 0.1836, 0.0000, 0.0813]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3294139.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(703.3510, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6495.8008, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15042.2812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1658.0503, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6979],
        [ 0.0570],
        [ 0.4266],
        ...,
        [-6.9895],
        [-7.2317],
        [-7.3659]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1149467.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(742.4330, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(742.4330, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0029, -0.0060, -0.0141],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0029, -0.0060, -0.0141],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0029, -0.0060, -0.0141],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0029, -0.0060, -0.0141],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0029, -0.0060, -0.0141],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0029, -0.0060, -0.0141]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-65871.4141, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.9250, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(91.5817, device='cuda:0')



h[100].sum tensor(97.8596, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-109.1644, device='cuda:0')



h[200].sum tensor(-481.6645, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.8433, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0117, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0117, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0117, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0117, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0117, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0117, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(292525.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1558, 0.0000, 0.0740],
        [0.0000, 0.0000, 0.0000,  ..., 0.1780, 0.0000, 0.0793],
        [0.0000, 0.0000, 0.0000,  ..., 0.1812, 0.0000, 0.0804],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1820, 0.0000, 0.0796],
        [0.0000, 0.0000, 0.0000,  ..., 0.1820, 0.0000, 0.0796],
        [0.0000, 0.0000, 0.0000,  ..., 0.1820, 0.0000, 0.0796]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3178138., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(638.7786, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6437.1919, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11224.0801, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1603.8634, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.9201],
        [-4.8692],
        [-5.3528],
        ...,
        [-7.3901],
        [-7.3823],
        [-7.3823]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1029867.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(708.3690, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(708.3690, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0028, -0.0060, -0.0142],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0028, -0.0060, -0.0142],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0028, -0.0060, -0.0142],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0028, -0.0060, -0.0142],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0028, -0.0060, -0.0142],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0028, -0.0060, -0.0142]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-66257.7969, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.6557, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(87.3798, device='cuda:0')



h[100].sum tensor(97.5827, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-104.1558, device='cuda:0')



h[200].sum tensor(-483.1881, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.4740, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(289010.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1805, 0.0000, 0.0796],
        [0.0000, 0.0000, 0.0000,  ..., 0.1805, 0.0000, 0.0796],
        [0.0000, 0.0000, 0.0000,  ..., 0.1802, 0.0000, 0.0798],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1811, 0.0000, 0.0791],
        [0.0000, 0.0000, 0.0000,  ..., 0.1811, 0.0000, 0.0791],
        [0.0000, 0.0000, 0.0000,  ..., 0.1811, 0.0000, 0.0791]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3144720.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(630.2207, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6440.1689, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10564.5527, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1589.0659, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.5695],
        [-7.5992],
        [-7.5997],
        ...,
        [-7.3780],
        [-7.3686],
        [-7.3670]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-977220.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 280.0 event: 4200 loss: tensor(481.0848, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(729.1576, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(729.1576, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0028, -0.0060, -0.0142],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0028, -0.0060, -0.0142],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0028, -0.0060, -0.0142],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0028, -0.0060, -0.0142],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0028, -0.0060, -0.0142],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0028, -0.0060, -0.0142]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-66180.5625, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.8066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(89.9441, device='cuda:0')



h[100].sum tensor(92.5575, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-107.2124, device='cuda:0')



h[200].sum tensor(-483.2378, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.3097, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0111, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0111, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0111, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0111, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0111, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0111, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(288327.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1682, 0.0000, 0.0769],
        [0.0000, 0.0000, 0.0000,  ..., 0.1783, 0.0000, 0.0795],
        [0.0000, 0.0000, 0.0000,  ..., 0.1799, 0.0000, 0.0804],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1812, 0.0000, 0.0798],
        [0.0000, 0.0000, 0.0000,  ..., 0.1812, 0.0000, 0.0798],
        [0.0000, 0.0000, 0.0000,  ..., 0.1812, 0.0000, 0.0798]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3141299., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(647.9167, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6501.3018, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11759.8584, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1578.6658, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.8022],
        [-5.7320],
        [-6.0317],
        ...,
        [-7.4019],
        [-7.3925],
        [-7.3909]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-994188.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(587.8887, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(587.8887, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0027, -0.0060, -0.0143],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0027, -0.0060, -0.0143],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0027, -0.0060, -0.0143],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0027, -0.0060, -0.0143],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0027, -0.0060, -0.0143],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0027, -0.0060, -0.0143]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-67579.3594, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.5110, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(72.5181, device='cuda:0')



h[100].sum tensor(85.2238, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-86.4408, device='cuda:0')



h[200].sum tensor(-488.6012, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.6311, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0106, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0106, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0106, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0106, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0106, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0106, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(278561.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1295, 0.0000, 0.0681],
        [0.0000, 0.0000, 0.0000,  ..., 0.1610, 0.0000, 0.0751],
        [0.0000, 0.0000, 0.0000,  ..., 0.1668, 0.0000, 0.0768],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1800, 0.0000, 0.0794],
        [0.0000, 0.0000, 0.0000,  ..., 0.1800, 0.0000, 0.0794],
        [0.0000, 0.0000, 0.0000,  ..., 0.1800, 0.0000, 0.0794]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3072816., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(639.1484, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6441.3931, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10327.9023, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1558.3374, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4617],
        [-2.6434],
        [-3.4169],
        ...,
        [-7.3768],
        [-7.3676],
        [-7.3661]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1022354.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1014.1361, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1014.1361, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0143],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0143],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0143],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0143],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0143],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0143]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-63300.9375, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-988.4477, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(125.0972, device='cuda:0')



h[100].sum tensor(118.5420, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-149.1146, device='cuda:0')



h[200].sum tensor(-473.0261, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.7648, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0101, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0101, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0101, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0101, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0101, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0101, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(316369.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1716, 0.0000, 0.0775],
        [0.0000, 0.0000, 0.0000,  ..., 0.1765, 0.0000, 0.0786],
        [0.0000, 0.0000, 0.0000,  ..., 0.1762, 0.0000, 0.0788],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1771, 0.0000, 0.0781],
        [0.0000, 0.0000, 0.0000,  ..., 0.1771, 0.0000, 0.0781],
        [0.0000, 0.0000, 0.0000,  ..., 0.1771, 0.0000, 0.0781]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3352202., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(705.5347, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6655.4028, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(17087.4336, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1639.8540, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.6903],
        [-7.1412],
        [-7.2881],
        ...,
        [-7.0483],
        [-7.2283],
        [-7.2769]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-727975.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(615.0874, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(615.0874, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0144],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0144],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0144],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0144],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0144],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0144]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-67318.0312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.7707, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(75.8732, device='cuda:0')



h[100].sum tensor(96.7613, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-90.4400, device='cuda:0')



h[200].sum tensor(-486.0322, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.7244, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0097, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(283974.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1615, 0.0000, 0.0748],
        [0.0000, 0.0000, 0.0000,  ..., 0.1139, 0.0000, 0.0644],
        [0.0000, 0.0000, 0.0000,  ..., 0.0595, 0.0000, 0.0489],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1765, 0.0000, 0.0777],
        [0.0000, 0.0000, 0.0000,  ..., 0.1765, 0.0000, 0.0777],
        [0.0000, 0.0000, 0.0000,  ..., 0.1731, 0.0000, 0.0769]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3126561., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(630.2097, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6508.3989, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11607.4111, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1524.1560, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.5793],
        [-2.0776],
        [-0.7081],
        ...,
        [-7.1619],
        [-6.7705],
        [-5.7983]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-784223.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(658.8330, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(658.8330, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0102, -0.0056, -0.0002,  ...,  0.0023, -0.0063, -0.0153],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0144],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0144],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0144],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0144],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0144]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-67178.5156, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.1548, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(81.2693, device='cuda:0')



h[100].sum tensor(93.4145, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-96.8722, device='cuda:0')



h[200].sum tensor(-483.9084, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.4829, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0093, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0101, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0101, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0101, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0101, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(286234.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0259, 0.0000, 0.0389],
        [0.0000, 0.0000, 0.0000,  ..., 0.0548, 0.0000, 0.0476],
        [0.0000, 0.0000, 0.0000,  ..., 0.1221, 0.0000, 0.0665],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1786, 0.0000, 0.0790],
        [0.0000, 0.0000, 0.0000,  ..., 0.1786, 0.0000, 0.0790],
        [0.0000, 0.0000, 0.0000,  ..., 0.1786, 0.0000, 0.0790]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3132056.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(624.5498, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6472.9375, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11465.2383, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1546.9507, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7706],
        [-1.5703],
        [-2.9612],
        ...,
        [-7.4034],
        [-7.3946],
        [-7.3933]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-962080.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5029],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.3301]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(784.0759, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5029],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.3301]], device='cuda:0') 
g.ndata[nfet].sum tensor(784.0759, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0108, -0.0050, -0.0003,  ...,  0.0021, -0.0065, -0.0162],
        [-0.0106, -0.0052, -0.0003,  ...,  0.0022, -0.0064, -0.0159],
        [-0.0142, -0.0015, -0.0013,  ...,  0.0008, -0.0078, -0.0213],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0144],
        [-0.0110, -0.0047, -0.0004,  ...,  0.0020, -0.0066, -0.0166],
        [-0.0103, -0.0055, -0.0002,  ...,  0.0023, -0.0063, -0.0154]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-66192.4688, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.3196, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(96.7185, device='cuda:0')



h[100].sum tensor(96.8882, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-115.2874, device='cuda:0')



h[200].sum tensor(-478.8254, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.5172, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0089, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0094, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0084, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(294920.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0054],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0505, 0.0000, 0.0413],
        [0.0000, 0.0000, 0.0000,  ..., 0.0179, 0.0000, 0.0277],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3176940.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(620.6449, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6452.1499, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11917.9219, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1586.0049, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4513],
        [ 0.2880],
        [ 0.1401],
        ...,
        [-2.4453],
        [-0.6749],
        [-0.0098]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1103009.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(812.3116, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(812.3116, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0144],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0144],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0144],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0144],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0144],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0144]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-66072.2344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.5643, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(100.2015, device='cuda:0')



h[100].sum tensor(100.4487, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-119.4391, device='cuda:0')



h[200].sum tensor(-477.4513, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.6522, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0102, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0102, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0102, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0102, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0102, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0102, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(295707.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1801, 0.0000, 0.0798],
        [0.0000, 0.0000, 0.0000,  ..., 0.1801, 0.0000, 0.0798],
        [0.0000, 0.0000, 0.0000,  ..., 0.1798, 0.0000, 0.0801],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1807, 0.0000, 0.0793],
        [0.0000, 0.0000, 0.0000,  ..., 0.1807, 0.0000, 0.0793],
        [0.0000, 0.0000, 0.0000,  ..., 0.1807, 0.0000, 0.0793]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3157418.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(601.7645, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6423.4370, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10730.8887, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1582.5463, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.6879],
        [-6.5830],
        [-6.4652],
        ...,
        [-7.4780],
        [-7.4636],
        [-7.4670]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1104954.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(633.8058, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(633.8058, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0101, -0.0057, -0.0001,  ...,  0.0023, -0.0062, -0.0152],
        [-0.0101, -0.0057, -0.0001,  ...,  0.0023, -0.0062, -0.0152],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0145],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0145],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0145],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0145]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-67888.8750, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.9529, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(78.1821, device='cuda:0')



h[100].sum tensor(92.8527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-93.1923, device='cuda:0')



h[200].sum tensor(-483.6755, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.4768, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0094, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0096, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0101, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0101, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0101, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(282362.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0346, 0.0000, 0.0409],
        [0.0000, 0.0000, 0.0000,  ..., 0.0485, 0.0000, 0.0460],
        [0.0000, 0.0000, 0.0000,  ..., 0.0839, 0.0000, 0.0561],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1809, 0.0000, 0.0785],
        [0.0000, 0.0000, 0.0000,  ..., 0.1809, 0.0000, 0.0785],
        [0.0000, 0.0000, 0.0000,  ..., 0.1809, 0.0000, 0.0785]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3093355.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(558.1443, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6416.7051, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9087.7598, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1500.8093, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9807],
        [-1.4632],
        [-2.6278],
        ...,
        [-7.5117],
        [-7.5024],
        [-7.5009]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1055727.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(730.3311, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(730.3311, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0145],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0145],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0145],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0145],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0145],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0145]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-67014.0938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.8536, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(90.0889, device='cuda:0')



h[100].sum tensor(93.7466, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-107.3850, device='cuda:0')



h[200].sum tensor(-480.9285, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.3568, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(296219.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1818, 0.0000, 0.0801],
        [0.0000, 0.0000, 0.0000,  ..., 0.1818, 0.0000, 0.0801],
        [0.0000, 0.0000, 0.0000,  ..., 0.1815, 0.0000, 0.0804],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1823, 0.0000, 0.0796],
        [0.0000, 0.0000, 0.0000,  ..., 0.1823, 0.0000, 0.0796],
        [0.0000, 0.0000, 0.0000,  ..., 0.1823, 0.0000, 0.0796]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3209806.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(591.0725, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6383.6689, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11821.4404, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1529.2043, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.1085],
        [-7.4453],
        [-7.6053],
        ...,
        [-7.5531],
        [-7.5437],
        [-7.5421]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1165651.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(647.3217, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(647.3217, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0145],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0145],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0145],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0145],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0145],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0145]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-67756.2734, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.0855, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(79.8494, device='cuda:0')



h[100].sum tensor(94.6156, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-95.1796, device='cuda:0')



h[200].sum tensor(-484.5141, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.0201, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0100, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0100, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0100, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0100, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0100, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0100, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(285495.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1642, 0.0000, 0.0750],
        [0.0000, 0.0000, 0.0000,  ..., 0.1764, 0.0000, 0.0779],
        [0.0000, 0.0000, 0.0000,  ..., 0.1800, 0.0000, 0.0790],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1809, 0.0000, 0.0782],
        [0.0000, 0.0000, 0.0000,  ..., 0.1809, 0.0000, 0.0782],
        [0.0000, 0.0000, 0.0000,  ..., 0.1809, 0.0000, 0.0782]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3113684.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(574.4894, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6401.9829, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9969.2344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1426.9681, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.6677],
        [-6.8697],
        [-6.6722],
        ...,
        [-7.4672],
        [-7.4583],
        [-7.4569]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-986975.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 290.0 event: 4350 loss: tensor(459.1264, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4370],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(662.5530, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4370],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(662.5530, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0108, -0.0050, -0.0003,  ...,  0.0020, -0.0065, -0.0163],
        [-0.0106, -0.0051, -0.0003,  ...,  0.0021, -0.0064, -0.0161],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0145],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0145],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0145],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0145]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-67607.1094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.2072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(81.7282, device='cuda:0')



h[100].sum tensor(101.0011, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-97.4192, device='cuda:0')



h[200].sum tensor(-485.0663, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.6324, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0075, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0091, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(286912.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0036],
        [0.0000, 0.0000, 0.0000,  ..., 0.0326, 0.0000, 0.0282],
        [0.0000, 0.0000, 0.0000,  ..., 0.0759, 0.0000, 0.0508],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1801, 0.0000, 0.0768],
        [0.0000, 0.0000, 0.0000,  ..., 0.1801, 0.0000, 0.0768],
        [0.0000, 0.0000, 0.0000,  ..., 0.1801, 0.0000, 0.0768]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3118416., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(575.4539, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6398.8853, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9851.6104, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1399.3324, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.7522],
        [-0.3696],
        [-2.0810],
        ...,
        [-7.4060],
        [-7.3974],
        [-7.3961]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-846906.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(733.9073, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(733.9073, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0145],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0145],
        [-0.0102, -0.0056, -0.0002,  ...,  0.0023, -0.0063, -0.0155],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0145],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0145],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0145]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-66980.5391, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.8881, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(90.5300, device='cuda:0')



h[100].sum tensor(102.7795, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-107.9108, device='cuda:0')



h[200].sum tensor(-483.6466, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.5006, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0100, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0100, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0100, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0100, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(293346.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1313, 0.0000, 0.0668],
        [0.0000, 0.0000, 0.0000,  ..., 0.0655, 0.0000, 0.0526],
        [0.0000, 0.0000, 0.0000,  ..., 0.0324, 0.0000, 0.0351],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1819, 0.0000, 0.0773],
        [0.0000, 0.0000, 0.0000,  ..., 0.1819, 0.0000, 0.0773],
        [0.0000, 0.0000, 0.0000,  ..., 0.1819, 0.0000, 0.0773]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3166468.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(586.1317, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6423.9756, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10971.9170, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1380.2037, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5649],
        [ 0.0941],
        [ 0.6201],
        ...,
        [-7.4201],
        [-7.4351],
        [-7.4358]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-869565.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(656.1218, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(656.1218, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0109, -0.0049, -0.0003,  ...,  0.0021, -0.0065, -0.0165],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0145],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0145],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0145],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0145],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0145]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-67863.7344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.1381, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(80.9349, device='cuda:0')



h[100].sum tensor(94.4765, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-96.4736, device='cuda:0')



h[200].sum tensor(-487.0137, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.3739, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0096, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0096, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0102, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0102, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0102, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0102, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(284891.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0276, 0.0000, 0.0381],
        [0.0000, 0.0000, 0.0000,  ..., 0.0571, 0.0000, 0.0461],
        [0.0000, 0.0000, 0.0000,  ..., 0.1353, 0.0000, 0.0666],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1837, 0.0000, 0.0775],
        [0.0000, 0.0000, 0.0000,  ..., 0.1837, 0.0000, 0.0775],
        [0.0000, 0.0000, 0.0000,  ..., 0.1837, 0.0000, 0.0775]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3095855.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(574.1598, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6315.6333, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9107.8330, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1347.9263, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0769],
        [-1.4162],
        [-3.0599],
        ...,
        [-7.5012],
        [-7.4923],
        [-7.4909]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1027315.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(709.5038, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(709.5038, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0146],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0146]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-67338.8438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.6040, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(87.5198, device='cuda:0')



h[100].sum tensor(102.6225, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-104.3226, device='cuda:0')



h[200].sum tensor(-485.4426, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.5196, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0094, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(288396.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0651, 0.0000, 0.0451],
        [0.0000, 0.0000, 0.0000,  ..., 0.1333, 0.0000, 0.0660],
        [0.0000, 0.0000, 0.0000,  ..., 0.1654, 0.0000, 0.0729],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1830, 0.0000, 0.0756],
        [0.0000, 0.0000, 0.0000,  ..., 0.1830, 0.0000, 0.0756],
        [0.0000, 0.0000, 0.0000,  ..., 0.1830, 0.0000, 0.0756]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3137496.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(593.4033, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6422.6377, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10443.4395, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1329.9441, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.3764],
        [-3.9034],
        [-4.5850],
        ...,
        [-7.4727],
        [-7.4640],
        [-7.4627]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-804096.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(761.7620, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(761.7620, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0109, -0.0049, -0.0003,  ...,  0.0020, -0.0065, -0.0165],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0146],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0146]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-66863.0781, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.0900, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(93.9660, device='cuda:0')



h[100].sum tensor(104.4435, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-112.0065, device='cuda:0')



h[200].sum tensor(-483.7209, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.6202, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0092, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0093, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(292599.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0210, 0.0000, 0.0233],
        [0.0000, 0.0000, 0.0000,  ..., 0.0541, 0.0000, 0.0384],
        [0.0000, 0.0000, 0.0000,  ..., 0.1278, 0.0000, 0.0636],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1841, 0.0000, 0.0748],
        [0.0000, 0.0000, 0.0000,  ..., 0.1841, 0.0000, 0.0748],
        [0.0000, 0.0000, 0.0000,  ..., 0.1841, 0.0000, 0.0748]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3160865., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(619.3774, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6413.7627, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10920.7510, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1352.8320, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3759],
        [-0.8850],
        [-2.7862],
        ...,
        [-7.3112],
        [-7.1641],
        [-7.0706]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-843562.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(726.4667, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(726.4667, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0146],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0146]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-67259.3750, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.7833, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(89.6122, device='cuda:0')



h[100].sum tensor(100.0842, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-106.8168, device='cuda:0')



h[200].sum tensor(-484.8734, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.2015, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0100, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0100, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0100, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0100, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0100, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0100, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(289990.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1852, 0.0000, 0.0748],
        [0.0000, 0.0000, 0.0000,  ..., 0.1852, 0.0000, 0.0748],
        [0.0000, 0.0000, 0.0000,  ..., 0.1696, 0.0000, 0.0716],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1858, 0.0000, 0.0743],
        [0.0000, 0.0000, 0.0000,  ..., 0.1858, 0.0000, 0.0743],
        [0.0000, 0.0000, 0.0000,  ..., 0.1858, 0.0000, 0.0743]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3154845.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(628.0808, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6400.9565, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10894.0957, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1348.9674, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.5153],
        [-7.1953],
        [-6.4482],
        ...,
        [-7.5941],
        [-7.5853],
        [-7.5841]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-914567.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(666.7825, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(666.7825, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0146],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0146]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-67920.2266, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.2159, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(82.2499, device='cuda:0')



h[100].sum tensor(97.0579, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-98.0411, device='cuda:0')



h[200].sum tensor(-486.8541, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.8024, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0101, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0101, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0101, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0101, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0101, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0101, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(285188.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1835, 0.0000, 0.0729],
        [0.0000, 0.0000, 0.0000,  ..., 0.1854, 0.0000, 0.0736],
        [0.0000, 0.0000, 0.0000,  ..., 0.1860, 0.0000, 0.0741],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1869, 0.0000, 0.0734],
        [0.0000, 0.0000, 0.0000,  ..., 0.1869, 0.0000, 0.0734],
        [0.0000, 0.0000, 0.0000,  ..., 0.1869, 0.0000, 0.0734]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3122208., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(620.5677, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6377.3623, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9810.0566, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1334.6771, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.0811],
        [-5.8368],
        [-6.4604],
        ...,
        [-7.6420],
        [-7.6328],
        [-7.6314]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-933438.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2690],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(522.6619, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2690],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(522.6619, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0106, -0.0052, -0.0002,  ...,  0.0022, -0.0064, -0.0161],
        [-0.0109, -0.0049, -0.0003,  ...,  0.0021, -0.0065, -0.0165],
        [-0.0115, -0.0042, -0.0004,  ...,  0.0018, -0.0068, -0.0176],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0146]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-69420.5234, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-983.8884, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(64.4722, device='cuda:0')



h[100].sum tensor(88.6878, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-76.8501, device='cuda:0')



h[200].sum tensor(-491.7654, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.0092, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0085, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0082, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0102, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0102, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0102, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(273356.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1883, 0.0000, 0.0726],
        [0.0000, 0.0000, 0.0000,  ..., 0.1883, 0.0000, 0.0726],
        [0.0000, 0.0000, 0.0000,  ..., 0.1883, 0.0000, 0.0726]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3040317.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(590.3547, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6303.7236, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7397.6260, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1305.2070, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.1349],
        [ 1.0695],
        [ 1.0017],
        ...,
        [-7.7025],
        [-7.6932],
        [-7.6916]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1034922.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1015.3805, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1015.3805, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0146],
        [-0.0102, -0.0056, -0.0001,  ...,  0.0023, -0.0062, -0.0155],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0146]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-64372.8398, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-988.5903, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(125.2507, device='cuda:0')



h[100].sum tensor(120.9853, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-149.2975, device='cuda:0')



h[200].sum tensor(-473.8376, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.8149, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0102, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0100, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0093, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(322261.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1502, 0.0000, 0.0642],
        [0.0000, 0.0000, 0.0000,  ..., 0.0842, 0.0000, 0.0491],
        [0.0000, 0.0000, 0.0000,  ..., 0.0340, 0.0000, 0.0285],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0390, 0.0000, 0.0334],
        [0.0000, 0.0000, 0.0000,  ..., 0.0158, 0.0000, 0.0209],
        [0.0000, 0.0000, 0.0000,  ..., 0.0478, 0.0000, 0.0352]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3413869.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(700.3976, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6469.9883, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15998.4395, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1474.2886, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.8047],
        [-2.1753],
        [-0.5920],
        ...,
        [-1.0358],
        [-0.3420],
        [-1.3542]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-903933.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(703.0710, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(703.0710, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0146],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0146]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-67645.3281, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.5631, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(86.7263, device='cuda:0')



h[100].sum tensor(102.7388, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-103.3768, device='cuda:0')



h[200].sum tensor(-485.8490, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.2611, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0102, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0102, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0102, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0102, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0102, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0102, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(290082.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1899, 0.0000, 0.0716],
        [0.0000, 0.0000, 0.0000,  ..., 0.1899, 0.0000, 0.0716],
        [0.0000, 0.0000, 0.0000,  ..., 0.1896, 0.0000, 0.0718],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1904, 0.0000, 0.0711],
        [0.0000, 0.0000, 0.0000,  ..., 0.1904, 0.0000, 0.0711],
        [0.0000, 0.0000, 0.0000,  ..., 0.1904, 0.0000, 0.0711]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3199207.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(628.8900, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6407.6714, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11020.9541, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1343.4604, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.6620],
        [-7.4110],
        [-6.9820],
        ...,
        [-7.7760],
        [-7.7660],
        [-7.7637]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-914296.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 300.0 event: 4500 loss: tensor(452.6344, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.2444]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(778.0095, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.2444]], device='cuda:0') 
g.ndata[nfet].sum tensor(778.0095, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0146],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0146],
        [-0.0102, -0.0056, -0.0001,  ...,  0.0024, -0.0062, -0.0155],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0146]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-66932.2969, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.2683, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(95.9702, device='cuda:0')



h[100].sum tensor(103.9817, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-114.3954, device='cuda:0')



h[200].sum tensor(-483.6464, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.2733, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0105, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0105, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0105, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(293773.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1909, 0.0000, 0.0710],
        [0.0000, 0.0000, 0.0000,  ..., 0.1916, 0.0000, 0.0713],
        [0.0000, 0.0000, 0.0000,  ..., 0.1917, 0.0000, 0.0717],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1567, 0.0000, 0.0621],
        [0.0000, 0.0000, 0.0000,  ..., 0.1478, 0.0000, 0.0597],
        [0.0000, 0.0000, 0.0000,  ..., 0.1147, 0.0000, 0.0514]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3202580., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(655.1711, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6383.6875, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11024.0371, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1350.4761, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.8048],
        [-7.2934],
        [-7.5629],
        ...,
        [-7.0181],
        [-6.7251],
        [-6.5597]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1026977.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(790.5546, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(790.5546, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0113, -0.0045, -0.0003,  ...,  0.0020, -0.0067, -0.0172],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0146],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0146]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-66619.6875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.4532, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(97.5177, device='cuda:0')



h[100].sum tensor(112.6921, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-116.2400, device='cuda:0')



h[200].sum tensor(-482.6559, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.7776, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0093, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0093, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(297754.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0101, 0.0000, 0.0142],
        [0.0000, 0.0000, 0.0000,  ..., 0.0322, 0.0000, 0.0242],
        [0.0000, 0.0000, 0.0000,  ..., 0.1060, 0.0000, 0.0536],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1917, 0.0000, 0.0697],
        [0.0000, 0.0000, 0.0000,  ..., 0.1917, 0.0000, 0.0697],
        [0.0000, 0.0000, 0.0000,  ..., 0.1917, 0.0000, 0.0697]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3239797.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(664.2228, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6392.6616, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11511.0449, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1339.4165, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4446],
        [-0.3591],
        [-1.3337],
        ...,
        [-7.7966],
        [-7.7871],
        [-7.7856]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-887515.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(703.7341, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(703.7341, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0114, -0.0044, -0.0004,  ...,  0.0020, -0.0067, -0.0174],
        [-0.0104, -0.0054, -0.0002,  ...,  0.0023, -0.0063, -0.0158],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0146],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0146]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-67441.8516, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.6122, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(86.8081, device='cuda:0')



h[100].sum tensor(111.0844, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-103.4743, device='cuda:0')



h[200].sum tensor(-485.4393, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.2877, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0090, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0096, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0104, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0104, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0104, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(291703.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0009],
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0000, 0.0128],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0173],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1917, 0.0000, 0.0688],
        [0.0000, 0.0000, 0.0000,  ..., 0.1917, 0.0000, 0.0688],
        [0.0000, 0.0000, 0.0000,  ..., 0.1917, 0.0000, 0.0688]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3209330.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(659.4388, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6349.0601, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10697.1611, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1295.4872, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.0643],
        [ 1.1575],
        [ 1.2632],
        ...,
        [-7.7767],
        [-7.7673],
        [-7.7658]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-836324.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(641.9656, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(641.9656, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0028, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0028, -0.0060, -0.0146],
        [-0.0113, -0.0044, -0.0003,  ...,  0.0022, -0.0067, -0.0173],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0028, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0028, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0028, -0.0060, -0.0146]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-68119.7500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.0120, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(79.1887, device='cuda:0')



h[100].sum tensor(96.8490, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-94.3921, device='cuda:0')



h[200].sum tensor(-487.7414, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.8048, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0100, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0100, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(290311.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1086, 0.0000, 0.0541],
        [0.0000, 0.0000, 0.0000,  ..., 0.0323, 0.0000, 0.0247],
        [0.0000, 0.0000, 0.0000,  ..., 0.0097, 0.0000, 0.0152],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1946, 0.0000, 0.0697],
        [0.0000, 0.0000, 0.0000,  ..., 0.1946, 0.0000, 0.0697],
        [0.0000, 0.0000, 0.0000,  ..., 0.1946, 0.0000, 0.0697]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3241634., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(704.9230, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6299.6064, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12411.3584, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1265.7051, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.9298],
        [-1.9872],
        [-0.7726],
        ...,
        [-7.9002],
        [-7.8903],
        [-7.8887]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1071151.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2462],
        [0.0000],
        [0.5493],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1011.5923, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2462],
        [0.0000],
        [0.5493],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1011.5923, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0102, -0.0056, -0.0001,  ...,  0.0025, -0.0062, -0.0156],
        [-0.0115, -0.0043, -0.0004,  ...,  0.0021, -0.0068, -0.0175],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0027, -0.0060, -0.0146],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0027, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0027, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0027, -0.0060, -0.0146]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-64102.1445, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-988.6734, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(124.7834, device='cuda:0')



h[100].sum tensor(126.0563, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-148.7405, device='cuda:0')



h[200].sum tensor(-473.6178, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.6626, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0092, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0100, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0085, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(325366.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1940, 0.0000, 0.0688],
        [0.0000, 0.0000, 0.0000,  ..., 0.1940, 0.0000, 0.0688],
        [0.0000, 0.0000, 0.0000,  ..., 0.1940, 0.0000, 0.0688]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3446868., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(790.2644, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6321.0977, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(16555.5312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1388.8918, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.2454],
        [ 1.3424],
        [ 1.3415],
        ...,
        [-7.8648],
        [-7.8551],
        [-7.8536]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-956328.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3728],
        [0.2800],
        [0.5835],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(748.0365, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3728],
        [0.2800],
        [0.5835],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(748.0365, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0123, -0.0035, -0.0005,  ...,  0.0018, -0.0070, -0.0187],
        [-0.0132, -0.0025, -0.0007,  ...,  0.0015, -0.0074, -0.0201],
        [-0.0129, -0.0028, -0.0006,  ...,  0.0015, -0.0073, -0.0197],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0027, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0027, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0027, -0.0060, -0.0146]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-66869.0625, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.0046, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(92.2729, device='cuda:0')



h[100].sum tensor(121.6223, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-109.9883, device='cuda:0')



h[200].sum tensor(-483.8380, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.0685, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0106, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0106, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0106, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(296764.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1919, 0.0000, 0.0670],
        [0.0000, 0.0000, 0.0000,  ..., 0.1919, 0.0000, 0.0670],
        [0.0000, 0.0000, 0.0000,  ..., 0.1919, 0.0000, 0.0670]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3245990.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(700.7532, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6314.9639, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10789.2217, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1219.0833, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.7184],
        [ 0.6521],
        [ 0.6362],
        ...,
        [-7.7531],
        [-7.7440],
        [-7.7426]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-723277.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(712.9070, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(712.9070, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0028, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0028, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0028, -0.0060, -0.0146],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0028, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0028, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0028, -0.0060, -0.0146]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-67238.7031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.7220, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(87.9396, device='cuda:0')



h[100].sum tensor(113.5149, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-104.8230, device='cuda:0')



h[200].sum tensor(-485.5040, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.6564, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(291555.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1911, 0.0000, 0.0671],
        [0.0000, 0.0000, 0.0000,  ..., 0.1925, 0.0000, 0.0677],
        [0.0000, 0.0000, 0.0000,  ..., 0.1785, 0.0000, 0.0658],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1937, 0.0000, 0.0675],
        [0.0000, 0.0000, 0.0000,  ..., 0.1937, 0.0000, 0.0675],
        [0.0000, 0.0000, 0.0000,  ..., 0.1937, 0.0000, 0.0675]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3217294.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(713.7957, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6317.7192, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10764.0801, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1168.9801, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.3234],
        [-6.5051],
        [-6.3201],
        ...,
        [-7.8367],
        [-7.8272],
        [-7.8255]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-800655.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(704.2026, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(704.2026, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0030, -0.0060, -0.0146],
        [-0.0103, -0.0055, -0.0001,  ...,  0.0027, -0.0063, -0.0157],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0030, -0.0060, -0.0146],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0030, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0030, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0030, -0.0060, -0.0146]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-67491.0312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.5992, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(86.8658, device='cuda:0')



h[100].sum tensor(93.0670, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-103.5432, device='cuda:0')



h[200].sum tensor(-486.2891, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.3066, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0117, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0117, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0119, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0119, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0119, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(290679.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1387, 0.0000, 0.0584],
        [0.0000, 0.0000, 0.0000,  ..., 0.1319, 0.0000, 0.0568],
        [0.0000, 0.0000, 0.0000,  ..., 0.0887, 0.0000, 0.0489],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1980, 0.0000, 0.0697],
        [0.0000, 0.0000, 0.0000,  ..., 0.1980, 0.0000, 0.0697],
        [0.0000, 0.0000, 0.0000,  ..., 0.1980, 0.0000, 0.0697]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3222505., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(776.2781, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6248.0938, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12181.8867, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1144.2295, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.9624],
        [-3.1158],
        [-3.2025],
        ...,
        [-8.0505],
        [-8.0401],
        [-8.0383]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1278858., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(669.5002, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(669.5002, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0028, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0028, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0028, -0.0060, -0.0146],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0028, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0028, -0.0060, -0.0146],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0028, -0.0060, -0.0146]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-67711.7500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.2654, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(82.5852, device='cuda:0')



h[100].sum tensor(103.8413, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-98.4407, device='cuda:0')



h[200].sum tensor(-486.8068, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.9116, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0112, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0112, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0112, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0112, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0112, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0112, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(289762.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1822, 0.0000, 0.0666],
        [0.0000, 0.0000, 0.0000,  ..., 0.1606, 0.0000, 0.0625],
        [0.0000, 0.0000, 0.0000,  ..., 0.1494, 0.0000, 0.0607],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1931, 0.0000, 0.0681],
        [0.0000, 0.0000, 0.0000,  ..., 0.1931, 0.0000, 0.0681],
        [0.0000, 0.0000, 0.0000,  ..., 0.1931, 0.0000, 0.0681]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3209055.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(793.8679, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6232.5601, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12062.9648, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1128.4923, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.2674],
        [-2.9928],
        [-2.9419],
        ...,
        [-7.8692],
        [-7.8598],
        [-7.8584]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-920099.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(667.4698, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(667.4698, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-67610.0781, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.2485, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(82.3347, device='cuda:0')



h[100].sum tensor(120.6921, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-98.1421, device='cuda:0')



h[200].sum tensor(-485.4142, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.8300, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(291062.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1872, 0.0000, 0.0667],
        [0.0000, 0.0000, 0.0000,  ..., 0.1872, 0.0000, 0.0667],
        [0.0000, 0.0000, 0.0000,  ..., 0.1869, 0.0000, 0.0669],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1879, 0.0000, 0.0662],
        [0.0000, 0.0000, 0.0000,  ..., 0.1879, 0.0000, 0.0662],
        [0.0000, 0.0000, 0.0000,  ..., 0.1879, 0.0000, 0.0662]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3220158., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(782.8063, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6244.9175, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11971.4219, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1127.0259, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.0481],
        [-7.2621],
        [-7.4089],
        ...,
        [-7.6863],
        [-7.6779],
        [-7.6767]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-594952.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 310.0 event: 4650 loss: tensor(474.2716, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(840.9241, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(840.9241, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0102, -0.0056, -0.0001,  ...,  0.0024, -0.0063, -0.0156],
        [-0.0109, -0.0049, -0.0002,  ...,  0.0022, -0.0065, -0.0167],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-65927.7656, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.8680, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(103.7309, device='cuda:0')



h[100].sum tensor(125.3913, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-123.6461, device='cuda:0')



h[200].sum tensor(-478.4971, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.8023, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0089, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0105, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0105, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0105, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(303671.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0117],
        [0.0000, 0.0000, 0.0000,  ..., 0.0216, 0.0000, 0.0249],
        [0.0000, 0.0000, 0.0000,  ..., 0.0547, 0.0000, 0.0362],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1878, 0.0000, 0.0666],
        [0.0000, 0.0000, 0.0000,  ..., 0.1878, 0.0000, 0.0666],
        [0.0000, 0.0000, 0.0000,  ..., 0.1878, 0.0000, 0.0666]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3283081., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(886.5094, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6276.4878, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15096.5342, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1158.0879, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.1410],
        [ 0.3630],
        [-1.0526],
        ...,
        [-7.7292],
        [-7.7207],
        [-7.7196]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-626077.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4651],
        [0.0000],
        [0.6201],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(604.2046, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4651],
        [0.0000],
        [0.6201],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(604.2046, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0122, -0.0036, -0.0004,  ...,  0.0019, -0.0070, -0.0187],
        [-0.0142, -0.0016, -0.0007,  ...,  0.0012, -0.0078, -0.0216],
        [-0.0105, -0.0053, -0.0001,  ...,  0.0025, -0.0063, -0.0160],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0028, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0028, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0028, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-68583.6719, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.6534, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(74.5307, device='cuda:0')



h[100].sum tensor(98.1394, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-88.8399, device='cuda:0')



h[200].sum tensor(-486.6461, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.2870, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0075, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0084, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0112, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0112, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0112, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(281201.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1909, 0.0000, 0.0675],
        [0.0000, 0.0000, 0.0000,  ..., 0.1909, 0.0000, 0.0675],
        [0.0000, 0.0000, 0.0000,  ..., 0.1909, 0.0000, 0.0675]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3144544.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(916.2079, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6252.6094, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13616.7158, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1050.8511, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.8213],
        [ 0.8095],
        [ 0.7984],
        ...,
        [-7.9064],
        [-7.8972],
        [-7.8958]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-842394.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(722.6207, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(722.6207, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0030, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0030, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0030, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0030, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0030, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0030, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-67748.5391, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.7658, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(89.1378, device='cuda:0')



h[100].sum tensor(99.5603, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-106.2513, device='cuda:0')



h[200].sum tensor(-481.9354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.0469, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(288992.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1917, 0.0000, 0.0680],
        [0.0000, 0.0000, 0.0000,  ..., 0.1926, 0.0000, 0.0683],
        [0.0000, 0.0000, 0.0000,  ..., 0.1927, 0.0000, 0.0687],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1936, 0.0000, 0.0680],
        [0.0000, 0.0000, 0.0000,  ..., 0.1936, 0.0000, 0.0680],
        [0.0000, 0.0000, 0.0000,  ..., 0.1936, 0.0000, 0.0680]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3173065., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(955.4917, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6199.7603, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13838.6719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1106.0498, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.2772],
        [-7.6370],
        [-7.9084],
        ...,
        [-8.0875],
        [-8.0776],
        [-8.0759]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1053151.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(577.7344, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(577.7344, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0029, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0029, -0.0060, -0.0147],
        [-0.0103, -0.0055, -0.0001,  ...,  0.0027, -0.0063, -0.0158],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0029, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0029, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0029, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-69476.7031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.3966, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(71.2655, device='cuda:0')



h[100].sum tensor(97.6720, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-84.9478, device='cuda:0')



h[200].sum tensor(-485.9671, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.2230, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0115, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0112, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(275578.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1713, 0.0000, 0.0641],
        [0.0000, 0.0000, 0.0000,  ..., 0.1144, 0.0000, 0.0540],
        [0.0000, 0.0000, 0.0000,  ..., 0.0585, 0.0000, 0.0421],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1935, 0.0000, 0.0677],
        [0.0000, 0.0000, 0.0000,  ..., 0.1935, 0.0000, 0.0677],
        [0.0000, 0.0000, 0.0000,  ..., 0.1935, 0.0000, 0.0677]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3077215.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(883.9919, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6091.2485, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10273.8965, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1074.0430, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.1171],
        [-3.0019],
        [-1.7416],
        ...,
        [-8.1202],
        [-8.1099],
        [-8.1082]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1026003., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3083],
        [0.2983],
        [0.3076],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(729.7410, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3083],
        [0.2983],
        [0.3076],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(729.7410, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0122, -0.0036, -0.0004,  ...,  0.0020, -0.0070, -0.0186],
        [-0.0122, -0.0036, -0.0004,  ...,  0.0020, -0.0070, -0.0187],
        [-0.0116, -0.0042, -0.0003,  ...,  0.0022, -0.0068, -0.0178],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0029, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0029, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0029, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-68087.4531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.8164, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(90.0161, device='cuda:0')



h[100].sum tensor(114.6694, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-107.2982, device='cuda:0')



h[200].sum tensor(-479.4389, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.3331, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0081, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0090, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0115, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0115, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0115, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(288342.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1928, 0.0000, 0.0674],
        [0.0000, 0.0000, 0.0000,  ..., 0.1928, 0.0000, 0.0674],
        [0.0000, 0.0000, 0.0000,  ..., 0.1928, 0.0000, 0.0674]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3155316.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(863.8181, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6100.7227, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11258.0332, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1129.3071, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.9777],
        [ 1.0571],
        [ 1.1803],
        ...,
        [-8.1166],
        [-8.1064],
        [-8.1047]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-905916.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(687.6119, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(687.6119, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0029, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0029, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0029, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0029, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0029, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0029, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-68436.0781, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.5182, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(84.8193, device='cuda:0')



h[100].sum tensor(104.5379, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-101.1037, device='cuda:0')



h[200].sum tensor(-479.4729, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.6397, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0116, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0116, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0116, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0116, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0116, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(285273.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0906, 0.0000, 0.0488],
        [0.0000, 0.0000, 0.0000,  ..., 0.1595, 0.0000, 0.0630],
        [0.0000, 0.0000, 0.0000,  ..., 0.1880, 0.0000, 0.0677],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1937, 0.0000, 0.0679],
        [0.0000, 0.0000, 0.0000,  ..., 0.1937, 0.0000, 0.0679],
        [0.0000, 0.0000, 0.0000,  ..., 0.1937, 0.0000, 0.0679]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3130869.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(911.8137, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6141.1138, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12805.6348, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1084.6055, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.3674],
        [-5.2820],
        [-6.3901],
        ...,
        [-8.1935],
        [-8.1831],
        [-8.1813]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1071524.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(628.3615, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(628.3615, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.2086e-03,  0.0000e+00,  ...,  2.7603e-03,
         -6.0055e-03, -1.4662e-02],
        [-1.0128e-02, -5.6828e-03, -7.3740e-05,  ...,  2.5658e-03,
         -6.2118e-03, -1.5464e-02],
        [-1.0128e-02, -5.6828e-03, -7.3740e-05,  ...,  2.5658e-03,
         -6.2118e-03, -1.5464e-02],
        ...,
        [-9.6037e-03, -6.2086e-03,  0.0000e+00,  ...,  2.7603e-03,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -6.2086e-03,  0.0000e+00,  ...,  2.7603e-03,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -6.2086e-03,  0.0000e+00,  ...,  2.7603e-03,
         -6.0055e-03, -1.4662e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-68988.6406, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.8879, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(77.5106, device='cuda:0')



h[100].sum tensor(99.7038, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-92.3918, device='cuda:0')



h[200].sum tensor(-480.6893, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.2580, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0105, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0105, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(280221.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0698, 0.0000, 0.0454],
        [0.0000, 0.0000, 0.0000,  ..., 0.0355, 0.0000, 0.0380],
        [0.0000, 0.0000, 0.0000,  ..., 0.0497, 0.0000, 0.0415],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1925, 0.0000, 0.0679],
        [0.0000, 0.0000, 0.0000,  ..., 0.1925, 0.0000, 0.0679],
        [0.0000, 0.0000, 0.0000,  ..., 0.1925, 0.0000, 0.0679]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3112097., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(914.4717, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6175.9487, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14245.7129, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1020.2529, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9987],
        [-0.9474],
        [-0.8678],
        ...,
        [-8.1076],
        [-8.0983],
        [-8.0981]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-959103.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(626.2655, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(626.2655, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-68924.3125, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.8958, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(77.2520, device='cuda:0')



h[100].sum tensor(106.3057, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-92.0836, device='cuda:0')



h[200].sum tensor(-479.6590, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.1737, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(279328.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1896, 0.0000, 0.0680],
        [0.0000, 0.0000, 0.0000,  ..., 0.1896, 0.0000, 0.0680],
        [0.0000, 0.0000, 0.0000,  ..., 0.1893, 0.0000, 0.0682],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1903, 0.0000, 0.0676],
        [0.0000, 0.0000, 0.0000,  ..., 0.1903, 0.0000, 0.0676],
        [0.0000, 0.0000, 0.0000,  ..., 0.1903, 0.0000, 0.0676]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3082992.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(880.2751, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6122.4097, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13533.8174, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1004.4575, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.8350],
        [-7.6891],
        [-7.4347],
        ...,
        [-7.9858],
        [-7.9769],
        [-7.9756]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-775512.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(798.7103, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(798.7103, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-67319.3984, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.4868, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(98.5237, device='cuda:0')



h[100].sum tensor(121.6830, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-117.4392, device='cuda:0')



h[200].sum tensor(-473.0964, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.1054, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(301495.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1828, 0.0000, 0.0661],
        [0.0000, 0.0000, 0.0000,  ..., 0.1884, 0.0000, 0.0673],
        [0.0000, 0.0000, 0.0000,  ..., 0.1889, 0.0000, 0.0678],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1899, 0.0000, 0.0671],
        [0.0000, 0.0000, 0.0000,  ..., 0.1899, 0.0000, 0.0671],
        [0.0000, 0.0000, 0.0000,  ..., 0.1899, 0.0000, 0.0671]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3251692.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(876.2020, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6076.7407, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(16444.9199, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1115.5013, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.8806],
        [-6.7840],
        [-7.2961],
        ...,
        [-7.9523],
        [-7.9435],
        [-7.9423]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-715463.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(802.8391, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(802.8391, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0025, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-67524.6094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.6039, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(99.0330, device='cuda:0')



h[100].sum tensor(115.1172, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-118.0463, device='cuda:0')



h[200].sum tensor(-473.7917, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.2714, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0100, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0100, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0100, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0100, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0100, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0100, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(295574.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1936, 0.0000, 0.0677],
        [0.0000, 0.0000, 0.0000,  ..., 0.1936, 0.0000, 0.0677],
        [0.0000, 0.0000, 0.0000,  ..., 0.1933, 0.0000, 0.0680],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1942, 0.0000, 0.0673],
        [0.0000, 0.0000, 0.0000,  ..., 0.1942, 0.0000, 0.0673],
        [0.0000, 0.0000, 0.0000,  ..., 0.1942, 0.0000, 0.0673]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3174274.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(835.7166, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6082.7241, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14489.2852, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1114.2400, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.0635],
        [-7.5855],
        [-7.8795],
        ...,
        [-8.1077],
        [-8.0981],
        [-8.0967]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-893760.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 320.0 event: 4800 loss: tensor(461.8529, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(654.0823, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(654.0823, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0026, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-69526.4844, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.0773, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(80.6833, device='cuda:0')



h[100].sum tensor(94.0591, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-96.1737, device='cuda:0')



h[200].sum tensor(-481.0170, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.2919, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0105, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0105, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0105, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0105, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0105, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0105, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(275079.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1990, 0.0000, 0.0680],
        [0.0000, 0.0000, 0.0000,  ..., 0.1953, 0.0000, 0.0674],
        [0.0000, 0.0000, 0.0000,  ..., 0.1804, 0.0000, 0.0649],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1996, 0.0000, 0.0675],
        [0.0000, 0.0000, 0.0000,  ..., 0.1996, 0.0000, 0.0675],
        [0.0000, 0.0000, 0.0000,  ..., 0.1996, 0.0000, 0.0675]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2988686., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(778.9331, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5996.2495, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9683.7646, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1083.0122, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.7455],
        [-6.2439],
        [-5.0135],
        ...,
        [-8.3101],
        [-8.2997],
        [-8.2979]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1288216.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3025],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(634.3517, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3025],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(634.3517, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0115e-02, -5.6956e-03, -6.3738e-05,  ...,  2.1862e-03,
         -6.2069e-03, -1.5449e-02],
        [-1.0327e-02, -5.4830e-03, -9.0140e-05,  ...,  2.0958e-03,
         -6.2903e-03, -1.5773e-02],
        [-1.1428e-02, -4.3787e-03, -2.2728e-04,  ...,  1.6263e-03,
         -6.7235e-03, -1.7457e-02],
        ...,
        [-9.6037e-03, -6.2089e-03,  0.0000e+00,  ...,  2.4044e-03,
         -6.0055e-03, -1.4666e-02],
        [-9.6037e-03, -6.2089e-03,  0.0000e+00,  ...,  2.4044e-03,
         -6.0055e-03, -1.4666e-02],
        [-9.6037e-03, -6.2089e-03,  0.0000e+00,  ...,  2.4044e-03,
         -6.0055e-03, -1.4666e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-69684.0938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.9385, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(78.2495, device='cuda:0')



h[100].sum tensor(104.6843, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-93.2726, device='cuda:0')



h[200].sum tensor(-482.5031, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.4988, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0079, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0082, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0096, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0096, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0096, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(276063.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0117],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0083],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0035],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1965, 0.0000, 0.0663],
        [0.0000, 0.0000, 0.0000,  ..., 0.1965, 0.0000, 0.0663],
        [0.0000, 0.0000, 0.0000,  ..., 0.1965, 0.0000, 0.0663]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3020101., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(752.8079, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5993.4780, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9976.5889, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1076.6477, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.5769],
        [ 1.5548],
        [ 1.4404],
        ...,
        [-8.2102],
        [-8.2003],
        [-8.1987]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-991945.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(804.0508, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(804.0508, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0023, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0023, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0023, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0023, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0023, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0023, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-67772.7031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.5056, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(99.1825, device='cuda:0')



h[100].sum tensor(115.1834, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-118.2244, device='cuda:0')



h[200].sum tensor(-477.3777, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.3201, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0084, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0091, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0091, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0091, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0091, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0091, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(295779.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0739, 0.0000, 0.0427],
        [0.0000, 0.0000, 0.0000,  ..., 0.1462, 0.0000, 0.0581],
        [0.0000, 0.0000, 0.0000,  ..., 0.1828, 0.0000, 0.0645],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1957, 0.0000, 0.0659],
        [0.0000, 0.0000, 0.0000,  ..., 0.1957, 0.0000, 0.0659],
        [0.0000, 0.0000, 0.0000,  ..., 0.1957, 0.0000, 0.0659]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3159355.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(978.1406, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5910.1899, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13016.1836, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1178.8904, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.4056],
        [-4.0189],
        [-5.1458],
        ...,
        [-8.1981],
        [-8.1886],
        [-8.1872]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1040308.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2842],
        [0.2742],
        [0.2646],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(621.1960, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2842],
        [0.2742],
        [0.2646],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(621.1960, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0115, -0.0043, -0.0002,  ...,  0.0012, -0.0068, -0.0176],
        [-0.0121, -0.0037, -0.0003,  ...,  0.0009, -0.0070, -0.0184],
        [-0.0125, -0.0033, -0.0003,  ...,  0.0007, -0.0071, -0.0191],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0021, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0021, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0021, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-69263.7734, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.7983, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(76.6267, device='cuda:0')



h[100].sum tensor(103.4863, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-91.3382, device='cuda:0')



h[200].sum tensor(-484.2787, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.9700, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0084, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0084, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0084, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(278019.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1941, 0.0000, 0.0654],
        [0.0000, 0.0000, 0.0000,  ..., 0.1941, 0.0000, 0.0654],
        [0.0000, 0.0000, 0.0000,  ..., 0.1941, 0.0000, 0.0654]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2992155.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1159.9338, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5811.0825, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10420.8477, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1112.5229, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.2474],
        [ 1.2407],
        [ 1.2453],
        ...,
        [-8.1366],
        [-8.1252],
        [-8.1224]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1037625.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(661.0267, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(661.0267, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0019, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0019, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0019, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0019, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0019, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0019, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-68781.4844, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.1474, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(81.5399, device='cuda:0')



h[100].sum tensor(110.6097, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-97.1948, device='cuda:0')



h[200].sum tensor(-482.5608, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.5710, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(282310.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1903, 0.0000, 0.0642],
        [0.0000, 0.0000, 0.0000,  ..., 0.1903, 0.0000, 0.0642],
        [0.0000, 0.0000, 0.0000,  ..., 0.1899, 0.0000, 0.0644],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1910, 0.0000, 0.0638],
        [0.0000, 0.0000, 0.0000,  ..., 0.1910, 0.0000, 0.0638],
        [0.0000, 0.0000, 0.0000,  ..., 0.1910, 0.0000, 0.0638]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3065555.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1083.3669, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5950.3633, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13365.0684, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1057.7520, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.5475],
        [-7.2219],
        [-6.6242],
        ...,
        [-7.9643],
        [-7.7928],
        [-7.2400]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-801522.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(644.8204, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(644.8204, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0019, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0019, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0019, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0019, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0019, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0019, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-68915.8984, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.0232, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(79.5408, device='cuda:0')



h[100].sum tensor(109.7665, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-94.8118, device='cuda:0')



h[200].sum tensor(-483.0376, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.9196, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(281129.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1802, 0.0000, 0.0627],
        [0.0000, 0.0000, 0.0000,  ..., 0.1903, 0.0000, 0.0642],
        [0.0000, 0.0000, 0.0000,  ..., 0.1899, 0.0000, 0.0644],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1910, 0.0000, 0.0638],
        [0.0000, 0.0000, 0.0000,  ..., 0.1910, 0.0000, 0.0638],
        [0.0000, 0.0000, 0.0000,  ..., 0.1844, 0.0000, 0.0629]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3056659.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1081.6621, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5952.2319, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13177.6211, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1050.9978, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.8223],
        [-7.5198],
        [-7.8172],
        ...,
        [-7.9486],
        [-7.7408],
        [-7.2113]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-807461.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2612],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(632.5498, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2612],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(632.5498, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0018, -0.0060, -0.0147],
        [-0.0109, -0.0049, -0.0001,  ...,  0.0011, -0.0065, -0.0166],
        [-0.0109, -0.0049, -0.0001,  ...,  0.0011, -0.0065, -0.0167],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0018, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0018, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0018, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-69154.5078, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.9271, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(78.0272, device='cuda:0')



h[100].sum tensor(113.9043, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-93.0076, device='cuda:0')



h[200].sum tensor(-482.7533, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.4264, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(280319.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0439, 0.0000, 0.0282],
        [0.0000, 0.0000, 0.0000,  ..., 0.0101, 0.0000, 0.0127],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1896, 0.0000, 0.0626],
        [0.0000, 0.0000, 0.0000,  ..., 0.1896, 0.0000, 0.0626],
        [0.0000, 0.0000, 0.0000,  ..., 0.1896, 0.0000, 0.0626]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3066143., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(887.3684, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6001.6533, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13108.4004, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(997.1332, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2861],
        [ 1.1171],
        [ 1.3775],
        ...,
        [-7.9745],
        [-7.9664],
        [-7.9654]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-768526.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6631],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(822.8882, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6631],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(822.8882, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0109, -0.0049, -0.0001,  ...,  0.0010, -0.0065, -0.0166],
        [-0.0112, -0.0046, -0.0002,  ...,  0.0008, -0.0066, -0.0171],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0017, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0017, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0017, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0017, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-67510.9922, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.7250, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(101.5061, device='cuda:0')



h[100].sum tensor(127.2204, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-120.9942, device='cuda:0')



h[200].sum tensor(-475.3327, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.0773, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(300104.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0126, 0.0000, 0.0173],
        [0.0000, 0.0000, 0.0000,  ..., 0.0611, 0.0000, 0.0352],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1902, 0.0000, 0.0621],
        [0.0000, 0.0000, 0.0000,  ..., 0.1902, 0.0000, 0.0621],
        [0.0000, 0.0000, 0.0000,  ..., 0.1902, 0.0000, 0.0621]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3230343.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(887.4027, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6017.2305, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(16283.8877, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1075.2667, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.8208],
        [ 0.0661],
        [-1.5174],
        ...,
        [-8.0371],
        [-8.0287],
        [-8.0277]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-820558.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(732.2382, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(732.2382, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.2132e-03,  0.0000e+00,  ...,  1.6302e-03,
         -6.0055e-03, -1.4669e-02],
        [-9.6037e-03, -6.2132e-03,  0.0000e+00,  ...,  1.6302e-03,
         -6.0055e-03, -1.4669e-02],
        [-1.0204e-02, -5.6153e-03, -6.4415e-05,  ...,  1.3017e-03,
         -6.2416e-03, -1.5586e-02],
        ...,
        [-9.6037e-03, -6.2132e-03,  0.0000e+00,  ...,  1.6302e-03,
         -6.0055e-03, -1.4669e-02],
        [-9.6037e-03, -6.2132e-03,  0.0000e+00,  ...,  1.6302e-03,
         -6.0055e-03, -1.4669e-02],
        [-9.6037e-03, -6.2132e-03,  0.0000e+00,  ...,  1.6302e-03,
         -6.0055e-03, -1.4669e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-68693.1094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.8441, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(90.3241, device='cuda:0')



h[100].sum tensor(122.5674, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-107.6654, device='cuda:0')



h[200].sum tensor(-480.1684, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.4335, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(288349., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1473, 0.0000, 0.0543],
        [0.0000, 0.0000, 0.0000,  ..., 0.0807, 0.0000, 0.0430],
        [0.0000, 0.0000, 0.0000,  ..., 0.0315, 0.0000, 0.0273],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1917, 0.0000, 0.0617],
        [0.0000, 0.0000, 0.0000,  ..., 0.1917, 0.0000, 0.0617],
        [0.0000, 0.0000, 0.0000,  ..., 0.1917, 0.0000, 0.0617]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3108197., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(777.2778, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6120.4785, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12723.1533, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(853.1132, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.3346],
        [-1.4691],
        [ 0.0346],
        ...,
        [-8.0890],
        [-8.0782],
        [-8.0749]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-795536.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.7988],
        [0.3286],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(650.9916, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.7988],
        [0.3286],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(650.9916, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0390e-02, -5.4378e-03, -8.2343e-05,  ...,  1.1682e-03,
         -6.3149e-03, -1.5872e-02],
        [-1.1515e-02, -4.3229e-03, -2.0017e-04,  ...,  5.4531e-04,
         -6.7576e-03, -1.7592e-02],
        [-1.0390e-02, -5.4378e-03, -8.2343e-05,  ...,  1.1682e-03,
         -6.3149e-03, -1.5872e-02],
        ...,
        [-9.6037e-03, -6.2169e-03,  0.0000e+00,  ...,  1.6035e-03,
         -6.0055e-03, -1.4669e-02],
        [-9.6037e-03, -6.2169e-03,  0.0000e+00,  ...,  1.6035e-03,
         -6.0055e-03, -1.4669e-02],
        [-9.6037e-03, -6.2169e-03,  0.0000e+00,  ...,  1.6035e-03,
         -6.0055e-03, -1.4669e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-69944.1016, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.1230, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(80.3021, device='cuda:0')



h[100].sum tensor(116.1232, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-95.7192, device='cuda:0')



h[200].sum tensor(-482.1068, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.1677, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(279886.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0017],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0122],
        [0.0000, 0.0000, 0.0000,  ..., 0.0341, 0.0000, 0.0287],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1931, 0.0000, 0.0617],
        [0.0000, 0.0000, 0.0000,  ..., 0.1931, 0.0000, 0.0617],
        [0.0000, 0.0000, 0.0000,  ..., 0.1931, 0.0000, 0.0617]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3057608., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(831.6293, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5960.5918, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11389.8613, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(978.6377, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.0840],
        [ 0.4021],
        [-1.1517],
        ...,
        [-8.1733],
        [-8.1646],
        [-8.1635]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-891842.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 330.0 event: 4950 loss: tensor(404.5870, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.7700],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(651.1021, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.7700],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(651.1021, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0107, -0.0051, -0.0001,  ...,  0.0010, -0.0064, -0.0163],
        [-0.0114, -0.0044, -0.0002,  ...,  0.0006, -0.0067, -0.0175],
        [-0.0113, -0.0045, -0.0002,  ...,  0.0007, -0.0067, -0.0173],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0016, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0016, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0016, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-70461.4609, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.0893, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(80.3157, device='cuda:0')



h[100].sum tensor(114.0271, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-95.7355, device='cuda:0')



h[200].sum tensor(-480.6671, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.1721, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(276778.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0046],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0112],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1942, 0.0000, 0.0615],
        [0.0000, 0.0000, 0.0000,  ..., 0.1942, 0.0000, 0.0615],
        [0.0000, 0.0000, 0.0000,  ..., 0.1942, 0.0000, 0.0615]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3020637., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(841.0449, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5904.2959, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10577.5156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(972.1479, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.5164],
        [ 1.4882],
        [ 1.1939],
        ...,
        [-7.8543],
        [-8.1078],
        [-8.1843]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-915976.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(751.5261, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(751.5261, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0115, -0.0044, -0.0002,  ...,  0.0006, -0.0068, -0.0176],
        [-0.0115, -0.0044, -0.0002,  ...,  0.0006, -0.0067, -0.0175],
        [-0.0115, -0.0044, -0.0002,  ...,  0.0006, -0.0067, -0.0175],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0016, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0016, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0016, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-69920.9844, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.0076, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(92.7034, device='cuda:0')



h[100].sum tensor(119.4615, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-110.5014, device='cuda:0')



h[200].sum tensor(-474.9786, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.2088, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(286287.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1949, 0.0000, 0.0606],
        [0.0000, 0.0000, 0.0000,  ..., 0.1949, 0.0000, 0.0606],
        [0.0000, 0.0000, 0.0000,  ..., 0.1949, 0.0000, 0.0606]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3090377., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(865.2832, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5888.6128, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12209.3379, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1000.8684, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5601],
        [ 0.5741],
        [ 0.5876],
        ...,
        [-8.2509],
        [-8.2420],
        [-8.2408]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-852889.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(849.3729, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(849.3729, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0124, -0.0035, -0.0003,  ...,  0.0002, -0.0071, -0.0190],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0017, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0017, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0017, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0017, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0017, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-69280.5703, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-987.0120, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(104.7731, device='cuda:0')



h[100].sum tensor(123.5993, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-124.8884, device='cuda:0')



h[200].sum tensor(-469.6490, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.1419, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(296299.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0150],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1968, 0.0000, 0.0601],
        [0.0000, 0.0000, 0.0000,  ..., 0.1968, 0.0000, 0.0601],
        [0.0000, 0.0000, 0.0000,  ..., 0.1968, 0.0000, 0.0601]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3147754.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(902.1229, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5871.0459, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13541.2676, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1036.2040, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.6419],
        [ 0.8412],
        [ 1.0119],
        ...,
        [-8.3161],
        [-8.3066],
        [-8.3051]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-902800.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(718.6407, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(718.6407, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0109, -0.0050, -0.0001,  ...,  0.0010, -0.0065, -0.0167],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0017, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0017, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0017, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0017, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0017, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-70671.8125, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.7267, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(88.6468, device='cuda:0')



h[100].sum tensor(114.8540, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-105.6661, device='cuda:0')



h[200].sum tensor(-474.5999, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.8869, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(279962.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0228, 0.0000, 0.0231],
        [0.0000, 0.0000, 0.0000,  ..., 0.0572, 0.0000, 0.0326],
        [0.0000, 0.0000, 0.0000,  ..., 0.1341, 0.0000, 0.0504],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1968, 0.0000, 0.0601],
        [0.0000, 0.0000, 0.0000,  ..., 0.1916, 0.0000, 0.0594],
        [0.0000, 0.0000, 0.0000,  ..., 0.1677, 0.0000, 0.0559]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3009901.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(877.4077, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5807.8984, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10231.6660, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(982.1907, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4300],
        [-0.8970],
        [-2.8245],
        ...,
        [-8.0914],
        [-7.5796],
        [-6.4767]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-950113.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2781],
        [0.2754],
        [0.2529],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(746.4032, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2781],
        [0.2754],
        [0.2529],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(746.4032, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1005e-02, -4.8639e-03, -1.3279e-04,  ...,  1.0257e-03,
         -6.5571e-03, -1.6814e-02],
        [-1.0874e-02, -4.9918e-03, -1.2034e-04,  ...,  1.0945e-03,
         -6.5054e-03, -1.6613e-02],
        [-1.0263e-02, -5.5871e-03, -6.2409e-05,  ...,  1.4147e-03,
         -6.2648e-03, -1.5678e-02],
        ...,
        [-9.6037e-03, -6.2285e-03,  0.0000e+00,  ...,  1.7597e-03,
         -6.0055e-03, -1.4670e-02],
        [-9.6037e-03, -6.2285e-03,  0.0000e+00,  ...,  1.7597e-03,
         -6.0055e-03, -1.4670e-02],
        [-9.6037e-03, -6.2285e-03,  0.0000e+00,  ...,  1.7597e-03,
         -6.0055e-03, -1.4670e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-70625.9688, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.0187, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(92.0714, device='cuda:0')



h[100].sum tensor(115.4412, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-109.7482, device='cuda:0')



h[200].sum tensor(-472.3734, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.0029, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(287543., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0007],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1986, 0.0000, 0.0593],
        [0.0000, 0.0000, 0.0000,  ..., 0.1986, 0.0000, 0.0593],
        [0.0000, 0.0000, 0.0000,  ..., 0.1986, 0.0000, 0.0593]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3074219.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(704.6748, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5984.6802, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11864.4580, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1187.7639, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.9085],
        [ 0.9140],
        [ 0.9507],
        ...,
        [-8.1549],
        [-8.1746],
        [-8.2105]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-912931.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6064],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(557.6959, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6064],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(557.6959, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0111, -0.0048, -0.0001,  ...,  0.0011, -0.0066, -0.0169],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0018, -0.0060, -0.0147],
        [-0.0123, -0.0036, -0.0002,  ...,  0.0004, -0.0071, -0.0188],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0018, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0018, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0018, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-72792.0469, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.2482, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(68.7937, device='cuda:0')



h[100].sum tensor(100.2467, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-82.0014, device='cuda:0')



h[200].sum tensor(-478.3892, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.4175, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(272292.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0467, 0.0000, 0.0251],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0008],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2014, 0.0000, 0.0590],
        [0.0000, 0.0000, 0.0000,  ..., 0.2014, 0.0000, 0.0590],
        [0.0000, 0.0000, 0.0000,  ..., 0.2014, 0.0000, 0.0590]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2935984., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(431.6951, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6108.4629, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8468.3701, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1395.3003, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2893],
        [ 0.4148],
        [ 1.1972],
        ...,
        [-8.5024],
        [-8.4923],
        [-8.4907]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1070595.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(849.8733, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(849.8733, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.2327e-03,  0.0000e+00,  ...,  1.8419e-03,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.2327e-03,  0.0000e+00,  ...,  1.8419e-03,
         -6.0055e-03, -1.4671e-02],
        [-1.0247e-02, -5.6103e-03, -5.7960e-05,  ...,  1.5156e-03,
         -6.2588e-03, -1.5655e-02],
        ...,
        [-9.6037e-03, -6.2327e-03,  0.0000e+00,  ...,  1.8419e-03,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.2327e-03,  0.0000e+00,  ...,  1.8419e-03,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.2327e-03,  0.0000e+00,  ...,  1.8419e-03,
         -6.0055e-03, -1.4671e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-70392.1172, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.9256, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(104.8348, device='cuda:0')



h[100].sum tensor(117.8322, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-124.9620, device='cuda:0')



h[200].sum tensor(-467.8248, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.1620, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0074, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0074, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0074, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0074, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(288267.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1785, 0.0000, 0.0527],
        [0.0000, 0.0000, 0.0000,  ..., 0.1554, 0.0000, 0.0489],
        [0.0000, 0.0000, 0.0000,  ..., 0.1360, 0.0000, 0.0456],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2029, 0.0000, 0.0579],
        [0.0000, 0.0000, 0.0000,  ..., 0.2029, 0.0000, 0.0579],
        [0.0000, 0.0000, 0.0000,  ..., 0.2029, 0.0000, 0.0579]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3031806.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(670.9348, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6068.1309, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11186.6914, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1247.1298, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.4879],
        [-4.3659],
        [-3.6888],
        ...,
        [-8.5275],
        [-8.5138],
        [-8.5044]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-926958.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(706.3986, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(706.3986, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0019, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0019, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0019, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0019, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0019, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0019, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-72176.7500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.6006, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(87.1367, device='cuda:0')



h[100].sum tensor(107.9379, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-103.8661, device='cuda:0')



h[200].sum tensor(-472.8984, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.3948, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0075, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0075, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0075, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0075, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0075, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(276993.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1974, 0.0000, 0.0557],
        [0.0000, 0.0000, 0.0000,  ..., 0.1617, 0.0000, 0.0494],
        [0.0000, 0.0000, 0.0000,  ..., 0.0786, 0.0000, 0.0351],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2048, 0.0000, 0.0569],
        [0.0000, 0.0000, 0.0000,  ..., 0.2048, 0.0000, 0.0569],
        [0.0000, 0.0000, 0.0000,  ..., 0.2048, 0.0000, 0.0569]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2940221., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(690.9895, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6008.1602, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8831.5820, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1208.1372, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.3566],
        [-3.4270],
        [-1.2189],
        ...,
        [-8.5884],
        [-8.5780],
        [-8.5764]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1025476.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(923.4279, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(923.4279, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0018, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0018, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0018, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0018, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0018, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0018, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-70374.3125, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-987.5403, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(113.9080, device='cuda:0')



h[100].sum tensor(123.7835, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-135.7772, device='cuda:0')



h[200].sum tensor(-465.0894, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.1187, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(300970.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2039, 0.0000, 0.0556],
        [0.0000, 0.0000, 0.0000,  ..., 0.2039, 0.0000, 0.0556],
        [0.0000, 0.0000, 0.0000,  ..., 0.2036, 0.0000, 0.0558],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2046, 0.0000, 0.0552],
        [0.0000, 0.0000, 0.0000,  ..., 0.2046, 0.0000, 0.0552],
        [0.0000, 0.0000, 0.0000,  ..., 0.2046, 0.0000, 0.0552]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3119772., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(900.8414, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5930.4570, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12653.3613, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1177.6566, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-8.3099],
        [-8.4394],
        [-8.5304],
        ...,
        [-8.5321],
        [-8.5221],
        [-8.5206]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-853457.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(752.6929, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(752.6929, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0018, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0018, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0018, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0018, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0018, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0018, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-72154.7031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.9745, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(92.8473, device='cuda:0')



h[100].sum tensor(109.9371, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-110.6730, device='cuda:0')



h[200].sum tensor(-471.4993, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.2557, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(288792.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1971, 0.0000, 0.0534],
        [0.0000, 0.0000, 0.0000,  ..., 0.1646, 0.0000, 0.0483],
        [0.0000, 0.0000, 0.0000,  ..., 0.0876, 0.0000, 0.0329],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1445, 0.0000, 0.0414],
        [0.0000, 0.0000, 0.0000,  ..., 0.1631, 0.0000, 0.0452],
        [0.0000, 0.0000, 0.0000,  ..., 0.1877, 0.0000, 0.0500]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3057474.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(792.8171, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6156.0361, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12574.5664, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1219.6393, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.6879],
        [-4.8784],
        [-2.5530],
        ...,
        [-5.3645],
        [-5.7097],
        [-6.5677]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1028106.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 340.0 event: 5100 loss: tensor(421.1680, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(583.3171, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(583.3171, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0087e-02, -5.7817e-03, -3.9299e-05,  ...,  1.5317e-03,
         -6.1957e-03, -1.5410e-02],
        [-9.6037e-03, -6.2420e-03,  0.0000e+00,  ...,  1.7670e-03,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.2420e-03,  0.0000e+00,  ...,  1.7670e-03,
         -6.0055e-03, -1.4671e-02],
        ...,
        [-9.6037e-03, -6.2420e-03,  0.0000e+00,  ...,  1.7670e-03,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.2420e-03,  0.0000e+00,  ...,  1.7670e-03,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.2420e-03,  0.0000e+00,  ...,  1.7670e-03,
         -6.0055e-03, -1.4671e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-74035.6719, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.4951, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(71.9542, device='cuda:0')



h[100].sum tensor(103.4968, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-85.7686, device='cuda:0')



h[200].sum tensor(-477.8140, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.4474, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(271207.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0408, 0.0000, 0.0185],
        [0.0000, 0.0000, 0.0000,  ..., 0.0886, 0.0000, 0.0319],
        [0.0000, 0.0000, 0.0000,  ..., 0.1680, 0.0000, 0.0462],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2067, 0.0000, 0.0529],
        [0.0000, 0.0000, 0.0000,  ..., 0.2067, 0.0000, 0.0529],
        [0.0000, 0.0000, 0.0000,  ..., 0.2067, 0.0000, 0.0529]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2913631.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(880.5011, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6045.5254, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8523.5186, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1025.3501, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4168],
        [-1.2149],
        [-3.3149],
        ...,
        [-8.5804],
        [-8.5705],
        [-8.5690]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-905583., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6313],
        [0.5386],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(842.8430, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6313],
        [0.5386],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(842.8430, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0109, -0.0050, -0.0001,  ...,  0.0011, -0.0065, -0.0166],
        [-0.0111, -0.0048, -0.0001,  ...,  0.0010, -0.0066, -0.0170],
        [-0.0109, -0.0050, -0.0001,  ...,  0.0011, -0.0065, -0.0166],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0017, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0017, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0017, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-71618.8750, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.8266, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(103.9676, device='cuda:0')



h[100].sum tensor(120.8259, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-123.9283, device='cuda:0')



h[200].sum tensor(-469.4014, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.8794, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(291874.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2068, 0.0000, 0.0517],
        [0.0000, 0.0000, 0.0000,  ..., 0.2068, 0.0000, 0.0517],
        [0.0000, 0.0000, 0.0000,  ..., 0.2068, 0.0000, 0.0517]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3044767.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(983.9774, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6152.9209, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12045.8926, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1054.3442, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.8436],
        [ 0.8310],
        [ 0.8327],
        ...,
        [-8.5725],
        [-8.5627],
        [-8.5613]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-807382.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(654.2903, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(654.2903, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.2386e-03,  0.0000e+00,  ...,  1.6968e-03,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.2386e-03,  0.0000e+00,  ...,  1.6968e-03,
         -6.0055e-03, -1.4671e-02],
        [-1.0222e-02, -5.6457e-03, -4.7725e-05,  ...,  1.3978e-03,
         -6.2487e-03, -1.5616e-02],
        ...,
        [-9.6037e-03, -6.2386e-03,  0.0000e+00,  ...,  1.6968e-03,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.2386e-03,  0.0000e+00,  ...,  1.6968e-03,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.2386e-03,  0.0000e+00,  ...,  1.6968e-03,
         -6.0055e-03, -1.4671e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-73531.8281, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.0605, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(80.7090, device='cuda:0')



h[100].sum tensor(105.4109, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-96.2043, device='cuda:0')



h[200].sum tensor(-475.6248, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.3003, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(282161.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1678, 0.0000, 0.0450],
        [0.0000, 0.0000, 0.0000,  ..., 0.0960, 0.0000, 0.0329],
        [0.0000, 0.0000, 0.0000,  ..., 0.0410, 0.0000, 0.0169],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2085, 0.0000, 0.0512],
        [0.0000, 0.0000, 0.0000,  ..., 0.2085, 0.0000, 0.0512],
        [0.0000, 0.0000, 0.0000,  ..., 0.2085, 0.0000, 0.0512]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3008374.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1178.0664, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6127.0547, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12850.7383, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(963.1852, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.2663],
        [-2.1799],
        [-0.3850],
        ...,
        [-8.6878],
        [-8.6777],
        [-8.6762]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1046925.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(682.5081, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(682.5081, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0224e-02, -5.6318e-03, -4.6696e-05,  ...,  1.3056e-03,
         -6.2497e-03, -1.5620e-02],
        [-1.0224e-02, -5.6318e-03, -4.6696e-05,  ...,  1.3056e-03,
         -6.2497e-03, -1.5620e-02],
        [-9.6037e-03, -6.2334e-03,  0.0000e+00,  ...,  1.6006e-03,
         -6.0055e-03, -1.4671e-02],
        ...,
        [-9.6037e-03, -6.2334e-03,  0.0000e+00,  ...,  1.6006e-03,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.2334e-03,  0.0000e+00,  ...,  1.6006e-03,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.2334e-03,  0.0000e+00,  ...,  1.6006e-03,
         -6.0055e-03, -1.4671e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-73532.3672, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.3819, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(84.1897, device='cuda:0')



h[100].sum tensor(113.8052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-100.3533, device='cuda:0')



h[200].sum tensor(-473.4883, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.4345, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(281727.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0878, 0.0000, 0.0261],
        [0.0000, 0.0000, 0.0000,  ..., 0.1107, 0.0000, 0.0305],
        [0.0000, 0.0000, 0.0000,  ..., 0.1361, 0.0000, 0.0357],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2058, 0.0000, 0.0496],
        [0.0000, 0.0000, 0.0000,  ..., 0.2058, 0.0000, 0.0496],
        [0.0000, 0.0000, 0.0000,  ..., 0.2058, 0.0000, 0.0496]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3014877.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1149.2161, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6100.1299, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12727.7705, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(828.3789, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2935],
        [-3.8535],
        [-5.3768],
        ...,
        [-8.5330],
        [-8.5237],
        [-8.5224]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-804304.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4944],
        [0.5615],
        [0.6328],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(694.0161, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4944],
        [0.5615],
        [0.6328],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(694.0161, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0947e-02, -4.9138e-03, -9.8522e-05,  ...,  9.3306e-04,
         -6.5341e-03, -1.6725e-02],
        [-1.2300e-02, -3.5893e-03, -1.9777e-04,  ...,  3.0471e-04,
         -7.0667e-03, -1.8793e-02],
        [-1.0947e-02, -4.9138e-03, -9.8522e-05,  ...,  9.3306e-04,
         -6.5341e-03, -1.6725e-02],
        ...,
        [-9.6037e-03, -6.2286e-03,  0.0000e+00,  ...,  1.5568e-03,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.2286e-03,  0.0000e+00,  ...,  1.5568e-03,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.2286e-03,  0.0000e+00,  ...,  1.5568e-03,
         -6.0055e-03, -1.4671e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-73800.7188, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.5177, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(85.6093, device='cuda:0')



h[100].sum tensor(118.9246, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-102.0454, device='cuda:0')



h[200].sum tensor(-472.5330, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.8971, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(279198.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2046, 0.0000, 0.0484],
        [0.0000, 0.0000, 0.0000,  ..., 0.2046, 0.0000, 0.0484],
        [0.0000, 0.0000, 0.0000,  ..., 0.2046, 0.0000, 0.0484]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2955796., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1145.2877, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6028.7832, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10569.4688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(777.5111, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.0429],
        [ 1.1013],
        [ 1.1068],
        ...,
        [-8.1173],
        [-8.2343],
        [-8.3347]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-624286.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(652.9395, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(652.9395, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0016, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0016, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0016, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0016, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0016, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0016, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-74781.0859, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.0900, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(80.5424, device='cuda:0')



h[100].sum tensor(108.0794, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-96.0056, device='cuda:0')



h[200].sum tensor(-474.5046, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.2460, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(272192.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1932, 0.0000, 0.0474],
        [0.0000, 0.0000, 0.0000,  ..., 0.2079, 0.0000, 0.0494],
        [0.0000, 0.0000, 0.0000,  ..., 0.2075, 0.0000, 0.0496],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2085, 0.0000, 0.0491],
        [0.0000, 0.0000, 0.0000,  ..., 0.2085, 0.0000, 0.0491],
        [0.0000, 0.0000, 0.0000,  ..., 0.2085, 0.0000, 0.0491]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2897966., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1215.4506, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6044.1211, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11201.0635, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(763.2509, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.8230],
        [-7.8758],
        [-8.3729],
        ...,
        [-8.6774],
        [-8.6676],
        [-8.6663]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1040702.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4395],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(931.8173, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4395],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(931.8173, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.2205e-03,  0.0000e+00,  ...,  1.5379e-03,
         -6.0055e-03, -1.4671e-02],
        [-1.0655e-02, -5.1750e-03, -7.3198e-05,  ...,  1.0577e-03,
         -6.4192e-03, -1.6278e-02],
        [-1.0443e-02, -5.3859e-03, -5.8436e-05,  ...,  1.1545e-03,
         -6.3358e-03, -1.5954e-02],
        ...,
        [-9.6037e-03, -6.2205e-03,  0.0000e+00,  ...,  1.5379e-03,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.2205e-03,  0.0000e+00,  ...,  1.5379e-03,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.2205e-03,  0.0000e+00,  ...,  1.5379e-03,
         -6.0055e-03, -1.4671e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-72148.1562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-987.7404, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(114.9429, device='cuda:0')



h[100].sum tensor(125.9033, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-137.0107, device='cuda:0')



h[200].sum tensor(-463.8195, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.4559, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(307995.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0997, 0.0000, 0.0302],
        [0.0000, 0.0000, 0.0000,  ..., 0.0555, 0.0000, 0.0188],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0034],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2093, 0.0000, 0.0482],
        [0.0000, 0.0000, 0.0000,  ..., 0.2093, 0.0000, 0.0482],
        [0.0000, 0.0000, 0.0000,  ..., 0.2093, 0.0000, 0.0482]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(3206590.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1340.4222, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5958.4097, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(19258.6426, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1041.5347, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4577],
        [-0.2205],
        [ 0.8125],
        ...,
        [-8.7080],
        [-8.6982],
        [-8.6968]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1000364.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(631.8638, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(631.8638, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0940e-02, -4.8793e-03, -9.0639e-05,  ...,  8.5358e-04,
         -6.5313e-03, -1.6714e-02],
        [-1.0940e-02, -4.8793e-03, -9.0639e-05,  ...,  8.5358e-04,
         -6.5313e-03, -1.6714e-02],
        [-9.6037e-03, -6.2170e-03,  0.0000e+00,  ...,  1.4525e-03,
         -6.0055e-03, -1.4671e-02],
        ...,
        [-9.6037e-03, -6.2170e-03,  0.0000e+00,  ...,  1.4525e-03,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.2170e-03,  0.0000e+00,  ...,  1.4525e-03,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.2170e-03,  0.0000e+00,  ...,  1.4525e-03,
         -6.0055e-03, -1.4671e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-75290.3906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.9594, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(77.9426, device='cuda:0')



h[100].sum tensor(113.5672, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-92.9068, device='cuda:0')



h[200].sum tensor(-474.6243, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.3988, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(273711.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0230, 0.0000, 0.0075],
        [0.0000, 0.0000, 0.0000,  ..., 0.0033, 0.0000, 0.0027],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2079, 0.0000, 0.0455],
        [0.0000, 0.0000, 0.0000,  ..., 0.2079, 0.0000, 0.0455],
        [0.0000, 0.0000, 0.0000,  ..., 0.2079, 0.0000, 0.0455]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2843103.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1288.3374, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5497.0640, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9065.8047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1087.8026, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.9837],
        [ 1.7692],
        [ 1.8830],
        ...,
        [-8.5211],
        [-8.5123],
        [-8.5112]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-748321.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(562.8892, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(562.8892, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0159e-02, -5.6548e-03, -3.6681e-05,  ...,  1.1622e-03,
         -6.2240e-03, -1.5520e-02],
        [-9.6037e-03, -6.2139e-03,  0.0000e+00,  ...,  1.4107e-03,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.2139e-03,  0.0000e+00,  ...,  1.4107e-03,
         -6.0055e-03, -1.4671e-02],
        ...,
        [-9.6037e-03, -6.2139e-03,  0.0000e+00,  ...,  1.4107e-03,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.2139e-03,  0.0000e+00,  ...,  1.4107e-03,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.2139e-03,  0.0000e+00,  ...,  1.4107e-03,
         -6.0055e-03, -1.4671e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-76564.3750, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.2924, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(69.4343, device='cuda:0')



h[100].sum tensor(105.8985, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-82.7650, device='cuda:0')



h[200].sum tensor(-477.1864, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.6262, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(266558.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1171, 0.0000, 0.0251],
        [0.0000, 0.0000, 0.0000,  ..., 0.1341, 0.0000, 0.0285],
        [0.0000, 0.0000, 0.0000,  ..., 0.1809, 0.0000, 0.0388],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2096, 0.0000, 0.0452],
        [0.0000, 0.0000, 0.0000,  ..., 0.2096, 0.0000, 0.0452],
        [0.0000, 0.0000, 0.0000,  ..., 0.2096, 0.0000, 0.0452]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2808968.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1296.7706, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5655.8418, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9441.4238, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(913.1187, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.8445],
        [-3.4404],
        [-4.4751],
        ...,
        [-8.6019],
        [-8.5925],
        [-8.5911]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-827748.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(625.3671, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(625.3671, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0014, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0014, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0014, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0014, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0014, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0014, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-76551.5078, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.8438, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(77.1412, device='cuda:0')



h[100].sum tensor(101.0746, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-91.9515, device='cuda:0')



h[200].sum tensor(-473.9246, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.1376, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(266439.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2038, 0.0000, 0.0449],
        [0.0000, 0.0000, 0.0000,  ..., 0.2107, 0.0000, 0.0458],
        [0.0000, 0.0000, 0.0000,  ..., 0.2083, 0.0000, 0.0451],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2122, 0.0000, 0.0458],
        [0.0000, 0.0000, 0.0000,  ..., 0.2122, 0.0000, 0.0458],
        [0.0000, 0.0000, 0.0000,  ..., 0.2122, 0.0000, 0.0458]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2822607., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1338.8435, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5947.3071, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12460.2607, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(726.9043, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.2352],
        [-7.3735],
        [-6.9268],
        ...,
        [-8.8115],
        [-8.8017],
        [-8.8003]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1060580., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 350.0 event: 5250 loss: tensor(461.0962, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5195],
        [0.6719],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(816.7725, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5195],
        [0.6719],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(816.7725, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1211e-02, -4.5730e-03, -1.0078e-04,  ...,  5.8106e-04,
         -6.6380e-03, -1.7128e-02],
        [-1.0847e-02, -4.9438e-03, -7.7927e-05,  ...,  7.4453e-04,
         -6.4946e-03, -1.6571e-02],
        [-1.1211e-02, -4.5730e-03, -1.0078e-04,  ...,  5.8106e-04,
         -6.6380e-03, -1.7128e-02],
        ...,
        [-9.6037e-03, -6.2085e-03,  0.0000e+00,  ...,  1.3020e-03,
         -6.0055e-03, -1.4672e-02],
        [-9.6037e-03, -6.2085e-03,  0.0000e+00,  ...,  1.3020e-03,
         -6.0055e-03, -1.4672e-02],
        [-9.6037e-03, -6.2085e-03,  0.0000e+00,  ...,  1.3020e-03,
         -6.0055e-03, -1.4672e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-75061.4531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.5776, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(100.7517, device='cuda:0')



h[100].sum tensor(113.6210, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-120.0950, device='cuda:0')



h[200].sum tensor(-465.7167, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.8315, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0026, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(281398.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0275, 0.0000, 0.0109],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2112, 0.0000, 0.0445],
        [0.0000, 0.0000, 0.0000,  ..., 0.2112, 0.0000, 0.0445],
        [0.0000, 0.0000, 0.0000,  ..., 0.2112, 0.0000, 0.0445]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2912236., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1381.4049, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6042.8218, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15603.4756, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(713.9777, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3221],
        [ 0.6944],
        [ 0.1178],
        ...,
        [-8.7877],
        [-8.7804],
        [-8.7783]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-966309.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(868.5375, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(868.5375, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0012, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0012, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0012, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0012, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0012, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0012, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-74752.6016, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-987.1513, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(107.1371, device='cuda:0')



h[100].sum tensor(125.0119, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-127.7063, device='cuda:0')



h[200].sum tensor(-462.4835, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.9123, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(284949.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1857, 0.0000, 0.0386],
        [0.0000, 0.0000, 0.0000,  ..., 0.2048, 0.0000, 0.0420],
        [0.0000, 0.0000, 0.0000,  ..., 0.2076, 0.0000, 0.0427],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2086, 0.0000, 0.0423],
        [0.0000, 0.0000, 0.0000,  ..., 0.2086, 0.0000, 0.0423],
        [0.0000, 0.0000, 0.0000,  ..., 0.2086, 0.0000, 0.0423]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2895816.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1366.5706, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5956.1411, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14186.1064, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(705.3477, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.9136],
        [-6.4013],
        [-7.2539],
        ...,
        [-8.6035],
        [-8.5949],
        [-8.5939]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-621882.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(620.3419, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(620.3419, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0012, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0012, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0012, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0012, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0012, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0012, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-77777.0703, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.7885, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(76.5213, device='cuda:0')



h[100].sum tensor(102.2441, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-91.2126, device='cuda:0')



h[200].sum tensor(-470.7590, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.9356, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(259457.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1837, 0.0000, 0.0353],
        [0.0000, 0.0000, 0.0000,  ..., 0.1575, 0.0000, 0.0297],
        [0.0000, 0.0000, 0.0000,  ..., 0.1297, 0.0000, 0.0246],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2105, 0.0000, 0.0419],
        [0.0000, 0.0000, 0.0000,  ..., 0.2105, 0.0000, 0.0419],
        [0.0000, 0.0000, 0.0000,  ..., 0.2105, 0.0000, 0.0419]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2722665.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1372.4813, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5944.3701, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12413.8271, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(613.0164, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.8056],
        [-3.8997],
        [-3.1041],
        ...,
        [-8.7602],
        [-8.7509],
        [-8.7497]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-848944.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(775.0824, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(775.0824, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0011, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0011, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0011, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0011, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0011, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0011, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-76576.1328, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.2526, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(95.6091, device='cuda:0')



h[100].sum tensor(107.8389, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-113.9650, device='cuda:0')



h[200].sum tensor(-463.5217, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.1557, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(272890., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1758, 0.0000, 0.0365],
        [0.0000, 0.0000, 0.0000,  ..., 0.1752, 0.0000, 0.0364],
        [0.0000, 0.0000, 0.0000,  ..., 0.1647, 0.0000, 0.0350],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2114, 0.0000, 0.0413],
        [0.0000, 0.0000, 0.0000,  ..., 0.2114, 0.0000, 0.0413],
        [0.0000, 0.0000, 0.0000,  ..., 0.2114, 0.0000, 0.0413]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2819105.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1427.4331, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6036.1123, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(16765.3047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(659.1333, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.3171],
        [-2.8648],
        [-2.1638],
        ...,
        [-8.8570],
        [-8.8474],
        [-8.8461]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-907001.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2612],
        [0.0000],
        [0.2466],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(647.9766, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2612],
        [0.0000],
        [0.2466],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(647.9766, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0088e-02, -5.6998e-03, -2.7340e-05,  ...,  8.6035e-04,
         -6.1963e-03, -1.5412e-02],
        [-1.1816e-02, -3.9154e-03, -1.2477e-04,  ...,  7.8386e-05,
         -6.8761e-03, -1.8052e-02],
        [-1.0116e-02, -5.6710e-03, -2.8913e-05,  ...,  8.4772e-04,
         -6.2072e-03, -1.5455e-02],
        ...,
        [-9.6037e-03, -6.2005e-03,  0.0000e+00,  ...,  1.0798e-03,
         -6.0055e-03, -1.4672e-02],
        [-9.6037e-03, -6.2005e-03,  0.0000e+00,  ...,  1.0798e-03,
         -6.0055e-03, -1.4672e-02],
        [-9.6037e-03, -6.2005e-03,  0.0000e+00,  ...,  1.0798e-03,
         -6.0055e-03, -1.4672e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-78245.2812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.0602, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(79.9302, device='cuda:0')



h[100].sum tensor(98.2819, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-95.2759, device='cuda:0')



h[200].sum tensor(-466.4245, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.0465, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(260250.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2113, 0.0000, 0.0405],
        [0.0000, 0.0000, 0.0000,  ..., 0.2113, 0.0000, 0.0405],
        [0.0000, 0.0000, 0.0000,  ..., 0.2113, 0.0000, 0.0405]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2708197.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1386.9160, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5882.7197, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13658.8291, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(638.8992, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.7730],
        [ 1.7724],
        [ 1.7494],
        ...,
        [-8.8672],
        [-8.8576],
        [-8.8563]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-925557.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.9429],
        [0.8540],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(543.7529, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.9429],
        [0.8540],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(543.7529, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1859e-02, -3.8624e-03, -1.2387e-04,  ...,  1.1740e-05,
         -6.8932e-03, -1.8118e-02],
        [-1.1647e-02, -4.0826e-03, -1.1220e-04,  ...,  1.0703e-04,
         -6.8095e-03, -1.7793e-02],
        [-1.1859e-02, -3.8624e-03, -1.2387e-04,  ...,  1.1740e-05,
         -6.8932e-03, -1.8118e-02],
        ...,
        [-9.6037e-03, -6.1990e-03,  0.0000e+00,  ...,  1.0228e-03,
         -6.0055e-03, -1.4672e-02],
        [-9.6037e-03, -6.1990e-03,  0.0000e+00,  ...,  1.0228e-03,
         -6.0055e-03, -1.4672e-02],
        [-9.6037e-03, -6.1990e-03,  0.0000e+00,  ...,  1.0228e-03,
         -6.0055e-03, -1.4672e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-79649.9375, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.0663, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(67.0738, device='cuda:0')



h[100].sum tensor(92.6444, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-79.9513, device='cuda:0')



h[200].sum tensor(-468.5140, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.8570, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(250020.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2104, 0.0000, 0.0395],
        [0.0000, 0.0000, 0.0000,  ..., 0.2104, 0.0000, 0.0395],
        [0.0000, 0.0000, 0.0000,  ..., 0.2104, 0.0000, 0.0395]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2603225.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1328.4894, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5689.1602, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9935.5820, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(627.6572, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.5708],
        [ 1.4061],
        [ 1.2406],
        ...,
        [-8.8237],
        [-8.8143],
        [-8.8131]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-846769.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(677.4012, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(677.4012, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1167e-02, -4.5744e-03, -8.3569e-05,  ...,  3.0566e-04,
         -6.6206e-03, -1.7059e-02],
        [-9.6037e-03, -6.1976e-03,  0.0000e+00,  ...,  1.0125e-03,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1976e-03,  0.0000e+00,  ...,  1.0125e-03,
         -6.0055e-03, -1.4671e-02],
        ...,
        [-9.6037e-03, -6.1976e-03,  0.0000e+00,  ...,  1.0125e-03,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1976e-03,  0.0000e+00,  ...,  1.0125e-03,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1976e-03,  0.0000e+00,  ...,  1.0125e-03,
         -6.0055e-03, -1.4671e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-78632.4141, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.3613, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(83.5598, device='cuda:0')



h[100].sum tensor(94.8207, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-99.6024, device='cuda:0')



h[200].sum tensor(-462.3346, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.2292, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(263727.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0621, 0.0000, 0.0138],
        [0.0000, 0.0000, 0.0000,  ..., 0.0974, 0.0000, 0.0220],
        [0.0000, 0.0000, 0.0000,  ..., 0.1706, 0.0000, 0.0336],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1910, 0.0000, 0.0359],
        [0.0000, 0.0000, 0.0000,  ..., 0.2072, 0.0000, 0.0385],
        [0.0000, 0.0000, 0.0000,  ..., 0.2122, 0.0000, 0.0392]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2701321., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1370.6199, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5703.2744, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13778.7861, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(699.2521, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0618],
        [-2.9147],
        [-4.8461],
        ...,
        [-7.8119],
        [-8.4171],
        [-8.7514]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-951997.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(643.9083, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(643.9083, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0010, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0010, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0010, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0010, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0010, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0010, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-79387.5703, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.0168, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(79.4283, device='cuda:0')



h[100].sum tensor(83.2020, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-94.6777, device='cuda:0')



h[200].sum tensor(-463.4850, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.8829, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(254197.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2106, 0.0000, 0.0388],
        [0.0000, 0.0000, 0.0000,  ..., 0.2144, 0.0000, 0.0394],
        [0.0000, 0.0000, 0.0000,  ..., 0.2140, 0.0000, 0.0395],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2150, 0.0000, 0.0391],
        [0.0000, 0.0000, 0.0000,  ..., 0.2069, 0.0000, 0.0379],
        [0.0000, 0.0000, 0.0000,  ..., 0.1484, 0.0000, 0.0296]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2621359.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1378.3572, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5706.9116, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13518.0264, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(685.2802, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.3846],
        [-8.3146],
        [-8.8051],
        ...,
        [-8.4813],
        [-7.2463],
        [-5.0063]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1255718.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(724.5037, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(724.5037, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0008, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0008, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0008, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0008, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0008, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0008, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-78718.2656, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.7335, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(89.3700, device='cuda:0')



h[100].sum tensor(94.5117, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-106.5282, device='cuda:0')



h[200].sum tensor(-460.2757, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.1226, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0033, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(262472.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1985, 0.0000, 0.0365],
        [0.0000, 0.0000, 0.0000,  ..., 0.1769, 0.0000, 0.0320],
        [0.0000, 0.0000, 0.0000,  ..., 0.1480, 0.0000, 0.0259],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2123, 0.0000, 0.0386],
        [0.0000, 0.0000, 0.0000,  ..., 0.2123, 0.0000, 0.0386],
        [0.0000, 0.0000, 0.0000,  ..., 0.2123, 0.0000, 0.0386]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2671283.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1386.1217, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5687.3486, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14478.9277, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(686.8059, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.9790],
        [-4.3207],
        [-3.6447],
        ...,
        [-9.0503],
        [-9.0403],
        [-9.0389]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1024141.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(685.2053, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(685.2053, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0006, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0006, -0.0060, -0.0147],
        [-0.0118, -0.0039, -0.0001,  ..., -0.0005, -0.0069, -0.0180],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0006, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0006, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0006, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-79060.8516, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.4370, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(84.5225, device='cuda:0')



h[100].sum tensor(102.3140, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-100.7499, device='cuda:0')



h[200].sum tensor(-460.8642, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.5429, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0011, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0023, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(257781.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0378, 0.0000, 0.0088],
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2083, 0.0000, 0.0380],
        [0.0000, 0.0000, 0.0000,  ..., 0.2083, 0.0000, 0.0380],
        [0.0000, 0.0000, 0.0000,  ..., 0.2083, 0.0000, 0.0380]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2627246.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1350.1069, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5711.8979, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12745.0986, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(597.3257, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.3052],
        [ 1.1787],
        [ 1.0374],
        ...,
        [-8.8525],
        [-8.8436],
        [-8.8426]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-673550.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 360.0 event: 5400 loss: tensor(477.4918, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8760],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(956.9963, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8760],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(956.9963, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.1937e-03,  0.0000e+00,  ...,  5.1161e-04,
         -6.0055e-03, -1.4671e-02],
        [-1.1699e-02, -4.0012e-03, -1.0063e-04,  ..., -4.9625e-04,
         -6.8302e-03, -1.7873e-02],
        [-1.0316e-02, -5.4488e-03, -3.4188e-05,  ...,  1.6919e-04,
         -6.2857e-03, -1.5759e-02],
        ...,
        [-9.6037e-03, -6.1937e-03,  0.0000e+00,  ...,  5.1161e-04,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1937e-03,  0.0000e+00,  ...,  5.1161e-04,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1937e-03,  0.0000e+00,  ...,  5.1161e-04,
         -6.0055e-03, -1.4671e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-76524.7031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-987.9993, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(118.0488, device='cuda:0')



h[100].sum tensor(116.3483, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-140.7130, device='cuda:0')



h[200].sum tensor(-451.6475, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.4680, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0020, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(279256.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0005],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2099, 0.0000, 0.0388],
        [0.0000, 0.0000, 0.0000,  ..., 0.2099, 0.0000, 0.0388],
        [0.0000, 0.0000, 0.0000,  ..., 0.2099, 0.0000, 0.0388]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2769922.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1418.7687, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5809.5757, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(17932.4805, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(659.8998, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.6382],
        [ 1.5191],
        [ 1.4039],
        ...,
        [-8.9939],
        [-8.9847],
        [-8.9836]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-807703.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(736.3228, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(736.3228, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0363e-02, -5.3976e-03, -3.5485e-05,  ...,  1.2004e-04,
         -6.3043e-03, -1.5831e-02],
        [-9.6037e-03, -6.1930e-03,  0.0000e+00,  ...,  4.8772e-04,
         -6.0055e-03, -1.4671e-02],
        [-1.0363e-02, -5.3976e-03, -3.5485e-05,  ...,  1.2004e-04,
         -6.3043e-03, -1.5831e-02],
        ...,
        [-9.6037e-03, -6.1930e-03,  0.0000e+00,  ...,  4.8772e-04,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1930e-03,  0.0000e+00,  ...,  4.8772e-04,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1930e-03,  0.0000e+00,  ...,  4.8772e-04,
         -6.0055e-03, -1.4671e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-79120.9219, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.8947, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(90.8280, device='cuda:0')



h[100].sum tensor(97.4569, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-108.2660, device='cuda:0')



h[200].sum tensor(-460.9963, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.5977, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0020, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(261569.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1440, 0.0000, 0.0280],
        [0.0000, 0.0000, 0.0000,  ..., 0.0894, 0.0000, 0.0188],
        [0.0000, 0.0000, 0.0000,  ..., 0.1208, 0.0000, 0.0236],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2121, 0.0000, 0.0397],
        [0.0000, 0.0000, 0.0000,  ..., 0.2121, 0.0000, 0.0397],
        [0.0000, 0.0000, 0.0000,  ..., 0.2121, 0.0000, 0.0397]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2666769.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1398.3123, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5736.8799, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15432.6045, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(596.6829, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.4446],
        [-4.0190],
        [-2.7321],
        ...,
        [-9.0039],
        [-8.9157],
        [-8.8175]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-980353.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(695.7719, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(695.7719, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.1925e-03,  0.0000e+00,  ...,  4.0915e-04,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1925e-03,  0.0000e+00,  ...,  4.0915e-04,
         -6.0055e-03, -1.4671e-02],
        [-1.0114e-02, -5.6569e-03, -2.3231e-05,  ...,  1.6101e-04,
         -6.2065e-03, -1.5452e-02],
        ...,
        [-9.6037e-03, -6.1925e-03,  0.0000e+00,  ...,  4.0915e-04,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1925e-03,  0.0000e+00,  ...,  4.0915e-04,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1925e-03,  0.0000e+00,  ...,  4.0915e-04,
         -6.0055e-03, -1.4671e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-79802.8359, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.5206, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(85.8259, device='cuda:0')



h[100].sum tensor(95.1482, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-102.3035, device='cuda:0')



h[200].sum tensor(-462.8201, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.9677, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(258440.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1949, 0.0000, 0.0355],
        [0.0000, 0.0000, 0.0000,  ..., 0.1381, 0.0000, 0.0249],
        [0.0000, 0.0000, 0.0000,  ..., 0.0798, 0.0000, 0.0166],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2125, 0.0000, 0.0391],
        [0.0000, 0.0000, 0.0000,  ..., 0.2125, 0.0000, 0.0391],
        [0.0000, 0.0000, 0.0000,  ..., 0.2125, 0.0000, 0.0391]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2638050.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1394.1364, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5669.7275, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13634.0811, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(571.6818, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.5224],
        [-3.1363],
        [-1.3947],
        ...,
        [-9.1168],
        [-9.1078],
        [-9.1075]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-948840.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(692.5455, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(692.5455, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0004, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0004, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0004, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0004, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0004, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0004, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-79818.3047, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.5063, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(85.4279, device='cuda:0')



h[100].sum tensor(95.0500, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-101.8292, device='cuda:0')



h[200].sum tensor(-462.8771, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.8380, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(254131.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0120, 0.0000, 0.0079],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0072],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0027],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1590, 0.0000, 0.0323],
        [0.0000, 0.0000, 0.0000,  ..., 0.2000, 0.0000, 0.0376],
        [0.0000, 0.0000, 0.0000,  ..., 0.2125, 0.0000, 0.0391]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2593793., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1386.2916, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5686.1982, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12663.8496, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(545.9622, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.9986],
        [ 1.0287],
        [ 1.0454],
        ...,
        [-6.4955],
        [-7.9124],
        [-8.6741]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-933515.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(608.3149, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(608.3149, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0003, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0003, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0003, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0003, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0003, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0003, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-80890.4531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.7131, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(75.0378, device='cuda:0')



h[100].sum tensor(90.0259, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-89.4442, device='cuda:0')



h[200].sum tensor(-465.7328, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.4522, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(246085.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2057, 0.0000, 0.0370],
        [0.0000, 0.0000, 0.0000,  ..., 0.2106, 0.0000, 0.0383],
        [0.0000, 0.0000, 0.0000,  ..., 0.2110, 0.0000, 0.0387],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2120, 0.0000, 0.0383],
        [0.0000, 0.0000, 0.0000,  ..., 0.2120, 0.0000, 0.0383],
        [0.0000, 0.0000, 0.0000,  ..., 0.2120, 0.0000, 0.0383]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2516982.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1376.2191, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5593.4204, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9585.6016, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(495.2756, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.6535],
        [-6.7709],
        [-7.5057],
        ...,
        [-9.0795],
        [-9.0705],
        [-9.0697]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-902918.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4812],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(740.8203, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4812],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(740.8203, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.1916e-03,  0.0000e+00,  ...,  3.2810e-04,
         -6.0055e-03, -1.4671e-02],
        [-1.0755e-02, -4.9823e-03, -4.9586e-05,  ..., -2.3632e-04,
         -6.4585e-03, -1.6430e-02],
        [-1.0366e-02, -5.3904e-03, -3.2856e-05,  ..., -4.5889e-05,
         -6.3057e-03, -1.5836e-02],
        ...,
        [-9.6037e-03, -6.1916e-03,  0.0000e+00,  ...,  3.2810e-04,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1916e-03,  0.0000e+00,  ...,  3.2810e-04,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1916e-03,  0.0000e+00,  ...,  3.2810e-04,
         -6.0055e-03, -1.4671e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-79738.2500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.9506, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(91.3828, device='cuda:0')



h[100].sum tensor(96.5289, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-108.9273, device='cuda:0')



h[200].sum tensor(-461.3463, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.7785, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(260441.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1024, 0.0000, 0.0226],
        [0.0000, 0.0000, 0.0000,  ..., 0.0534, 0.0000, 0.0121],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2121, 0.0000, 0.0384],
        [0.0000, 0.0000, 0.0000,  ..., 0.2121, 0.0000, 0.0384],
        [0.0000, 0.0000, 0.0000,  ..., 0.2121, 0.0000, 0.0384]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2619010.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1415.6608, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5612.6230, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12856.9805, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(486.9023, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6372],
        [-0.7294],
        [ 0.5870],
        ...,
        [-9.1212],
        [-9.1119],
        [-9.1109]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-934479.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(690.5193, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(690.5193, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0003, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0003, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0003, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0003, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0003, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0003, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-80318.4844, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.4946, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(85.1780, device='cuda:0')



h[100].sum tensor(91.8711, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-101.5312, device='cuda:0')



h[200].sum tensor(-462.5814, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.7565, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(253123.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2104, 0.0000, 0.0385],
        [0.0000, 0.0000, 0.0000,  ..., 0.2104, 0.0000, 0.0385],
        [0.0000, 0.0000, 0.0000,  ..., 0.2101, 0.0000, 0.0386],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2111, 0.0000, 0.0382],
        [0.0000, 0.0000, 0.0000,  ..., 0.2111, 0.0000, 0.0382],
        [0.0000, 0.0000, 0.0000,  ..., 0.2111, 0.0000, 0.0382]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2564167., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1427.1533, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5622.1621, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12967.9619, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(378.2745, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.4547],
        [-6.8206],
        [-5.8219],
        ...,
        [-9.1356],
        [-9.1264],
        [-9.1254]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-999580.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.9438],
        [0.7729],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(793.1112, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.9438],
        [0.7729],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(793.1112, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2611e-02, -3.0254e-03, -1.2263e-04,  ..., -1.2652e-03,
         -7.1889e-03, -1.9264e-02],
        [-1.2202e-02, -3.4557e-03, -1.0596e-04,  ..., -1.0617e-03,
         -7.0280e-03, -1.8639e-02],
        [-1.1862e-02, -3.8139e-03, -9.2079e-05,  ..., -8.9228e-04,
         -6.8941e-03, -1.8120e-02],
        ...,
        [-9.6037e-03, -6.1906e-03,  0.0000e+00,  ...,  2.3152e-04,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1906e-03,  0.0000e+00,  ...,  2.3152e-04,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1906e-03,  0.0000e+00,  ...,  2.3152e-04,
         -6.0055e-03, -1.4671e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-79334.1094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.3955, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(97.8330, device='cuda:0')



h[100].sum tensor(99.6941, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-116.6159, device='cuda:0')



h[200].sum tensor(-457.4140, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.8804, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(260439.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2080, 0.0000, 0.0366],
        [0.0000, 0.0000, 0.0000,  ..., 0.2080, 0.0000, 0.0366],
        [0.0000, 0.0000, 0.0000,  ..., 0.2080, 0.0000, 0.0366]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2590887., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1488.3008, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5644.0020, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14413.5859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(362.1295, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3718],
        [ 0.5476],
        [ 0.5612],
        ...,
        [-9.0131],
        [-9.0137],
        [-9.0204]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-859794.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(624.6146, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(624.6146, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0001, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0001, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0001, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0001, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0001, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0001, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-81119.4766, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.8164, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(77.0484, device='cuda:0')



h[100].sum tensor(89.4408, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-91.8409, device='cuda:0')



h[200].sum tensor(-462.7350, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.1074, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(246078.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1731, 0.0000, 0.0306],
        [0.0000, 0.0000, 0.0000,  ..., 0.0921, 0.0000, 0.0200],
        [0.0000, 0.0000, 0.0000,  ..., 0.0357, 0.0000, 0.0085],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2054, 0.0000, 0.0349],
        [0.0000, 0.0000, 0.0000,  ..., 0.2054, 0.0000, 0.0349],
        [0.0000, 0.0000, 0.0000,  ..., 0.2054, 0.0000, 0.0349]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2501400.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1500.9442, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5625.0586, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12097.3076, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(267.9134, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.6792],
        [-1.6492],
        [-0.1547],
        ...,
        [-8.9484],
        [-8.9402],
        [-8.9394]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-734449.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4111],
        [0.3630],
        [0.4265],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(641.5825, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4111],
        [0.3630],
        [0.4265],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(641.5825, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0472e-02, -5.2743e-03, -3.3521e-05,  ..., -2.8116e-04,
         -6.3473e-03, -1.5997e-02],
        [-1.1608e-02, -4.0775e-03, -7.7344e-05,  ..., -8.5308e-04,
         -6.7941e-03, -1.7731e-02],
        [-1.0472e-02, -5.2743e-03, -3.3521e-05,  ..., -2.8116e-04,
         -6.3473e-03, -1.5997e-02],
        ...,
        [-9.6037e-03, -6.1898e-03,  0.0000e+00,  ...,  1.5631e-04,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1898e-03,  0.0000e+00,  ...,  1.5631e-04,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1898e-03,  0.0000e+00,  ...,  1.5631e-04,
         -6.0055e-03, -1.4671e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-81173.3047, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.9731, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(79.1414, device='cuda:0')



h[100].sum tensor(85.8637, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-94.3358, device='cuda:0')



h[200].sum tensor(-461.7870, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.7894, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(246617.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0003],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2052, 0.0000, 0.0340],
        [0.0000, 0.0000, 0.0000,  ..., 0.2052, 0.0000, 0.0340],
        [0.0000, 0.0000, 0.0000,  ..., 0.2052, 0.0000, 0.0340]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2484157.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1523.5251, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5570.4424, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11386.0898, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(268.9173, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.8821],
        [ 1.5341],
        [ 0.6063],
        ...,
        [-8.9829],
        [-8.9747],
        [-8.9740]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-758094.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 370.0 event: 5550 loss: tensor(412.2236, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(843.4880, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(843.4880, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0942e-02, -4.7777e-03, -5.0254e-05,  ..., -3.7005e-04,
         -6.5322e-03, -1.6715e-02],
        [-9.6037e-03, -6.1895e-03,  0.0000e+00,  ...,  3.0459e-04,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1895e-03,  0.0000e+00,  ...,  3.0459e-04,
         -6.0055e-03, -1.4671e-02],
        ...,
        [-9.6037e-03, -6.1895e-03,  0.0000e+00,  ...,  3.0459e-04,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1895e-03,  0.0000e+00,  ...,  3.0459e-04,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1895e-03,  0.0000e+00,  ...,  3.0459e-04,
         -6.0055e-03, -1.4671e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-79501.6562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.8931, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(104.0472, device='cuda:0')



h[100].sum tensor(88.6700, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-124.0231, device='cuda:0')



h[200].sum tensor(-455.1501, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.9054, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(269483., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0227, 0.0000, 0.0031],
        [0.0000, 0.0000, 0.0000,  ..., 0.0578, 0.0000, 0.0103],
        [0.0000, 0.0000, 0.0000,  ..., 0.1394, 0.0000, 0.0244],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2080, 0.0000, 0.0340],
        [0.0000, 0.0000, 0.0000,  ..., 0.2080, 0.0000, 0.0340],
        [0.0000, 0.0000, 0.0000,  ..., 0.2080, 0.0000, 0.0340]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2676706.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1575.8711, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5596.7866, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(17074.9824, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(347.5700, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.6882],
        [-0.5723],
        [-2.5921],
        ...,
        [-9.1629],
        [-9.1523],
        [-9.1492]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-941033.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(608.0802, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(608.0802, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0129e-02, -5.6346e-03, -1.9192e-05,  ...,  1.8219e-04,
         -6.2123e-03, -1.5474e-02],
        [-9.6037e-03, -6.1892e-03,  0.0000e+00,  ...,  4.4669e-04,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1892e-03,  0.0000e+00,  ...,  4.4669e-04,
         -6.0055e-03, -1.4671e-02],
        ...,
        [-9.6037e-03, -6.1892e-03,  0.0000e+00,  ...,  4.4669e-04,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1892e-03,  0.0000e+00,  ...,  4.4669e-04,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1892e-03,  0.0000e+00,  ...,  4.4669e-04,
         -6.0055e-03, -1.4671e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-82301.7109, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.7366, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(75.0088, device='cuda:0')



h[100].sum tensor(64.4809, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-89.4097, device='cuda:0')



h[200].sum tensor(-466.0346, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.4428, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0011, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(240340.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0479, 0.0000, 0.0050],
        [0.0000, 0.0000, 0.0000,  ..., 0.0856, 0.0000, 0.0117],
        [0.0000, 0.0000, 0.0000,  ..., 0.1554, 0.0000, 0.0239],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2112, 0.0000, 0.0341],
        [0.0000, 0.0000, 0.0000,  ..., 0.2112, 0.0000, 0.0341],
        [0.0000, 0.0000, 0.0000,  ..., 0.2112, 0.0000, 0.0341]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2418831.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1522.6685, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5439.6904, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9730.2373, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(267.6749, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1441],
        [-2.1507],
        [-3.8300],
        ...,
        [-9.3233],
        [-9.3132],
        [-9.3118]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1272757.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(655.8273, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(655.8273, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0003, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0003, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0003, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0003, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0003, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0003, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-81997.4531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.1363, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(80.8986, device='cuda:0')



h[100].sum tensor(71.2531, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-96.4303, device='cuda:0')



h[200].sum tensor(-466.3586, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.3620, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(247378., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2094, 0.0000, 0.0340],
        [0.0000, 0.0000, 0.0000,  ..., 0.2094, 0.0000, 0.0340],
        [0.0000, 0.0000, 0.0000,  ..., 0.2091, 0.0000, 0.0341],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2101, 0.0000, 0.0337],
        [0.0000, 0.0000, 0.0000,  ..., 0.2101, 0.0000, 0.0337],
        [0.0000, 0.0000, 0.0000,  ..., 0.2101, 0.0000, 0.0337]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2497518., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1544.5588, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5522.7666, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11404.7520, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(256.5096, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-8.5518],
        [-8.7650],
        [-8.9211],
        ...,
        [-9.2238],
        [-9.2142],
        [-9.2131]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-985712., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4412],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(758.1050, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4412],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(758.1050, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0659e-02, -5.0738e-03, -3.6459e-05,  ..., -2.6958e-04,
         -6.4208e-03, -1.6282e-02],
        [-9.6037e-03, -6.1887e-03,  0.0000e+00,  ...,  2.7921e-04,
         -6.0055e-03, -1.4671e-02],
        [-1.0659e-02, -5.0738e-03, -3.6459e-05,  ..., -2.6958e-04,
         -6.4208e-03, -1.6282e-02],
        ...,
        [-9.6037e-03, -6.1887e-03,  0.0000e+00,  ...,  2.7921e-04,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1887e-03,  0.0000e+00,  ...,  2.7921e-04,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1887e-03,  0.0000e+00,  ...,  2.7921e-04,
         -6.0055e-03, -1.4671e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-81010.7969, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.1730, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(93.5149, device='cuda:0')



h[100].sum tensor(78.1312, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-111.4688, device='cuda:0')



h[200].sum tensor(-464.6821, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.4732, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0011, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0011, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0011, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(255539.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1041, 0.0000, 0.0200],
        [0.0000, 0.0000, 0.0000,  ..., 0.0273, 0.0000, 0.0101],
        [0.0000, 0.0000, 0.0000,  ..., 0.1037, 0.0000, 0.0201],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2101, 0.0000, 0.0335],
        [0.0000, 0.0000, 0.0000,  ..., 0.2101, 0.0000, 0.0335],
        [0.0000, 0.0000, 0.0000,  ..., 0.2101, 0.0000, 0.0335]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2523557., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1560.5784, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5542.9312, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12132.6895, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(268.8552, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.2265],
        [-3.6661],
        [-4.3066],
        ...,
        [-9.2103],
        [-9.2008],
        [-9.1997]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-993453., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(631.9487, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(631.9487, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ...,  0.0002, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0002, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0002, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ...,  0.0002, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0002, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ...,  0.0002, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-82390., device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.9639, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(77.9531, device='cuda:0')



h[100].sum tensor(70.5016, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-92.9192, device='cuda:0')



h[200].sum tensor(-471.8168, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.4022, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(243627.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1952, 0.0000, 0.0304],
        [0.0000, 0.0000, 0.0000,  ..., 0.1563, 0.0000, 0.0258],
        [0.0000, 0.0000, 0.0000,  ..., 0.0940, 0.0000, 0.0183],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2097, 0.0000, 0.0331],
        [0.0000, 0.0000, 0.0000,  ..., 0.2097, 0.0000, 0.0331],
        [0.0000, 0.0000, 0.0000,  ..., 0.2097, 0.0000, 0.0331]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2436741.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1539.9827, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5545.1484, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10316.3867, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(199.8375, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.7235],
        [-2.4032],
        [-0.9454],
        ...,
        [-9.1820],
        [-9.1728],
        [-9.1717]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1019851.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2576],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(647.5200, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2576],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(647.5200, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0223e-02, -5.5337e-03, -2.0225e-05,  ..., -1.9077e-04,
         -6.2491e-03, -1.5616e-02],
        [-1.0220e-02, -5.5368e-03, -2.0129e-05,  ..., -1.8920e-04,
         -6.2480e-03, -1.5612e-02],
        [-9.6037e-03, -6.1883e-03,  0.0000e+00,  ...,  1.4105e-04,
         -6.0055e-03, -1.4671e-02],
        ...,
        [-9.6037e-03, -6.1883e-03,  0.0000e+00,  ...,  1.4105e-04,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1883e-03,  0.0000e+00,  ...,  1.4105e-04,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1883e-03,  0.0000e+00,  ...,  1.4105e-04,
         -6.0055e-03, -1.4671e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-82265.3281, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.0712, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(79.8738, device='cuda:0')



h[100].sum tensor(73.9583, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-95.2088, device='cuda:0')



h[200].sum tensor(-473.4988, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.0281, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(244289.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0245, 0.0000, 0.0045],
        [0.0000, 0.0000, 0.0000,  ..., 0.0755, 0.0000, 0.0120],
        [0.0000, 0.0000, 0.0000,  ..., 0.0917, 0.0000, 0.0141],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1848, 0.0000, 0.0296],
        [0.0000, 0.0000, 0.0000,  ..., 0.1433, 0.0000, 0.0245],
        [0.0000, 0.0000, 0.0000,  ..., 0.0550, 0.0000, 0.0103]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2455665.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1538.8678, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5651.6104, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11535.3906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(144.9103, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3609],
        [-2.4931],
        [-3.4272],
        ...,
        [-7.3654],
        [-5.6570],
        [-3.5273]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-890646.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(953.6681, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(953.6681, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.1881e-03,  0.0000e+00,  ...,  7.4360e-05,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1881e-03,  0.0000e+00,  ...,  7.4360e-05,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1881e-03,  0.0000e+00,  ...,  7.4360e-05,
         -6.0055e-03, -1.4671e-02],
        ...,
        [-9.6037e-03, -6.1881e-03,  0.0000e+00,  ...,  7.4360e-05,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1881e-03,  0.0000e+00,  ...,  7.4360e-05,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1881e-03,  0.0000e+00,  ...,  7.4360e-05,
         -6.0055e-03, -1.4671e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-79260.3125, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-987.8831, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(117.6383, device='cuda:0')



h[100].sum tensor(95.2105, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-140.2236, device='cuda:0')



h[200].sum tensor(-464.5037, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.3342, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(279645.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2071, 0.0000, 0.0322],
        [0.0000, 0.0000, 0.0000,  ..., 0.2071, 0.0000, 0.0322],
        [0.0000, 0.0000, 0.0000,  ..., 0.2026, 0.0000, 0.0317],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2079, 0.0000, 0.0320],
        [0.0000, 0.0000, 0.0000,  ..., 0.2079, 0.0000, 0.0320],
        [0.0000, 0.0000, 0.0000,  ..., 0.2079, 0.0000, 0.0320]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2712318., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1601.1646, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5636.3389, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(17185., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(290.6521, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-8.2506],
        [-7.9894],
        [-7.3660],
        ...,
        [-9.0656],
        [-9.0574],
        [-9.0567]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-906757.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(526.9036, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(526.9036, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.1881e-03,  0.0000e+00,  ...,  6.6974e-06,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1881e-03,  0.0000e+00,  ...,  6.6974e-06,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1881e-03,  0.0000e+00,  ...,  6.6974e-06,
         -6.0055e-03, -1.4671e-02],
        ...,
        [-9.6037e-03, -6.1881e-03,  0.0000e+00,  ...,  6.6974e-06,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1881e-03,  0.0000e+00,  ...,  6.6974e-06,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1881e-03,  0.0000e+00,  ...,  6.6974e-06,
         -6.0055e-03, -1.4671e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-83513.4375, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-983.9332, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(64.9954, device='cuda:0')



h[100].sum tensor(70.7260, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-77.4738, device='cuda:0')



h[200].sum tensor(-481.6266, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.1797, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.6790e-05, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.6790e-05, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.6790e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.6790e-05, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.6790e-05, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.6790e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(237080.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2031, 0.0000, 0.0308],
        [0.0000, 0.0000, 0.0000,  ..., 0.2062, 0.0000, 0.0314],
        [0.0000, 0.0000, 0.0000,  ..., 0.2059, 0.0000, 0.0315],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2070, 0.0000, 0.0312],
        [0.0000, 0.0000, 0.0000,  ..., 0.2070, 0.0000, 0.0312],
        [0.0000, 0.0000, 0.0000,  ..., 0.2070, 0.0000, 0.0312]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2408899.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1510.5764, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5643.0679, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9826.5664, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(90.7176, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.2778],
        [-8.0251],
        [-8.3675],
        ...,
        [-8.9978],
        [-8.9901],
        [-8.9896]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-848912., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3005],
        [0.3391],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(579.9313, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3005],
        [0.3391],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(579.9313, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1101e-02, -4.6045e-03, -4.4947e-05,  ..., -8.1955e-04,
         -6.5946e-03, -1.6956e-02],
        [-1.0323e-02, -5.4275e-03, -2.1588e-05,  ..., -3.8787e-04,
         -6.2884e-03, -1.5768e-02],
        [-1.0415e-02, -5.3299e-03, -2.4359e-05,  ..., -4.3908e-04,
         -6.3248e-03, -1.5909e-02],
        ...,
        [-9.6037e-03, -6.1880e-03,  0.0000e+00,  ...,  1.1069e-05,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1880e-03,  0.0000e+00,  ...,  1.1069e-05,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1880e-03,  0.0000e+00,  ...,  1.1069e-05,
         -6.0055e-03, -1.4671e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-83057.0156, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.4156, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(71.5365, device='cuda:0')



h[100].sum tensor(72.9466, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-85.2708, device='cuda:0')



h[200].sum tensor(-481.3647, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.3113, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.2138e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.4276e-05, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.4276e-05, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.4276e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(239285.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0023],
        [0.0000, 0.0000, 0.0000,  ..., 0.0678, 0.0000, 0.0123],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2079, 0.0000, 0.0313],
        [0.0000, 0.0000, 0.0000,  ..., 0.2079, 0.0000, 0.0313],
        [0.0000, 0.0000, 0.0000,  ..., 0.2079, 0.0000, 0.0313]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2418755.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1491.5168, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5689.1333, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10207.9912, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(72.1060, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.8855],
        [ 0.1961],
        [-1.4043],
        ...,
        [-9.0323],
        [-9.0246],
        [-9.0240]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-880410.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5474],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(754.3313, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5474],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(754.3313, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0913e-02, -4.8027e-03, -3.8222e-05,  ..., -6.7881e-04,
         -6.5208e-03, -1.6670e-02],
        [-9.6037e-03, -6.1880e-03,  0.0000e+00,  ...,  5.4404e-05,
         -6.0055e-03, -1.4671e-02],
        [-1.0913e-02, -4.8027e-03, -3.8222e-05,  ..., -6.7881e-04,
         -6.5208e-03, -1.6670e-02],
        ...,
        [-9.6037e-03, -6.1880e-03,  0.0000e+00,  ...,  5.4404e-05,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1880e-03,  0.0000e+00,  ...,  5.4404e-05,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1880e-03,  0.0000e+00,  ...,  5.4404e-05,
         -6.0055e-03, -1.4671e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-81379.0312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.0709, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(93.0494, device='cuda:0')



h[100].sum tensor(80.7832, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-110.9139, device='cuda:0')



h[200].sum tensor(-476.0894, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.3216, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(254027.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0575, 0.0000, 0.0116],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0017],
        [0.0000, 0.0000, 0.0000,  ..., 0.0787, 0.0000, 0.0147],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2099, 0.0000, 0.0319],
        [0.0000, 0.0000, 0.0000,  ..., 0.2099, 0.0000, 0.0319],
        [0.0000, 0.0000, 0.0000,  ..., 0.2099, 0.0000, 0.0319]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2511751., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1485.1279, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5742.7373, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12525.8730, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(101.7252, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.2128],
        [ 0.4262],
        [-1.2839],
        ...,
        [-9.1212],
        [-9.1154],
        [-9.1166]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-977905.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 380.0 event: 5700 loss: tensor(454.8805, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3098],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(856.0952, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3098],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(856.0952, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.1879e-03,  0.0000e+00,  ...,  1.0440e-05,
         -6.0055e-03, -1.4671e-02],
        [-1.0345e-02, -5.4037e-03, -2.1030e-05,  ..., -4.0795e-04,
         -6.2972e-03, -1.5802e-02],
        [-1.0157e-02, -5.6026e-03, -1.5696e-05,  ..., -3.0183e-04,
         -6.2232e-03, -1.5515e-02],
        ...,
        [-9.6037e-03, -6.1879e-03,  0.0000e+00,  ...,  1.0440e-05,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1879e-03,  0.0000e+00,  ...,  1.0440e-05,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1879e-03,  0.0000e+00,  ...,  1.0440e-05,
         -6.0055e-03, -1.4671e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-80323.6250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.9935, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(105.6023, device='cuda:0')



h[100].sum tensor(86.5673, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-125.8769, device='cuda:0')



h[200].sum tensor(-471.5610, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.4121, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.1321e-05, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.0881e-05, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.1761e-05, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.1761e-05, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.1761e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(264551.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1069, 0.0000, 0.0177],
        [0.0000, 0.0000, 0.0000,  ..., 0.0527, 0.0000, 0.0085],
        [0.0000, 0.0000, 0.0000,  ..., 0.0024, 0.0000, 0.0005],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2085, 0.0000, 0.0305],
        [0.0000, 0.0000, 0.0000,  ..., 0.2085, 0.0000, 0.0305],
        [0.0000, 0.0000, 0.0000,  ..., 0.2085, 0.0000, 0.0305]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2571545.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1518.2739, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5719.6401, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13544.6895, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(189.7626, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9322],
        [ 0.0489],
        [ 1.2696],
        ...,
        [-9.1046],
        [-9.0965],
        [-9.0959]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-939948.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3162],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(930.6744, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3162],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(930.6744, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.1878e-03,  0.0000e+00,  ..., -4.0031e-05,
         -6.0055e-03, -1.4671e-02],
        [-1.0865e-02, -4.8530e-03, -3.4788e-05,  ..., -7.5727e-04,
         -6.5019e-03, -1.6596e-02],
        [-1.0109e-02, -5.6534e-03, -1.3928e-05,  ..., -3.2719e-04,
         -6.2042e-03, -1.5442e-02],
        ...,
        [-9.6037e-03, -6.1878e-03,  0.0000e+00,  ..., -4.0031e-05,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1878e-03,  0.0000e+00,  ..., -4.0031e-05,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1878e-03,  0.0000e+00,  ..., -4.0031e-05,
         -6.0055e-03, -1.4671e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-79472.0078, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-987.7021, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(114.8019, device='cuda:0')



h[100].sum tensor(91.8406, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-136.8427, device='cuda:0')



h[200].sum tensor(-467.6252, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.4100, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(278208.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0762, 0.0000, 0.0111],
        [0.0000, 0.0000, 0.0000,  ..., 0.0381, 0.0000, 0.0045],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2071, 0.0000, 0.0289],
        [0.0000, 0.0000, 0.0000,  ..., 0.2071, 0.0000, 0.0289],
        [0.0000, 0.0000, 0.0000,  ..., 0.2071, 0.0000, 0.0289]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2726754.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1551.3496, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5835.8584, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(17637.8359, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(231.1169, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.3147],
        [-0.4612],
        [ 0.6712],
        ...,
        [-9.0737],
        [-9.0660],
        [-9.0655]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-815336.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(773.3264, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(773.3264, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.1877e-03,  0.0000e+00,  ..., -8.5529e-05,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1877e-03,  0.0000e+00,  ..., -8.5529e-05,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1877e-03,  0.0000e+00,  ..., -8.5529e-05,
         -6.0055e-03, -1.4671e-02],
        ...,
        [-9.6037e-03, -6.1877e-03,  0.0000e+00,  ..., -8.5529e-05,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1877e-03,  0.0000e+00,  ..., -8.5529e-05,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1877e-03,  0.0000e+00,  ..., -8.5529e-05,
         -6.0055e-03, -1.4671e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-81179.9219, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.2581, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(95.3925, device='cuda:0')



h[100].sum tensor(78.1456, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-113.7069, device='cuda:0')



h[200].sum tensor(-473.1205, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.0851, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(257528.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2065, 0.0000, 0.0287],
        [0.0000, 0.0000, 0.0000,  ..., 0.2010, 0.0000, 0.0276],
        [0.0000, 0.0000, 0.0000,  ..., 0.1837, 0.0000, 0.0245],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2077, 0.0000, 0.0287],
        [0.0000, 0.0000, 0.0000,  ..., 0.2077, 0.0000, 0.0287],
        [0.0000, 0.0000, 0.0000,  ..., 0.2077, 0.0000, 0.0287]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2533937.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1516.6384, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5817.0488, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13954.2070, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(190.1836, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.7648],
        [-6.8881],
        [-5.4823],
        ...,
        [-9.1785],
        [-9.1704],
        [-9.1698]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-955890.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6206],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(697.3671, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6206],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(697.3671, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3113e-02, -2.4728e-03, -9.1415e-05,  ..., -2.1466e-03,
         -7.3864e-03, -2.0026e-02],
        [-1.1712e-02, -3.9555e-03, -5.4927e-05,  ..., -1.3403e-03,
         -6.8352e-03, -1.7888e-02],
        [-1.1193e-02, -4.5046e-03, -4.1416e-05,  ..., -1.0418e-03,
         -6.6311e-03, -1.7097e-02],
        ...,
        [-9.6037e-03, -6.1876e-03,  0.0000e+00,  ..., -1.2655e-04,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1876e-03,  0.0000e+00,  ..., -1.2655e-04,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1876e-03,  0.0000e+00,  ..., -1.2655e-04,
         -6.0055e-03, -1.4671e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-82127.6250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.5101, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(86.0227, device='cuda:0')



h[100].sum tensor(72.3958, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-102.5381, device='cuda:0')



h[200].sum tensor(-476.8110, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.0318, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(250668.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2078, 0.0000, 0.0282],
        [0.0000, 0.0000, 0.0000,  ..., 0.2078, 0.0000, 0.0282],
        [0.0000, 0.0000, 0.0000,  ..., 0.2078, 0.0000, 0.0282]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2499861., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1504.9211, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5834.9883, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13444.4453, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(191.7377, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4774],
        [ 0.5933],
        [ 0.6896],
        ...,
        [-9.2264],
        [-9.2182],
        [-9.2175]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-931662.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(664.9961, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(664.9961, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0002, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0002, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0002, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0002, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0002, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0002, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-82570.2656, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.2441, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(82.0296, device='cuda:0')



h[100].sum tensor(70.8299, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-97.7784, device='cuda:0')



h[200].sum tensor(-478.9350, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.7306, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(251647.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1816, 0.0000, 0.0247],
        [0.0000, 0.0000, 0.0000,  ..., 0.2036, 0.0000, 0.0274],
        [0.0000, 0.0000, 0.0000,  ..., 0.2067, 0.0000, 0.0280],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2079, 0.0000, 0.0277],
        [0.0000, 0.0000, 0.0000,  ..., 0.2079, 0.0000, 0.0277],
        [0.0000, 0.0000, 0.0000,  ..., 0.2079, 0.0000, 0.0277]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2508310., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1501.7716, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5804.0913, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13170.9053, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(222.9177, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.8009],
        [-7.3237],
        [-8.1415],
        ...,
        [-9.2441],
        [-9.2358],
        [-9.2351]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-949262.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(792.7361, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(792.7361, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0002, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0002, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0002, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0002, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0002, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0002, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-81356.3047, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.4608, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(97.7868, device='cuda:0')



h[100].sum tensor(79.9661, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-116.5608, device='cuda:0')



h[200].sum tensor(-475.0685, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.8653, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(264641.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2071, 0.0000, 0.0275],
        [0.0000, 0.0000, 0.0000,  ..., 0.2071, 0.0000, 0.0275],
        [0.0000, 0.0000, 0.0000,  ..., 0.2068, 0.0000, 0.0276],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2079, 0.0000, 0.0273],
        [0.0000, 0.0000, 0.0000,  ..., 0.2079, 0.0000, 0.0273],
        [0.0000, 0.0000, 0.0000,  ..., 0.2079, 0.0000, 0.0273]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2601411., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1515.9523, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5838.1382, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15417.7969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(271.4352, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-8.4741],
        [-8.6535],
        [-8.6770],
        ...,
        [-9.2568],
        [-9.2486],
        [-9.2479]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-883142.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3342],
        [0.3333],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(731.8540, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3342],
        [0.3333],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(731.8540, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0403e-02, -5.3400e-03, -1.9112e-05,  ..., -6.9374e-04,
         -6.3202e-03, -1.5891e-02],
        [-1.0401e-02, -5.3425e-03, -1.9056e-05,  ..., -6.9237e-04,
         -6.3192e-03, -1.5887e-02],
        [-1.1639e-02, -4.0307e-03, -4.8653e-05,  ..., -1.4153e-03,
         -6.8065e-03, -1.7776e-02],
        ...,
        [-9.6037e-03, -6.1871e-03,  0.0000e+00,  ..., -2.2690e-04,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1871e-03,  0.0000e+00,  ..., -2.2690e-04,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1871e-03,  0.0000e+00,  ..., -2.2690e-04,
         -6.0055e-03, -1.4671e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-82095.3516, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.8763, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(90.2767, device='cuda:0')



h[100].sum tensor(74.2044, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-107.6089, device='cuda:0')



h[200].sum tensor(-478.3606, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.4180, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(253337.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0507, 0.0000, 0.0053],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2088, 0.0000, 0.0273],
        [0.0000, 0.0000, 0.0000,  ..., 0.2088, 0.0000, 0.0273],
        [0.0000, 0.0000, 0.0000,  ..., 0.2088, 0.0000, 0.0273]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2514185.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1495.3518, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5850.1621, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13913.1016, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(221.8890, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.0480],
        [ 1.6221],
        [ 1.8351],
        ...,
        [-9.2913],
        [-9.2965],
        [-9.2979]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-940067.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2820],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(552.2292, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2820],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(552.2292, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0278e-02, -5.4724e-03, -1.6124e-05,  ..., -6.2076e-04,
         -6.2710e-03, -1.5700e-02],
        [-9.6037e-03, -6.1871e-03,  0.0000e+00,  ..., -2.2690e-04,
         -6.0055e-03, -1.4671e-02],
        [-1.0278e-02, -5.4724e-03, -1.6124e-05,  ..., -6.2076e-04,
         -6.2710e-03, -1.5700e-02],
        ...,
        [-9.6037e-03, -6.1871e-03,  0.0000e+00,  ..., -2.2690e-04,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1871e-03,  0.0000e+00,  ..., -2.2690e-04,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1871e-03,  0.0000e+00,  ..., -2.2690e-04,
         -6.0055e-03, -1.4671e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-83941.1875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.1675, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(68.1194, device='cuda:0')



h[100].sum tensor(62.4294, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-81.1976, device='cuda:0')



h[200].sum tensor(-485.1512, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.1977, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(237782.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0473, 0.0000, 0.0067],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2088, 0.0000, 0.0273],
        [0.0000, 0.0000, 0.0000,  ..., 0.2088, 0.0000, 0.0273],
        [0.0000, 0.0000, 0.0000,  ..., 0.2088, 0.0000, 0.0273]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2406861., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1471.4811, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5776.7051, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11047.3438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(185.4737, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.3320],
        [ 1.1915],
        [ 0.4812],
        ...,
        [-9.3213],
        [-9.3129],
        [-9.3121]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1010590.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(761.7009, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(761.7009, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0003, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0003, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0003, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0003, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0003, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0003, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-81777.2266, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.1660, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(93.9585, device='cuda:0')



h[100].sum tensor(77.9446, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-111.9975, device='cuda:0')



h[200].sum tensor(-477.9182, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.6178, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(260162.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2077, 0.0000, 0.0272],
        [0.0000, 0.0000, 0.0000,  ..., 0.2077, 0.0000, 0.0272],
        [0.0000, 0.0000, 0.0000,  ..., 0.2073, 0.0000, 0.0273],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2085, 0.0000, 0.0270],
        [0.0000, 0.0000, 0.0000,  ..., 0.2085, 0.0000, 0.0270],
        [0.0000, 0.0000, 0.0000,  ..., 0.2085, 0.0000, 0.0270]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2581495.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1499.4451, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5831.2515, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15257.4785, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(252.8483, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-8.7868],
        [-8.8748],
        [-8.9224],
        ...,
        [-9.2807],
        [-9.2832],
        [-9.2834]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-928630., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2488],
        [0.2424],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(846.4626, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2488],
        [0.2424],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(846.4626, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1063e-02, -4.6394e-03, -3.2930e-05,  ..., -1.1367e-03,
         -6.5799e-03, -1.6897e-02],
        [-1.0199e-02, -5.5558e-03, -1.3428e-05,  ..., -6.2839e-04,
         -6.2397e-03, -1.5579e-02],
        [-1.0184e-02, -5.5718e-03, -1.3085e-05,  ..., -6.1946e-04,
         -6.2337e-03, -1.5555e-02],
        ...,
        [-9.6037e-03, -6.1867e-03,  0.0000e+00,  ..., -2.7840e-04,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1867e-03,  0.0000e+00,  ..., -2.7840e-04,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1867e-03,  0.0000e+00,  ..., -2.7840e-04,
         -6.0055e-03, -1.4671e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-80925.0156, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.9155, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(104.4141, device='cuda:0')



h[100].sum tensor(86.4739, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-124.4605, device='cuda:0')



h[200].sum tensor(-475.6402, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.0249, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(268137.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0240, 0.0000, 0.0030],
        [0.0000, 0.0000, 0.0000,  ..., 0.0882, 0.0000, 0.0113],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2076, 0.0000, 0.0264],
        [0.0000, 0.0000, 0.0000,  ..., 0.2076, 0.0000, 0.0264],
        [0.0000, 0.0000, 0.0000,  ..., 0.2076, 0.0000, 0.0264]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2645625., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1495.4037, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5894.4521, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(16580.8398, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(233.5801, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.4510],
        [ 0.1729],
        [-1.8696],
        ...,
        [-9.2416],
        [-9.2337],
        [-9.2332]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-797377.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 390.0 event: 5850 loss: tensor(407.8298, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(616.9645, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(616.9645, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.1869e-03,  0.0000e+00,  ..., -3.0042e-04,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1869e-03,  0.0000e+00,  ..., -3.0042e-04,
         -6.0055e-03, -1.4671e-02],
        [-1.0327e-02, -5.4199e-03, -1.5860e-05,  ..., -7.2726e-04,
         -6.2903e-03, -1.5775e-02],
        ...,
        [-9.6037e-03, -6.1869e-03,  0.0000e+00,  ..., -3.0042e-04,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1869e-03,  0.0000e+00,  ..., -3.0042e-04,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1869e-03,  0.0000e+00,  ..., -3.0042e-04,
         -6.0055e-03, -1.4671e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-83334.6484, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.7700, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(76.1047, device='cuda:0')



h[100].sum tensor(72.0344, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-90.7160, device='cuda:0')



h[200].sum tensor(-486.6664, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.7999, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(247237.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1783, 0.0000, 0.0224],
        [0.0000, 0.0000, 0.0000,  ..., 0.1054, 0.0000, 0.0143],
        [0.0000, 0.0000, 0.0000,  ..., 0.0554, 0.0000, 0.0073],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2094, 0.0000, 0.0268],
        [0.0000, 0.0000, 0.0000,  ..., 0.2094, 0.0000, 0.0268],
        [0.0000, 0.0000, 0.0000,  ..., 0.2094, 0.0000, 0.0268]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2491412.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1450.6947, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5776.8198, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11788.5596, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(184.1180, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.5177],
        [-2.2314],
        [-0.2689],
        ...,
        [-9.2940],
        [-9.2860],
        [-9.2854]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-903676.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(628.9801, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(628.9801, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0003, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0003, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0003, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0003, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0003, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0003, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-83255.8438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.9033, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(77.5869, device='cuda:0')



h[100].sum tensor(70.9792, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-92.4827, device='cuda:0')



h[200].sum tensor(-487.0972, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.2829, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(246114.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1842, 0.0000, 0.0239],
        [0.0000, 0.0000, 0.0000,  ..., 0.2061, 0.0000, 0.0268],
        [0.0000, 0.0000, 0.0000,  ..., 0.2099, 0.0000, 0.0277],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2111, 0.0000, 0.0274],
        [0.0000, 0.0000, 0.0000,  ..., 0.2111, 0.0000, 0.0274],
        [0.0000, 0.0000, 0.0000,  ..., 0.2111, 0.0000, 0.0274]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2481690.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1445.0923, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5795.1094, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12206.2988, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(169.1370, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.9617],
        [-6.6542],
        [-7.6976],
        ...,
        [-9.3857],
        [-9.3773],
        [-9.3766]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-950496.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2466],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(626.4885, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2466],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(626.4885, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0736e-02, -4.9878e-03, -2.3410e-05,  ..., -1.0094e-03,
         -6.4510e-03, -1.6397e-02],
        [-9.6037e-03, -6.1872e-03,  0.0000e+00,  ..., -3.3814e-04,
         -6.0055e-03, -1.4671e-02],
        [-1.0194e-02, -5.5622e-03, -1.2198e-05,  ..., -6.8791e-04,
         -6.2376e-03, -1.5570e-02],
        ...,
        [-9.6037e-03, -6.1872e-03,  0.0000e+00,  ..., -3.3814e-04,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1872e-03,  0.0000e+00,  ..., -3.3814e-04,
         -6.0055e-03, -1.4671e-02],
        [-9.6037e-03, -6.1872e-03,  0.0000e+00,  ..., -3.3814e-04,
         -6.0055e-03, -1.4671e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-83039.3281, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.9119, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(77.2795, device='cuda:0')



h[100].sum tensor(71.9159, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-92.1164, device='cuda:0')



h[200].sum tensor(-486.9490, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.1827, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(251089.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0728, 0.0000, 0.0099],
        [0.0000, 0.0000, 0.0000,  ..., 0.0453, 0.0000, 0.0071],
        [0.0000, 0.0000, 0.0000,  ..., 0.1340, 0.0000, 0.0175],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2110, 0.0000, 0.0270],
        [0.0000, 0.0000, 0.0000,  ..., 0.2110, 0.0000, 0.0270],
        [0.0000, 0.0000, 0.0000,  ..., 0.2110, 0.0000, 0.0270]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2505299.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1227.9784, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5803.4927, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12281.0645, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(365.0625, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.5522],
        [-3.1610],
        [-4.3832],
        ...,
        [-9.3781],
        [-9.3699],
        [-9.3693]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-983802., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(707.3094, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(707.3094, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-82288.5938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.6547, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(87.2491, device='cuda:0')



h[100].sum tensor(81.0634, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-104.0000, device='cuda:0')



h[200].sum tensor(-483.1422, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.4314, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(256686.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2085, 0.0000, 0.0258],
        [0.0000, 0.0000, 0.0000,  ..., 0.2078, 0.0000, 0.0255],
        [0.0000, 0.0000, 0.0000,  ..., 0.2054, 0.0000, 0.0249],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2093, 0.0000, 0.0256],
        [0.0000, 0.0000, 0.0000,  ..., 0.2093, 0.0000, 0.0256],
        [0.0000, 0.0000, 0.0000,  ..., 0.2093, 0.0000, 0.0256]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2557545.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1437.0625, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5753.5801, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13078.3906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(181.6213, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-8.2658],
        [-7.8724],
        [-7.2496],
        ...,
        [-9.2698],
        [-9.2635],
        [-9.2640]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-802324.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(613.9088, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(613.9088, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-83512.5625, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.7419, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(75.7278, device='cuda:0')



h[100].sum tensor(74.3257, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-90.2667, device='cuda:0')



h[200].sum tensor(-486.8793, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.6771, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(247409.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2095, 0.0000, 0.0260],
        [0.0000, 0.0000, 0.0000,  ..., 0.2095, 0.0000, 0.0260],
        [0.0000, 0.0000, 0.0000,  ..., 0.2091, 0.0000, 0.0261],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2103, 0.0000, 0.0258],
        [0.0000, 0.0000, 0.0000,  ..., 0.2103, 0.0000, 0.0258],
        [0.0000, 0.0000, 0.0000,  ..., 0.2103, 0.0000, 0.0258]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2482296., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1416.7184, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5674.1289, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11097.9824, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(158.2839, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-8.4907],
        [-8.5607],
        [-8.5523],
        ...,
        [-9.3243],
        [-9.3165],
        [-9.3161]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-872405., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(703.2878, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(703.2878, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-82884.3906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.5677, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(86.7530, device='cuda:0')



h[100].sum tensor(78.1445, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-103.4087, device='cuda:0')



h[200].sum tensor(-483.6211, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.2698, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(254496.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0443, 0.0000, 0.0074],
        [0.0000, 0.0000, 0.0000,  ..., 0.0971, 0.0000, 0.0156],
        [0.0000, 0.0000, 0.0000,  ..., 0.1641, 0.0000, 0.0206],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2120, 0.0000, 0.0263],
        [0.0000, 0.0000, 0.0000,  ..., 0.2120, 0.0000, 0.0263],
        [0.0000, 0.0000, 0.0000,  ..., 0.2120, 0.0000, 0.0263]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2531756., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1415.9598, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5643.9229, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12152.4570, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(188.8661, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.0296],
        [ 0.1470],
        [-0.6589],
        ...,
        [-9.3445],
        [-9.3377],
        [-9.3434]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-959264.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(879.1224, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(879.1224, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.1879e-03,  0.0000e+00,  ..., -3.9372e-04,
         -6.0055e-03, -1.4670e-02],
        [-9.6037e-03, -6.1879e-03,  0.0000e+00,  ..., -3.9372e-04,
         -6.0055e-03, -1.4670e-02],
        [-1.0557e-02, -5.1789e-03, -1.7530e-05,  ..., -9.6320e-04,
         -6.3806e-03, -1.6124e-02],
        ...,
        [-9.6037e-03, -6.1879e-03,  0.0000e+00,  ..., -3.9372e-04,
         -6.0055e-03, -1.4670e-02],
        [-9.6037e-03, -6.1879e-03,  0.0000e+00,  ..., -3.9372e-04,
         -6.0055e-03, -1.4670e-02],
        [-9.6037e-03, -6.1879e-03,  0.0000e+00,  ..., -3.9372e-04,
         -6.0055e-03, -1.4670e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-81371.3984, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-987.1962, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(108.4428, device='cuda:0')



h[100].sum tensor(88.1128, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-129.2627, device='cuda:0')



h[200].sum tensor(-477.1221, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.3377, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(268166.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1830, 0.0000, 0.0231],
        [0.0000, 0.0000, 0.0000,  ..., 0.1060, 0.0000, 0.0157],
        [0.0000, 0.0000, 0.0000,  ..., 0.0548, 0.0000, 0.0084],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2129, 0.0000, 0.0257],
        [0.0000, 0.0000, 0.0000,  ..., 0.2129, 0.0000, 0.0257],
        [0.0000, 0.0000, 0.0000,  ..., 0.2129, 0.0000, 0.0257]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2618811.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1411.2139, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5686.8740, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13516.0156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(227.8366, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.3099],
        [-3.9826],
        [-1.5254],
        ...,
        [-9.4377],
        [-9.4294],
        [-9.4289]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-864421.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(603.9034, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(603.9034, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-84406., device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.6414, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(74.4936, device='cuda:0')



h[100].sum tensor(67.1390, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-88.7956, device='cuda:0')



h[200].sum tensor(-487.9041, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.2749, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(244610.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1970, 0.0000, 0.0221],
        [0.0000, 0.0000, 0.0000,  ..., 0.2075, 0.0000, 0.0242],
        [0.0000, 0.0000, 0.0000,  ..., 0.2121, 0.0000, 0.0258],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2148, 0.0000, 0.0261],
        [0.0000, 0.0000, 0.0000,  ..., 0.2148, 0.0000, 0.0261],
        [0.0000, 0.0000, 0.0000,  ..., 0.2148, 0.0000, 0.0261]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2463017.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1353.5947, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5612.0776, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9294.9824, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(166.7988, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.6696],
        [-6.2434],
        [-6.5513],
        ...,
        [-9.5399],
        [-9.5313],
        [-9.5307]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-952150.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6294],
        [0.6362],
        [0.5654],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(681.1509, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6294],
        [0.6362],
        [0.5654],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(681.1509, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1126e-02, -4.5781e-03, -2.6386e-05,  ..., -1.3258e-03,
         -6.6045e-03, -1.6991e-02],
        [-1.3432e-02, -2.1383e-03, -6.6368e-05,  ..., -2.7076e-03,
         -7.5121e-03, -2.0509e-02],
        [-1.3264e-02, -2.3160e-03, -6.3456e-05,  ..., -2.6069e-03,
         -7.4460e-03, -2.0252e-02],
        ...,
        [-9.6037e-03, -6.1883e-03,  0.0000e+00,  ..., -4.1396e-04,
         -6.0055e-03, -1.4670e-02],
        [-9.6037e-03, -6.1883e-03,  0.0000e+00,  ..., -4.1396e-04,
         -6.0055e-03, -1.4670e-02],
        [-9.6037e-03, -6.1883e-03,  0.0000e+00,  ..., -4.1396e-04,
         -6.0055e-03, -1.4670e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-83753.3984, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.3799, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(84.0223, device='cuda:0')



h[100].sum tensor(69.5028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-100.1537, device='cuda:0')



h[200].sum tensor(-485.8882, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.3800, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(249901.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2163, 0.0000, 0.0269],
        [0.0000, 0.0000, 0.0000,  ..., 0.2163, 0.0000, 0.0269],
        [0.0000, 0.0000, 0.0000,  ..., 0.2163, 0.0000, 0.0269]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2493455.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1342.6455, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5655.2900, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10154.7031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(208.3865, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2106],
        [ 0.2228],
        [ 0.3732],
        ...,
        [-9.6557],
        [-9.6473],
        [-9.6467]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1049488.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(628.4335, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(628.4335, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-84237.9531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.8606, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(77.5195, device='cuda:0')



h[100].sum tensor(67.6369, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-92.4024, device='cuda:0')



h[200].sum tensor(-487.9870, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.2609, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(249403.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2069, 0.0000, 0.0242],
        [0.0000, 0.0000, 0.0000,  ..., 0.2049, 0.0000, 0.0236],
        [0.0000, 0.0000, 0.0000,  ..., 0.2007, 0.0000, 0.0226],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2152, 0.0000, 0.0259],
        [0.0000, 0.0000, 0.0000,  ..., 0.2152, 0.0000, 0.0259],
        [0.0000, 0.0000, 0.0000,  ..., 0.2152, 0.0000, 0.0259]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2512393.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1338.0233, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5709.1826, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10563.3252, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(232.5921, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.5663],
        [-6.5665],
        [-6.5528],
        ...,
        [-9.6336],
        [-9.6257],
        [-9.6253]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-946393.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 400.0 event: 6000 loss: tensor(441.7317, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(541.7950, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(541.7950, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-85038.6875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.1006, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(66.8323, device='cuda:0')



h[100].sum tensor(64.3347, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-79.6634, device='cuda:0')



h[200].sum tensor(-492.3928, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.7783, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(243721.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0833, 0.0000, 0.0125],
        [0.0000, 0.0000, 0.0000,  ..., 0.1574, 0.0000, 0.0207],
        [0.0000, 0.0000, 0.0000,  ..., 0.2004, 0.0000, 0.0236],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2145, 0.0000, 0.0252],
        [0.0000, 0.0000, 0.0000,  ..., 0.2145, 0.0000, 0.0252],
        [0.0000, 0.0000, 0.0000,  ..., 0.2145, 0.0000, 0.0252]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2476736., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1327.1709, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5762.5381, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9670.0723, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(230.0386, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1506],
        [-2.6157],
        [-3.9909],
        ...,
        [-9.6138],
        [-9.6065],
        [-9.6065]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-884655.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(664.5807, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(664.5807, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.1914e-03,  0.0000e+00,  ..., -4.3744e-04,
         -6.0055e-03, -1.4670e-02],
        [-9.6037e-03, -6.1914e-03,  0.0000e+00,  ..., -4.3744e-04,
         -6.0055e-03, -1.4670e-02],
        [-1.0889e-02, -4.8376e-03, -2.0381e-05,  ..., -1.2099e-03,
         -6.5113e-03, -1.6630e-02],
        ...,
        [-9.6037e-03, -6.1914e-03,  0.0000e+00,  ..., -4.3744e-04,
         -6.0055e-03, -1.4670e-02],
        [-9.6037e-03, -6.1914e-03,  0.0000e+00,  ..., -4.3744e-04,
         -6.0055e-03, -1.4670e-02],
        [-9.6037e-03, -6.1914e-03,  0.0000e+00,  ..., -4.3744e-04,
         -6.0055e-03, -1.4670e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-83860.2969, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.2260, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(81.9783, device='cuda:0')



h[100].sum tensor(73.1860, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-97.7173, device='cuda:0')



h[200].sum tensor(-489.3189, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.7139, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(253412.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1574, 0.0000, 0.0190],
        [0.0000, 0.0000, 0.0000,  ..., 0.0684, 0.0000, 0.0092],
        [0.0000, 0.0000, 0.0000,  ..., 0.0286, 0.0000, 0.0035],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2141, 0.0000, 0.0246],
        [0.0000, 0.0000, 0.0000,  ..., 0.2141, 0.0000, 0.0246],
        [0.0000, 0.0000, 0.0000,  ..., 0.2141, 0.0000, 0.0246]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2542784.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1349.2002, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5855.1958, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11392.8467, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(267.4579, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.1515],
        [-1.5874],
        [ 0.2895],
        ...,
        [-9.6123],
        [-9.6048],
        [-9.6045]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-789144.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(746.7725, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(746.7725, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-83202.4766, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.9939, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(92.1170, device='cuda:0')



h[100].sum tensor(77.0545, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-109.8025, device='cuda:0')



h[200].sum tensor(-487.5989, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.0177, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(267519.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2139, 0.0000, 0.0247],
        [0.0000, 0.0000, 0.0000,  ..., 0.2120, 0.0000, 0.0241],
        [0.0000, 0.0000, 0.0000,  ..., 0.2018, 0.0000, 0.0228],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2152, 0.0000, 0.0247],
        [0.0000, 0.0000, 0.0000,  ..., 0.2152, 0.0000, 0.0247],
        [0.0000, 0.0000, 0.0000,  ..., 0.2152, 0.0000, 0.0247]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2660280.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1390.7100, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5827.5591, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13577.5352, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(354.8292, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-8.1906],
        [-7.4968],
        [-6.3096],
        ...,
        [-9.6727],
        [-9.6651],
        [-9.6647]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-846836.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(755.4601, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(755.4601, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0004, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-83407.2656, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.0084, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(93.1886, device='cuda:0')



h[100].sum tensor(75.3539, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-111.0799, device='cuda:0')



h[200].sum tensor(-487.7582, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.3669, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(262250.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1998, 0.0000, 0.0223],
        [0.0000, 0.0000, 0.0000,  ..., 0.1741, 0.0000, 0.0194],
        [0.0000, 0.0000, 0.0000,  ..., 0.1408, 0.0000, 0.0155],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2165, 0.0000, 0.0248],
        [0.0000, 0.0000, 0.0000,  ..., 0.2165, 0.0000, 0.0248],
        [0.0000, 0.0000, 0.0000,  ..., 0.2165, 0.0000, 0.0248]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2605423.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1396.3009, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5837.8662, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12439.8545, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(320.5503, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.3926],
        [-2.8583],
        [-1.1866],
        ...,
        [-9.7328],
        [-9.7253],
        [-9.7251]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-831854.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(664.2594, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(664.2594, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.1939e-03,  0.0000e+00,  ..., -4.5464e-04,
         -6.0055e-03, -1.4670e-02],
        [-9.6037e-03, -6.1939e-03,  0.0000e+00,  ..., -4.5464e-04,
         -6.0055e-03, -1.4670e-02],
        [-1.0159e-02, -5.6107e-03, -8.0529e-06,  ..., -7.8933e-04,
         -6.2241e-03, -1.5517e-02],
        ...,
        [-9.6037e-03, -6.1939e-03,  0.0000e+00,  ..., -4.5464e-04,
         -6.0055e-03, -1.4670e-02],
        [-9.6037e-03, -6.1939e-03,  0.0000e+00,  ..., -4.5464e-04,
         -6.0055e-03, -1.4670e-02],
        [-9.6037e-03, -6.1939e-03,  0.0000e+00,  ..., -4.5464e-04,
         -6.0055e-03, -1.4670e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-84491.0781, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.2395, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(81.9387, device='cuda:0')



h[100].sum tensor(67.1534, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-97.6701, device='cuda:0')



h[200].sum tensor(-490.5025, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.7010, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(252037.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1662, 0.0000, 0.0189],
        [0.0000, 0.0000, 0.0000,  ..., 0.0905, 0.0000, 0.0097],
        [0.0000, 0.0000, 0.0000,  ..., 0.0516, 0.0000, 0.0046],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2181, 0.0000, 0.0251],
        [0.0000, 0.0000, 0.0000,  ..., 0.2181, 0.0000, 0.0251],
        [0.0000, 0.0000, 0.0000,  ..., 0.2181, 0.0000, 0.0251]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2520210.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1400.6453, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5783.6602, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10422.3281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(272.7708, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.4558],
        [-2.3695],
        [-1.0579],
        ...,
        [-9.7872],
        [-9.6738],
        [-9.2961]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-866729., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(609.7260, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(609.7260, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-85286.5234, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.7249, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(75.2118, device='cuda:0')



h[100].sum tensor(59.5798, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-89.6517, device='cuda:0')



h[200].sum tensor(-492.5992, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.5089, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(247188.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2017, 0.0000, 0.0230],
        [0.0000, 0.0000, 0.0000,  ..., 0.1557, 0.0000, 0.0185],
        [0.0000, 0.0000, 0.0000,  ..., 0.0674, 0.0000, 0.0087],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2202, 0.0000, 0.0259],
        [0.0000, 0.0000, 0.0000,  ..., 0.2202, 0.0000, 0.0259],
        [0.0000, 0.0000, 0.0000,  ..., 0.2202, 0.0000, 0.0259]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2488695.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1408.9131, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5714.4170, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9615.1523, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(244.8898, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.6212],
        [-3.4646],
        [-1.0956],
        ...,
        [-9.9256],
        [-9.9171],
        [-9.9165]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-980467.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(650.0696, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(650.0696, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-85089.1250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.0315, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(80.1883, device='cuda:0')



h[100].sum tensor(60.8806, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-95.5837, device='cuda:0')



h[200].sum tensor(-490.9295, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.1306, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(247591.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1864, 0.0000, 0.0220],
        [0.0000, 0.0000, 0.0000,  ..., 0.1808, 0.0000, 0.0217],
        [0.0000, 0.0000, 0.0000,  ..., 0.1749, 0.0000, 0.0214],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2205, 0.0000, 0.0252],
        [0.0000, 0.0000, 0.0000,  ..., 0.2205, 0.0000, 0.0252],
        [0.0000, 0.0000, 0.0000,  ..., 0.2205, 0.0000, 0.0252]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2479396., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1408.5330, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5694.1631, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8916.2637, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(214.5240, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.7346],
        [-3.8108],
        [-2.8204],
        ...,
        [-9.9112],
        [-9.9026],
        [-9.9019]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-925196.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3999],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(615.9929, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3999],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(615.9929, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.1969e-03,  0.0000e+00,  ..., -4.6724e-04,
         -6.0055e-03, -1.4670e-02],
        [-1.1841e-02, -3.8577e-03, -2.9627e-05,  ..., -1.8175e-03,
         -6.8861e-03, -1.8081e-02],
        [-1.0885e-02, -4.8578e-03, -1.6960e-05,  ..., -1.2402e-03,
         -6.5096e-03, -1.6623e-02],
        ...,
        [-9.6037e-03, -6.1969e-03,  0.0000e+00,  ..., -4.6724e-04,
         -6.0055e-03, -1.4670e-02],
        [-9.6037e-03, -6.1969e-03,  0.0000e+00,  ..., -4.6724e-04,
         -6.0055e-03, -1.4670e-02],
        [-9.6037e-03, -6.1969e-03,  0.0000e+00,  ..., -4.6724e-04,
         -6.0055e-03, -1.4670e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-85358.8906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.7778, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(75.9849, device='cuda:0')



h[100].sum tensor(57.9247, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-90.5732, device='cuda:0')



h[200].sum tensor(-490.8390, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.7608, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(246133.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0324, 0.0000, 0.0053],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2205, 0.0000, 0.0251],
        [0.0000, 0.0000, 0.0000,  ..., 0.2205, 0.0000, 0.0251],
        [0.0000, 0.0000, 0.0000,  ..., 0.2205, 0.0000, 0.0251]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2457465.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1416.6239, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5643.5698, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8225.6309, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(193.7513, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0138],
        [ 0.2772],
        [ 0.2025],
        ...,
        [-9.9186],
        [-9.9100],
        [-9.9093]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-972988.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(702.2404, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(702.2404, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1197e-02, -4.5334e-03, -2.0472e-05,  ..., -1.4328e-03,
         -6.6327e-03, -1.7099e-02],
        [-9.6037e-03, -6.1977e-03,  0.0000e+00,  ..., -4.7064e-04,
         -6.0055e-03, -1.4670e-02],
        [-9.6037e-03, -6.1977e-03,  0.0000e+00,  ..., -4.7064e-04,
         -6.0055e-03, -1.4670e-02],
        ...,
        [-9.6037e-03, -6.1977e-03,  0.0000e+00,  ..., -4.7064e-04,
         -6.0055e-03, -1.4670e-02],
        [-9.6037e-03, -6.1977e-03,  0.0000e+00,  ..., -4.7064e-04,
         -6.0055e-03, -1.4670e-02],
        [-9.6037e-03, -6.1977e-03,  0.0000e+00,  ..., -4.7064e-04,
         -6.0055e-03, -1.4670e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-84378.5000, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.5573, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(86.6238, device='cuda:0')



h[100].sum tensor(63.5593, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-103.2546, device='cuda:0')



h[200].sum tensor(-486.9841, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.2277, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(254455.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0492, 0.0000, 0.0074],
        [0.0000, 0.0000, 0.0000,  ..., 0.1296, 0.0000, 0.0176],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2196, 0.0000, 0.0248],
        [0.0000, 0.0000, 0.0000,  ..., 0.2196, 0.0000, 0.0248],
        [0.0000, 0.0000, 0.0000,  ..., 0.2196, 0.0000, 0.0248]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2533760.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1430.2676, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5715.9873, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10773.5918, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(195.5240, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2533],
        [-1.2500],
        [-3.1816],
        ...,
        [-9.9020],
        [-9.8938],
        [-9.8933]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-939542.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(659.4929, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(659.4929, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-84779.7812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.1810, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(81.3507, device='cuda:0')



h[100].sum tensor(62.2690, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-96.9692, device='cuda:0')



h[200].sum tensor(-487.4490, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.5094, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(250656.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2167, 0.0000, 0.0238],
        [0.0000, 0.0000, 0.0000,  ..., 0.2060, 0.0000, 0.0219],
        [0.0000, 0.0000, 0.0000,  ..., 0.1752, 0.0000, 0.0182],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2180, 0.0000, 0.0238],
        [0.0000, 0.0000, 0.0000,  ..., 0.2180, 0.0000, 0.0238],
        [0.0000, 0.0000, 0.0000,  ..., 0.2180, 0.0000, 0.0238]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2485140.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1414.4868, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5712.6816, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9422.1973, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(169.8860, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.6379],
        [-6.3251],
        [-4.6109],
        ...,
        [-9.8335],
        [-9.8258],
        [-9.8255]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-898220.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 410.0 event: 6150 loss: tensor(432.6250, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(667.6072, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(667.6072, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.1991e-03,  0.0000e+00,  ..., -4.7646e-04,
         -6.0055e-03, -1.4670e-02],
        [-9.6037e-03, -6.1991e-03,  0.0000e+00,  ..., -4.7646e-04,
         -6.0055e-03, -1.4670e-02],
        [-1.0269e-02, -5.5051e-03, -8.0475e-06,  ..., -8.7876e-04,
         -6.2675e-03, -1.5685e-02],
        ...,
        [-9.6037e-03, -6.1991e-03,  0.0000e+00,  ..., -4.7646e-04,
         -6.0055e-03, -1.4670e-02],
        [-9.6037e-03, -6.1991e-03,  0.0000e+00,  ..., -4.7646e-04,
         -6.0055e-03, -1.4670e-02],
        [-9.6037e-03, -6.1991e-03,  0.0000e+00,  ..., -4.7646e-04,
         -6.0055e-03, -1.4670e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-84744.0156, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.2800, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(82.3517, device='cuda:0')



h[100].sum tensor(63.5394, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-98.1623, device='cuda:0')



h[200].sum tensor(-486.2164, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.8355, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(254524.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1772, 0.0000, 0.0187],
        [0.0000, 0.0000, 0.0000,  ..., 0.0926, 0.0000, 0.0104],
        [0.0000, 0.0000, 0.0000,  ..., 0.0320, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2169, 0.0000, 0.0229],
        [0.0000, 0.0000, 0.0000,  ..., 0.2169, 0.0000, 0.0229],
        [0.0000, 0.0000, 0.0000,  ..., 0.2169, 0.0000, 0.0229]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2525188., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1397.4225, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5771.5088, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10233.7314, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(161.8757, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6765],
        [-1.2146],
        [-0.3073],
        ...,
        [-9.7829],
        [-9.7757],
        [-9.7755]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-807625.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(525.0773, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(525.0773, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-86355.9453, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-983.9222, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(64.7701, device='cuda:0')



h[100].sum tensor(52.3838, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-77.2053, device='cuda:0')



h[200].sum tensor(-491.9823, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.1063, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(243697.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2172, 0.0000, 0.0235],
        [0.0000, 0.0000, 0.0000,  ..., 0.2172, 0.0000, 0.0235],
        [0.0000, 0.0000, 0.0000,  ..., 0.2169, 0.0000, 0.0236],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2180, 0.0000, 0.0234],
        [0.0000, 0.0000, 0.0000,  ..., 0.2180, 0.0000, 0.0234],
        [0.0000, 0.0000, 0.0000,  ..., 0.2180, 0.0000, 0.0234]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2464868., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1379.3542, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5736.2656, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8922.5693, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(140.7974, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-8.5315],
        [-8.9324],
        [-9.1337],
        ...,
        [-9.8733],
        [-9.8658],
        [-9.8655]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-942831.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9775],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(609.2792, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9775],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(609.2792, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.2002e-03,  0.0000e+00,  ..., -4.8120e-04,
         -6.0055e-03, -1.4670e-02],
        [-1.1942e-02, -3.7664e-03, -2.6587e-05,  ..., -1.8951e-03,
         -6.9258e-03, -1.8234e-02],
        [-9.6037e-03, -6.2002e-03,  0.0000e+00,  ..., -4.8120e-04,
         -6.0055e-03, -1.4670e-02],
        ...,
        [-9.6037e-03, -6.2002e-03,  0.0000e+00,  ..., -4.8120e-04,
         -6.0055e-03, -1.4670e-02],
        [-9.6037e-03, -6.2002e-03,  0.0000e+00,  ..., -4.8120e-04,
         -6.0055e-03, -1.4670e-02],
        [-9.6037e-03, -6.2002e-03,  0.0000e+00,  ..., -4.8120e-04,
         -6.0055e-03, -1.4670e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-85688.1250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.7058, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(75.1567, device='cuda:0')



h[100].sum tensor(57.0269, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-89.5860, device='cuda:0')



h[200].sum tensor(-489.1133, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.4910, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(248541.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0493, 0.0000, 0.0106],
        [0.0000, 0.0000, 0.0000,  ..., 0.0236, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2187, 0.0000, 0.0235],
        [0.0000, 0.0000, 0.0000,  ..., 0.2187, 0.0000, 0.0235],
        [0.0000, 0.0000, 0.0000,  ..., 0.2187, 0.0000, 0.0235]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2479252.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1389.3990, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5771.2461, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9357.0205, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(162.9482, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.0361],
        [ 1.2541],
        [ 1.4250],
        ...,
        [-9.9304],
        [-9.9225],
        [-9.9221]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-929903.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(651.0555, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(651.0555, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-85429.3906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.1172, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(80.3100, device='cuda:0')



h[100].sum tensor(60.4260, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-95.7286, device='cuda:0')



h[200].sum tensor(-487.4194, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.1702, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(255752.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2177, 0.0000, 0.0233],
        [0.0000, 0.0000, 0.0000,  ..., 0.2177, 0.0000, 0.0233],
        [0.0000, 0.0000, 0.0000,  ..., 0.2173, 0.0000, 0.0233],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2185, 0.0000, 0.0231],
        [0.0000, 0.0000, 0.0000,  ..., 0.2185, 0.0000, 0.0231],
        [0.0000, 0.0000, 0.0000,  ..., 0.2185, 0.0000, 0.0231]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2543431.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1407.1577, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5804.4058, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10646.4932, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(176.0811, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-9.3067],
        [-9.3160],
        [-9.2857],
        ...,
        [-9.9320],
        [-9.9242],
        [-9.9239]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-861555.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2705],
        [0.0000],
        [0.5278],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(629.6385, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2705],
        [0.0000],
        [0.5278],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(629.6385, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0230e-02, -5.5495e-03, -6.6993e-06,  ..., -8.6409e-04,
         -6.2521e-03, -1.5625e-02],
        [-1.2140e-02, -3.5639e-03, -2.7118e-05,  ..., -2.0194e-03,
         -7.0037e-03, -1.8535e-02],
        [-9.6037e-03, -6.2010e-03,  0.0000e+00,  ..., -4.8504e-04,
         -6.0055e-03, -1.4670e-02],
        ...,
        [-9.6037e-03, -6.2010e-03,  0.0000e+00,  ..., -4.8504e-04,
         -6.0055e-03, -1.4670e-02],
        [-9.6037e-03, -6.2010e-03,  0.0000e+00,  ..., -4.8504e-04,
         -6.0055e-03, -1.4670e-02],
        [-9.6037e-03, -6.2010e-03,  0.0000e+00,  ..., -4.8504e-04,
         -6.0055e-03, -1.4670e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-85825.5391, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.9075, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(77.6681, device='cuda:0')



h[100].sum tensor(58.4597, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-92.5796, device='cuda:0')



h[200].sum tensor(-488.5605, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.3093, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(250432.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2190, 0.0000, 0.0233],
        [0.0000, 0.0000, 0.0000,  ..., 0.2190, 0.0000, 0.0233],
        [0.0000, 0.0000, 0.0000,  ..., 0.2190, 0.0000, 0.0233]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2493264.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1420.2407, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5809.2959, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9854.1328, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(157.5291, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.5691],
        [ 2.2712],
        [ 2.3833],
        ...,
        [-9.9937],
        [-9.9857],
        [-9.9853]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-888181.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(541.5868, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(541.5868, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-86890.2031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.0676, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(66.8066, device='cuda:0')



h[100].sum tensor(51.5409, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-79.6328, device='cuda:0')



h[200].sum tensor(-492.0324, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.7700, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(242244.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2188, 0.0000, 0.0239],
        [0.0000, 0.0000, 0.0000,  ..., 0.2188, 0.0000, 0.0239],
        [0.0000, 0.0000, 0.0000,  ..., 0.2184, 0.0000, 0.0240],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2196, 0.0000, 0.0238],
        [0.0000, 0.0000, 0.0000,  ..., 0.2196, 0.0000, 0.0238],
        [0.0000, 0.0000, 0.0000,  ..., 0.2196, 0.0000, 0.0238]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2426280.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1427.3176, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5750.3530, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8475.4277, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(151.0949, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -8.4871],
        [ -8.8803],
        [ -9.1544],
        ...,
        [-10.0779],
        [-10.0696],
        [-10.0692]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-997320.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(615.4276, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(615.4276, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1353e-02, -4.3841e-03, -1.7584e-05,  ..., -1.5469e-03,
         -6.6940e-03, -1.7335e-02],
        [-9.6037e-03, -6.2016e-03,  0.0000e+00,  ..., -4.8817e-04,
         -6.0055e-03, -1.4670e-02],
        [-9.6037e-03, -6.2016e-03,  0.0000e+00,  ..., -4.8817e-04,
         -6.0055e-03, -1.4670e-02],
        ...,
        [-9.6037e-03, -6.2016e-03,  0.0000e+00,  ..., -4.8817e-04,
         -6.0055e-03, -1.4670e-02],
        [-9.6037e-03, -6.2016e-03,  0.0000e+00,  ..., -4.8817e-04,
         -6.0055e-03, -1.4670e-02],
        [-9.6037e-03, -6.2016e-03,  0.0000e+00,  ..., -4.8817e-04,
         -6.0055e-03, -1.4670e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-86280.0469, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.7327, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(75.9151, device='cuda:0')



h[100].sum tensor(57.3590, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-90.4900, device='cuda:0')



h[200].sum tensor(-489.1555, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.7381, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(249166.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0261, 0.0000, 0.0022],
        [0.0000, 0.0000, 0.0000,  ..., 0.0585, 0.0000, 0.0071],
        [0.0000, 0.0000, 0.0000,  ..., 0.1511, 0.0000, 0.0170],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2186, 0.0000, 0.0232],
        [0.0000, 0.0000, 0.0000,  ..., 0.2186, 0.0000, 0.0232],
        [0.0000, 0.0000, 0.0000,  ..., 0.2186, 0.0000, 0.0232]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2472205.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1433.5663, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5786.7700, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9279.1279, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(177.1033, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.0283],
        [ -0.9273],
        [ -3.6843],
        ...,
        [-10.0670],
        [-10.0590],
        [-10.0585]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-893522.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3616],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(561.1880, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3616],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(561.1880, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1172e-02, -4.5728e-03, -1.5285e-05,  ..., -1.4390e-03,
         -6.6229e-03, -1.7060e-02],
        [-9.6037e-03, -6.2019e-03,  0.0000e+00,  ..., -4.8950e-04,
         -6.0055e-03, -1.4670e-02],
        [-1.0469e-02, -5.3037e-03, -8.4280e-06,  ..., -1.0131e-03,
         -6.3459e-03, -1.5988e-02],
        ...,
        [-9.6037e-03, -6.2019e-03,  0.0000e+00,  ..., -4.8950e-04,
         -6.0055e-03, -1.4670e-02],
        [-9.6037e-03, -6.2019e-03,  0.0000e+00,  ..., -4.8950e-04,
         -6.0055e-03, -1.4670e-02],
        [-9.6037e-03, -6.2019e-03,  0.0000e+00,  ..., -4.8950e-04,
         -6.0055e-03, -1.4670e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-86928.2500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.2585, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(69.2245, device='cuda:0')



h[100].sum tensor(53.7024, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-82.5149, device='cuda:0')



h[200].sum tensor(-491.2195, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.5579, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(244566.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0474, 0.0000, 0.0038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0261, 0.0000, 0.0043],
        [0.0000, 0.0000, 0.0000,  ..., 0.1199, 0.0000, 0.0134],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2185, 0.0000, 0.0233],
        [0.0000, 0.0000, 0.0000,  ..., 0.2185, 0.0000, 0.0233],
        [0.0000, 0.0000, 0.0000,  ..., 0.2185, 0.0000, 0.0233]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2450610., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1438.3307, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5820.1641, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9327.4346, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(162.1480, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.0578],
        [ -1.2905],
        [ -3.6769],
        ...,
        [-10.1194],
        [-10.1113],
        [-10.1108]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-884225.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(696.6160, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(696.6160, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-85685.6953, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.5302, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(85.9300, device='cuda:0')



h[100].sum tensor(60.8300, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-102.4277, device='cuda:0')



h[200].sum tensor(-486.4007, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.0016, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(257429.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2140, 0.0000, 0.0228],
        [0.0000, 0.0000, 0.0000,  ..., 0.2066, 0.0000, 0.0209],
        [0.0000, 0.0000, 0.0000,  ..., 0.2022, 0.0000, 0.0199],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2165, 0.0000, 0.0236],
        [0.0000, 0.0000, 0.0000,  ..., 0.2191, 0.0000, 0.0239],
        [0.0000, 0.0000, 0.0000,  ..., 0.2191, 0.0000, 0.0239]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2546017.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1466.0521, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5885.6660, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12430.2070, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(216.7600, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.8723],
        [-7.3263],
        [-6.8895],
        ...,
        [-8.5745],
        [-9.4024],
        [-9.9194]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-922078.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(615.3260, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(615.3260, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.2023e-03,  0.0000e+00,  ..., -4.9179e-04,
         -6.0055e-03, -1.4670e-02],
        [-9.6037e-03, -6.2023e-03,  0.0000e+00,  ..., -4.9179e-04,
         -6.0055e-03, -1.4670e-02],
        [-1.1925e-02, -3.7936e-03, -2.1250e-05,  ..., -1.8971e-03,
         -6.9189e-03, -1.8206e-02],
        ...,
        [-9.6037e-03, -6.2023e-03,  0.0000e+00,  ..., -4.9179e-04,
         -6.0055e-03, -1.4670e-02],
        [-9.6037e-03, -6.2023e-03,  0.0000e+00,  ..., -4.9179e-04,
         -6.0055e-03, -1.4670e-02],
        [-9.6037e-03, -6.2023e-03,  0.0000e+00,  ..., -4.9179e-04,
         -6.0055e-03, -1.4670e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-86345.7422, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.8104, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(75.9026, device='cuda:0')



h[100].sum tensor(56.2881, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-90.4751, device='cuda:0')



h[200].sum tensor(-489.0292, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.7340, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(253300.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1511, 0.0000, 0.0190],
        [0.0000, 0.0000, 0.0000,  ..., 0.0615, 0.0000, 0.0110],
        [0.0000, 0.0000, 0.0000,  ..., 0.0251, 0.0000, 0.0069],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2180, 0.0000, 0.0231],
        [0.0000, 0.0000, 0.0000,  ..., 0.2180, 0.0000, 0.0231],
        [0.0000, 0.0000, 0.0000,  ..., 0.2180, 0.0000, 0.0231]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2481782.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1220.8453, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5728.9517, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10112.8086, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(395.7141, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -2.9729],
        [ -0.8764],
        [  0.4378],
        ...,
        [-10.1703],
        [-10.1693],
        [-10.1766]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-884898.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 420.0 event: 6300 loss: tensor(408.3969, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(692.3978, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(692.3978, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-85818.0156, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.4751, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(85.4097, device='cuda:0')



h[100].sum tensor(60.2069, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-101.8074, device='cuda:0')



h[200].sum tensor(-486.1932, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.8320, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(256303.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2013, 0.0000, 0.0211],
        [0.0000, 0.0000, 0.0000,  ..., 0.2126, 0.0000, 0.0225],
        [0.0000, 0.0000, 0.0000,  ..., 0.2117, 0.0000, 0.0226],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2175, 0.0000, 0.0228],
        [0.0000, 0.0000, 0.0000,  ..., 0.2175, 0.0000, 0.0228],
        [0.0000, 0.0000, 0.0000,  ..., 0.2175, 0.0000, 0.0228]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2512490.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1413.1815, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5865.6045, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11266.5615, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(233.8580, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -5.0944],
        [ -6.6361],
        [ -7.4469],
        ...,
        [-10.2237],
        [-10.2158],
        [-10.2155]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-829208.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2864],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(616.4651, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2864],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(616.4651, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0815e-02, -4.9465e-03, -1.0415e-05,  ..., -1.2271e-03,
         -6.4821e-03, -1.6514e-02],
        [-1.0130e-02, -5.6571e-03, -4.5236e-06,  ..., -8.1220e-04,
         -6.2125e-03, -1.5471e-02],
        [-1.1234e-02, -4.5112e-03, -1.4024e-05,  ..., -1.4812e-03,
         -6.6473e-03, -1.7154e-02],
        ...,
        [-9.6037e-03, -6.2027e-03,  0.0000e+00,  ..., -4.9365e-04,
         -6.0055e-03, -1.4669e-02],
        [-9.6037e-03, -6.2027e-03,  0.0000e+00,  ..., -4.9365e-04,
         -6.0055e-03, -1.4669e-02],
        [-9.6037e-03, -6.2027e-03,  0.0000e+00,  ..., -4.9365e-04,
         -6.0055e-03, -1.4669e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-86449.7734, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.7888, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(76.0431, device='cuda:0')



h[100].sum tensor(52.1615, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-90.6426, device='cuda:0')



h[200].sum tensor(-488.8663, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.7798, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(253499., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0308, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2191, 0.0000, 0.0242],
        [0.0000, 0.0000, 0.0000,  ..., 0.2191, 0.0000, 0.0242],
        [0.0000, 0.0000, 0.0000,  ..., 0.2191, 0.0000, 0.0242]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2490624.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1214.6147, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5743.0942, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12112.5215, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(424.1014, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.1228],
        [  0.7797],
        [  0.7432],
        ...,
        [-10.3779],
        [-10.3694],
        [-10.3689]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1006206.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(490.5768, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(490.5768, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-87896.1875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-983.6089, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(60.5144, device='cuda:0')



h[100].sum tensor(43.5374, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-72.1325, device='cuda:0')



h[200].sum tensor(-491.9443, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.7195, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(241525.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1290, 0.0000, 0.0109],
        [0.0000, 0.0000, 0.0000,  ..., 0.1879, 0.0000, 0.0189],
        [0.0000, 0.0000, 0.0000,  ..., 0.2088, 0.0000, 0.0220],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2179, 0.0000, 0.0230],
        [0.0000, 0.0000, 0.0000,  ..., 0.2179, 0.0000, 0.0230],
        [0.0000, 0.0000, 0.0000,  ..., 0.2179, 0.0000, 0.0230]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2421211.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1356.6056, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5776.8135, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9360.4688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(210.6418, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -5.3758],
        [ -7.0944],
        [ -8.1869],
        ...,
        [-10.3250],
        [-10.3168],
        [-10.3165]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-908211.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2810],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(588.2004, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2810],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(588.2004, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0276e-02, -5.5058e-03, -5.4289e-06,  ..., -9.0236e-04,
         -6.2701e-03, -1.5693e-02],
        [-1.0144e-02, -5.6431e-03, -4.3595e-06,  ..., -8.2215e-04,
         -6.2179e-03, -1.5492e-02],
        [-1.0816e-02, -4.9460e-03, -9.7884e-06,  ..., -1.2294e-03,
         -6.4825e-03, -1.6516e-02],
        ...,
        [-9.6037e-03, -6.2029e-03,  0.0000e+00,  ..., -4.9516e-04,
         -6.0055e-03, -1.4669e-02],
        [-9.6037e-03, -6.2029e-03,  0.0000e+00,  ..., -4.9516e-04,
         -6.0055e-03, -1.4669e-02],
        [-9.6037e-03, -6.2029e-03,  0.0000e+00,  ..., -4.9516e-04,
         -6.0055e-03, -1.4669e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-87054.4609, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.5012, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(72.5566, device='cuda:0')



h[100].sum tensor(48.7434, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-86.4867, device='cuda:0')



h[200].sum tensor(-486.9250, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.6437, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(247512.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0833, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0211, 0.0000, 0.0010],
        [0.0000, 0.0000, 0.0000,  ..., 0.0644, 0.0000, 0.0035],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2176, 0.0000, 0.0225],
        [0.0000, 0.0000, 0.0000,  ..., 0.2176, 0.0000, 0.0225],
        [0.0000, 0.0000, 0.0000,  ..., 0.2176, 0.0000, 0.0225]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2455920., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1469.5018, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5839.0566, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10107.3848, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(138.8654, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -1.5837],
        [ -0.3891],
        [ -0.7635],
        ...,
        [-10.3339],
        [-10.3259],
        [-10.3256]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-854361.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2668],
        [0.0000],
        [0.6699],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(654.3776, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2668],
        [0.0000],
        [0.6699],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(654.3776, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0143e-02, -5.6437e-03, -4.2203e-06,  ..., -8.2254e-04,
         -6.2178e-03, -1.5491e-02],
        [-1.3168e-02, -2.5066e-03, -2.7894e-05,  ..., -2.6553e-03,
         -7.4084e-03, -2.0099e-02],
        [-1.2830e-02, -2.8572e-03, -2.5248e-05,  ..., -2.4505e-03,
         -7.2753e-03, -1.9584e-02],
        ...,
        [-9.6037e-03, -6.2030e-03,  0.0000e+00,  ..., -4.9581e-04,
         -6.0055e-03, -1.4669e-02],
        [-9.6037e-03, -6.2030e-03,  0.0000e+00,  ..., -4.9581e-04,
         -6.0055e-03, -1.4669e-02],
        [-9.6037e-03, -6.2030e-03,  0.0000e+00,  ..., -4.9581e-04,
         -6.0055e-03, -1.4669e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-86379.4688, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.2036, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(80.7197, device='cuda:0')



h[100].sum tensor(51.8485, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-96.2171, device='cuda:0')



h[200].sum tensor(-483.5000, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.3038, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(255504.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2186, 0.0000, 0.0232],
        [0.0000, 0.0000, 0.0000,  ..., 0.2186, 0.0000, 0.0232],
        [0.0000, 0.0000, 0.0000,  ..., 0.2186, 0.0000, 0.0232]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2514828.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1531.7609, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5875.9111, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13152.4980, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(176.6298, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.1141],
        [  0.9884],
        [  0.8014],
        ...,
        [-10.4131],
        [-10.4142],
        [-10.4240]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-999690.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(608.2659, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(608.2659, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.2030e-03,  0.0000e+00,  ..., -4.9639e-04,
         -6.0055e-03, -1.4669e-02],
        [-9.6037e-03, -6.2030e-03,  0.0000e+00,  ..., -4.9639e-04,
         -6.0055e-03, -1.4669e-02],
        [-9.6037e-03, -6.2030e-03,  0.0000e+00,  ..., -4.9639e-04,
         -6.0055e-03, -1.4669e-02],
        ...,
        [-9.6037e-03, -6.2030e-03,  0.0000e+00,  ..., -4.9639e-04,
         -6.0055e-03, -1.4669e-02],
        [-9.6037e-03, -6.2030e-03,  0.0000e+00,  ..., -4.9639e-04,
         -6.0055e-03, -1.4669e-02],
        [-1.0622e-02, -5.1476e-03, -7.7176e-06,  ..., -1.1131e-03,
         -6.4061e-03, -1.6220e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-86651.2812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.6930, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(75.0317, device='cuda:0')



h[100].sum tensor(48.3782, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-89.4370, device='cuda:0')



h[200].sum tensor(-482.8792, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.4502, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(251669.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2157, 0.0000, 0.0217],
        [0.0000, 0.0000, 0.0000,  ..., 0.2157, 0.0000, 0.0217],
        [0.0000, 0.0000, 0.0000,  ..., 0.2153, 0.0000, 0.0218],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1871, 0.0000, 0.0191],
        [0.0000, 0.0000, 0.0000,  ..., 0.1127, 0.0000, 0.0133],
        [0.0000, 0.0000, 0.0000,  ..., 0.0605, 0.0000, 0.0075]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2475990., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1480.6974, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5630.3892, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11509.5127, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-6.7576, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-8.7662],
        [-9.1700],
        [-9.4021],
        ...,
        [-8.0366],
        [-5.8075],
        [-3.0685]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-843089.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3042],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(550.3356, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3042],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(550.3356, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.2031e-03,  0.0000e+00,  ..., -4.9691e-04,
         -6.0055e-03, -1.4669e-02],
        [-1.0331e-02, -5.4487e-03, -5.3447e-06,  ..., -9.3783e-04,
         -6.2919e-03, -1.5778e-02],
        [-1.0092e-02, -5.6973e-03, -3.5829e-06,  ..., -7.9249e-04,
         -6.1975e-03, -1.5412e-02],
        ...,
        [-9.6037e-03, -6.2031e-03,  0.0000e+00,  ..., -4.9691e-04,
         -6.0055e-03, -1.4669e-02],
        [-9.6037e-03, -6.2031e-03,  0.0000e+00,  ..., -4.9691e-04,
         -6.0055e-03, -1.4669e-02],
        [-9.6037e-03, -6.2031e-03,  0.0000e+00,  ..., -4.9691e-04,
         -6.0055e-03, -1.4669e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-87429.6406, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.1171, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(67.8858, device='cuda:0')



h[100].sum tensor(43.0868, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-80.9192, device='cuda:0')



h[200].sum tensor(-483.5852, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.1216, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(244190.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1254, 0.0000, 0.0124],
        [0.0000, 0.0000, 0.0000,  ..., 0.0629, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0004],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2158, 0.0000, 0.0208],
        [0.0000, 0.0000, 0.0000,  ..., 0.2158, 0.0000, 0.0208],
        [0.0000, 0.0000, 0.0000,  ..., 0.2158, 0.0000, 0.0208]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2426386.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1496.5291, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5831.3906, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9974.0527, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(64.2291, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -3.2567],
        [ -0.7220],
        [  1.0290],
        ...,
        [-10.1972],
        [-10.2087],
        [-10.2324]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-824040.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2720],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(699.9802, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2720],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(699.9802, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.2032e-03,  0.0000e+00,  ..., -4.9739e-04,
         -6.0055e-03, -1.4669e-02],
        [-1.0254e-02, -5.5287e-03, -4.6289e-06,  ..., -8.9162e-04,
         -6.2616e-03, -1.5660e-02],
        [-1.0144e-02, -5.6426e-03, -3.8474e-06,  ..., -8.2505e-04,
         -6.2183e-03, -1.5493e-02],
        ...,
        [-9.6037e-03, -6.2032e-03,  0.0000e+00,  ..., -4.9739e-04,
         -6.0055e-03, -1.4669e-02],
        [-9.6037e-03, -6.2032e-03,  0.0000e+00,  ..., -4.9739e-04,
         -6.0055e-03, -1.4669e-02],
        [-9.6037e-03, -6.2032e-03,  0.0000e+00,  ..., -4.9739e-04,
         -6.0055e-03, -1.4669e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-85804.6719, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.5129, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(86.3450, device='cuda:0')



h[100].sum tensor(50.7241, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-102.9223, device='cuda:0')



h[200].sum tensor(-477.5988, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.1368, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(259836.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1432e-01, 0.0000e+00,
         1.0604e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.4081e-02, 0.0000e+00,
         5.0011e-03],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.0973e-03, 0.0000e+00,
         1.7870e-04],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.1723e-01, 0.0000e+00,
         2.1233e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.1723e-01, 0.0000e+00,
         2.1233e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.1723e-01, 0.0000e+00,
         2.1233e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2525064.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1546.1096, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5615.0610, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13901.9121, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-3.9235, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -2.6443],
        [ -0.3420],
        [  0.9452],
        ...,
        [-10.3997],
        [-10.3920],
        [-10.3918]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-964520.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(665.8905, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(665.8905, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-86270.5234, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.1840, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(82.1399, device='cuda:0')



h[100].sum tensor(47.8539, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-97.9099, device='cuda:0')



h[200].sum tensor(-477.8689, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.7665, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(254570.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1610, 0.0000, 0.0155],
        [0.0000, 0.0000, 0.0000,  ..., 0.2023, 0.0000, 0.0191],
        [0.0000, 0.0000, 0.0000,  ..., 0.2149, 0.0000, 0.0206],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2162, 0.0000, 0.0204],
        [0.0000, 0.0000, 0.0000,  ..., 0.2162, 0.0000, 0.0204],
        [0.0000, 0.0000, 0.0000,  ..., 0.2162, 0.0000, 0.0204]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2480057.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1554.4812, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5720.5186, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11976.9492, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1.6519, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -4.2185],
        [ -6.5131],
        [ -8.0444],
        ...,
        [-10.3086],
        [-10.3013],
        [-10.3013]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-864968.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4509],
        [0.4463],
        [0.4287],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(897.0768, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4509],
        [0.4463],
        [0.4287],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(897.0768, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0671e-02, -5.0967e-03, -7.1259e-06,  ..., -1.1452e-03,
         -6.4257e-03, -1.6295e-02],
        [-1.2580e-02, -3.1176e-03, -1.9870e-05,  ..., -2.3022e-03,
         -7.1771e-03, -1.9202e-02],
        [-1.1544e-02, -4.1921e-03, -1.2951e-05,  ..., -1.6740e-03,
         -6.7691e-03, -1.7624e-02],
        ...,
        [-9.6037e-03, -6.2033e-03,  0.0000e+00,  ..., -4.9820e-04,
         -6.0055e-03, -1.4669e-02],
        [-9.6037e-03, -6.2033e-03,  0.0000e+00,  ..., -4.9820e-04,
         -6.0055e-03, -1.4669e-02],
        [-9.6037e-03, -6.2033e-03,  0.0000e+00,  ..., -4.9820e-04,
         -6.0055e-03, -1.4669e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-84080.5391, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-987.3138, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(110.6575, device='cuda:0')



h[100].sum tensor(61.1427, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-131.9026, device='cuda:0')



h[200].sum tensor(-468.3459, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.0594, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(277698.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2160, 0.0000, 0.0198],
        [0.0000, 0.0000, 0.0000,  ..., 0.2160, 0.0000, 0.0198],
        [0.0000, 0.0000, 0.0000,  ..., 0.2160, 0.0000, 0.0198]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2640289., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1612.8707, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5838.9463, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15219.3486, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(113.0627, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.6804],
        [  1.7532],
        [  1.8410],
        ...,
        [-10.2676],
        [-10.2606],
        [-10.2607]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-746506.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 430.0 event: 6450 loss: tensor(443.9327, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(526.1150, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(526.1150, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-88070.8516, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-983.9471, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(64.8981, device='cuda:0')



h[100].sum tensor(35.9814, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-77.3579, device='cuda:0')



h[200].sum tensor(-483.2904, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.1480, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(241912.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2179, 0.0000, 0.0210],
        [0.0000, 0.0000, 0.0000,  ..., 0.2129, 0.0000, 0.0201],
        [0.0000, 0.0000, 0.0000,  ..., 0.1979, 0.0000, 0.0182],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2202, 0.0000, 0.0214],
        [0.0000, 0.0000, 0.0000,  ..., 0.2202, 0.0000, 0.0214],
        [0.0000, 0.0000, 0.0000,  ..., 0.2202, 0.0000, 0.0214]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2399629.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1625.3448, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5801.1543, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12045.2695, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(48.0784, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -7.2964],
        [ -6.3831],
        [ -4.8441],
        ...,
        [-10.5441],
        [-10.5361],
        [-10.5358]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1115581.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(673.5509, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(673.5509, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-86598.2969, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.3042, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(83.0848, device='cuda:0')



h[100].sum tensor(48.5463, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-99.0363, device='cuda:0')



h[200].sum tensor(-476.4315, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.0745, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(252411.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2182, 0.0000, 0.0210],
        [0.0000, 0.0000, 0.0000,  ..., 0.2101, 0.0000, 0.0204],
        [0.0000, 0.0000, 0.0000,  ..., 0.1758, 0.0000, 0.0182],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2191, 0.0000, 0.0208],
        [0.0000, 0.0000, 0.0000,  ..., 0.2191, 0.0000, 0.0208],
        [0.0000, 0.0000, 0.0000,  ..., 0.2191, 0.0000, 0.0208]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2457803.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1654.8651, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5907.1094, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13533.5645, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(36.7613, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -8.1946],
        [ -6.6787],
        [ -4.4332],
        ...,
        [-10.4665],
        [-10.4590],
        [-10.4589]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-898270.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(571.4802, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(571.4802, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1110e-02, -4.6573e-03, -9.1333e-06,  ..., -1.4124e-03,
         -6.5985e-03, -1.6963e-02],
        [-9.6037e-03, -6.2076e-03,  0.0000e+00,  ..., -4.9914e-04,
         -6.0055e-03, -1.4669e-02],
        [-9.6037e-03, -6.2076e-03,  0.0000e+00,  ..., -4.9914e-04,
         -6.0055e-03, -1.4669e-02],
        ...,
        [-9.6037e-03, -6.2076e-03,  0.0000e+00,  ..., -4.9914e-04,
         -6.0055e-03, -1.4669e-02],
        [-9.6037e-03, -6.2076e-03,  0.0000e+00,  ..., -4.9914e-04,
         -6.0055e-03, -1.4669e-02],
        [-9.6037e-03, -6.2076e-03,  0.0000e+00,  ..., -4.9914e-04,
         -6.0055e-03, -1.4669e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-87574.6562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.3586, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(70.4941, device='cuda:0')



h[100].sum tensor(42.7147, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-84.0282, device='cuda:0')



h[200].sum tensor(-478.4665, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.9716, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(246018.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0728, 0.0000, 0.0087],
        [0.0000, 0.0000, 0.0000,  ..., 0.0766, 0.0000, 0.0095],
        [0.0000, 0.0000, 0.0000,  ..., 0.0850, 0.0000, 0.0098],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2166, 0.0000, 0.0193],
        [0.0000, 0.0000, 0.0000,  ..., 0.2166, 0.0000, 0.0193],
        [0.0000, 0.0000, 0.0000,  ..., 0.2166, 0.0000, 0.0193]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2407221.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1599.6234, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5851.1841, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10054.4492, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-81.1032, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -1.5011],
        [ -1.5284],
        [ -1.3534],
        ...,
        [-10.2602],
        [-10.2539],
        [-10.2542]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-616171., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(766.0366, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(766.0366, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-85892.0625, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.1573, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(94.4933, device='cuda:0')



h[100].sum tensor(56.3024, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-112.6350, device='cuda:0')



h[200].sum tensor(-471.6182, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.7921, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(259480.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1683, 0.0000, 0.0174],
        [0.0000, 0.0000, 0.0000,  ..., 0.2154, 0.0000, 0.0207],
        [0.0000, 0.0000, 0.0000,  ..., 0.2026, 0.0000, 0.0196],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2203, 0.0000, 0.0212],
        [0.0000, 0.0000, 0.0000,  ..., 0.2203, 0.0000, 0.0212],
        [0.0000, 0.0000, 0.0000,  ..., 0.2203, 0.0000, 0.0212]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2491710., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1718.2839, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5911.3159, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14742.8848, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4.0818, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -3.2653],
        [ -4.6346],
        [ -4.8871],
        ...,
        [-10.5004],
        [-10.4932],
        [-10.4933]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-819493.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3223],
        [0.3923],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(654.9940, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3223],
        [0.3923],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(654.9940, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1501e-02, -4.2656e-03, -1.0778e-05,  ..., -1.6494e-03,
         -6.7520e-03, -1.7557e-02],
        [-1.0542e-02, -5.2479e-03, -5.3327e-06,  ..., -1.0685e-03,
         -6.3749e-03, -1.6098e-02],
        [-1.1501e-02, -4.2656e-03, -1.0778e-05,  ..., -1.6494e-03,
         -6.7520e-03, -1.7557e-02],
        ...,
        [-9.6037e-03, -6.2098e-03,  0.0000e+00,  ..., -4.9962e-04,
         -6.0055e-03, -1.4669e-02],
        [-9.6037e-03, -6.2098e-03,  0.0000e+00,  ..., -4.9962e-04,
         -6.0055e-03, -1.4669e-02],
        [-9.6037e-03, -6.2098e-03,  0.0000e+00,  ..., -4.9962e-04,
         -6.0055e-03, -1.4669e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-87143.5625, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.0961, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(80.7958, device='cuda:0')



h[100].sum tensor(47.6429, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-96.3077, device='cuda:0')



h[200].sum tensor(-474.1550, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.3285, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(249938.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2223, 0.0000, 0.0215],
        [0.0000, 0.0000, 0.0000,  ..., 0.2223, 0.0000, 0.0215],
        [0.0000, 0.0000, 0.0000,  ..., 0.2223, 0.0000, 0.0215]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2436564., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1738.4772, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5895.4844, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14354.6465, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-58.5636, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.7431],
        [  0.9894],
        [  1.3913],
        ...,
        [-10.5935],
        [-10.5861],
        [-10.5861]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-881461.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(750.8248, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(750.8248, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-86345.9062, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.0361, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(92.6169, device='cuda:0')



h[100].sum tensor(51.2021, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-110.3983, device='cuda:0')



h[200].sum tensor(-467.0201, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.1806, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(258713.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2169, 0.0000, 0.0187],
        [0.0000, 0.0000, 0.0000,  ..., 0.2122, 0.0000, 0.0175],
        [0.0000, 0.0000, 0.0000,  ..., 0.2070, 0.0000, 0.0167],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2229, 0.0000, 0.0204],
        [0.0000, 0.0000, 0.0000,  ..., 0.2229, 0.0000, 0.0204],
        [0.0000, 0.0000, 0.0000,  ..., 0.2229, 0.0000, 0.0204]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2478154.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1740.9490, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5871.3760, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14106.7529, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-96.6416, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -7.3659],
        [ -7.0291],
        [ -6.7437],
        ...,
        [-10.5656],
        [-10.5585],
        [-10.5586]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-740594., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(784.8184, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(784.8184, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-86321.4688, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.3176, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(96.8101, device='cuda:0')



h[100].sum tensor(47.5234, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-115.3966, device='cuda:0')



h[200].sum tensor(-463.5923, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.5470, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(259406.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2243, 0.0000, 0.0204],
        [0.0000, 0.0000, 0.0000,  ..., 0.2235, 0.0000, 0.0201],
        [0.0000, 0.0000, 0.0000,  ..., 0.2145, 0.0000, 0.0190],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2232, 0.0000, 0.0200],
        [0.0000, 0.0000, 0.0000,  ..., 0.2251, 0.0000, 0.0203],
        [0.0000, 0.0000, 0.0000,  ..., 0.2251, 0.0000, 0.0203]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2475846.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1702.4617, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5845.9424, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13116.2080, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-138.0598, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -9.1334],
        [ -8.4683],
        [ -7.2510],
        ...,
        [ -9.7479],
        [-10.1637],
        [-10.4421]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-678021.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(625.8523, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(625.8523, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-88436.2031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.8377, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(77.2011, device='cuda:0')



h[100].sum tensor(32.9404, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-92.0229, device='cuda:0')



h[200].sum tensor(-470.4656, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.1571, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(243648.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1538, 0.0000, 0.0152],
        [0.0000, 0.0000, 0.0000,  ..., 0.1808, 0.0000, 0.0183],
        [0.0000, 0.0000, 0.0000,  ..., 0.1375, 0.0000, 0.0145],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2308, 0.0000, 0.0236],
        [0.0000, 0.0000, 0.0000,  ..., 0.2308, 0.0000, 0.0236],
        [0.0000, 0.0000, 0.0000,  ..., 0.2308, 0.0000, 0.0236]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2381730., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1684.8247, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5825.4136, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13167.7412, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-165.2720, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -2.5061],
        [ -2.8110],
        [ -1.9493],
        ...,
        [-10.9807],
        [-10.9719],
        [-10.9715]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1039508.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(648.7888, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(648.7888, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-88479.5703, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.0962, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(80.0304, device='cuda:0')



h[100].sum tensor(30.4122, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-95.3953, device='cuda:0')



h[200].sum tensor(-468.7960, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.0791, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(243602.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2277, 0.0000, 0.0218],
        [0.0000, 0.0000, 0.0000,  ..., 0.2308, 0.0000, 0.0222],
        [0.0000, 0.0000, 0.0000,  ..., 0.2304, 0.0000, 0.0223],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2315, 0.0000, 0.0221],
        [0.0000, 0.0000, 0.0000,  ..., 0.2315, 0.0000, 0.0221],
        [0.0000, 0.0000, 0.0000,  ..., 0.2315, 0.0000, 0.0221]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2354245.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1710.5714, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5786.5957, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13258.0908, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-152.3165, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -8.0936],
        [ -9.1599],
        [ -9.7558],
        ...,
        [-11.0280],
        [-11.0193],
        [-11.0189]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1036100.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2832],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(631.9517, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2832],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(631.9517, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.2137e-03,  0.0000e+00,  ..., -5.0046e-04,
         -6.0055e-03, -1.4669e-02],
        [-1.0281e-02, -5.5239e-03, -3.2701e-06,  ..., -9.1114e-04,
         -6.2721e-03, -1.5700e-02],
        [-1.1196e-02, -4.5920e-03, -7.6884e-06,  ..., -1.4660e-03,
         -6.6324e-03, -1.7093e-02],
        ...,
        [-9.6037e-03, -6.2137e-03,  0.0000e+00,  ..., -5.0046e-04,
         -6.0055e-03, -1.4669e-02],
        [-9.6037e-03, -6.2137e-03,  0.0000e+00,  ..., -5.0046e-04,
         -6.0055e-03, -1.4669e-02],
        [-9.6037e-03, -6.2137e-03,  0.0000e+00,  ..., -5.0046e-04,
         -6.0055e-03, -1.4669e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-88841.9531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.9083, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(77.9534, device='cuda:0')



h[100].sum tensor(24.9993, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-92.9197, device='cuda:0')



h[200].sum tensor(-467.9467, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.4023, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(241835.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 9.8192e-02, 0.0000e+00,
         7.7778e-03],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.0227e-02, 0.0000e+00,
         3.6310e-03],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.0035e-04, 0.0000e+00,
         4.9116e-05],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.9617e-01, 0.0000e+00,
         1.5737e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.4783e-01, 0.0000e+00,
         1.2640e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 8.8996e-02, 0.0000e+00,
         7.5802e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2325683.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1730.0388, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5758.4199, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12191.2402, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-186.4857, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3873],
        [ 0.1640],
        [ 1.0024],
        ...,
        [-7.9116],
        [-5.5141],
        [-3.1795]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-793477.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 440.0 event: 6600 loss: tensor(455.8621, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2571],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(667.1073, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2571],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(667.1073, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0219e-02, -5.5888e-03, -2.8725e-06,  ..., -8.7339e-04,
         -6.2475e-03, -1.5605e-02],
        [-9.6037e-03, -6.2142e-03,  0.0000e+00,  ..., -5.0059e-04,
         -6.0055e-03, -1.4669e-02],
        [-1.0219e-02, -5.5888e-03, -2.8725e-06,  ..., -8.7339e-04,
         -6.2475e-03, -1.5605e-02],
        ...,
        [-9.6037e-03, -6.2142e-03,  0.0000e+00,  ..., -5.0059e-04,
         -6.0055e-03, -1.4669e-02],
        [-9.6037e-03, -6.2142e-03,  0.0000e+00,  ..., -5.0059e-04,
         -6.0055e-03, -1.4669e-02],
        [-9.6037e-03, -6.2142e-03,  0.0000e+00,  ..., -5.0059e-04,
         -6.0055e-03, -1.4669e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-88729.9531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.2040, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(82.2900, device='cuda:0')



h[100].sum tensor(22.9649, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-98.0888, device='cuda:0')



h[200].sum tensor(-466.1550, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.8155, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(247338.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0710, 0.0000, 0.0030],
        [0.0000, 0.0000, 0.0000,  ..., 0.0087, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0323, 0.0000, 0.0005],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2275, 0.0000, 0.0146],
        [0.0000, 0.0000, 0.0000,  ..., 0.2275, 0.0000, 0.0146],
        [0.0000, 0.0000, 0.0000,  ..., 0.2275, 0.0000, 0.0146]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2352903.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1771.4824, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5699.1147, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13296.9785, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-153.8506, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -0.3575],
        [  1.0754],
        [  0.7870],
        ...,
        [-10.7863],
        [-10.7789],
        [-10.7789]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-735030.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(740.3474, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(740.3474, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-88346.0312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.9322, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(91.3244, device='cuda:0')



h[100].sum tensor(23.7498, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-108.8578, device='cuda:0')



h[200].sum tensor(-464.6235, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.7595, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(255320.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2272, 0.0000, 0.0125],
        [0.0000, 0.0000, 0.0000,  ..., 0.2272, 0.0000, 0.0125],
        [0.0000, 0.0000, 0.0000,  ..., 0.2268, 0.0000, 0.0125],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2281, 0.0000, 0.0124],
        [0.0000, 0.0000, 0.0000,  ..., 0.2281, 0.0000, 0.0124],
        [0.0000, 0.0000, 0.0000,  ..., 0.2281, 0.0000, 0.0124]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2435824.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1823.2682, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5762.2305, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(18127.0781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-103.6061, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.1845],
        [-10.1488],
        [-10.0484],
        ...,
        [-10.8783],
        [-10.8706],
        [-10.8706]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-814770.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(729.8859, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(729.8859, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.2155e-03,  0.0000e+00,  ..., -5.0080e-04,
         -6.0055e-03, -1.4669e-02],
        [-9.6037e-03, -6.2155e-03,  0.0000e+00,  ..., -5.0080e-04,
         -6.0055e-03, -1.4669e-02],
        [-1.0256e-02, -5.5535e-03, -2.8525e-06,  ..., -8.9627e-04,
         -6.2622e-03, -1.5661e-02],
        ...,
        [-9.6037e-03, -6.2155e-03,  0.0000e+00,  ..., -5.0080e-04,
         -6.0055e-03, -1.4669e-02],
        [-9.6037e-03, -6.2155e-03,  0.0000e+00,  ..., -5.0080e-04,
         -6.0055e-03, -1.4669e-02],
        [-9.6037e-03, -6.2155e-03,  0.0000e+00,  ..., -5.0080e-04,
         -6.0055e-03, -1.4669e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-88971.1250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.8575, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(90.0340, device='cuda:0')



h[100].sum tensor(19.3932, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-107.3195, device='cuda:0')



h[200].sum tensor(-467.3559, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.3389, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(246572.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0983, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.1397, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.1439, 0.0000, 0.0017],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2298, 0.0000, 0.0107],
        [0.0000, 0.0000, 0.0000,  ..., 0.2298, 0.0000, 0.0107],
        [0.0000, 0.0000, 0.0000,  ..., 0.2298, 0.0000, 0.0107]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2338656.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1858.4275, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5689.1401, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(17939.5625, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-62.0802, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -0.5212],
        [ -2.0379],
        [ -3.1434],
        ...,
        [-11.0417],
        [-11.0334],
        [-11.0332]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-990884.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(854.0763, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(854.0763, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-88052.5469, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-987.0378, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(105.3533, device='cuda:0')



h[100].sum tensor(23.9348, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-125.5800, device='cuda:0')



h[200].sum tensor(-462.9122, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.3310, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(257055.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2300, 0.0000, 0.0098],
        [0.0000, 0.0000, 0.0000,  ..., 0.2300, 0.0000, 0.0098],
        [0.0000, 0.0000, 0.0000,  ..., 0.2296, 0.0000, 0.0098],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2308, 0.0000, 0.0098],
        [0.0000, 0.0000, 0.0000,  ..., 0.2308, 0.0000, 0.0098],
        [0.0000, 0.0000, 0.0000,  ..., 0.2308, 0.0000, 0.0098]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2425861., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1883.5254, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5737.9536, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(20725.7109, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-6.3769, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.4874],
        [-10.3995],
        [-10.2275],
        ...,
        [-11.1132],
        [-11.1047],
        [-11.1044]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1002357.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2479],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(633.3660, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2479],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(633.3660, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0197e-02, -5.6158e-03, -2.4269e-06,  ..., -8.6051e-04,
         -6.2389e-03, -1.5571e-02],
        [-9.6037e-03, -6.2166e-03,  0.0000e+00,  ..., -5.0097e-04,
         -6.0055e-03, -1.4669e-02],
        [-1.0197e-02, -5.6158e-03, -2.4269e-06,  ..., -8.6051e-04,
         -6.2389e-03, -1.5571e-02],
        ...,
        [-9.6037e-03, -6.2166e-03,  0.0000e+00,  ..., -5.0097e-04,
         -6.0055e-03, -1.4669e-02],
        [-9.6037e-03, -6.2166e-03,  0.0000e+00,  ..., -5.0097e-04,
         -6.0055e-03, -1.4669e-02],
        [-9.6037e-03, -6.2166e-03,  0.0000e+00,  ..., -5.0097e-04,
         -6.0055e-03, -1.4669e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-90467.9531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.9497, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(78.1279, device='cuda:0')



h[100].sum tensor(7.8860, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-93.1276, device='cuda:0')



h[200].sum tensor(-470.7354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.4592, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(237131.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2309, 0.0000, 0.0095],
        [0.0000, 0.0000, 0.0000,  ..., 0.2309, 0.0000, 0.0095],
        [0.0000, 0.0000, 0.0000,  ..., 0.2309, 0.0000, 0.0095]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2272301.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1850.9729, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5694.4531, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15203.4502, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-66.7832, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.1840],
        [  2.0775],
        [  1.8768],
        ...,
        [-11.0529],
        [-11.0448],
        [-11.0446]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-877253.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(678.5162, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(678.5162, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-90084.2500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.3740, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(83.6973, device='cuda:0')



h[100].sum tensor(7.5421, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-99.7663, device='cuda:0')



h[200].sum tensor(-457.0049, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.2741, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(245365.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1601, 0.0000, 0.0042],
        [0.0000, 0.0000, 0.0000,  ..., 0.2114, 0.0000, 0.0077],
        [0.0000, 0.0000, 0.0000,  ..., 0.2260, 0.0000, 0.0098],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2311, 0.0000, 0.0101],
        [0.0000, 0.0000, 0.0000,  ..., 0.2311, 0.0000, 0.0101],
        [0.0000, 0.0000, 0.0000,  ..., 0.2311, 0.0000, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2329273.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1893.2172, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5610.8550, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(17552.0742, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-9.9229, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -6.6279],
        [ -8.3817],
        [ -9.1100],
        ...,
        [-11.1303],
        [-11.1220],
        [-11.1217]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1022532.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(588.3982, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(588.3982, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-91206.5000, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.4742, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(72.5810, device='cuda:0')



h[100].sum tensor(-1.7524, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-86.5157, device='cuda:0')



h[200].sum tensor(-452.1683, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.6516, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(231468.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2291, 0.0000, 0.0099],
        [0.0000, 0.0000, 0.0000,  ..., 0.2130, 0.0000, 0.0077],
        [0.0000, 0.0000, 0.0000,  ..., 0.1607, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2315, 0.0000, 0.0106],
        [0.0000, 0.0000, 0.0000,  ..., 0.2315, 0.0000, 0.0106],
        [0.0000, 0.0000, 0.0000,  ..., 0.2315, 0.0000, 0.0106]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2214468.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1883.7666, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5607.3037, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(16218.6523, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-55.3820, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -8.1920],
        [ -6.3587],
        [ -3.8246],
        ...,
        [-11.2164],
        [-11.2078],
        [-11.2075]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1060258.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(741.0505, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(741.0505, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-89750.8516, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.9218, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(91.4112, device='cuda:0')



h[100].sum tensor(6.1900, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-108.9611, device='cuda:0')



h[200].sum tensor(-439.4156, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.7877, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(250640.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2291, 0.0000, 0.0106],
        [0.0000, 0.0000, 0.0000,  ..., 0.2291, 0.0000, 0.0106],
        [0.0000, 0.0000, 0.0000,  ..., 0.2287, 0.0000, 0.0106],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2299, 0.0000, 0.0105],
        [0.0000, 0.0000, 0.0000,  ..., 0.2299, 0.0000, 0.0105],
        [0.0000, 0.0000, 0.0000,  ..., 0.2299, 0.0000, 0.0105]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2358828., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1892.3701, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5635.9956, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(20255.0859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-26.3848, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.2175],
        [-10.3795],
        [-10.4825],
        ...,
        [-11.1629],
        [-11.1548],
        [-11.1546]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1006014.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(909.7769, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(909.7769, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-88154.2656, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-987.6235, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(112.2241, device='cuda:0')



h[100].sum tensor(16.2022, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-133.7700, device='cuda:0')



h[200].sum tensor(-428.1958, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.5699, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(266483.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2272, 0.0000, 0.0103],
        [0.0000, 0.0000, 0.0000,  ..., 0.2272, 0.0000, 0.0103],
        [0.0000, 0.0000, 0.0000,  ..., 0.2267, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2280, 0.0000, 0.0103],
        [0.0000, 0.0000, 0.0000,  ..., 0.2280, 0.0000, 0.0103],
        [0.0000, 0.0000, 0.0000,  ..., 0.2280, 0.0000, 0.0103]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2417588.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1912.3169, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5587.6504, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(21169.0449, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(18.9465, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.2591],
        [-10.1457],
        [ -9.9384],
        ...,
        [-11.0419],
        [-11.0286],
        [-11.0254]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-913139.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(685.6897, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(685.6897, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-91301.7031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.3563, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(84.5822, device='cuda:0')



h[100].sum tensor(-2.0652, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-100.8211, device='cuda:0')



h[200].sum tensor(-440.6255, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.5624, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(244540.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1854, 0.0000, 0.0051],
        [0.0000, 0.0000, 0.0000,  ..., 0.2086, 0.0000, 0.0071],
        [0.0000, 0.0000, 0.0000,  ..., 0.2120, 0.0000, 0.0074],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2294, 0.0000, 0.0101],
        [0.0000, 0.0000, 0.0000,  ..., 0.2294, 0.0000, 0.0101],
        [0.0000, 0.0000, 0.0000,  ..., 0.2294, 0.0000, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2292557.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1917.5060, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5576.6890, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(19008.2695, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-9.1356, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -3.8920],
        [ -5.2892],
        [ -6.0850],
        ...,
        [-11.1103],
        [-11.1059],
        [-11.1100]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1034337., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 450.0 event: 6750 loss: tensor(455.0037, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4387],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(594.1953, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4387],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(594.1953, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.2187e-03,  0.0000e+00,  ..., -5.0131e-04,
         -6.0055e-03, -1.4668e-02],
        [-1.0653e-02, -5.1593e-03, -3.5128e-06,  ..., -1.1376e-03,
         -6.4185e-03, -1.6265e-02],
        [-1.1089e-02, -4.7197e-03, -4.9703e-06,  ..., -1.4016e-03,
         -6.5899e-03, -1.6927e-02],
        ...,
        [-9.6037e-03, -6.2187e-03,  0.0000e+00,  ..., -5.0131e-04,
         -6.0055e-03, -1.4668e-02],
        [-9.6037e-03, -6.2187e-03,  0.0000e+00,  ..., -5.0131e-04,
         -6.0055e-03, -1.4668e-02],
        [-9.6037e-03, -6.2187e-03,  0.0000e+00,  ..., -5.0131e-04,
         -6.0055e-03, -1.4668e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-92950.2969, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.5334, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(73.2961, device='cuda:0')



h[100].sum tensor(-10.3405, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-87.3681, device='cuda:0')



h[200].sum tensor(-448.2376, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.8846, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(230898.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2309, 0.0000, 0.0098],
        [0.0000, 0.0000, 0.0000,  ..., 0.2309, 0.0000, 0.0098],
        [0.0000, 0.0000, 0.0000,  ..., 0.2309, 0.0000, 0.0098]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2193730., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1940.3457, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5574.4502, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(17168.4434, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2.6600, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.0403],
        [  1.9397],
        [  1.8055],
        ...,
        [-11.2343],
        [-11.2259],
        [-11.2257]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1112144.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(614.9224, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(614.9224, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-93229.7578, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.7361, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(75.8528, device='cuda:0')



h[100].sum tensor(-10.2692, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-90.4158, device='cuda:0')



h[200].sum tensor(-451.4328, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.7178, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(235639.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2282, 0.0000, 0.0094],
        [0.0000, 0.0000, 0.0000,  ..., 0.2278, 0.0000, 0.0092],
        [0.0000, 0.0000, 0.0000,  ..., 0.2264, 0.0000, 0.0088],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2290, 0.0000, 0.0093],
        [0.0000, 0.0000, 0.0000,  ..., 0.2290, 0.0000, 0.0093],
        [0.0000, 0.0000, 0.0000,  ..., 0.2290, 0.0000, 0.0093]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2226812.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1940.7253, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5592.5884, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(17339.2871, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(18.2281, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -9.9289],
        [ -9.5415],
        [ -8.9307],
        ...,
        [-11.0780],
        [-11.0710],
        [-11.0741]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1018460.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.8457],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(805.0355, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.8457],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(805.0355, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3999e-02, -1.7857e-03, -1.3750e-05,  ..., -3.1663e-03,
         -7.7354e-03, -2.1354e-02],
        [-1.1165e-02, -4.6445e-03, -4.8837e-06,  ..., -1.4479e-03,
         -6.6199e-03, -1.7043e-02],
        [-1.3850e-02, -1.9364e-03, -1.3283e-05,  ..., -3.0757e-03,
         -7.6766e-03, -2.1127e-02],
        ...,
        [-9.6037e-03, -6.2192e-03,  0.0000e+00,  ..., -5.0138e-04,
         -6.0055e-03, -1.4668e-02],
        [-9.6037e-03, -6.2192e-03,  0.0000e+00,  ..., -5.0138e-04,
         -6.0055e-03, -1.4668e-02],
        [-9.6037e-03, -6.2192e-03,  0.0000e+00,  ..., -5.0138e-04,
         -6.0055e-03, -1.4668e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-91829.6719, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.6013, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(99.3039, device='cuda:0')



h[100].sum tensor(0.9769, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-118.3692, device='cuda:0')



h[200].sum tensor(-449.9297, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.3597, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(257839.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2282, 0.0000, 0.0084],
        [0.0000, 0.0000, 0.0000,  ..., 0.2282, 0.0000, 0.0084],
        [0.0000, 0.0000, 0.0000,  ..., 0.2282, 0.0000, 0.0084]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2387491.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1985.2639, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5626.7188, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(21969.8945, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(113.3161, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -0.4031],
        [ -0.5159],
        [ -0.4483],
        ...,
        [-11.0449],
        [-11.0374],
        [-11.0374]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1013699.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(608.5293, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(608.5293, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-94616.8203, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.6708, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(75.0642, device='cuda:0')



h[100].sum tensor(-14.2701, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-89.4757, device='cuda:0')



h[200].sum tensor(-467.1558, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.4608, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(234273.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1046, 0.0000, 0.0019],
        [0.0000, 0.0000, 0.0000,  ..., 0.1882, 0.0000, 0.0040],
        [0.0000, 0.0000, 0.0000,  ..., 0.2124, 0.0000, 0.0053],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2281, 0.0000, 0.0072],
        [0.0000, 0.0000, 0.0000,  ..., 0.2281, 0.0000, 0.0072],
        [0.0000, 0.0000, 0.0000,  ..., 0.2281, 0.0000, 0.0072]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2198206.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1979.5890, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5616.9204, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(17988.2070, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(57.4787, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -1.8822],
        [ -4.1005],
        [ -5.2061],
        ...,
        [-11.0329],
        [-11.0291],
        [-11.0316]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1093161.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2979],
        [0.3430],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(594.2109, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2979],
        [0.3430],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(594.2109, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2060e-02, -3.7461e-03, -7.1783e-06,  ..., -1.9906e-03,
         -6.9722e-03, -1.8404e-02],
        [-1.0316e-02, -5.5024e-03, -2.0823e-06,  ..., -9.3342e-04,
         -6.2859e-03, -1.5752e-02],
        [-1.0424e-02, -5.3936e-03, -2.3980e-06,  ..., -9.9893e-04,
         -6.3284e-03, -1.5916e-02],
        ...,
        [-9.6037e-03, -6.2201e-03,  0.0000e+00,  ..., -5.0144e-04,
         -6.0055e-03, -1.4668e-02],
        [-9.6037e-03, -6.2201e-03,  0.0000e+00,  ..., -5.0144e-04,
         -6.0055e-03, -1.4668e-02],
        [-9.6037e-03, -6.2201e-03,  0.0000e+00,  ..., -5.0144e-04,
         -6.0055e-03, -1.4668e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-95147.0625, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.6119, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(73.2980, device='cuda:0')



h[100].sum tensor(-15.7798, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-87.3704, device='cuda:0')



h[200].sum tensor(-474.8618, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.8853, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(237368.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0617, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2259, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.2259, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.2259, 0.0000, 0.0062]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2233518.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1956.7847, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5739.7344, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(19648.9570, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(20.9417, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.0493],
        [  0.8858],
        [  0.1343],
        ...,
        [-10.9272],
        [-10.9222],
        [-10.9234]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-987996., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2651],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(755.8292, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2651],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(755.8292, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0261e-02, -5.5591e-03, -1.8557e-06,  ..., -8.9981e-04,
         -6.2641e-03, -1.5667e-02],
        [-1.0238e-02, -5.5820e-03, -1.7914e-06,  ..., -8.8600e-04,
         -6.2551e-03, -1.5633e-02],
        [-9.6037e-03, -6.2205e-03,  0.0000e+00,  ..., -5.0147e-04,
         -6.0055e-03, -1.4668e-02],
        ...,
        [-9.6037e-03, -6.2205e-03,  0.0000e+00,  ..., -5.0147e-04,
         -6.0055e-03, -1.4668e-02],
        [-9.6037e-03, -6.2205e-03,  0.0000e+00,  ..., -5.0147e-04,
         -6.0055e-03, -1.4668e-02],
        [-9.6037e-03, -6.2205e-03,  0.0000e+00,  ..., -5.0147e-04,
         -6.0055e-03, -1.4668e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-93880.7344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.1052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(93.2342, device='cuda:0')



h[100].sum tensor(-6.0039, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-111.1341, device='cuda:0')



h[200].sum tensor(-475.3917, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.3818, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(252620.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0319, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1018, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1564, 0.0000, 0.0008],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2225, 0.0000, 0.0053],
        [0.0000, 0.0000, 0.0000,  ..., 0.2225, 0.0000, 0.0053],
        [0.0000, 0.0000, 0.0000,  ..., 0.2225, 0.0000, 0.0053]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2321436.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1940.8074, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5827.7456, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(21833.5488, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(15.2342, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -0.3302],
        [ -2.2078],
        [ -4.3518],
        ...,
        [-10.7761],
        [-10.7699],
        [-10.7704]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-887959., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4675],
        [0.2751],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(675.2899, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4675],
        [0.2751],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(675.2899, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0722e-02, -5.0957e-03, -3.0525e-06,  ..., -1.1796e-03,
         -6.4457e-03, -1.6369e-02],
        [-1.0262e-02, -5.5587e-03, -1.7964e-06,  ..., -9.0054e-04,
         -6.2645e-03, -1.5669e-02],
        [-1.0722e-02, -5.0957e-03, -3.0525e-06,  ..., -1.1796e-03,
         -6.4457e-03, -1.6369e-02],
        ...,
        [-9.6037e-03, -6.2208e-03,  0.0000e+00,  ..., -5.0149e-04,
         -6.0055e-03, -1.4668e-02],
        [-9.6037e-03, -6.2208e-03,  0.0000e+00,  ..., -5.0149e-04,
         -6.0055e-03, -1.4668e-02],
        [-9.6037e-03, -6.2208e-03,  0.0000e+00,  ..., -5.0149e-04,
         -6.0055e-03, -1.4668e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-95057., device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.3202, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(83.2994, device='cuda:0')



h[100].sum tensor(-11.7296, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-99.2920, device='cuda:0')



h[200].sum tensor(-484.1380, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.1444, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(245005.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2199, 0.0000, 0.0050],
        [0.0000, 0.0000, 0.0000,  ..., 0.2199, 0.0000, 0.0050],
        [0.0000, 0.0000, 0.0000,  ..., 0.2199, 0.0000, 0.0050]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2275656.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1888.4624, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5940.4287, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(20660.5098, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-89.3987, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.6098],
        [  2.6095],
        [  2.5646],
        ...,
        [-10.6536],
        [-10.6480],
        [-10.6486]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-789991.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(722.9321, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(722.9321, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-94777.3203, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.7556, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(89.1762, device='cuda:0')



h[100].sum tensor(-10.1476, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-106.2971, device='cuda:0')



h[200].sum tensor(-486.8959, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.0594, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(258461.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2184, 0.0000, 0.0051],
        [0.0000, 0.0000, 0.0000,  ..., 0.2184, 0.0000, 0.0051],
        [0.0000, 0.0000, 0.0000,  ..., 0.2180, 0.0000, 0.0051],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2193, 0.0000, 0.0051],
        [0.0000, 0.0000, 0.0000,  ..., 0.2193, 0.0000, 0.0051],
        [0.0000, 0.0000, 0.0000,  ..., 0.2120, 0.0000, 0.0039]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2374727.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1710.1787, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5822.9561, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(23216.3477, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-309.5345, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -9.6018],
        [ -9.5126],
        [ -9.2588],
        ...,
        [-10.2763],
        [ -9.6575],
        [ -8.6904]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-823448.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(621.4728, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(621.4728, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-96288.8906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.8744, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(76.6608, device='cuda:0')



h[100].sum tensor(-17.7572, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-91.3789, device='cuda:0')



h[200].sum tensor(-494.1763, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.9811, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(243510.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2183, 0.0000, 0.0056],
        [0.0000, 0.0000, 0.0000,  ..., 0.2183, 0.0000, 0.0056],
        [0.0000, 0.0000, 0.0000,  ..., 0.2179, 0.0000, 0.0056],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2193, 0.0000, 0.0056],
        [0.0000, 0.0000, 0.0000,  ..., 0.2193, 0.0000, 0.0056],
        [0.0000, 0.0000, 0.0000,  ..., 0.2193, 0.0000, 0.0056]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2259246., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1872.9218, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5991.3672, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(20592.2461, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-139.4993, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -9.4761],
        [ -9.5809],
        [ -9.6414],
        ...,
        [-10.6777],
        [-10.6721],
        [-10.6727]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-847654.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(625.5175, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(625.5175, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.2218e-03,  0.0000e+00,  ..., -5.0155e-04,
         -6.0055e-03, -1.4668e-02],
        [-9.6037e-03, -6.2218e-03,  0.0000e+00,  ..., -5.0155e-04,
         -6.0055e-03, -1.4668e-02],
        [-1.1076e-02, -4.7426e-03, -3.6250e-06,  ..., -1.3943e-03,
         -6.5850e-03, -1.6907e-02],
        ...,
        [-9.6037e-03, -6.2218e-03,  0.0000e+00,  ..., -5.0155e-04,
         -6.0055e-03, -1.4668e-02],
        [-9.6037e-03, -6.2218e-03,  0.0000e+00,  ..., -5.0155e-04,
         -6.0055e-03, -1.4668e-02],
        [-9.6037e-03, -6.2218e-03,  0.0000e+00,  ..., -5.0155e-04,
         -6.0055e-03, -1.4668e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-96316.7969, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.8538, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(77.1598, device='cuda:0')



h[100].sum tensor(-19.5708, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-91.9736, device='cuda:0')



h[200].sum tensor(-496.2803, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.1437, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(246599.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2195, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.2195, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.2195, 0.0000, 0.0063]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2238890.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1944.5168, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5828.2158, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(19526.5703, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-204.7327, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.5113],
        [  1.7193],
        [  1.7071],
        ...,
        [-10.7207],
        [-10.7150],
        [-10.7156]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-892377.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(552.7987, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(552.7987, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-97021.5781, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.1949, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(68.1896, device='cuda:0')



h[100].sum tensor(-24.0945, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-81.2813, device='cuda:0')



h[200].sum tensor(-499.0341, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.2206, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(241858.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2160, 0.0000, 0.0054],
        [0.0000, 0.0000, 0.0000,  ..., 0.2153, 0.0000, 0.0052],
        [0.0000, 0.0000, 0.0000,  ..., 0.2105, 0.0000, 0.0045],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2195, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.2195, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.2195, 0.0000, 0.0063]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2215822., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1943.2871, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5812.9502, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(18918.1992, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-216.6558, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -8.3422],
        [ -7.8785],
        [ -7.0407],
        ...,
        [-10.7207],
        [-10.7150],
        [-10.7156]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-913876.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(582.2802, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(582.2802, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-96653.1250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.4667, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(71.8263, device='cuda:0')



h[100].sum tensor(-23.4338, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-85.6162, device='cuda:0')



h[200].sum tensor(-498.7958, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.4057, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(248680.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1966, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.1716, 0.0000, 0.0018],
        [0.0000, 0.0000, 0.0000,  ..., 0.1046, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2193, 0.0000, 0.0067],
        [0.0000, 0.0000, 0.0000,  ..., 0.2193, 0.0000, 0.0067],
        [0.0000, 0.0000, 0.0000,  ..., 0.2193, 0.0000, 0.0067]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2214749.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2028.2089, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5639.5166, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(17673.6172, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-316.1385, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -6.6044],
        [ -4.7226],
        [ -2.8337],
        ...,
        [-10.7366],
        [-10.7309],
        [-10.7315]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-879709.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(626.7279, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(626.7279, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-96929.1250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.8269, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(77.3091, device='cuda:0')



h[100].sum tensor(-23.1175, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-92.1516, device='cuda:0')



h[200].sum tensor(-498.3033, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.1923, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(246661.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2206, 0.0000, 0.0081],
        [0.0000, 0.0000, 0.0000,  ..., 0.2206, 0.0000, 0.0081],
        [0.0000, 0.0000, 0.0000,  ..., 0.2202, 0.0000, 0.0081],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2215, 0.0000, 0.0081],
        [0.0000, 0.0000, 0.0000,  ..., 0.2215, 0.0000, 0.0081],
        [0.0000, 0.0000, 0.0000,  ..., 0.2215, 0.0000, 0.0081]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2201684., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1879.6030, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5637.3325, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(17739.6094, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-406.0822, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -9.1752],
        [ -9.0829],
        [ -8.6985],
        ...,
        [-10.8969],
        [-10.8906],
        [-10.8910]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1034031.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(619.0948, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(619.0948, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1090e-02, -4.7305e-03, -3.2987e-06,  ..., -1.4030e-03,
         -6.5906e-03, -1.6928e-02],
        [-1.1090e-02, -4.7305e-03, -3.2987e-06,  ..., -1.4030e-03,
         -6.5906e-03, -1.6928e-02],
        [-1.0115e-02, -5.7095e-03, -1.1341e-06,  ..., -8.1152e-04,
         -6.2067e-03, -1.5445e-02],
        ...,
        [-9.6037e-03, -6.2225e-03,  0.0000e+00,  ..., -5.0159e-04,
         -6.0055e-03, -1.4668e-02],
        [-9.6037e-03, -6.2225e-03,  0.0000e+00,  ..., -5.0159e-04,
         -6.0055e-03, -1.4668e-02],
        [-9.6037e-03, -6.2225e-03,  0.0000e+00,  ..., -5.0159e-04,
         -6.0055e-03, -1.4668e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-97426.5625, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.8030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(76.3675, device='cuda:0')



h[100].sum tensor(-24.0340, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-91.0293, device='cuda:0')



h[200].sum tensor(-498.5891, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.8855, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(244600.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0106, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2221, 0.0000, 0.0088],
        [0.0000, 0.0000, 0.0000,  ..., 0.2221, 0.0000, 0.0088],
        [0.0000, 0.0000, 0.0000,  ..., 0.2221, 0.0000, 0.0088]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2194299.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1676.6016, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5639.8496, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(16967.9922, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-561.8594, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.0059],
        [  1.8657],
        [  1.7500],
        ...,
        [-10.9339],
        [-10.9275],
        [-10.9280]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1008445.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(602.5038, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(602.5038, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0131e-02, -5.6941e-03, -1.1290e-06,  ..., -8.2107e-04,
         -6.2129e-03, -1.5469e-02],
        [-1.1078e-02, -4.7440e-03, -3.1581e-06,  ..., -1.3952e-03,
         -6.5856e-03, -1.6908e-02],
        [-1.1648e-02, -4.1714e-03, -4.3811e-06,  ..., -1.7413e-03,
         -6.8102e-03, -1.7776e-02],
        ...,
        [-9.6037e-03, -6.2227e-03,  0.0000e+00,  ..., -5.0160e-04,
         -6.0055e-03, -1.4668e-02],
        [-9.6037e-03, -6.2227e-03,  0.0000e+00,  ..., -5.0160e-04,
         -6.0055e-03, -1.4668e-02],
        [-9.6037e-03, -6.2227e-03,  0.0000e+00,  ..., -5.0160e-04,
         -6.0055e-03, -1.4668e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-97902.3906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.6200, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(74.3209, device='cuda:0')



h[100].sum tensor(-25.2548, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-88.5898, device='cuda:0')



h[200].sum tensor(-498.4250, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.2186, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(243673.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0264, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2214, 0.0000, 0.0091],
        [0.0000, 0.0000, 0.0000,  ..., 0.2214, 0.0000, 0.0091],
        [0.0000, 0.0000, 0.0000,  ..., 0.2214, 0.0000, 0.0091]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2169914.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1607.9780, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5593.4517, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14963.6543, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-674.1742, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.2280],
        [  2.3245],
        [  2.0414],
        ...,
        [-10.8847],
        [-10.8787],
        [-10.8792]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-891349.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5352],
        [0.0000],
        [0.5889],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(650.2831, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5352],
        [0.0000],
        [0.5889],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(650.2831, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2208e-02, -3.6110e-03, -5.3879e-06,  ..., -2.0804e-03,
         -7.0303e-03, -1.8626e-02],
        [-1.3292e-02, -2.5235e-03, -7.6314e-06,  ..., -2.7378e-03,
         -7.4571e-03, -2.0274e-02],
        [-9.6037e-03, -6.2228e-03,  0.0000e+00,  ..., -5.0161e-04,
         -6.0055e-03, -1.4668e-02],
        ...,
        [-9.6037e-03, -6.2228e-03,  0.0000e+00,  ..., -5.0161e-04,
         -6.0055e-03, -1.4668e-02],
        [-9.6037e-03, -6.2228e-03,  0.0000e+00,  ..., -5.0161e-04,
         -6.0055e-03, -1.4668e-02],
        [-9.6037e-03, -6.2228e-03,  0.0000e+00,  ..., -5.0161e-04,
         -6.0055e-03, -1.4668e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-97864.4688, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.0776, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(80.2147, device='cuda:0')



h[100].sum tensor(-22.8969, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-95.6151, device='cuda:0')



h[200].sum tensor(-496.0005, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.1392, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(246927.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2227, 0.0000, 0.0097],
        [0.0000, 0.0000, 0.0000,  ..., 0.2227, 0.0000, 0.0097],
        [0.0000, 0.0000, 0.0000,  ..., 0.2227, 0.0000, 0.0097]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2188826., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1712.5219, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5636.7485, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15757.8848, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-560.3824, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.2781],
        [  0.4038],
        [  0.2186],
        ...,
        [-10.9813],
        [-10.9750],
        [-10.9754]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-938147.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2411],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(639.4592, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2411],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(639.4592, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1373e-02, -4.4487e-03, -3.5348e-06,  ..., -1.5743e-03,
         -6.7018e-03, -1.7357e-02],
        [-9.6037e-03, -6.2229e-03,  0.0000e+00,  ..., -5.0162e-04,
         -6.0055e-03, -1.4668e-02],
        [-1.0833e-02, -4.9900e-03, -2.4564e-06,  ..., -1.2471e-03,
         -6.4894e-03, -1.6537e-02],
        ...,
        [-9.6037e-03, -6.2229e-03,  0.0000e+00,  ..., -5.0162e-04,
         -6.0055e-03, -1.4668e-02],
        [-9.6037e-03, -6.2229e-03,  0.0000e+00,  ..., -5.0162e-04,
         -6.0055e-03, -1.4668e-02],
        [-9.6037e-03, -6.2229e-03,  0.0000e+00,  ..., -5.0162e-04,
         -6.0055e-03, -1.4668e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-98457.1875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.9731, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(78.8795, device='cuda:0')



h[100].sum tensor(-24.4718, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-94.0236, device='cuda:0')



h[200].sum tensor(-495.7135, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.7041, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(245374.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2251, 0.0000, 0.0107],
        [0.0000, 0.0000, 0.0000,  ..., 0.2251, 0.0000, 0.0107],
        [0.0000, 0.0000, 0.0000,  ..., 0.2251, 0.0000, 0.0107]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2191895.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1820.5742, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5692.5381, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(16646.4453, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-423.3730, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.1888],
        [  2.1612],
        [  2.1472],
        ...,
        [-11.1536],
        [-11.1464],
        [-11.1466]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1081985., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6543],
        [0.4158],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(847.5975, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6543],
        [0.4158],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(847.5975, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1733e-02, -4.0876e-03, -4.1085e-06,  ..., -1.7930e-03,
         -6.8437e-03, -1.7905e-02],
        [-1.1994e-02, -3.8267e-03, -4.6104e-06,  ..., -1.9508e-03,
         -6.9461e-03, -1.8300e-02],
        [-1.1423e-02, -4.3988e-03, -3.5097e-06,  ..., -1.6048e-03,
         -6.7216e-03, -1.7433e-02],
        ...,
        [-9.6037e-03, -6.2230e-03,  0.0000e+00,  ..., -5.0163e-04,
         -6.0055e-03, -1.4668e-02],
        [-9.6037e-03, -6.2230e-03,  0.0000e+00,  ..., -5.0163e-04,
         -6.0055e-03, -1.4668e-02],
        [-9.6037e-03, -6.2230e-03,  0.0000e+00,  ..., -5.0163e-04,
         -6.0055e-03, -1.4668e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-96798.9062, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.9067, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(104.5541, device='cuda:0')



h[100].sum tensor(-10.9088, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-124.6274, device='cuda:0')



h[200].sum tensor(-487.1011, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.0705, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(261447.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0106, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2272, 0.0000, 0.0115],
        [0.0000, 0.0000, 0.0000,  ..., 0.2272, 0.0000, 0.0115],
        [0.0000, 0.0000, 0.0000,  ..., 0.2272, 0.0000, 0.0115]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2282365.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1890.9240, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5703.6973, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(18041.7812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-269.3034, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.1693],
        [  0.2277],
        [ -0.0566],
        ...,
        [-11.2731],
        [-11.2654],
        [-11.2654]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1106372., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(590.0802, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(590.0802, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-99338.8594, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.5262, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(72.7884, device='cuda:0')



h[100].sum tensor(-27.0827, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-86.7631, device='cuda:0')



h[200].sum tensor(-497.0865, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.7192, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(237610.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2263, 0.0000, 0.0116],
        [0.0000, 0.0000, 0.0000,  ..., 0.2263, 0.0000, 0.0116],
        [0.0000, 0.0000, 0.0000,  ..., 0.2259, 0.0000, 0.0116],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2272, 0.0000, 0.0115],
        [0.0000, 0.0000, 0.0000,  ..., 0.2272, 0.0000, 0.0115],
        [0.0000, 0.0000, 0.0000,  ..., 0.2272, 0.0000, 0.0115]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2129326., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1860.5597, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5689.7412, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14358.0195, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-354.3359, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.4665],
        [-10.4100],
        [-10.2793],
        ...,
        [-11.2731],
        [-11.2654],
        [-11.2654]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1120940.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(728.6844, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(728.6844, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.2232e-03,  0.0000e+00,  ..., -5.0164e-04,
         -6.0055e-03, -1.4668e-02],
        [-1.0149e-02, -5.6763e-03, -1.0159e-06,  ..., -8.3238e-04,
         -6.2202e-03, -1.5497e-02],
        [-1.0149e-02, -5.6763e-03, -1.0159e-06,  ..., -8.3238e-04,
         -6.2202e-03, -1.5497e-02],
        ...,
        [-9.6037e-03, -6.2232e-03,  0.0000e+00,  ..., -5.0164e-04,
         -6.0055e-03, -1.4668e-02],
        [-9.6037e-03, -6.2232e-03,  0.0000e+00,  ..., -5.0164e-04,
         -6.0055e-03, -1.4668e-02],
        [-9.6037e-03, -6.2232e-03,  0.0000e+00,  ..., -5.0164e-04,
         -6.0055e-03, -1.4668e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-98089.6875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.8180, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(89.8858, device='cuda:0')



h[100].sum tensor(-16.4155, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-107.1429, device='cuda:0')



h[200].sum tensor(-490.9404, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.2906, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(250805.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1601, 0.0000, 0.0015],
        [0.0000, 0.0000, 0.0000,  ..., 0.1460, 0.0000, 0.0005],
        [0.0000, 0.0000, 0.0000,  ..., 0.1455, 0.0000, 0.0005],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2265, 0.0000, 0.0118],
        [0.0000, 0.0000, 0.0000,  ..., 0.2265, 0.0000, 0.0118],
        [0.0000, 0.0000, 0.0000,  ..., 0.2265, 0.0000, 0.0118]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2211922.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1880.8914, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5750.1787, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14447.2119, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-283.3724, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -4.1833],
        [ -3.5335],
        [ -3.1039],
        ...,
        [-11.2000],
        [-11.1927],
        [-11.1929]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-933580.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 470.0 event: 7050 loss: tensor(812.3443, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5723],
        [0.5127],
        [0.5645],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(676.9849, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5723],
        [0.5127],
        [0.5645],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(676.9849, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0830e-02, -4.9940e-03, -2.2047e-06,  ..., -1.2452e-03,
         -6.4882e-03, -1.6531e-02],
        [-1.2323e-02, -3.4978e-03, -4.8881e-06,  ..., -2.1503e-03,
         -7.0757e-03, -1.8800e-02],
        [-1.2326e-02, -3.4943e-03, -4.8944e-06,  ..., -2.1524e-03,
         -7.0770e-03, -1.8805e-02],
        ...,
        [-9.6037e-03, -6.2232e-03,  0.0000e+00,  ..., -5.0164e-04,
         -6.0055e-03, -1.4668e-02],
        [-9.6037e-03, -6.2232e-03,  0.0000e+00,  ..., -5.0164e-04,
         -6.0055e-03, -1.4668e-02],
        [-9.6037e-03, -6.2232e-03,  0.0000e+00,  ..., -5.0164e-04,
         -6.0055e-03, -1.4668e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-98919.9219, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.3517, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(83.5084, device='cuda:0')



h[100].sum tensor(-18.6652, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-99.5412, device='cuda:0')



h[200].sum tensor(-493.3091, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.2125, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(249216.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2287, 0.0000, 0.0126],
        [0.0000, 0.0000, 0.0000,  ..., 0.2287, 0.0000, 0.0126],
        [0.0000, 0.0000, 0.0000,  ..., 0.2287, 0.0000, 0.0126]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2235413.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2011.9468, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5816.3110, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15691.3594, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-87.0257, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.1972],
        [  1.1414],
        [  1.3937],
        ...,
        [-11.3198],
        [-11.3106],
        [-11.3052]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1103293.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(581.2863, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(581.2863, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-100300.4297, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.4474, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(71.7037, device='cuda:0')



h[100].sum tensor(-23.1820, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-85.4700, device='cuda:0')



h[200].sum tensor(-497.1706, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.3657, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(235410.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2287, 0.0000, 0.0131],
        [0.0000, 0.0000, 0.0000,  ..., 0.2287, 0.0000, 0.0131],
        [0.0000, 0.0000, 0.0000,  ..., 0.2283, 0.0000, 0.0131],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2296, 0.0000, 0.0130],
        [0.0000, 0.0000, 0.0000,  ..., 0.2296, 0.0000, 0.0130],
        [0.0000, 0.0000, 0.0000,  ..., 0.2296, 0.0000, 0.0130]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2141436.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2082.2007, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5885.3550, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14068.6846, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-25.9347, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.6489],
        [-10.5424],
        [-10.3543],
        ...,
        [-11.4057],
        [-11.3978],
        [-11.3978]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1173231.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2678],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(504.4647, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2678],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(504.4647, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.2233e-03,  0.0000e+00,  ..., -5.0165e-04,
         -6.0055e-03, -1.4668e-02],
        [-1.0244e-02, -5.5813e-03, -1.1116e-06,  ..., -8.9009e-04,
         -6.2576e-03, -1.5641e-02],
        [-9.6037e-03, -6.2233e-03,  0.0000e+00,  ..., -5.0165e-04,
         -6.0055e-03, -1.4668e-02],
        ...,
        [-9.6037e-03, -6.2233e-03,  0.0000e+00,  ..., -5.0165e-04,
         -6.0055e-03, -1.4668e-02],
        [-9.6037e-03, -6.2233e-03,  0.0000e+00,  ..., -5.0165e-04,
         -6.0055e-03, -1.4668e-02],
        [-9.6037e-03, -6.2233e-03,  0.0000e+00,  ..., -5.0165e-04,
         -6.0055e-03, -1.4668e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-101090.8516, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-983.7063, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(62.2275, device='cuda:0')



h[100].sum tensor(-28.2161, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-74.1745, device='cuda:0')



h[200].sum tensor(-500.2857, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.2778, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(229856.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0509, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1146, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.1607, 0.0000, 0.0043],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2296, 0.0000, 0.0130],
        [0.0000, 0.0000, 0.0000,  ..., 0.2296, 0.0000, 0.0130],
        [0.0000, 0.0000, 0.0000,  ..., 0.2296, 0.0000, 0.0130]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2109579.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2075.4868, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5850.4150, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13114.9170, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-37.3428, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -1.6106],
        [ -2.9109],
        [ -4.9333],
        ...,
        [-11.4058],
        [-11.3979],
        [-11.3978]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1196354.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(785.0918, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(785.0918, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.2234e-03,  0.0000e+00,  ..., -5.0166e-04,
         -6.0055e-03, -1.4668e-02],
        [-9.6037e-03, -6.2234e-03,  0.0000e+00,  ..., -5.0166e-04,
         -6.0055e-03, -1.4668e-02],
        [-1.0194e-02, -5.6317e-03, -9.8878e-07,  ..., -8.5964e-04,
         -6.2379e-03, -1.5565e-02],
        ...,
        [-9.6037e-03, -6.2234e-03,  0.0000e+00,  ..., -5.0166e-04,
         -6.0055e-03, -1.4668e-02],
        [-9.6037e-03, -6.2234e-03,  0.0000e+00,  ..., -5.0166e-04,
         -6.0055e-03, -1.4668e-02],
        [-1.1399e-02, -4.4244e-03, -3.0064e-06,  ..., -1.5901e-03,
         -6.7120e-03, -1.7396e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-98484.5938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.2855, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(96.8438, device='cuda:0')



h[100].sum tensor(-7.5169, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-115.4368, device='cuda:0')



h[200].sum tensor(-488.7574, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.5580, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(255879.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.9355e-01, 0.0000e+00,
         3.0007e-03],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.8076e-01, 0.0000e+00,
         1.8242e-03],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.6361e-01, 0.0000e+00,
         1.7408e-03],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.7260e-01, 0.0000e+00,
         7.9042e-03],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 8.2468e-02, 0.0000e+00,
         3.7165e-03],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.6246e-02, 0.0000e+00,
         1.1899e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2276795.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2091.0317, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5926.5420, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(16459.7773, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4.8300, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.0187],
        [-4.2944],
        [-2.9469],
        ...,
        [-7.6486],
        [-4.8453],
        [-2.2115]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-950252.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4495],
        [0.0000],
        [0.2793],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(643.4769, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4495],
        [0.0000],
        [0.2793],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(643.4769, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0207e-02, -5.6186e-03, -9.7564e-07,  ..., -8.6768e-04,
         -6.2431e-03, -1.5585e-02],
        [-1.1951e-02, -3.8719e-03, -3.7930e-06,  ..., -1.9246e-03,
         -6.9292e-03, -1.8234e-02],
        [-9.6037e-03, -6.2235e-03,  0.0000e+00,  ..., -5.0166e-04,
         -6.0055e-03, -1.4668e-02],
        ...,
        [-9.6037e-03, -6.2235e-03,  0.0000e+00,  ..., -5.0166e-04,
         -6.0055e-03, -1.4668e-02],
        [-9.6037e-03, -6.2235e-03,  0.0000e+00,  ..., -5.0166e-04,
         -6.0055e-03, -1.4668e-02],
        [-9.6037e-03, -6.2235e-03,  0.0000e+00,  ..., -5.0166e-04,
         -6.0055e-03, -1.4668e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-99770.9844, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.0441, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(79.3751, device='cuda:0')



h[100].sum tensor(-12.9846, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-94.6143, device='cuda:0')



h[200].sum tensor(-493.4106, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.8656, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(244013.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0209, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2258, 0.0000, 0.0117],
        [0.0000, 0.0000, 0.0000,  ..., 0.2134, 0.0000, 0.0098],
        [0.0000, 0.0000, 0.0000,  ..., 0.1641, 0.0000, 0.0055]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2175378.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2291.8042, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5892.2803, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13623.9736, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4.7939, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.8680],
        [  2.0619],
        [  1.4703],
        ...,
        [-10.0602],
        [ -8.2788],
        [ -5.2562]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-919864.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(596.5765, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(596.5765, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-100559.2734, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.5894, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(73.5898, device='cuda:0')



h[100].sum tensor(-13.5533, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-87.7183, device='cuda:0')



h[200].sum tensor(-494.7432, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.9804, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(239886.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.5806e-01, 0.0000e+00,
         2.0685e-04],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.6522e-01, 0.0000e+00,
         5.4468e-04],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1802e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.2486e-01, 0.0000e+00,
         1.1139e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.2486e-01, 0.0000e+00,
         1.1139e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.2486e-01, 0.0000e+00,
         1.1139e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2179689.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2062.1431, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5911.9443, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14326.2832, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-79.0480, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -2.5919],
        [ -1.8970],
        [ -0.9583],
        ...,
        [-11.1288],
        [-11.1219],
        [-11.1221]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-884177.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4099],
        [0.4167],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(718.4080, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4099],
        [0.4167],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(718.4080, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0584e-02, -5.2411e-03, -1.5292e-06,  ..., -1.0962e-03,
         -6.3914e-03, -1.6157e-02],
        [-1.0601e-02, -5.2247e-03, -1.5547e-06,  ..., -1.1061e-03,
         -6.3978e-03, -1.6182e-02],
        [-1.0584e-02, -5.2411e-03, -1.5292e-06,  ..., -1.0962e-03,
         -6.3914e-03, -1.6157e-02],
        ...,
        [-9.6037e-03, -6.2235e-03,  0.0000e+00,  ..., -5.0166e-04,
         -6.0055e-03, -1.4668e-02],
        [-9.6037e-03, -6.2235e-03,  0.0000e+00,  ..., -5.0166e-04,
         -6.0055e-03, -1.4668e-02],
        [-9.6037e-03, -6.2235e-03,  0.0000e+00,  ..., -5.0166e-04,
         -6.0055e-03, -1.4668e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-99351.2031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.7217, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(88.6181, device='cuda:0')



h[100].sum tensor(-5.8547, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-105.6319, device='cuda:0')



h[200].sum tensor(-489.9671, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.8776, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(248191.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0679, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2249, 0.0000, 0.0111],
        [0.0000, 0.0000, 0.0000,  ..., 0.2249, 0.0000, 0.0111],
        [0.0000, 0.0000, 0.0000,  ..., 0.2249, 0.0000, 0.0111]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2229152.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2073.9707, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5958.5063, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15823.7637, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-59.7395, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -2.0602],
        [  0.2688],
        [  1.2895],
        ...,
        [-11.1288],
        [-11.1219],
        [-11.1221]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-849667.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4744],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(575.6266, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4744],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(575.6266, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.2236e-03,  0.0000e+00,  ..., -5.0167e-04,
         -6.0055e-03, -1.4667e-02],
        [-1.0738e-02, -5.0868e-03, -1.7075e-06,  ..., -1.1897e-03,
         -6.4521e-03, -1.6391e-02],
        [-9.6037e-03, -6.2236e-03,  0.0000e+00,  ..., -5.0167e-04,
         -6.0055e-03, -1.4667e-02],
        ...,
        [-9.6037e-03, -6.2236e-03,  0.0000e+00,  ..., -5.0167e-04,
         -6.0055e-03, -1.4667e-02],
        [-9.6037e-03, -6.2236e-03,  0.0000e+00,  ..., -5.0167e-04,
         -6.0055e-03, -1.4667e-02],
        [-9.6037e-03, -6.2236e-03,  0.0000e+00,  ..., -5.0167e-04,
         -6.0055e-03, -1.4667e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-101070.9219, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.3817, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(71.0055, device='cuda:0')



h[100].sum tensor(-12.9657, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-84.6379, device='cuda:0')



h[200].sum tensor(-494.7659, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.1382, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(236473.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0407, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0114, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2249, 0.0000, 0.0109],
        [0.0000, 0.0000, 0.0000,  ..., 0.2249, 0.0000, 0.0109],
        [0.0000, 0.0000, 0.0000,  ..., 0.2249, 0.0000, 0.0109]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2160583.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2052.0496, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5906.2544, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14736.5078, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-75.5106, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.4041],
        [  2.3208],
        [  2.2386],
        ...,
        [-11.1739],
        [-11.1668],
        [-11.1671]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-949352.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(567.2323, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(567.2323, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.2236e-03,  0.0000e+00,  ..., -5.0167e-04,
         -6.0055e-03, -1.4667e-02],
        [-9.6037e-03, -6.2236e-03,  0.0000e+00,  ..., -5.0167e-04,
         -6.0055e-03, -1.4667e-02],
        [-1.1488e-02, -4.3364e-03, -2.7352e-06,  ..., -1.6440e-03,
         -6.7470e-03, -1.7530e-02],
        ...,
        [-9.6037e-03, -6.2236e-03,  0.0000e+00,  ..., -5.0167e-04,
         -6.0055e-03, -1.4667e-02],
        [-9.6037e-03, -6.2236e-03,  0.0000e+00,  ..., -5.0167e-04,
         -6.0055e-03, -1.4667e-02],
        [-9.6037e-03, -6.2236e-03,  0.0000e+00,  ..., -5.0167e-04,
         -6.0055e-03, -1.4667e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-101451.5312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.2902, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(69.9701, device='cuda:0')



h[100].sum tensor(-12.1795, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-83.4036, device='cuda:0')



h[200].sum tensor(-494.0645, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.8008, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(238038.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1584, 0.0000, 0.0078],
        [0.0000, 0.0000, 0.0000,  ..., 0.0756, 0.0000, 0.0036],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0000, 0.0005],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2255, 0.0000, 0.0112],
        [0.0000, 0.0000, 0.0000,  ..., 0.2255, 0.0000, 0.0112],
        [0.0000, 0.0000, 0.0000,  ..., 0.2255, 0.0000, 0.0112]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2173789., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2019.8206, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5850.4688, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14458.9785, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-24.4142, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -3.9146],
        [ -1.5642],
        [  0.0755],
        ...,
        [-11.2338],
        [-11.2265],
        [-11.2267]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1035453.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(673.8251, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(673.8251, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.2242e-03,  0.0000e+00,  ..., -5.0168e-04,
         -6.0055e-03, -1.4667e-02],
        [-9.6037e-03, -6.2242e-03,  0.0000e+00,  ..., -5.0168e-04,
         -6.0055e-03, -1.4667e-02],
        [-1.0170e-02, -5.6573e-03, -7.9338e-07,  ..., -8.4514e-04,
         -6.2285e-03, -1.5528e-02],
        ...,
        [-9.6037e-03, -6.2242e-03,  0.0000e+00,  ..., -5.0168e-04,
         -6.0055e-03, -1.4667e-02],
        [-9.6037e-03, -6.2242e-03,  0.0000e+00,  ..., -5.0168e-04,
         -6.0055e-03, -1.4667e-02],
        [-9.6037e-03, -6.2242e-03,  0.0000e+00,  ..., -5.0168e-04,
         -6.0055e-03, -1.4667e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-100609.2812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.2911, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(83.1187, device='cuda:0')



h[100].sum tensor(-4.0622, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-99.0766, device='cuda:0')



h[200].sum tensor(-488.2342, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.0855, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(245757.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1986, 0.0000, 0.0054],
        [0.0000, 0.0000, 0.0000,  ..., 0.1551, 0.0000, 0.0016],
        [0.0000, 0.0000, 0.0000,  ..., 0.1396, 0.0000, 0.0005],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2250, 0.0000, 0.0113],
        [0.0000, 0.0000, 0.0000,  ..., 0.2250, 0.0000, 0.0113],
        [0.0000, 0.0000, 0.0000,  ..., 0.2250, 0.0000, 0.0113]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2221344., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1988.3788, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5881.5718, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14751.8848, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-6.6122, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -6.0041],
        [ -4.7987],
        [ -4.0609],
        ...,
        [-11.2050],
        [-11.1978],
        [-11.1981]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-915333., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 480.0 event: 7200 loss: tensor(349.5797, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(574.5188, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(574.5188, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-101882.0312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.3464, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(70.8689, device='cuda:0')



h[100].sum tensor(-8.9879, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-84.4750, device='cuda:0')



h[200].sum tensor(-490.9622, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.0937, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(236441.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2219, 0.0000, 0.0113],
        [0.0000, 0.0000, 0.0000,  ..., 0.2249, 0.0000, 0.0118],
        [0.0000, 0.0000, 0.0000,  ..., 0.2244, 0.0000, 0.0118],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2258, 0.0000, 0.0117],
        [0.0000, 0.0000, 0.0000,  ..., 0.2258, 0.0000, 0.0117],
        [0.0000, 0.0000, 0.0000,  ..., 0.2258, 0.0000, 0.0117]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2157958.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1952.4937, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5822.0166, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12949.6514, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-23.5150, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -8.2262],
        [ -9.2165],
        [ -9.7447],
        ...,
        [-11.2755],
        [-11.2689],
        [-11.2690]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-977262.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2656],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(813.1508, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2656],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(813.1508, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.2252e-03,  0.0000e+00,  ..., -5.0168e-04,
         -6.0055e-03, -1.4667e-02],
        [-1.0239e-02, -5.5903e-03, -8.2794e-07,  ..., -8.8693e-04,
         -6.2556e-03, -1.5632e-02],
        [-9.6037e-03, -6.2252e-03,  0.0000e+00,  ..., -5.0168e-04,
         -6.0055e-03, -1.4667e-02],
        ...,
        [-9.6037e-03, -6.2252e-03,  0.0000e+00,  ..., -5.0168e-04,
         -6.0055e-03, -1.4667e-02],
        [-9.6037e-03, -6.2252e-03,  0.0000e+00,  ..., -5.0168e-04,
         -6.0055e-03, -1.4667e-02],
        [-9.6037e-03, -6.2252e-03,  0.0000e+00,  ..., -5.0168e-04,
         -6.0055e-03, -1.4667e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-99817.6250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.5583, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(100.3050, device='cuda:0')



h[100].sum tensor(7.2613, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-119.5625, device='cuda:0')



h[200].sum tensor(-481.0567, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.6859, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(256024.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0478, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0962, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1050, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2277, 0.0000, 0.0122],
        [0.0000, 0.0000, 0.0000,  ..., 0.2277, 0.0000, 0.0122],
        [0.0000, 0.0000, 0.0000,  ..., 0.2277, 0.0000, 0.0122]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2290531.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1959.8862, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5788.0752, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(16546.2383, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(67.1355, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.0444],
        [  0.0282],
        [ -0.5896],
        ...,
        [-10.7493],
        [-10.1536],
        [ -8.9364]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1100938.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(640.2166, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(640.2166, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-101555.4609, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.9826, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(78.9729, device='cuda:0')



h[100].sum tensor(-1.1031, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-94.1349, device='cuda:0')



h[200].sum tensor(-486.1699, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.7345, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(241712., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0706, 0.0000, 0.0027],
        [0.0000, 0.0000, 0.0000,  ..., 0.1654, 0.0000, 0.0077],
        [0.0000, 0.0000, 0.0000,  ..., 0.2046, 0.0000, 0.0106],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2266, 0.0000, 0.0121],
        [0.0000, 0.0000, 0.0000,  ..., 0.2266, 0.0000, 0.0121],
        [0.0000, 0.0000, 0.0000,  ..., 0.2266, 0.0000, 0.0121]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2189690.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1908.7955, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5758.3906, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13066.1016, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-40.0486, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -1.4763],
        [ -4.3782],
        [ -6.9484],
        ...,
        [-11.3466],
        [-11.3390],
        [-11.3390]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1052733.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(578.2625, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(578.2625, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-102156.0703, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.4368, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(71.3307, device='cuda:0')



h[100].sum tensor(-2.3595, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-85.0254, device='cuda:0')



h[200].sum tensor(-487.2399, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.2442, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(237771.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1561, 0.0000, 0.0067],
        [0.0000, 0.0000, 0.0000,  ..., 0.2058, 0.0000, 0.0095],
        [0.0000, 0.0000, 0.0000,  ..., 0.2202, 0.0000, 0.0101],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2255, 0.0000, 0.0117],
        [0.0000, 0.0000, 0.0000,  ..., 0.2255, 0.0000, 0.0117],
        [0.0000, 0.0000, 0.0000,  ..., 0.2255, 0.0000, 0.0117]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2161254.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1880.7708, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5735.9419, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11622.9023, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-94.7646, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -3.8624],
        [ -6.2135],
        [ -7.7287],
        ...,
        [-11.2810],
        [-11.2736],
        [-11.2738]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-966773.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(655.6462, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(655.6462, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-101683.2109, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.1290, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(80.8762, device='cuda:0')



h[100].sum tensor(4.6436, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-96.4036, device='cuda:0')



h[200].sum tensor(-484.3403, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.3548, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(248513.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2231, 0.0000, 0.0112],
        [0.0000, 0.0000, 0.0000,  ..., 0.1736, 0.0000, 0.0081],
        [0.0000, 0.0000, 0.0000,  ..., 0.0899, 0.0000, 0.0036],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2268, 0.0000, 0.0116],
        [0.0000, 0.0000, 0.0000,  ..., 0.2268, 0.0000, 0.0116],
        [0.0000, 0.0000, 0.0000,  ..., 0.2268, 0.0000, 0.0116]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2272040., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1890.6185, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5765.1602, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15700.0830, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-66.5245, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -7.5189],
        [ -5.0138],
        [ -2.0794],
        ...,
        [-11.4039],
        [-11.3962],
        [-11.3963]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1026065.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(637.5782, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(637.5782, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-102058.3906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.9478, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(78.6475, device='cuda:0')



h[100].sum tensor(5.8356, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-93.7470, device='cuda:0')



h[200].sum tensor(-483.7977, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.6285, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(240812.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2228, 0.0000, 0.0109],
        [0.0000, 0.0000, 0.0000,  ..., 0.2031, 0.0000, 0.0087],
        [0.0000, 0.0000, 0.0000,  ..., 0.1412, 0.0000, 0.0040],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2265, 0.0000, 0.0113],
        [0.0000, 0.0000, 0.0000,  ..., 0.2265, 0.0000, 0.0113],
        [0.0000, 0.0000, 0.0000,  ..., 0.2265, 0.0000, 0.0113]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2194746.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1872.3788, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5794.7080, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13868.6172, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-144.8961, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -7.9069],
        [ -6.1431],
        [ -3.6704],
        ...,
        [-11.4039],
        [-11.3963],
        [-11.3965]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-980066.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(757.0585, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(757.0585, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-100949.9141, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.1188, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(93.3858, device='cuda:0')



h[100].sum tensor(15.8072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-111.3149, device='cuda:0')



h[200].sum tensor(-475.9958, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.4312, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(254920.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0854, 0.0000, 0.0067],
        [0.0000, 0.0000, 0.0000,  ..., 0.1628, 0.0000, 0.0094],
        [0.0000, 0.0000, 0.0000,  ..., 0.2162, 0.0000, 0.0109],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2252, 0.0000, 0.0112],
        [0.0000, 0.0000, 0.0000,  ..., 0.2252, 0.0000, 0.0112],
        [0.0000, 0.0000, 0.0000,  ..., 0.2252, 0.0000, 0.0112]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2291674., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1873.5078, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5802.5239, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15091.7227, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-138.9192, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -2.6563],
        [ -5.0838],
        [ -7.1656],
        ...,
        [-11.3337],
        [-11.3265],
        [-11.3267]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-827154.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3928],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(787.4545, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3928],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(787.4545, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0252e-02, -5.5810e-03, -6.7870e-07,  ..., -8.9473e-04,
         -6.2606e-03, -1.5651e-02],
        [-1.0543e-02, -5.2906e-03, -9.8381e-07,  ..., -1.0714e-03,
         -6.3753e-03, -1.6094e-02],
        [-9.6037e-03, -6.2269e-03,  0.0000e+00,  ..., -5.0169e-04,
         -6.0055e-03, -1.4667e-02],
        ...,
        [-9.6037e-03, -6.2269e-03,  0.0000e+00,  ..., -5.0169e-04,
         -6.0055e-03, -1.4667e-02],
        [-9.6037e-03, -6.2269e-03,  0.0000e+00,  ..., -5.0169e-04,
         -6.0055e-03, -1.4667e-02],
        [-9.6037e-03, -6.2269e-03,  0.0000e+00,  ..., -5.0169e-04,
         -6.0055e-03, -1.4667e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-100874.5312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.3939, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(97.1352, device='cuda:0')



h[100].sum tensor(19.7287, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-115.7842, device='cuda:0')



h[200].sum tensor(-473.4532, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.6530, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(253903.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0636, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.1023, 0.0000, 0.0026],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2276, 0.0000, 0.0123],
        [0.0000, 0.0000, 0.0000,  ..., 0.2276, 0.0000, 0.0123],
        [0.0000, 0.0000, 0.0000,  ..., 0.2276, 0.0000, 0.0123]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2273476.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1865.6652, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5751.9214, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14455.3330, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-143.9475, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.0335],
        [  1.1063],
        [  0.1565],
        ...,
        [-11.4942],
        [-11.4865],
        [-11.4866]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-954769.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(569.4669, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(569.4669, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.2271e-03,  0.0000e+00,  ..., -5.0169e-04,
         -6.0055e-03, -1.4667e-02],
        [-9.6037e-03, -6.2271e-03,  0.0000e+00,  ..., -5.0169e-04,
         -6.0055e-03, -1.4667e-02],
        [-1.0255e-02, -5.5779e-03, -6.5772e-07,  ..., -8.9686e-04,
         -6.2620e-03, -1.5657e-02],
        ...,
        [-9.6037e-03, -6.2271e-03,  0.0000e+00,  ..., -5.0169e-04,
         -6.0055e-03, -1.4667e-02],
        [-9.6037e-03, -6.2271e-03,  0.0000e+00,  ..., -5.0169e-04,
         -6.0055e-03, -1.4667e-02],
        [-9.6037e-03, -6.2271e-03,  0.0000e+00,  ..., -5.0169e-04,
         -6.0055e-03, -1.4667e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-103054.5312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.3655, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(70.2457, device='cuda:0')



h[100].sum tensor(7.4846, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-83.7322, device='cuda:0')



h[200].sum tensor(-479.8179, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.8906, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(239310.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2050, 0.0000, 0.0103],
        [0.0000, 0.0000, 0.0000,  ..., 0.1446, 0.0000, 0.0055],
        [0.0000, 0.0000, 0.0000,  ..., 0.0720, 0.0000, 0.0023],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2305, 0.0000, 0.0136],
        [0.0000, 0.0000, 0.0000,  ..., 0.2305, 0.0000, 0.0136],
        [0.0000, 0.0000, 0.0000,  ..., 0.2305, 0.0000, 0.0136]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2173748., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1861.1099, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5412.6333, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11348.9922, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-88.1874, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -5.9588],
        [ -3.2581],
        [ -0.6250],
        ...,
        [-11.6900],
        [-11.6818],
        [-11.6817]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1132339., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2871],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(718.2025, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2871],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(718.2025, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1762e-02, -4.0799e-03, -2.0993e-06,  ..., -1.8104e-03,
         -6.8550e-03, -1.7944e-02],
        [-9.6037e-03, -6.2278e-03,  0.0000e+00,  ..., -5.0169e-04,
         -6.0055e-03, -1.4667e-02],
        [-1.0290e-02, -5.5443e-03, -6.6797e-07,  ..., -9.1811e-04,
         -6.2758e-03, -1.5710e-02],
        ...,
        [-9.6037e-03, -6.2278e-03,  0.0000e+00,  ..., -5.0169e-04,
         -6.0055e-03, -1.4667e-02],
        [-9.6037e-03, -6.2278e-03,  0.0000e+00,  ..., -5.0169e-04,
         -6.0055e-03, -1.4667e-02],
        [-9.6037e-03, -6.2278e-03,  0.0000e+00,  ..., -5.0169e-04,
         -6.0055e-03, -1.4667e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-101424.6250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.7960, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(88.5928, device='cuda:0')



h[100].sum tensor(19.3943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-105.6017, device='cuda:0')



h[200].sum tensor(-470.3149, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.8693, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(254770.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0048],
        [0.0000, 0.0000, 0.0000,  ..., 0.0154, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0775, 0.0000, 0.0030],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2303, 0.0000, 0.0137],
        [0.0000, 0.0000, 0.0000,  ..., 0.2303, 0.0000, 0.0137],
        [0.0000, 0.0000, 0.0000,  ..., 0.2303, 0.0000, 0.0137]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2259122.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1895.9813, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5219.6851, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12449.1094, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(31.4586, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.9207],
        [  0.7655],
        [  0.0628],
        ...,
        [-11.7048],
        [-11.6966],
        [-11.6966]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-987364.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 490.0 event: 7350 loss: tensor(440.8333, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(526.2548, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(526.2548, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-103726.5312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-983.9030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(64.9154, device='cuda:0')



h[100].sum tensor(8.1806, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-77.3784, device='cuda:0')



h[200].sum tensor(-476.0920, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.1537, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(232205.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2308, 0.0000, 0.0149],
        [0.0000, 0.0000, 0.0000,  ..., 0.2287, 0.0000, 0.0146],
        [0.0000, 0.0000, 0.0000,  ..., 0.2168, 0.0000, 0.0133],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0759, 0.0000, 0.0048],
        [0.0000, 0.0000, 0.0000,  ..., 0.1718, 0.0000, 0.0109],
        [0.0000, 0.0000, 0.0000,  ..., 0.2200, 0.0000, 0.0137]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2127180.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1857.8143, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5440.2339, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9409.3828, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-122.2506, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-9.6646],
        [-8.8856],
        [-7.5463],
        ...,
        [-2.0290],
        [-5.8611],
        [-8.9386]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1050618.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(701.3899, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(701.3899, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.2289e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4667e-02],
        [-1.0268e-02, -5.5694e-03, -5.9970e-07,  ..., -9.0430e-04,
         -6.2668e-03, -1.5675e-02],
        [-9.6037e-03, -6.2289e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4667e-02],
        ...,
        [-9.6037e-03, -6.2289e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4667e-02],
        [-9.6037e-03, -6.2289e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4667e-02],
        [-9.6037e-03, -6.2289e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4667e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-102177.5938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.5571, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(86.5189, device='cuda:0')



h[100].sum tensor(21.5751, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-103.1296, device='cuda:0')



h[200].sum tensor(-467.3398, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.1935, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(246675.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1411, 0.0000, 0.0035],
        [0.0000, 0.0000, 0.0000,  ..., 0.1635, 0.0000, 0.0077],
        [0.0000, 0.0000, 0.0000,  ..., 0.1450, 0.0000, 0.0093],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2323, 0.0000, 0.0156],
        [0.0000, 0.0000, 0.0000,  ..., 0.2177, 0.0000, 0.0140],
        [0.0000, 0.0000, 0.0000,  ..., 0.1963, 0.0000, 0.0116]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2246458.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1874.2847, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5707.7656, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12904.4062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-136.0288, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -3.4940],
        [ -3.2701],
        [ -2.5743],
        ...,
        [-11.1605],
        [-10.1029],
        [ -8.5312]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1119568.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(696.5654, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(696.5654, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-102231.5156, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.5031, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(85.9238, device='cuda:0')



h[100].sum tensor(23.7503, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-102.4202, device='cuda:0')



h[200].sum tensor(-463.8835, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.9996, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(244712.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1140, 0.0000, 0.0130],
        [0.0000, 0.0000, 0.0000,  ..., 0.1912, 0.0000, 0.0135],
        [0.0000, 0.0000, 0.0000,  ..., 0.2191, 0.0000, 0.0141],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2290, 0.0000, 0.0141],
        [0.0000, 0.0000, 0.0000,  ..., 0.2290, 0.0000, 0.0141],
        [0.0000, 0.0000, 0.0000,  ..., 0.2290, 0.0000, 0.0141]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2223226., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1891.3542, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5820.8916, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12940.9102, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-181.1246, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -4.6801],
        [ -7.3873],
        [ -9.1649],
        ...,
        [-11.7030],
        [-11.6954],
        [-11.6956]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-927717.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4255],
        [0.3010],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(708.1216, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4255],
        [0.3010],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(708.1216, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1201e-02, -4.6454e-03, -1.3390e-06,  ..., -1.4701e-03,
         -6.6341e-03, -1.7091e-02],
        [-1.0622e-02, -5.2200e-03, -8.5335e-07,  ..., -1.1189e-03,
         -6.4061e-03, -1.6212e-02],
        [-1.1092e-02, -4.7537e-03, -1.2475e-06,  ..., -1.4039e-03,
         -6.5911e-03, -1.6926e-02],
        ...,
        [-9.6037e-03, -6.2297e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4667e-02],
        [-9.6037e-03, -6.2297e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4667e-02],
        [-9.6037e-03, -6.2297e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4667e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-101997.3047, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.6892, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(87.3493, device='cuda:0')



h[100].sum tensor(26.9816, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-104.1194, device='cuda:0')



h[200].sum tensor(-459.1657, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.4641, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(252217.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0046],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0118],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2256, 0.0000, 0.0127],
        [0.0000, 0.0000, 0.0000,  ..., 0.2256, 0.0000, 0.0127],
        [0.0000, 0.0000, 0.0000,  ..., 0.2256, 0.0000, 0.0127]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2285931.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1916.8040, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5879.3389, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14595.0859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-186.7119, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.6010],
        [  1.5729],
        [  1.3581],
        ...,
        [-11.5111],
        [-11.5041],
        [-11.5044]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-784917.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(657.3640, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(657.3640, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-102694.2578, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.1395, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(81.0881, device='cuda:0')



h[100].sum tensor(24.5372, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-96.6562, device='cuda:0')



h[200].sum tensor(-459.3605, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.4238, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(244723.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2237, 0.0000, 0.0123],
        [0.0000, 0.0000, 0.0000,  ..., 0.2237, 0.0000, 0.0123],
        [0.0000, 0.0000, 0.0000,  ..., 0.2158, 0.0000, 0.0123],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2247, 0.0000, 0.0122],
        [0.0000, 0.0000, 0.0000,  ..., 0.2247, 0.0000, 0.0122],
        [0.0000, 0.0000, 0.0000,  ..., 0.2247, 0.0000, 0.0122]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2226666., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1920.9338, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5898.6245, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13670.8906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-208.3696, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -9.6827],
        [ -9.4700],
        [ -8.7529],
        ...,
        [-11.4906],
        [-11.4837],
        [-11.4842]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-766603.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(625.3265, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(625.3265, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-103243.2500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.8678, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(77.1362, device='cuda:0')



h[100].sum tensor(23.3691, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-91.9455, device='cuda:0')



h[200].sum tensor(-458.8331, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.1360, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(241714.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2112, 0.0000, 0.0109],
        [0.0000, 0.0000, 0.0000,  ..., 0.2250, 0.0000, 0.0127],
        [0.0000, 0.0000, 0.0000,  ..., 0.2249, 0.0000, 0.0128],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2263, 0.0000, 0.0127],
        [0.0000, 0.0000, 0.0000,  ..., 0.2263, 0.0000, 0.0127],
        [0.0000, 0.0000, 0.0000,  ..., 0.2263, 0.0000, 0.0127]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2210187., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1916.6904, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5885.3823, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13898.3828, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-179.9986, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -6.6533],
        [ -7.9562],
        [ -8.0281],
        ...,
        [-11.6223],
        [-11.6143],
        [-11.5998]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-879293.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4756],
        [0.5186],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(651.5790, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4756],
        [0.5186],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(651.5790, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0741e-02, -5.1040e-03, -8.5226e-07,  ..., -1.1915e-03,
         -6.4532e-03, -1.6393e-02],
        [-1.0844e-02, -5.0022e-03, -9.2926e-07,  ..., -1.2538e-03,
         -6.4937e-03, -1.6549e-02],
        [-1.2059e-02, -3.7993e-03, -1.8393e-06,  ..., -1.9903e-03,
         -6.9718e-03, -1.8393e-02],
        ...,
        [-9.6037e-03, -6.2307e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4667e-02],
        [-9.6037e-03, -6.2307e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4667e-02],
        [-9.6037e-03, -6.2307e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4667e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-103354.6328, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.0811, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(80.3745, device='cuda:0')



h[100].sum tensor(24.9519, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-95.8056, device='cuda:0')



h[200].sum tensor(-454.9658, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.1913, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(242856.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0609, 0.0000, 0.0161],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0254],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0321],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2287, 0.0000, 0.0136],
        [0.0000, 0.0000, 0.0000,  ..., 0.2287, 0.0000, 0.0136],
        [0.0000, 0.0000, 0.0000,  ..., 0.2287, 0.0000, 0.0136]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2203569.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1911.4469, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5803.6299, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12862.3203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-160.3176, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -1.6660],
        [  0.3602],
        [  1.2661],
        ...,
        [-11.7732],
        [-11.7662],
        [-11.7666]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-957333.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(628.5654, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(628.5654, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-103813.9688, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.8888, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(77.5357, device='cuda:0')



h[100].sum tensor(23.7530, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-92.4218, device='cuda:0')



h[200].sum tensor(-452.3546, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.2662, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(239075.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2267, 0.0000, 0.0134],
        [0.0000, 0.0000, 0.0000,  ..., 0.2284, 0.0000, 0.0140],
        [0.0000, 0.0000, 0.0000,  ..., 0.2219, 0.0000, 0.0136],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2301, 0.0000, 0.0142],
        [0.0000, 0.0000, 0.0000,  ..., 0.2301, 0.0000, 0.0142],
        [0.0000, 0.0000, 0.0000,  ..., 0.2301, 0.0000, 0.0142]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2168683.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1892.5317, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5765.8384, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10913.2676, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-195.3880, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -7.7569],
        [ -7.3789],
        [ -6.2665],
        ...,
        [-11.8404],
        [-11.8353],
        [-11.8317]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-923883.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2952],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(715.8369, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2952],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(715.8369, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0310e-02, -5.5325e-03, -4.9049e-07,  ..., -9.2980e-04,
         -6.2834e-03, -1.5738e-02],
        [-9.6037e-03, -6.2312e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4667e-02],
        [-1.0310e-02, -5.5325e-03, -4.9049e-07,  ..., -9.2980e-04,
         -6.2834e-03, -1.5738e-02],
        ...,
        [-9.6037e-03, -6.2312e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4667e-02],
        [-9.6037e-03, -6.2312e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4667e-02],
        [-9.6037e-03, -6.2312e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4667e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-103327.8359, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.6844, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(88.3010, device='cuda:0')



h[100].sum tensor(28.7826, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-105.2538, device='cuda:0')



h[200].sum tensor(-448.5284, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.7742, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(244618.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1777, 0.0000, 0.0099],
        [0.0000, 0.0000, 0.0000,  ..., 0.1344, 0.0000, 0.0061],
        [0.0000, 0.0000, 0.0000,  ..., 0.1667, 0.0000, 0.0090],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2336, 0.0000, 0.0153],
        [0.0000, 0.0000, 0.0000,  ..., 0.2336, 0.0000, 0.0153],
        [0.0000, 0.0000, 0.0000,  ..., 0.2336, 0.0000, 0.0153]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2203793.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1895.4553, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5743.3379, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12542.6211, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-150.9467, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -7.4666],
        [ -6.0587],
        [ -4.8710],
        ...,
        [-12.0763],
        [-12.0680],
        [-12.0681]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1135400.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(642.3165, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(642.3165, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-104005.7031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.0497, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(79.2320, device='cuda:0')



h[100].sum tensor(24.4685, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-94.4437, device='cuda:0')



h[200].sum tensor(-451.3293, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.8189, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(241159.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1000, 0.0000, 0.0111],
        [0.0000, 0.0000, 0.0000,  ..., 0.0475, 0.0000, 0.0139],
        [0.0000, 0.0000, 0.0000,  ..., 0.0376, 0.0000, 0.0167],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2336, 0.0000, 0.0153],
        [0.0000, 0.0000, 0.0000,  ..., 0.2336, 0.0000, 0.0153],
        [0.0000, 0.0000, 0.0000,  ..., 0.2336, 0.0000, 0.0153]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2186014.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1898.3655, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5681.4204, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11696.6953, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-142.5654, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -0.3363],
        [  0.6404],
        [  0.8875],
        ...,
        [-12.0763],
        [-12.0680],
        [-12.0681]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1166075.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 500.0 event: 7500 loss: tensor(392.0400, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(607.7573, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(607.7573, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-104489.6484, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.6611, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(74.9690, device='cuda:0')



h[100].sum tensor(23.1939, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-89.3622, device='cuda:0')



h[200].sum tensor(-451.9789, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.4298, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(238045.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2322, 0.0000, 0.0152],
        [0.0000, 0.0000, 0.0000,  ..., 0.2322, 0.0000, 0.0152],
        [0.0000, 0.0000, 0.0000,  ..., 0.2317, 0.0000, 0.0153],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2331, 0.0000, 0.0151],
        [0.0000, 0.0000, 0.0000,  ..., 0.2331, 0.0000, 0.0151],
        [0.0000, 0.0000, 0.0000,  ..., 0.2331, 0.0000, 0.0151]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2164179.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1864.0034, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5693.8647, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10395.2598, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-209.5301, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.6914],
        [-10.7382],
        [-10.6756],
        ...,
        [-12.0122],
        [-12.0035],
        [-12.0031]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1063288.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4534],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(784.6996, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4534],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(784.6996, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.2316e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4667e-02],
        [-1.0688e-02, -5.1591e-03, -6.9833e-07,  ..., -1.1593e-03,
         -6.4323e-03, -1.6312e-02],
        [-1.0878e-02, -4.9711e-03, -8.2080e-07,  ..., -1.2746e-03,
         -6.5072e-03, -1.6601e-02],
        ...,
        [-9.6037e-03, -6.2316e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4667e-02],
        [-9.6037e-03, -6.2316e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4667e-02],
        [-9.6037e-03, -6.2316e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4667e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-102520.1797, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.3884, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(96.7954, device='cuda:0')



h[100].sum tensor(36.2433, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-115.3791, device='cuda:0')



h[200].sum tensor(-445.0870, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.5423, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(255290.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0921, 0.0000, 0.0133],
        [0.0000, 0.0000, 0.0000,  ..., 0.0406, 0.0000, 0.0162],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0202],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2323, 0.0000, 0.0146],
        [0.0000, 0.0000, 0.0000,  ..., 0.2323, 0.0000, 0.0146],
        [0.0000, 0.0000, 0.0000,  ..., 0.2323, 0.0000, 0.0146]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2256767., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1978.1470, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5658.8613, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12907.9609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-76.3060, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -0.5778],
        [  1.0700],
        [  2.0326],
        ...,
        [-11.9360],
        [-11.9336],
        [-11.9381]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-977541., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(826.9149, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(826.9149, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-102088.8828, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.6879, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(102.0028, device='cuda:0')



h[100].sum tensor(39.6554, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-121.5863, device='cuda:0')



h[200].sum tensor(-445.3281, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.2392, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(260880.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2316, 0.0000, 0.0144],
        [0.0000, 0.0000, 0.0000,  ..., 0.2308, 0.0000, 0.0141],
        [0.0000, 0.0000, 0.0000,  ..., 0.2227, 0.0000, 0.0135],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2325, 0.0000, 0.0144],
        [0.0000, 0.0000, 0.0000,  ..., 0.2325, 0.0000, 0.0144],
        [0.0000, 0.0000, 0.0000,  ..., 0.2325, 0.0000, 0.0144]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2278779.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2095.2480, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5602.4976, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14256.4424, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(34.5800, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -9.9233],
        [ -9.3383],
        [ -8.2186],
        ...,
        [-11.9563],
        [-11.9489],
        [-11.9493]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-998023.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(605.2921, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(605.2921, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-104436.8125, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.6703, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(74.6649, device='cuda:0')



h[100].sum tensor(27.4835, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-88.9998, device='cuda:0')



h[200].sum tensor(-454.4724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.3307, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(239916.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2296, 0.0000, 0.0133],
        [0.0000, 0.0000, 0.0000,  ..., 0.2296, 0.0000, 0.0133],
        [0.0000, 0.0000, 0.0000,  ..., 0.2291, 0.0000, 0.0133],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2305, 0.0000, 0.0132],
        [0.0000, 0.0000, 0.0000,  ..., 0.2305, 0.0000, 0.0132],
        [0.0000, 0.0000, 0.0000,  ..., 0.2305, 0.0000, 0.0132]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2162982.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1970.1035, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5667.9155, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10908.3672, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-242.8058, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.6711],
        [-10.5580],
        [-10.2869],
        ...,
        [-11.7929],
        [-11.7853],
        [-11.7855]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-893023., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(717.2118, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(717.2118, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-103458.9062, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.7172, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(88.4706, device='cuda:0')



h[100].sum tensor(34.8263, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-105.4560, device='cuda:0')



h[200].sum tensor(-450.3635, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.8295, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(251033.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1995, 0.0000, 0.0109],
        [0.0000, 0.0000, 0.0000,  ..., 0.2183, 0.0000, 0.0119],
        [0.0000, 0.0000, 0.0000,  ..., 0.2286, 0.0000, 0.0126],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2300, 0.0000, 0.0125],
        [0.0000, 0.0000, 0.0000,  ..., 0.2300, 0.0000, 0.0125],
        [0.0000, 0.0000, 0.0000,  ..., 0.2300, 0.0000, 0.0125]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2251803., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1894.4146, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5744.9863, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13300.6719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-326.3888, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -6.7033],
        [ -7.7931],
        [ -8.0598],
        ...,
        [-11.7721],
        [-11.7648],
        [-11.7651]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-915666.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(743.2458, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(743.2458, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1181e-02, -4.6739e-03, -8.7159e-07,  ..., -1.4581e-03,
         -6.6263e-03, -1.7059e-02],
        [-9.6037e-03, -6.2322e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-9.6037e-03, -6.2322e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        ...,
        [-9.6037e-03, -6.2322e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-9.6037e-03, -6.2322e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-9.6037e-03, -6.2322e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-103317.4531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.9244, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(91.6820, device='cuda:0')



h[100].sum tensor(35.0341, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-109.2839, device='cuda:0')



h[200].sum tensor(-449.1547, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.8760, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(250554.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0279, 0.0000, 0.0221],
        [0.0000, 0.0000, 0.0000,  ..., 0.0626, 0.0000, 0.0204],
        [0.0000, 0.0000, 0.0000,  ..., 0.1541, 0.0000, 0.0155],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2304, 0.0000, 0.0128],
        [0.0000, 0.0000, 0.0000,  ..., 0.2304, 0.0000, 0.0128],
        [0.0000, 0.0000, 0.0000,  ..., 0.2304, 0.0000, 0.0128]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2252889.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1840.2495, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5847.1548, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13827.4248, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-382.4880, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.0392e-02],
        [-2.1983e+00],
        [-4.8760e+00],
        ...,
        [-1.1775e+01],
        [-1.1735e+01],
        [-1.1651e+01]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-924355.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(512.5269, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(512.5269, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.2323e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-9.6037e-03, -6.2323e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-1.0288e-02, -5.5568e-03, -3.6360e-07,  ..., -9.1635e-04,
         -6.2747e-03, -1.5704e-02],
        ...,
        [-9.6037e-03, -6.2323e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-9.6037e-03, -6.2323e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-9.6037e-03, -6.2323e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-105671.3750, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-983.8074, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(63.2220, device='cuda:0')



h[100].sum tensor(20.6449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-75.3599, device='cuda:0')



h[200].sum tensor(-456.6248, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.6018, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(234989.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0111, 0.0000, 0.0210],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0253],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0398],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2293, 0.0000, 0.0123],
        [0.0000, 0.0000, 0.0000,  ..., 0.2293, 0.0000, 0.0123],
        [0.0000, 0.0000, 0.0000,  ..., 0.2293, 0.0000, 0.0123]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2132532.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1739.0822, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5724.6270, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10132.0254, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-545.2373, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.2933],
        [  1.2518],
        [  0.9900],
        ...,
        [-11.7878],
        [-11.7458],
        [-11.6232]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-993992.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(569.4043, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(569.4043, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-105279.4375, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.3101, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(70.2380, device='cuda:0')



h[100].sum tensor(24.2061, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-83.7230, device='cuda:0')



h[200].sum tensor(-452.7875, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.8881, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(242533.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1591, 0.0000, 0.0113],
        [0.0000, 0.0000, 0.0000,  ..., 0.1992, 0.0000, 0.0107],
        [0.0000, 0.0000, 0.0000,  ..., 0.1906, 0.0000, 0.0099],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2283, 0.0000, 0.0116],
        [0.0000, 0.0000, 0.0000,  ..., 0.2199, 0.0000, 0.0111],
        [0.0000, 0.0000, 0.0000,  ..., 0.2022, 0.0000, 0.0100]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2176488., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1657.2030, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5751.4194, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11074.7266, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-668.4011, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -4.4221],
        [ -5.2919],
        [ -5.1073],
        ...,
        [-10.4297],
        [ -9.2245],
        [ -7.9856]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-915001.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(556.8002, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(556.8002, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0117e-02, -5.7254e-03, -2.5273e-07,  ..., -8.1308e-04,
         -6.2076e-03, -1.5445e-02],
        [-9.6037e-03, -6.2325e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-9.6037e-03, -6.2325e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        ...,
        [-9.6037e-03, -6.2325e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-9.6037e-03, -6.2325e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-9.6037e-03, -6.2325e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-105963.1250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.2249, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(68.6832, device='cuda:0')



h[100].sum tensor(23.4271, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-81.8697, device='cuda:0')



h[200].sum tensor(-452.7325, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.3815, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(239742.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0900, 0.0000, 0.0026],
        [0.0000, 0.0000, 0.0000,  ..., 0.1339, 0.0000, 0.0051],
        [0.0000, 0.0000, 0.0000,  ..., 0.1873, 0.0000, 0.0090],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2286, 0.0000, 0.0113],
        [0.0000, 0.0000, 0.0000,  ..., 0.2286, 0.0000, 0.0113],
        [0.0000, 0.0000, 0.0000,  ..., 0.2286, 0.0000, 0.0113]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2165630., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1792.4784, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5705.8193, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11577.3848, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-529.8492, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -1.1883],
        [ -2.9138],
        [ -4.7965],
        ...,
        [-11.8431],
        [-11.8366],
        [-11.8373]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1008407.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(573.3396, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(573.3396, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-106324.6406, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.3480, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(70.7234, device='cuda:0')



h[100].sum tensor(24.3188, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-84.3016, device='cuda:0')



h[200].sum tensor(-450.6149, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.0463, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(239081.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2266, 0.0000, 0.0104],
        [0.0000, 0.0000, 0.0000,  ..., 0.2266, 0.0000, 0.0104],
        [0.0000, 0.0000, 0.0000,  ..., 0.2261, 0.0000, 0.0105],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2276, 0.0000, 0.0104],
        [0.0000, 0.0000, 0.0000,  ..., 0.2276, 0.0000, 0.0104],
        [0.0000, 0.0000, 0.0000,  ..., 0.2276, 0.0000, 0.0104]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2167074.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1905.2731, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5673.4009, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11612.2637, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-442.7534, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -9.8957],
        [ -9.7661],
        [ -9.3487],
        ...,
        [-11.8109],
        [-11.8044],
        [-11.8035]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-981831.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 510.0 event: 7650 loss: tensor(438.1641, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(638.0793, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(638.0793, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1581e-02, -4.2809e-03, -9.0020e-07,  ..., -1.7005e-03,
         -6.7837e-03, -1.7665e-02],
        [-1.1273e-02, -4.5842e-03, -7.6028e-07,  ..., -1.5142e-03,
         -6.6627e-03, -1.7199e-02],
        [-1.3030e-02, -2.8502e-03, -1.5601e-06,  ..., -2.5793e-03,
         -7.3541e-03, -1.9863e-02],
        ...,
        [-9.6037e-03, -6.2326e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-9.6037e-03, -6.2326e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-9.6037e-03, -6.2326e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-106142.8594, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.0129, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(78.7093, device='cuda:0')



h[100].sum tensor(28.7014, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-93.8207, device='cuda:0')



h[200].sum tensor(-446.1597, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.6486, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(244931.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0972],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0983],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1132],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2265, 0.0000, 0.0098],
        [0.0000, 0.0000, 0.0000,  ..., 0.2265, 0.0000, 0.0098],
        [0.0000, 0.0000, 0.0000,  ..., 0.2265, 0.0000, 0.0098]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2193629.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1944.3911, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5641.1284, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11549.8945, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-439.4943, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.1340],
        [  0.1077],
        [  0.0710],
        ...,
        [-11.7645],
        [-11.7586],
        [-11.7594]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-904054.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2732],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(766.1799, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2732],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(766.1799, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0601e-02, -5.2487e-03, -4.3650e-07,  ..., -1.1061e-03,
         -6.3978e-03, -1.6178e-02],
        [-1.0257e-02, -5.5877e-03, -2.8614e-07,  ..., -8.9793e-04,
         -6.2627e-03, -1.5657e-02],
        [-1.0180e-02, -5.6637e-03, -2.5242e-07,  ..., -8.5124e-04,
         -6.2324e-03, -1.5540e-02],
        ...,
        [-9.6037e-03, -6.2327e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-9.6037e-03, -6.2327e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-9.6037e-03, -6.2327e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-105368.1719, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.1360, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(94.5110, device='cuda:0')



h[100].sum tensor(36.5254, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-112.6561, device='cuda:0')



h[200].sum tensor(-440.7096, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.7978, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(256587.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0186],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0157],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0197],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2265, 0.0000, 0.0092],
        [0.0000, 0.0000, 0.0000,  ..., 0.2265, 0.0000, 0.0092],
        [0.0000, 0.0000, 0.0000,  ..., 0.2265, 0.0000, 0.0092]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2284734.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1955.0459, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5652.3677, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13829.4766, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-393.4157, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.2649],
        [  2.3832],
        [  2.3798],
        ...,
        [-11.7999],
        [-11.7940],
        [-11.7948]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-882462.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4006],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(800.6370, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4006],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(800.6370, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.2327e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-1.0562e-02, -5.2869e-03, -4.0351e-07,  ..., -1.0828e-03,
         -6.3827e-03, -1.6119e-02],
        [-1.0905e-02, -4.9480e-03, -5.4813e-07,  ..., -1.2910e-03,
         -6.5179e-03, -1.6640e-02],
        ...,
        [-9.6037e-03, -6.2327e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-9.6037e-03, -6.2327e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-9.6037e-03, -6.2327e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-105441.1250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.5117, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(98.7614, device='cuda:0')



h[100].sum tensor(39.7791, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-117.7225, device='cuda:0')



h[200].sum tensor(-442.2466, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.1829, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(258007.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0141],
        [0.0000, 0.0000, 0.0000,  ..., 0.0093, 0.0000, 0.0083],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0065],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2291, 0.0000, 0.0087],
        [0.0000, 0.0000, 0.0000,  ..., 0.2291, 0.0000, 0.0087],
        [0.0000, 0.0000, 0.0000,  ..., 0.2291, 0.0000, 0.0087]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2276441.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1963.8369, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5638.0742, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14541.2148, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-322.1526, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.6179],
        [  2.6691],
        [  2.6386],
        ...,
        [-12.0057],
        [-11.9989],
        [-11.9995]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1030411.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(583.8279, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(583.8279, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-107958.4453, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.4819, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(72.0172, device='cuda:0')



h[100].sum tensor(26.7262, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-85.8437, device='cuda:0')



h[200].sum tensor(-452.2339, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.4679, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(239472.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2283, 0.0000, 0.0087],
        [0.0000, 0.0000, 0.0000,  ..., 0.2283, 0.0000, 0.0087],
        [0.0000, 0.0000, 0.0000,  ..., 0.2278, 0.0000, 0.0087],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2293, 0.0000, 0.0086],
        [0.0000, 0.0000, 0.0000,  ..., 0.2293, 0.0000, 0.0086],
        [0.0000, 0.0000, 0.0000,  ..., 0.2293, 0.0000, 0.0086]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2162957.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1893.7002, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5652.4561, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10711.6445, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-408.6687, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.8097],
        [-10.7643],
        [-10.6638],
        ...,
        [-12.0093],
        [-12.0027],
        [-12.0034]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-958173.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(723.6144, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(723.6144, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0189e-02, -5.6552e-03, -2.2787e-07,  ..., -8.5668e-04,
         -6.2359e-03, -1.5554e-02],
        [-9.6037e-03, -6.2329e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-9.6037e-03, -6.2329e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        ...,
        [-9.6037e-03, -6.2329e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-9.6037e-03, -6.2329e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-9.6037e-03, -6.2329e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-106979.4297, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.7487, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(89.2603, device='cuda:0')



h[100].sum tensor(36.1525, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-106.3974, device='cuda:0')



h[200].sum tensor(-448.5718, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.0868, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(251144.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1386, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.1792, 0.0000, 0.0053],
        [0.0000, 0.0000, 0.0000,  ..., 0.1906, 0.0000, 0.0067],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2300, 0.0000, 0.0086],
        [0.0000, 0.0000, 0.0000,  ..., 0.2300, 0.0000, 0.0086],
        [0.0000, 0.0000, 0.0000,  ..., 0.2300, 0.0000, 0.0086]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2240156.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1876.3339, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5699.4258, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12159.7402, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-389.4087, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -4.6324],
        [ -5.0470],
        [ -4.4948],
        ...,
        [-12.0411],
        [-12.0345],
        [-12.0352]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-901218.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(537.1479, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(537.1479, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0894e-02, -4.9593e-03, -4.8297e-07,  ..., -1.2844e-03,
         -6.5135e-03, -1.6623e-02],
        [-9.6037e-03, -6.2329e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-1.0200e-02, -5.6448e-03, -2.2301e-07,  ..., -8.6310e-04,
         -6.2401e-03, -1.5570e-02],
        ...,
        [-9.6037e-03, -6.2329e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-9.6037e-03, -6.2329e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-9.6037e-03, -6.2329e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-109239.0938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.0142, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(66.2591, device='cuda:0')



h[100].sum tensor(25.8672, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-78.9801, device='cuda:0')



h[200].sum tensor(-459.7577, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.5915, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(234954.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0128, 0.0000, 0.0110],
        [0.0000, 0.0000, 0.0000,  ..., 0.0182, 0.0000, 0.0050],
        [0.0000, 0.0000, 0.0000,  ..., 0.0422, 0.0000, 0.0012],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2326, 0.0000, 0.0089],
        [0.0000, 0.0000, 0.0000,  ..., 0.2326, 0.0000, 0.0089],
        [0.0000, 0.0000, 0.0000,  ..., 0.2326, 0.0000, 0.0089]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2122652.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1875.1384, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5583.1226, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9592.8789, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-402.2768, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.3447],
        [  2.2671],
        [  1.8532],
        ...,
        [-12.1623],
        [-12.1871],
        [-12.1931]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1115905.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(744.9062, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(744.9062, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1337e-02, -4.5229e-03, -6.2334e-07,  ..., -1.5527e-03,
         -6.6877e-03, -1.7294e-02],
        [-9.6037e-03, -6.2329e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-9.6037e-03, -6.2329e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        ...,
        [-9.6037e-03, -6.2329e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-9.6037e-03, -6.2329e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-9.6037e-03, -6.2329e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-107337.2656, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.9686, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(91.8868, device='cuda:0')



h[100].sum tensor(41.5669, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-109.5281, device='cuda:0')



h[200].sum tensor(-451.9155, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.9427, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(254177.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0849, 0.0000, 0.0137],
        [0.0000, 0.0000, 0.0000,  ..., 0.1123, 0.0000, 0.0130],
        [0.0000, 0.0000, 0.0000,  ..., 0.1902, 0.0000, 0.0091],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2307, 0.0000, 0.0074],
        [0.0000, 0.0000, 0.0000,  ..., 0.2307, 0.0000, 0.0074],
        [0.0000, 0.0000, 0.0000,  ..., 0.2307, 0.0000, 0.0074]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2251966.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1884.7883, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5701.6475, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12804.8213, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-373.9088, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -2.2310],
        [ -3.5518],
        [ -4.7385],
        ...,
        [-12.0776],
        [-12.0714],
        [-12.0726]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-913927.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(650.2198, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(650.2198, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0255e-02, -5.5908e-03, -2.2506e-07,  ..., -8.9652e-04,
         -6.2618e-03, -1.5653e-02],
        [-9.6037e-03, -6.2331e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-9.6037e-03, -6.2331e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        ...,
        [-9.6037e-03, -6.2331e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-9.6037e-03, -6.2331e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-9.6037e-03, -6.2331e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-108481.0703, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.1063, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(80.2069, device='cuda:0')



h[100].sum tensor(37.9973, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-95.6058, device='cuda:0')



h[200].sum tensor(-457.7062, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.1366, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(246258.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0602, 0.0000, 0.0043],
        [0.0000, 0.0000, 0.0000,  ..., 0.1258, 0.0000, 0.0035],
        [0.0000, 0.0000, 0.0000,  ..., 0.1893, 0.0000, 0.0047],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2294, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.2294, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.2294, 0.0000, 0.0065]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2204282., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1871.8060, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5723.6919, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11301.9951, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-424.2274, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.3228],
        [ -2.1645],
        [ -4.9473],
        ...,
        [-11.9864],
        [-11.9798],
        [-11.9803]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-826973.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(641.1031, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(641.1031, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-108866.4141, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.0076, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(79.0823, device='cuda:0')



h[100].sum tensor(39.9191, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-94.2653, device='cuda:0')



h[200].sum tensor(-461.3979, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.7702, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(245853.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2236, 0.0000, 0.0046],
        [0.0000, 0.0000, 0.0000,  ..., 0.2281, 0.0000, 0.0053],
        [0.0000, 0.0000, 0.0000,  ..., 0.2293, 0.0000, 0.0059],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2312, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.2312, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.2312, 0.0000, 0.0060]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2202631.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1901.9606, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5702.2954, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12580.4590, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-363.6209, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -7.5184],
        [ -8.7480],
        [ -9.4952],
        ...,
        [-12.1336],
        [-12.1270],
        [-12.1277]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-993647.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5352],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(635.9329, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5352],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(635.9329, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.2334e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-1.1865e-02, -4.0038e-03, -7.2172e-07,  ..., -1.8729e-03,
         -6.8955e-03, -1.8094e-02],
        [-1.2875e-02, -3.0075e-03, -1.0442e-06,  ..., -2.4856e-03,
         -7.2932e-03, -1.9626e-02],
        ...,
        [-9.6037e-03, -6.2334e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-9.6037e-03, -6.2334e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-9.6037e-03, -6.2334e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-109048.3906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.9698, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(78.4445, device='cuda:0')



h[100].sum tensor(41.7630, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-93.5051, device='cuda:0')



h[200].sum tensor(-460.5106, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.5623, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(244967.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0286, 0.0000, 0.0628],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1302],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.2082],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2299, 0.0000, 0.0055],
        [0.0000, 0.0000, 0.0000,  ..., 0.2299, 0.0000, 0.0055],
        [0.0000, 0.0000, 0.0000,  ..., 0.2299, 0.0000, 0.0055]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2188491.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1912.6228, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5721.6460, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11843.6270, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-410.4107, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -0.3559],
        [  0.1051],
        [  0.2131],
        ...,
        [-11.9587],
        [-11.9549],
        [-11.9600]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-901429.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 520.0 event: 7800 loss: tensor(441.0815, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3232],
        [0.4253],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(859.4908, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3232],
        [0.4253],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(859.4908, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0377e-02, -5.4713e-03, -2.3708e-07,  ..., -9.7052e-04,
         -6.3098e-03, -1.5838e-02],
        [-1.0621e-02, -5.2307e-03, -3.1193e-07,  ..., -1.1185e-03,
         -6.4059e-03, -1.6208e-02],
        [-1.0377e-02, -5.4713e-03, -2.3708e-07,  ..., -9.7052e-04,
         -6.3098e-03, -1.5838e-02],
        ...,
        [-9.6037e-03, -6.2335e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-9.6037e-03, -6.2335e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-9.6037e-03, -6.2335e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-106897.5312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-987.0726, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(106.0212, device='cuda:0')



h[100].sum tensor(59.2653, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-126.3761, device='cuda:0')



h[200].sum tensor(-447.1020, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.5486, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(262096.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0633, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0234, 0.0000, 0.0015],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0032],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2280, 0.0000, 0.0055],
        [0.0000, 0.0000, 0.0000,  ..., 0.2280, 0.0000, 0.0055],
        [0.0000, 0.0000, 0.0000,  ..., 0.2167, 0.0000, 0.0051]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2290043., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1942.8795, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5810.4438, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13145.7480, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-444.1680, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.9723],
        [  2.1280],
        [  1.8181],
        ...,
        [-11.4808],
        [-10.5038],
        [ -8.3975]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-663792.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(674.7272, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(674.7272, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1624e-02, -4.2421e-03, -5.9517e-07,  ..., -1.7269e-03,
         -6.8008e-03, -1.7728e-02],
        [-9.6037e-03, -6.2336e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-1.0902e-02, -4.9542e-03, -3.8237e-07,  ..., -1.2888e-03,
         -6.5164e-03, -1.6633e-02],
        ...,
        [-9.6037e-03, -6.2336e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-9.6037e-03, -6.2336e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-9.6037e-03, -6.2336e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-108987.5625, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.3190, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(83.2299, device='cuda:0')



h[100].sum tensor(42.9532, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-99.2092, device='cuda:0')



h[200].sum tensor(-453.9510, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.1217, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(245177.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0594],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0385],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0334],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2309, 0.0000, 0.0050],
        [0.0000, 0.0000, 0.0000,  ..., 0.2309, 0.0000, 0.0050],
        [0.0000, 0.0000, 0.0000,  ..., 0.2309, 0.0000, 0.0050]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2180739., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1996.6791, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5736.0532, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12257.2764, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-456.9737, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.6549],
        [  0.8619],
        [  0.9890],
        ...,
        [-12.1059],
        [-12.0996],
        [-12.1003]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-901839., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(639.1042, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(639.1042, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-109465.5312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.0222, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(78.8357, device='cuda:0')



h[100].sum tensor(37.0751, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-93.9714, device='cuda:0')



h[200].sum tensor(-451.2136, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.6898, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(242947.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2115, 0.0000, 0.0053],
        [0.0000, 0.0000, 0.0000,  ..., 0.2212, 0.0000, 0.0061],
        [0.0000, 0.0000, 0.0000,  ..., 0.2138, 0.0000, 0.0066],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2328, 0.0000, 0.0059],
        [0.0000, 0.0000, 0.0000,  ..., 0.2328, 0.0000, 0.0059],
        [0.0000, 0.0000, 0.0000,  ..., 0.2328, 0.0000, 0.0059]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2166881., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2021.0651, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5709.3647, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11736.2383, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-524.4394, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -8.0375],
        [ -8.7998],
        [ -8.4698],
        ...,
        [-12.2056],
        [-12.1986],
        [-12.1989]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-906597.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(671.5453, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(671.5453, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-109208.5234, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.3165, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(82.8374, device='cuda:0')



h[100].sum tensor(34.4995, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-98.7414, device='cuda:0')



h[200].sum tensor(-444.4689, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.9938, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(246990.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2273, 0.0000, 0.0068],
        [0.0000, 0.0000, 0.0000,  ..., 0.2312, 0.0000, 0.0074],
        [0.0000, 0.0000, 0.0000,  ..., 0.2327, 0.0000, 0.0077],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2341, 0.0000, 0.0077],
        [0.0000, 0.0000, 0.0000,  ..., 0.2341, 0.0000, 0.0077],
        [0.0000, 0.0000, 0.0000,  ..., 0.2341, 0.0000, 0.0077]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2187609.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2014.6807, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5642.2822, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10348.5352, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-610.1117, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -7.7402],
        [ -8.0084],
        [ -7.5281],
        ...,
        [-12.2275],
        [-12.2213],
        [-12.2222]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-828016.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2732],
        [0.2839],
        [0.3171],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(665.2507, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2732],
        [0.2839],
        [0.3171],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(665.2507, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1928e-02, -3.9438e-03, -6.0659e-07,  ..., -1.9112e-03,
         -6.9204e-03, -1.8188e-02],
        [-1.1511e-02, -4.3546e-03, -4.9777e-07,  ..., -1.6583e-03,
         -6.7563e-03, -1.7556e-02],
        [-1.1501e-02, -4.3645e-03, -4.9516e-07,  ..., -1.6522e-03,
         -6.7523e-03, -1.7541e-02],
        ...,
        [-9.6037e-03, -6.2338e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-9.6037e-03, -6.2338e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-9.6037e-03, -6.2338e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-109347.5781, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.2496, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(82.0610, device='cuda:0')



h[100].sum tensor(30.1144, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-97.8158, device='cuda:0')



h[200].sum tensor(-442.6110, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.7408, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(244694.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0817],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0935],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1009],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2373, 0.0000, 0.0097],
        [0.0000, 0.0000, 0.0000,  ..., 0.2373, 0.0000, 0.0097],
        [0.0000, 0.0000, 0.0000,  ..., 0.2373, 0.0000, 0.0097]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2165908.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2025.7028, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5647.5801, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10039.7344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-661.3720, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.4847],
        [  0.4089],
        [  0.3824],
        ...,
        [-12.4116],
        [-12.4048],
        [-12.4055]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-912428.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6079],
        [0.6045],
        [0.6064],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(619.9705, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6079],
        [0.6045],
        [0.6064],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(619.9705, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.5002e-02, -9.1620e-04, -1.3527e-06,  ..., -3.7749e-03,
         -8.1302e-03, -2.2846e-02],
        [-1.3756e-02, -2.1438e-03, -1.0404e-06,  ..., -3.0193e-03,
         -7.6397e-03, -2.0958e-02],
        [-1.2571e-02, -3.3104e-03, -7.4364e-07,  ..., -2.3012e-03,
         -7.1736e-03, -1.9163e-02],
        ...,
        [-9.6037e-03, -6.2339e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-9.6037e-03, -6.2339e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02],
        [-9.6037e-03, -6.2339e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4666e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-109785.1094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.7926, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(76.4755, device='cuda:0')



h[100].sum tensor(23.5457, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-91.1580, device='cuda:0')



h[200].sum tensor(-442.2946, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.9207, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(238762.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1477],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1592],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1299],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2396, 0.0000, 0.0114],
        [0.0000, 0.0000, 0.0000,  ..., 0.2396, 0.0000, 0.0114],
        [0.0000, 0.0000, 0.0000,  ..., 0.2396, 0.0000, 0.0114]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2140379.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2041.6479, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5681.2661, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10675.1719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-693.3729, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -0.2821],
        [ -0.1138],
        [ -0.0234],
        ...,
        [-12.5768],
        [-12.5694],
        [-12.5699]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1039405.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(575.0438, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(575.0438, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-109990.2500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.4001, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(70.9337, device='cuda:0')



h[100].sum tensor(18.6636, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-84.5522, device='cuda:0')



h[200].sum tensor(-441.0690, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.1148, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(238011.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2363, 0.0000, 0.0109],
        [0.0000, 0.0000, 0.0000,  ..., 0.2363, 0.0000, 0.0109],
        [0.0000, 0.0000, 0.0000,  ..., 0.2358, 0.0000, 0.0109],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2372, 0.0000, 0.0109],
        [0.0000, 0.0000, 0.0000,  ..., 0.2372, 0.0000, 0.0109],
        [0.0000, 0.0000, 0.0000,  ..., 0.2372, 0.0000, 0.0109]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2130098.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2051.4504, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5632.7080, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9833.2285, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-718.0044, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.8403],
        [-10.8852],
        [-10.7250],
        ...,
        [-12.4351],
        [-12.4334],
        [-12.4364]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-982351.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(599.7601, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(599.7601, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-109560.6094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.6042, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(73.9825, device='cuda:0')



h[100].sum tensor(18.3347, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-88.1864, device='cuda:0')



h[200].sum tensor(-437.5832, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.1083, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(241303.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2295, 0.0000, 0.0098],
        [0.0000, 0.0000, 0.0000,  ..., 0.2157, 0.0000, 0.0093],
        [0.0000, 0.0000, 0.0000,  ..., 0.1751, 0.0000, 0.0089],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2338, 0.0000, 0.0098],
        [0.0000, 0.0000, 0.0000,  ..., 0.2338, 0.0000, 0.0098],
        [0.0000, 0.0000, 0.0000,  ..., 0.2338, 0.0000, 0.0098]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2145700.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2065.4463, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5632.1299, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9786.5645, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-744.9592, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.0943],
        [ -8.9575],
        [ -7.0576],
        ...,
        [-12.2785],
        [-12.2727],
        [-12.2737]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-849228.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(632.7098, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(632.7098, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-109196.0781, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.9236, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(78.0470, device='cuda:0')



h[100].sum tensor(19.3726, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-93.0312, device='cuda:0')



h[200].sum tensor(-437.6368, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.4328, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(244093.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2324, 0.0000, 0.0098],
        [0.0000, 0.0000, 0.0000,  ..., 0.2324, 0.0000, 0.0098],
        [0.0000, 0.0000, 0.0000,  ..., 0.2319, 0.0000, 0.0098],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2334, 0.0000, 0.0097],
        [0.0000, 0.0000, 0.0000,  ..., 0.2334, 0.0000, 0.0097],
        [0.0000, 0.0000, 0.0000,  ..., 0.2334, 0.0000, 0.0097]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2168133.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2112.4546, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5632.2251, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11149.4434, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-751.4742, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.4533],
        [-10.6426],
        [-10.7537],
        ...,
        [-12.2676],
        [-12.2618],
        [-12.2629]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-891648.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(480.3881, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(480.3881, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-110760.9766, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-983.4926, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(59.2575, device='cuda:0')



h[100].sum tensor(8.8846, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-70.6344, device='cuda:0')



h[200].sum tensor(-446.6487, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.3100, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(231718.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2324, 0.0000, 0.0099],
        [0.0000, 0.0000, 0.0000,  ..., 0.2321, 0.0000, 0.0098],
        [0.0000, 0.0000, 0.0000,  ..., 0.2306, 0.0000, 0.0095],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2334, 0.0000, 0.0099],
        [0.0000, 0.0000, 0.0000,  ..., 0.2334, 0.0000, 0.0099],
        [0.0000, 0.0000, 0.0000,  ..., 0.2334, 0.0000, 0.0099]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2088921.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2137.5315, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5565.9976, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9665.4629, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-794.2725, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.4382],
        [-10.0273],
        [ -9.3533],
        ...,
        [-11.3222],
        [-11.9144],
        [-12.1814]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-994312., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 530.0 event: 7950 loss: tensor(442.6681, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5869],
        [0.6304],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(703.5680, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5869],
        [0.6304],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(703.5680, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1845e-02, -4.0267e-03, -4.5773e-07,  ..., -1.8611e-03,
         -6.8879e-03, -1.8062e-02],
        [-1.1008e-02, -4.8518e-03, -2.8663e-07,  ..., -1.3529e-03,
         -6.5580e-03, -1.6792e-02],
        [-1.1111e-02, -4.7495e-03, -3.0785e-07,  ..., -1.4160e-03,
         -6.5990e-03, -1.6950e-02],
        ...,
        [-9.6037e-03, -6.2342e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        [-9.6037e-03, -6.2342e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        [-9.6037e-03, -6.2342e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-108616.4766, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.5695, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(86.7876, device='cuda:0')



h[100].sum tensor(21.8029, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-103.4499, device='cuda:0')



h[200].sum tensor(-439.1318, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.2810, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(249852.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0753],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0871],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0732],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2327, 0.0000, 0.0098],
        [0.0000, 0.0000, 0.0000,  ..., 0.2327, 0.0000, 0.0098],
        [0.0000, 0.0000, 0.0000,  ..., 0.2327, 0.0000, 0.0098]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2200801., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2186.4087, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5584.3911, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12523.6738, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-766.8649, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.5977],
        [  0.5293],
        [  0.5023],
        ...,
        [-12.2185],
        [-12.2119],
        [-12.2127]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-913121., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(715.4167, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(715.4167, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0585e-02, -5.2682e-03, -1.9220e-07,  ..., -1.0966e-03,
         -6.3916e-03, -1.6152e-02],
        [-9.6037e-03, -6.2342e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        [-9.6037e-03, -6.2342e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        ...,
        [-9.6037e-03, -6.2342e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        [-9.6037e-03, -6.2342e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        [-9.6037e-03, -6.2342e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-108547.2188, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.6991, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(88.2491, device='cuda:0')



h[100].sum tensor(21.3395, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-105.1920, device='cuda:0')



h[200].sum tensor(-439.6523, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.7573, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(250212.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0493, 0.0000, 0.0227],
        [0.0000, 0.0000, 0.0000,  ..., 0.0681, 0.0000, 0.0260],
        [0.0000, 0.0000, 0.0000,  ..., 0.1069, 0.0000, 0.0315],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2321, 0.0000, 0.0096],
        [0.0000, 0.0000, 0.0000,  ..., 0.2321, 0.0000, 0.0096],
        [0.0000, 0.0000, 0.0000,  ..., 0.2321, 0.0000, 0.0096]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2190858.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2182.8643, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5580.6494, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11885.5293, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-792.3458, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.8534],
        [  0.1587],
        [ -0.6366],
        ...,
        [-12.1835],
        [-12.1770],
        [-12.1778]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-862319., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2462],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(765.1541, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2462],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(765.1541, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0972e-02, -4.8866e-03, -2.5728e-07,  ..., -1.3317e-03,
         -6.5443e-03, -1.6739e-02],
        [-1.0193e-02, -5.6544e-03, -1.1069e-07,  ..., -8.5881e-04,
         -6.2373e-03, -1.5557e-02],
        [-9.6037e-03, -6.2342e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        ...,
        [-9.6037e-03, -6.2342e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        [-9.6037e-03, -6.2342e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        [-9.6037e-03, -6.2342e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-108194.8672, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.1251, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(94.3844, device='cuda:0')



h[100].sum tensor(23.2845, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-112.5052, device='cuda:0')



h[200].sum tensor(-439.4221, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.7566, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(252656.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0226, 0.0000, 0.0365],
        [0.0000, 0.0000, 0.0000,  ..., 0.0846, 0.0000, 0.0202],
        [0.0000, 0.0000, 0.0000,  ..., 0.1629, 0.0000, 0.0118],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2334, 0.0000, 0.0101],
        [0.0000, 0.0000, 0.0000,  ..., 0.2334, 0.0000, 0.0101],
        [0.0000, 0.0000, 0.0000,  ..., 0.2334, 0.0000, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2191907., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2167.0630, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5530.9146, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11631.0723, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-748.9417, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -0.4547],
        [ -2.6192],
        [ -5.1736],
        ...,
        [-12.3066],
        [-12.2997],
        [-12.3004]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-950403.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4011],
        [0.3823],
        [0.4502],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(593.4536, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4011],
        [0.3823],
        [0.4502],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(593.4536, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1622e-02, -4.2473e-03, -3.6390e-07,  ..., -1.7254e-03,
         -6.7998e-03, -1.7722e-02],
        [-1.1640e-02, -4.2295e-03, -3.6716e-07,  ..., -1.7364e-03,
         -6.8070e-03, -1.7750e-02],
        [-1.0518e-02, -5.3339e-03, -1.6489e-07,  ..., -1.0562e-03,
         -6.3654e-03, -1.6050e-02],
        ...,
        [-9.6037e-03, -6.2343e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        [-9.6037e-03, -6.2343e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        [-9.6037e-03, -6.2343e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-109902.4375, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.5107, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(73.2046, device='cuda:0')



h[100].sum tensor(11.3632, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-87.2591, device='cuda:0')



h[200].sum tensor(-447.1352, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.8548, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(236631.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0393],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0391],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0341],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2342, 0.0000, 0.0101],
        [0.0000, 0.0000, 0.0000,  ..., 0.2342, 0.0000, 0.0101],
        [0.0000, 0.0000, 0.0000,  ..., 0.2342, 0.0000, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2086304., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2123.7253, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5521.7275, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9239.4463, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-783.5207, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.7492],
        [  2.0850],
        [  1.7190],
        ...,
        [-12.3869],
        [-12.3186],
        [-12.0199]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-990638.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4116],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(803.1136, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4116],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(803.1136, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1576e-02, -4.2924e-03, -3.4114e-07,  ..., -1.6978e-03,
         -6.7819e-03, -1.7653e-02],
        [-1.0588e-02, -5.2650e-03, -1.7028e-07,  ..., -1.0987e-03,
         -6.3930e-03, -1.6156e-02],
        [-1.0638e-02, -5.2161e-03, -1.7886e-07,  ..., -1.1288e-03,
         -6.4126e-03, -1.6232e-02],
        ...,
        [-9.6037e-03, -6.2343e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        [-9.6037e-03, -6.2343e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        [-9.6037e-03, -6.2343e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-107700.0625, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.5439, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(99.0669, device='cuda:0')



h[100].sum tensor(24.0962, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-118.0867, device='cuda:0')



h[200].sum tensor(-437.6417, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.2824, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(256807.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0925],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0745],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0662],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2347, 0.0000, 0.0102],
        [0.0000, 0.0000, 0.0000,  ..., 0.2347, 0.0000, 0.0102],
        [0.0000, 0.0000, 0.0000,  ..., 0.2347, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2214610.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2100.8257, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5571.0249, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11898.1016, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-716.5558, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.5608],
        [  0.5787],
        [  0.5934],
        ...,
        [-12.4575],
        [-12.4499],
        [-12.4384]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-923430.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(560.9187, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(560.9187, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.2343e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        [-1.0147e-02, -5.6996e-03, -9.3925e-08,  ..., -8.3101e-04,
         -6.2193e-03, -1.5488e-02],
        [-1.0147e-02, -5.6996e-03, -9.3925e-08,  ..., -8.3101e-04,
         -6.2193e-03, -1.5488e-02],
        ...,
        [-9.6037e-03, -6.2343e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        [-9.6037e-03, -6.2343e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        [-9.6037e-03, -6.2343e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-110124.2812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.2637, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(69.1913, device='cuda:0')



h[100].sum tensor(8.5934, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-82.4753, device='cuda:0')



h[200].sum tensor(-447.7673, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.5470, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(236448.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1070, 0.0000, 0.0095],
        [0.0000, 0.0000, 0.0000,  ..., 0.0772, 0.0000, 0.0088],
        [0.0000, 0.0000, 0.0000,  ..., 0.0997, 0.0000, 0.0073],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2347, 0.0000, 0.0102],
        [0.0000, 0.0000, 0.0000,  ..., 0.2347, 0.0000, 0.0102],
        [0.0000, 0.0000, 0.0000,  ..., 0.2347, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2076672.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2067.8269, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5498.2026, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8004.3535, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-767.8854, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -1.3314],
        [ -0.2008],
        [ -0.7771],
        ...,
        [-12.4575],
        [-12.4507],
        [-12.4515]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1001913.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(587.1412, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(587.1412, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-109851.6562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.5201, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(72.4259, device='cuda:0')



h[100].sum tensor(10.3369, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-86.3309, device='cuda:0')



h[200].sum tensor(-446.6286, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.6011, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(238960.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2306, 0.0000, 0.0091],
        [0.0000, 0.0000, 0.0000,  ..., 0.2244, 0.0000, 0.0086],
        [0.0000, 0.0000, 0.0000,  ..., 0.2003, 0.0000, 0.0084],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2347, 0.0000, 0.0102],
        [0.0000, 0.0000, 0.0000,  ..., 0.2347, 0.0000, 0.0102],
        [0.0000, 0.0000, 0.0000,  ..., 0.2347, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2096146.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2072.2261, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5514.5537, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8605.6602, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-764.0392, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -8.9421],
        [ -7.7496],
        [ -6.0106],
        ...,
        [-12.4575],
        [-12.4507],
        [-12.4515]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-989443.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6479],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(664.1135, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6479],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(664.1135, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.2343e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        [-1.1153e-02, -4.7086e-03, -2.5707e-07,  ..., -1.4415e-03,
         -6.6155e-03, -1.7012e-02],
        [-9.6037e-03, -6.2343e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        ...,
        [-9.6037e-03, -6.2343e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        [-9.6037e-03, -6.2343e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        [-9.6037e-03, -6.2343e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-109081.3438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.2065, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(81.9207, device='cuda:0')



h[100].sum tensor(14.1208, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-97.6486, device='cuda:0')



h[200].sum tensor(-444.7471, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.6951, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(243471.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0403],
        [0.0000, 0.0000, 0.0000,  ..., 0.0662, 0.0000, 0.0259],
        [0.0000, 0.0000, 0.0000,  ..., 0.1155, 0.0000, 0.0214],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2363, 0.0000, 0.0107],
        [0.0000, 0.0000, 0.0000,  ..., 0.2363, 0.0000, 0.0107],
        [0.0000, 0.0000, 0.0000,  ..., 0.2363, 0.0000, 0.0107]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2114170.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2040.3074, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5532.1860, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9305.0664, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-717.2221, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.3044],
        [ -0.5601],
        [ -3.3123],
        ...,
        [-12.5945],
        [-12.5877],
        [-12.5885]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1039701.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(654.2939, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(654.2939, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-109083.9453, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.1083, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(80.7094, device='cuda:0')



h[100].sum tensor(13.2236, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-96.2048, device='cuda:0')



h[200].sum tensor(-445.9725, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.3004, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(243534.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2328, 0.0000, 0.0107],
        [0.0000, 0.0000, 0.0000,  ..., 0.2139, 0.0000, 0.0110],
        [0.0000, 0.0000, 0.0000,  ..., 0.1697, 0.0000, 0.0107],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2367, 0.0000, 0.0106],
        [0.0000, 0.0000, 0.0000,  ..., 0.2367, 0.0000, 0.0106],
        [0.0000, 0.0000, 0.0000,  ..., 0.2367, 0.0000, 0.0106]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2115082.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2019.0258, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5520.7598, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9597.8379, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-690.3454, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -9.3589],
        [ -7.6354],
        [ -5.1662],
        ...,
        [-12.6528],
        [-12.6459],
        [-12.6466]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1114593.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2446],
        [0.2413],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(620.4393, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2446],
        [0.2413],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(620.4393, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2250e-02, -3.6289e-03, -4.0366e-07,  ..., -2.1066e-03,
         -7.0473e-03, -1.8673e-02],
        [-1.1532e-02, -4.3360e-03, -2.9411e-07,  ..., -1.6711e-03,
         -6.7646e-03, -1.7586e-02],
        [-1.0807e-02, -5.0501e-03, -1.8348e-07,  ..., -1.2312e-03,
         -6.4790e-03, -1.6487e-02],
        ...,
        [-9.6037e-03, -6.2344e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        [-9.6037e-03, -6.2344e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        [-9.6037e-03, -6.2344e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-108999.0625, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.8221, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(76.5333, device='cuda:0')



h[100].sum tensor(12.2944, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-91.2269, device='cuda:0')



h[200].sum tensor(-445.9402, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.9396, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(244097.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0490],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0473],
        [0.0000, 0.0000, 0.0000,  ..., 0.0167, 0.0000, 0.0319],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2330, 0.0000, 0.0080],
        [0.0000, 0.0000, 0.0000,  ..., 0.2330, 0.0000, 0.0080],
        [0.0000, 0.0000, 0.0000,  ..., 0.2330, 0.0000, 0.0080]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2110208.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2252.3965, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5741.1982, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9647.9473, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-850.2567, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.1460],
        [  2.1284],
        [  1.9356],
        ...,
        [-12.4397],
        [-12.4328],
        [-12.4335]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-880914.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 540.0 event: 8100 loss: tensor(351.0427, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2759],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(603.2545, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2759],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(603.2545, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0264e-02, -5.5848e-03, -9.6480e-08,  ..., -9.0183e-04,
         -6.2652e-03, -1.5664e-02],
        [-9.6037e-03, -6.2344e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        [-1.0919e-02, -4.9400e-03, -1.9227e-07,  ..., -1.2991e-03,
         -6.5231e-03, -1.6656e-02],
        ...,
        [-9.6037e-03, -6.2344e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        [-9.6037e-03, -6.2344e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        [-9.6037e-03, -6.2344e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-109396.5234, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.6206, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(74.4135, device='cuda:0')



h[100].sum tensor(11.9325, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-88.7002, device='cuda:0')



h[200].sum tensor(-448.5547, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.2488, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(241528.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1272, 0.0000, 0.0079],
        [0.0000, 0.0000, 0.0000,  ..., 0.0419, 0.0000, 0.0141],
        [0.0000, 0.0000, 0.0000,  ..., 0.0232, 0.0000, 0.0195],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2327, 0.0000, 0.0067],
        [0.0000, 0.0000, 0.0000,  ..., 0.2327, 0.0000, 0.0067],
        [0.0000, 0.0000, 0.0000,  ..., 0.2327, 0.0000, 0.0067]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2113026.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2050.4585, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5555.8145, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10573.6816, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-662.0520, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -1.3339],
        [  1.0554],
        [  2.4015],
        ...,
        [-12.4673],
        [-12.4602],
        [-12.4608]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-950409.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3740],
        [0.4832],
        [0.4106],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(593.3538, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3740],
        [0.4832],
        [0.4106],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(593.3538, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2135e-02, -3.7424e-03, -3.5483e-07,  ..., -2.0368e-03,
         -7.0020e-03, -1.8499e-02],
        [-1.1963e-02, -3.9116e-03, -3.3074e-07,  ..., -1.9326e-03,
         -6.9343e-03, -1.8238e-02],
        [-1.1994e-02, -3.8813e-03, -3.3505e-07,  ..., -1.9513e-03,
         -6.9464e-03, -1.8285e-02],
        ...,
        [-9.6037e-03, -6.2344e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        [-9.6037e-03, -6.2344e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        [-9.6037e-03, -6.2344e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-109598.1953, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.5187, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(73.1922, device='cuda:0')



h[100].sum tensor(12.0947, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-87.2444, device='cuda:0')



h[200].sum tensor(-448.9655, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.8508, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(238529.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1136],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1168],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1067],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2309, 0.0000, 0.0059],
        [0.0000, 0.0000, 0.0000,  ..., 0.2280, 0.0000, 0.0056],
        [0.0000, 0.0000, 0.0000,  ..., 0.2260, 0.0000, 0.0054]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2085686.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2064.6355, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5555.1123, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10404.2812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-652.2559, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.4113],
        [  0.3911],
        [  0.4876],
        ...,
        [-11.2426],
        [-10.8034],
        [-10.6739]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-957724.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(701.5530, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(701.5530, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-108630.3906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.5344, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(86.5390, device='cuda:0')



h[100].sum tensor(19.0539, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-103.1536, device='cuda:0')



h[200].sum tensor(-443.0391, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.2001, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(247224.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2299, 0.0000, 0.0058],
        [0.0000, 0.0000, 0.0000,  ..., 0.2299, 0.0000, 0.0058],
        [0.0000, 0.0000, 0.0000,  ..., 0.2294, 0.0000, 0.0058],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2309, 0.0000, 0.0058],
        [0.0000, 0.0000, 0.0000,  ..., 0.2309, 0.0000, 0.0058],
        [0.0000, 0.0000, 0.0000,  ..., 0.2309, 0.0000, 0.0058]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2144674.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2072.6755, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5629.2651, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11860.8232, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-644.4649, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -9.1294],
        [ -9.5621],
        [ -9.7817],
        ...,
        [-12.2474],
        [-11.8469],
        [-10.9291]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-848475.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(587.9990, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(587.9990, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-109800.2266, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.5138, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(72.5317, device='cuda:0')



h[100].sum tensor(12.5982, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-86.4571, device='cuda:0')



h[200].sum tensor(-446.0990, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.6356, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(239398.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1819, 0.0000, 0.0070],
        [0.0000, 0.0000, 0.0000,  ..., 0.2106, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.1780, 0.0000, 0.0122],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2311, 0.0000, 0.0059],
        [0.0000, 0.0000, 0.0000,  ..., 0.2311, 0.0000, 0.0059],
        [0.0000, 0.0000, 0.0000,  ..., 0.2311, 0.0000, 0.0059]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2099002.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2082.4460, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5571.8008, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10635.1953, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-652.6166, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -3.9942],
        [ -5.5076],
        [ -5.1945],
        ...,
        [-12.3979],
        [-12.3908],
        [-12.3915]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-879245., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(662.9185, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(662.9185, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0209e-02, -5.6390e-03, -7.4628e-08,  ..., -8.6854e-04,
         -6.2436e-03, -1.5581e-02],
        [-9.6037e-03, -6.2344e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        [-9.6037e-03, -6.2344e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        ...,
        [-9.6037e-03, -6.2344e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        [-9.6037e-03, -6.2344e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        [-9.6037e-03, -6.2344e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-109197.1094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.2028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(81.7733, device='cuda:0')



h[100].sum tensor(17.5429, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-97.4729, device='cuda:0')



h[200].sum tensor(-442.6894, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.6471, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(244311.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1207, 0.0000, 0.0079],
        [0.0000, 0.0000, 0.0000,  ..., 0.1620, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.2022, 0.0000, 0.0067],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2333, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.2333, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.2333, 0.0000, 0.0063]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2126407., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2110.1792, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5530.0723, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11933.5645, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-597.1998, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -1.9275],
        [ -4.0138],
        [ -5.9517],
        ...,
        [-12.5882],
        [-12.5805],
        [-12.5809]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1022444.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4956],
        [0.3542],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(600.4810, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4956],
        [0.3542],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(600.4810, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1430e-02, -4.4372e-03, -2.1581e-07,  ..., -1.6090e-03,
         -6.7242e-03, -1.7429e-02],
        [-1.0451e-02, -5.4005e-03, -1.0014e-07,  ..., -1.0155e-03,
         -6.3390e-03, -1.5948e-02],
        [-1.1556e-02, -4.3130e-03, -2.3072e-07,  ..., -1.6854e-03,
         -6.7739e-03, -1.7620e-02],
        ...,
        [-9.6037e-03, -6.2344e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        [-9.6037e-03, -6.2344e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        [-9.6037e-03, -6.2344e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-109803.9688, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.6307, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(74.0714, device='cuda:0')



h[100].sum tensor(14.7022, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-88.2923, device='cuda:0')



h[200].sum tensor(-440.9249, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.1373, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(237272.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0262],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0353],
        [0.0000, 0.0000, 0.0000,  ..., 0.0034, 0.0000, 0.0270],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2324, 0.0000, 0.0070],
        [0.0000, 0.0000, 0.0000,  ..., 0.2324, 0.0000, 0.0070],
        [0.0000, 0.0000, 0.0000,  ..., 0.2324, 0.0000, 0.0070]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2064996.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2080.5583, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5517.4771, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9140.7412, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-658.5499, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.7845],
        [  2.6564],
        [  2.2575],
        ...,
        [-12.5084],
        [-12.4975],
        [-12.4353]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-920471.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(498.6869, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(498.6869, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-110773.1797, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-983.6804, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(61.5148, device='cuda:0')



h[100].sum tensor(8.9570, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-73.3250, device='cuda:0')



h[200].sum tensor(-440.1094, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.0455, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(230777.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2256, 0.0000, 0.0082],
        [0.0000, 0.0000, 0.0000,  ..., 0.1989, 0.0000, 0.0095],
        [0.0000, 0.0000, 0.0000,  ..., 0.1215, 0.0000, 0.0158],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2321, 0.0000, 0.0078],
        [0.0000, 0.0000, 0.0000,  ..., 0.2321, 0.0000, 0.0078],
        [0.0000, 0.0000, 0.0000,  ..., 0.2321, 0.0000, 0.0078]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2031232., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2059.5750, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5509.6738, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7809.0391, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-704.2209, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -8.7500],
        [ -6.4738],
        [ -3.4239],
        ...,
        [-12.5274],
        [-12.5201],
        [-12.5207]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-888767.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(588.6569, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(588.6569, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.2345e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        [-9.6037e-03, -6.2345e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        [-1.0894e-02, -4.9645e-03, -1.3993e-07,  ..., -1.2841e-03,
         -6.5134e-03, -1.6618e-02],
        ...,
        [-9.6037e-03, -6.2345e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        [-9.6037e-03, -6.2345e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        [-9.6037e-03, -6.2345e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-109739.8672, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.5118, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(72.6129, device='cuda:0')



h[100].sum tensor(15.4564, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-86.5538, device='cuda:0')



h[200].sum tensor(-434.5369, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.6620, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(239545.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1845, 0.0000, 0.0099],
        [0.0000, 0.0000, 0.0000,  ..., 0.0946, 0.0000, 0.0141],
        [0.0000, 0.0000, 0.0000,  ..., 0.0623, 0.0000, 0.0145],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2338, 0.0000, 0.0093],
        [0.0000, 0.0000, 0.0000,  ..., 0.2338, 0.0000, 0.0093],
        [0.0000, 0.0000, 0.0000,  ..., 0.2338, 0.0000, 0.0093]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2076826.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2306.0366, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5349.4575, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9697.5840, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-939.4765, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -4.7348],
        [ -1.6728],
        [  0.4806],
        ...,
        [-12.6762],
        [-12.6684],
        [-12.6689]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-983726., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(700.0138, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(700.0138, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-108607.4844, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.5776, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(86.3491, device='cuda:0')



h[100].sum tensor(22.7157, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-102.9273, device='cuda:0')



h[200].sum tensor(-429.7574, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.1382, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(249202.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1948, 0.0000, 0.0098],
        [0.0000, 0.0000, 0.0000,  ..., 0.2260, 0.0000, 0.0085],
        [0.0000, 0.0000, 0.0000,  ..., 0.2308, 0.0000, 0.0089],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2338, 0.0000, 0.0093],
        [0.0000, 0.0000, 0.0000,  ..., 0.2338, 0.0000, 0.0093],
        [0.0000, 0.0000, 0.0000,  ..., 0.2338, 0.0000, 0.0093]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2146825.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2321.5547, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5394.7788, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11781.2061, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-915.2219, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -5.0552],
        [ -7.4884],
        [ -8.9630],
        ...,
        [-12.6760],
        [-12.6684],
        [-12.6688]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-937513.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(590.5974, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(590.5974, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1761e-02, -4.1114e-03, -2.2404e-07,  ..., -1.8097e-03,
         -6.8545e-03, -1.7930e-02],
        [-1.1085e-02, -4.7769e-03, -1.5381e-07,  ..., -1.3997e-03,
         -6.5884e-03, -1.6906e-02],
        [-1.0958e-02, -4.9018e-03, -1.4064e-07,  ..., -1.3228e-03,
         -6.5385e-03, -1.6714e-02],
        ...,
        [-9.6037e-03, -6.2345e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        [-9.6037e-03, -6.2345e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        [-9.6037e-03, -6.2345e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-109888.5391, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.5485, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(72.8522, device='cuda:0')



h[100].sum tensor(15.7529, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-86.8391, device='cuda:0')



h[200].sum tensor(-431.3381, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.7400, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(239053., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0369],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0414],
        [0.0000, 0.0000, 0.0000,  ..., 0.0167, 0.0000, 0.0290],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2343, 0.0000, 0.0102],
        [0.0000, 0.0000, 0.0000,  ..., 0.2343, 0.0000, 0.0102],
        [0.0000, 0.0000, 0.0000,  ..., 0.2343, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2096213., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2075.7556, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5525.4683, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10230.7090, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-708.3162, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.4333],
        [  2.4939],
        [  1.4849],
        ...,
        [-12.7285],
        [-12.7205],
        [-12.7209]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1003377.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 550.0 event: 8250 loss: tensor(390.4316, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2654],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(593.8440, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2654],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(593.8440, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.2345e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        [-1.0238e-02, -5.6097e-03, -6.3129e-08,  ..., -8.8660e-04,
         -6.2553e-03, -1.5625e-02],
        [-9.6037e-03, -6.2345e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        ...,
        [-9.6037e-03, -6.2345e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        [-9.6037e-03, -6.2345e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02],
        [-9.6037e-03, -6.2345e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4665e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-109972.9766, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.5428, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(73.2527, device='cuda:0')



h[100].sum tensor(15.0097, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-87.3165, device='cuda:0')



h[200].sum tensor(-424.5728, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.8705, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(239997.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0832],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0755],
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0558],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2319, 0.0000, 0.0091],
        [0.0000, 0.0000, 0.0000,  ..., 0.2319, 0.0000, 0.0091],
        [0.0000, 0.0000, 0.0000,  ..., 0.2319, 0.0000, 0.0091]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2093564.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2062.6003, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5524.0864, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9093.8047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-769.2609, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.5145],
        [  0.5836],
        [  0.6044],
        ...,
        [-12.5597],
        [-12.5524],
        [-12.5530]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-842541.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(631.7926, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(631.7926, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-109638.0391, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.9351, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(77.9338, device='cuda:0')



h[100].sum tensor(17.9557, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-92.8963, device='cuda:0')



h[200].sum tensor(-419.9100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.3959, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(244165.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1002, 0.0000, 0.0229],
        [0.0000, 0.0000, 0.0000,  ..., 0.0540, 0.0000, 0.0279],
        [0.0000, 0.0000, 0.0000,  ..., 0.1000, 0.0000, 0.0229],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2331, 0.0000, 0.0103],
        [0.0000, 0.0000, 0.0000,  ..., 0.2331, 0.0000, 0.0103],
        [0.0000, 0.0000, 0.0000,  ..., 0.2331, 0.0000, 0.0103]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2124276., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2074.2510, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5518.9658, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10302.5322, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-786.9490, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -2.1196],
        [ -1.6056],
        [ -2.6032],
        ...,
        [-12.6411],
        [-12.6335],
        [-12.6338]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-948888.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(662.1312, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(662.1312, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-109434.3281, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.1972, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(81.6762, device='cuda:0')



h[100].sum tensor(20.4872, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-97.3572, device='cuda:0')



h[200].sum tensor(-415.7087, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.6154, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(244041.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0928, 0.0000, 0.0252],
        [0.0000, 0.0000, 0.0000,  ..., 0.1367, 0.0000, 0.0187],
        [0.0000, 0.0000, 0.0000,  ..., 0.1320, 0.0000, 0.0193],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2346, 0.0000, 0.0114],
        [0.0000, 0.0000, 0.0000,  ..., 0.2346, 0.0000, 0.0114],
        [0.0000, 0.0000, 0.0000,  ..., 0.2346, 0.0000, 0.0114]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2118458.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2077.0715, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5516.5571, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10436.9453, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-816.2092, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.1875],
        [ -0.0889],
        [ -0.2270],
        ...,
        [-12.7198],
        [-12.7123],
        [-12.7135]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1026280.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2837],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(667.3877, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2837],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(667.3877, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.2345e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02],
        [-1.0859e-02, -4.9989e-03, -1.0955e-07,  ..., -1.2630e-03,
         -6.4997e-03, -1.6564e-02],
        [-1.0181e-02, -5.6667e-03, -5.0341e-08,  ..., -8.5153e-04,
         -6.2326e-03, -1.5538e-02],
        ...,
        [-9.6037e-03, -6.2345e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02],
        [-9.6037e-03, -6.2345e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02],
        [-9.6037e-03, -6.2345e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-109470.7188, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.2164, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(82.3246, device='cuda:0')



h[100].sum tensor(20.6232, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-98.1301, device='cuda:0')



h[200].sum tensor(-409.6171, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.8267, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(247293.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0825, 0.0000, 0.0135],
        [0.0000, 0.0000, 0.0000,  ..., 0.0536, 0.0000, 0.0141],
        [0.0000, 0.0000, 0.0000,  ..., 0.0182, 0.0000, 0.0197],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2328, 0.0000, 0.0082],
        [0.0000, 0.0000, 0.0000,  ..., 0.2328, 0.0000, 0.0082],
        [0.0000, 0.0000, 0.0000,  ..., 0.2328, 0.0000, 0.0082]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2154381.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2096.3337, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5503.3057, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10877.9785, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-836.8801, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -0.7637],
        [  1.0494],
        [  1.5195],
        ...,
        [-12.5345],
        [-12.5229],
        [-12.5181]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-895904.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(541.2347, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(541.2347, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-111052.5703, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.0627, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(66.7632, device='cuda:0')



h[100].sum tensor(13.3809, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-79.5810, device='cuda:0')



h[200].sum tensor(-413.2877, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.7558, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(234162.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2323, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.2323, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.2318, 0.0000, 0.0062],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2333, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.2333, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.2333, 0.0000, 0.0062]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2039191., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2115.5679, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5407.4468, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8168.4644, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-865.2639, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.8311],
        [-10.5102],
        [ -9.9266],
        ...,
        [-12.6394],
        [-12.6319],
        [-12.6324]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-999838.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2849],
        ...,
        [0.0000],
        [0.3513],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(572.4608, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2849],
        ...,
        [0.0000],
        [0.3513],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(572.4608, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.2345e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02],
        [-1.0285e-02, -5.5638e-03, -5.4459e-08,  ..., -9.1493e-04,
         -6.2737e-03, -1.5696e-02],
        [-1.0119e-02, -5.7277e-03, -4.1151e-08,  ..., -8.1395e-04,
         -6.2082e-03, -1.5444e-02],
        ...,
        [-1.1011e-02, -4.8496e-03, -1.1246e-07,  ..., -1.3550e-03,
         -6.5594e-03, -1.6794e-02],
        [-1.0171e-02, -5.6766e-03, -4.5304e-08,  ..., -8.4546e-04,
         -6.2286e-03, -1.5522e-02],
        [-1.0444e-02, -5.4075e-03, -6.7152e-08,  ..., -1.0112e-03,
         -6.3363e-03, -1.5936e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-111051.4922, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.3764, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(70.6150, device='cuda:0')



h[100].sum tensor(16.3956, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-84.1724, device='cuda:0')



h[200].sum tensor(-410.0562, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.0110, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(236545.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0619, 0.0005, 0.0128],
        [0.0000, 0.0000, 0.0000,  ..., 0.0525, 0.0004, 0.0095],
        [0.0000, 0.0000, 0.0000,  ..., 0.0268, 0.0002, 0.0098],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0615, 0.0005, 0.0074],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0001, 0.0130],
        [0.0000, 0.0000, 0.0000,  ..., 0.0847, 0.0007, 0.0094]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2060029.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2161.9211, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5427.6890, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9459.5752, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-875.8221, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 2.2246],
        [ 2.3697],
        [ 2.2713],
        ...,
        [-0.0098],
        [ 0.9572],
        [-1.2201]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-957648.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4077],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(585.3773, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4077],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(585.3773, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0579e-02, -5.2748e-03, -7.4565e-08,  ..., -1.0930e-03,
         -6.3893e-03, -1.6140e-02],
        [-9.6037e-03, -6.2345e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02],
        [-1.1295e-02, -4.5696e-03, -1.2935e-07,  ..., -1.5275e-03,
         -6.6714e-03, -1.7224e-02],
        ...,
        [-9.6037e-03, -6.2345e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02],
        [-9.6037e-03, -6.2345e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02],
        [-9.6037e-03, -6.2345e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-111265.8438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.4724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(72.2083, device='cuda:0')



h[100].sum tensor(17.6153, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-86.0716, device='cuda:0')



h[200].sum tensor(-407.7256, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.5302, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(238978.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1195, 0.0250, 0.0051],
        [0.0000, 0.0000, 0.0000,  ..., 0.0319, 0.0348, 0.0148],
        [0.0000, 0.0000, 0.0000,  ..., 0.0227, 0.0392, 0.0200],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2319, 0.0171, 0.0024],
        [0.0000, 0.0000, 0.0000,  ..., 0.2319, 0.0171, 0.0024],
        [0.0000, 0.0000, 0.0000,  ..., 0.2319, 0.0171, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2092691.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2185.0435, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5410.8296, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9973.9180, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-895.5410, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -1.6158],
        [  0.8024],
        [  2.0878],
        ...,
        [-12.5598],
        [-12.5528],
        [-12.5535]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-882905.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(574.8933, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(574.8933, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-111674.7031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.3922, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(70.9151, device='cuda:0')



h[100].sum tensor(18.1290, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-84.5300, device='cuda:0')



h[200].sum tensor(-410.4609, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.1088, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(235764.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2325, 0.0016, 0.0008],
        [0.0000, 0.0000, 0.0000,  ..., 0.2292, 0.0017, 0.0006],
        [0.0000, 0.0000, 0.0000,  ..., 0.2163, 0.0020, 0.0003],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2335, 0.0016, 0.0008],
        [0.0000, 0.0000, 0.0000,  ..., 0.2335, 0.0016, 0.0008],
        [0.0000, 0.0000, 0.0000,  ..., 0.2335, 0.0016, 0.0008]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2048311.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2196.7302, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5388.8721, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9708.3662, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-858.1675, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -9.2452],
        [ -8.1044],
        [ -6.2624],
        ...,
        [-12.6849],
        [-12.6774],
        [-12.6780]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-993294.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2401],
        [0.2761],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(508.7311, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2401],
        [0.2761],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(508.7311, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2160e-02, -3.7192e-03, -1.7883e-07,  ..., -2.0515e-03,
         -7.0115e-03, -1.8531e-02],
        [-1.0992e-02, -4.8685e-03, -9.7118e-08,  ..., -1.3434e-03,
         -6.5518e-03, -1.6764e-02],
        [-1.0993e-02, -4.8671e-03, -9.7222e-08,  ..., -1.3443e-03,
         -6.5524e-03, -1.6766e-02],
        ...,
        [-9.6037e-03, -6.2345e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02],
        [-9.6037e-03, -6.2345e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02],
        [-9.6037e-03, -6.2345e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-112560.3281, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-983.7491, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(62.7537, device='cuda:0')



h[100].sum tensor(14.6171, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-74.8018, device='cuda:0')



h[200].sum tensor(-412.0469, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.4493, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(229853.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0228, 0.0409],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0213, 0.0402],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0110, 0.0246],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2336, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.2336, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.2336, 0.0000, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2016690., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2167.9519, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5364.2314, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8013.6211, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-870.1049, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.3133],
        [  2.2647],
        [  1.8333],
        ...,
        [-12.6973],
        [-12.6890],
        [-12.6888]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-975775.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2524],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(718.6469, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2524],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(718.6469, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.2345e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02],
        [-1.0208e-02, -5.6403e-03, -4.0405e-08,  ..., -8.6784e-04,
         -6.2432e-03, -1.5578e-02],
        [-1.0230e-02, -5.6180e-03, -4.1924e-08,  ..., -8.8160e-04,
         -6.2521e-03, -1.5612e-02],
        ...,
        [-9.6037e-03, -6.2345e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02],
        [-9.6037e-03, -6.2345e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02],
        [-9.6037e-03, -6.2345e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-110630.2734, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.7352, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(88.6476, device='cuda:0')



h[100].sum tensor(29.1203, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-105.6670, device='cuda:0')



h[200].sum tensor(-400.2903, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.8872, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(247421.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1256, 0.0000, 0.0020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0812, 0.0000, 0.0027],
        [0.0000, 0.0000, 0.0000,  ..., 0.0225, 0.0000, 0.0044],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2330, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.2330, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.2330, 0.0000, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2133765.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2166.6816, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5416.9404, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9978.0566, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-832.4014, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.0728],
        [  1.6127],
        [  1.8856],
        ...,
        [-12.6719],
        [-12.6648],
        [-12.6655]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-821713., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 560.0 event: 8400 loss: tensor(436.1968, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.8057],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(617.1600, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.8057],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(617.1600, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.2345e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02],
        [-1.1531e-02, -4.3382e-03, -1.2331e-07,  ..., -1.6702e-03,
         -6.7640e-03, -1.7579e-02],
        [-9.6037e-03, -6.2345e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02],
        ...,
        [-9.6037e-03, -6.2345e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02],
        [-9.6037e-03, -6.2345e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02],
        [-9.6037e-03, -6.2345e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-112014.5156, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.7826, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(76.1288, device='cuda:0')



h[100].sum tensor(23.1241, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-90.7448, device='cuda:0')



h[200].sum tensor(-409.0573, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.8077, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(237313.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0418],
        [0.0000, 0.0000, 0.0000,  ..., 0.0573, 0.0000, 0.0193],
        [0.0000, 0.0000, 0.0000,  ..., 0.0669, 0.0000, 0.0142],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2366, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2366, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2366, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2058011.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2163.7788, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5340.1348, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8904.1445, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-803.1071, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.8109],
        [  1.6547],
        [  1.5948],
        ...,
        [-12.9610],
        [-12.9530],
        [-12.9535]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1062771.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(659.2135, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(659.2135, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-111819.0156, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.1826, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(81.3163, device='cuda:0')



h[100].sum tensor(26.7478, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-96.9281, device='cuda:0')



h[200].sum tensor(-409.5936, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.4981, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(242999.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2161, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2334, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2073, 0.0000, 0.0026],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2372, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2372, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2372, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2097205., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2153.7913, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5363.1260, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10145.1406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-776.1724, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -7.5219],
        [ -8.6137],
        [ -8.2483],
        ...,
        [-13.0479],
        [-13.0400],
        [-13.0405]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1043168.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2974],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(783.8651, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2974],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(783.8651, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0638e-02, -5.2168e-03, -6.3272e-08,  ..., -1.1288e-03,
         -6.4126e-03, -1.6228e-02],
        [-1.0879e-02, -4.9791e-03, -7.8054e-08,  ..., -1.2753e-03,
         -6.5077e-03, -1.6594e-02],
        [-1.1788e-02, -4.0847e-03, -1.3366e-07,  ..., -1.8264e-03,
         -6.8654e-03, -1.7969e-02],
        ...,
        [-9.6037e-03, -6.2346e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02],
        [-9.6037e-03, -6.2346e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02],
        [-9.6037e-03, -6.2346e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-110511.9922, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.4148, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(96.6925, device='cuda:0')



h[100].sum tensor(35.1607, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-115.2564, device='cuda:0')



h[200].sum tensor(-403.9793, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.5087, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(253262.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0143],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0207],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0183],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2372, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2372, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2372, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2154463., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2164.7903, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5381.0015, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11472.0801, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-746.1780, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.8326],
        [  2.7452],
        [  2.6657],
        ...,
        [-13.0479],
        [-13.0400],
        [-13.0405]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1016012.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(537.6055, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(537.6055, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-113021.2969, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.0491, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(66.3155, device='cuda:0')



h[100].sum tensor(19.0091, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-79.0474, device='cuda:0')



h[200].sum tensor(-414.7579, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.6099, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(232787.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2326, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2358, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2220, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2337, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2372, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2032443.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2139.2065, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5356.0454, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8437.7285, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-815.3924, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -7.5692],
        [ -9.2631],
        [-10.2316],
        ...,
        [-11.5470],
        [-12.1412],
        [-12.1817]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1056522.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(573.1383, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(573.1383, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.2346e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02],
        [-1.0831e-02, -5.0269e-03, -7.1780e-08,  ..., -1.2459e-03,
         -6.4886e-03, -1.6520e-02],
        [-1.0831e-02, -5.0269e-03, -7.1780e-08,  ..., -1.2459e-03,
         -6.4886e-03, -1.6520e-02],
        ...,
        [-9.6037e-03, -6.2346e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02],
        [-9.6037e-03, -6.2346e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02],
        [-9.6037e-03, -6.2346e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-112966.7656, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.3724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(70.6986, device='cuda:0')



h[100].sum tensor(22.4969, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-84.2720, device='cuda:0')



h[200].sum tensor(-413.0699, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.0382, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(236893.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0409],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0284],
        [0.0000, 0.0000, 0.0000,  ..., 0.0315, 0.0000, 0.0164],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2348, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2348, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2348, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2070782., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2106.8213, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5429.7910, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8700.5078, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-858.1964, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.2245],
        [  1.3765],
        [  1.4354],
        ...,
        [-12.8976],
        [-12.8906],
        [-12.8913]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-843437.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(568.9377, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(568.9377, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-113526.9375, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.3277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(70.1805, device='cuda:0')



h[100].sum tensor(24.7616, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-83.6544, device='cuda:0')



h[200].sum tensor(-420.7600, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.8694, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(235963.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2306, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2153, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1758, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2368, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2368, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2368, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2059669.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2158.9148, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5379.3496, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10130.9902, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-840.1719, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -9.5872],
        [ -8.2205],
        [ -6.2444],
        ...,
        [-12.9319],
        [-12.9793],
        [-13.0120]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1064786., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4319],
        [0.2874],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(702.0966, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4319],
        [0.2874],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(702.0966, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0637e-02, -5.2182e-03, -5.5191e-08,  ..., -1.1281e-03,
         -6.4121e-03, -1.6226e-02],
        [-1.0853e-02, -5.0057e-03, -6.6730e-08,  ..., -1.2590e-03,
         -6.4971e-03, -1.6553e-02],
        [-1.1198e-02, -4.6656e-03, -8.5199e-08,  ..., -1.4687e-03,
         -6.6332e-03, -1.7076e-02],
        ...,
        [-9.6037e-03, -6.2347e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02],
        [-9.6037e-03, -6.2347e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02],
        [-9.6037e-03, -6.2347e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-112294.4922, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.5947, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(86.6060, device='cuda:0')



h[100].sum tensor(35.5619, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-103.2335, device='cuda:0')



h[200].sum tensor(-415.1389, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.2219, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(248113.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0656],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0601],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0409],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2361, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2361, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2361, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2119977.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2187.8384, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5630.1006, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12465.1582, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-618.0168, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.8263],
        [  0.9538],
        [  0.8160],
        ...,
        [-13.0131],
        [-13.0062],
        [-13.0070]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-985331.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(544.4449, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(544.4449, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-113801.7500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.1036, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(67.1592, device='cuda:0')



h[100].sum tensor(26.0987, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-80.0530, device='cuda:0')



h[200].sum tensor(-415.0042, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.8848, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(238667.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2326, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2326, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2321, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2336, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2336, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2336, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2037007.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2146.7937, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5810.7402, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8845.1797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-497.5411, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-11.0842],
        [-10.9435],
        [-10.6915],
        ...,
        [-12.7583],
        [-12.7521],
        [-12.7538]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-776899.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(611.9121, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(611.9121, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-113637.5078, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.7318, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(75.4815, device='cuda:0')



h[100].sum tensor(31.1650, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-89.9731, device='cuda:0')



h[200].sum tensor(-410.0629, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.5968, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(242169.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2337, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2337, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2332, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2347, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2347, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2347, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2074438.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2195.7451, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5643.2314, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10006.5713, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-712.5433, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.9265],
        [-10.9792],
        [-10.8909],
        ...,
        [-12.8574],
        [-12.8511],
        [-12.8511]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-780628., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(870.8616, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(870.8616, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0216e-02, -5.6324e-03, -2.8525e-08,  ..., -8.7293e-04,
         -6.2465e-03, -1.5590e-02],
        [-1.0216e-02, -5.6324e-03, -2.8525e-08,  ..., -8.7293e-04,
         -6.2465e-03, -1.5590e-02],
        [-9.6037e-03, -6.2347e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02],
        ...,
        [-9.6037e-03, -6.2347e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02],
        [-9.6037e-03, -6.2347e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02],
        [-9.6037e-03, -6.2347e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-111784.2812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-987.1364, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(107.4238, device='cuda:0')



h[100].sum tensor(47.6433, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-128.0480, device='cuda:0')



h[200].sum tensor(-402.3214, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.0057, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(264610.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1096, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0925, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1162, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2384, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2384, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2384, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2264841.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2268.7754, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5491.7598, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(16341.1729, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-840.5712, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -1.5658],
        [ -0.8297],
        [ -1.3952],
        ...,
        [-13.1285],
        [-13.1214],
        [-13.1220]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-944916.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 570.0 event: 8550 loss: tensor(351.7825, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(616.0559, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(616.0559, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.2347e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02],
        [-1.0858e-02, -5.0011e-03, -5.5804e-08,  ..., -1.2621e-03,
         -6.4991e-03, -1.6560e-02],
        [-1.0858e-02, -5.0011e-03, -5.5804e-08,  ..., -1.2621e-03,
         -6.4991e-03, -1.6560e-02],
        ...,
        [-9.6037e-03, -6.2347e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02],
        [-9.6037e-03, -6.2347e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02],
        [-9.6037e-03, -6.2347e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-114678.5312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.7673, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(75.9926, device='cuda:0')



h[100].sum tensor(30.4168, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-90.5824, device='cuda:0')



h[200].sum tensor(-409.4722, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.7634, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(239803.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0984, 0.0000, 0.0036],
        [0.0000, 0.0000, 0.0000,  ..., 0.0687, 0.0000, 0.0048],
        [0.0000, 0.0000, 0.0000,  ..., 0.0675, 0.0000, 0.0044],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2398, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2398, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2398, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2071213.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2228.1616, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5411.8579, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11189.3652, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-959.6373, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -4.0120],
        [ -1.7003],
        [ -0.6529],
        ...,
        [-13.2015],
        [-13.1945],
        [-13.1954]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1059828., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(750.5299, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(750.5299, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-113581.2031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.0806, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(92.5805, device='cuda:0')



h[100].sum tensor(38.0105, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-110.3549, device='cuda:0')



h[200].sum tensor(-398.2043, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.1688, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(250721.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2257, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2343, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2380, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2394, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2394, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2394, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2135296.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2252.7808, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5453.3970, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12474.7002, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-973.8603, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -7.4847],
        [ -8.7982],
        [ -9.6334],
        ...,
        [-13.1335],
        [-13.1264],
        [-13.1272]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-915603.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(524.6238, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(524.6238, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-116440.1641, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-983.9057, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(64.7142, device='cuda:0')



h[100].sum tensor(20.8290, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-77.1386, device='cuda:0')



h[200].sum tensor(-412.6470, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.0881, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(227845.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2392, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2385, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2363, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2401, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2401, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2401, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1970151.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2238.4146, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5409.9180, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8530.8115, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1097.2642, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.9001],
        [-10.4223],
        [ -9.6393],
        ...,
        [-13.0930],
        [-13.0856],
        [-13.0863]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-958477.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2808],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(610.9866, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2808],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(610.9866, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0275e-02, -5.5522e-03, -2.6014e-08,  ..., -9.0891e-04,
         -6.2698e-03, -1.5679e-02],
        [-1.0144e-02, -5.6836e-03, -2.0926e-08,  ..., -8.2927e-04,
         -6.2181e-03, -1.5480e-02],
        [-1.0815e-02, -5.0117e-03, -4.6940e-08,  ..., -1.2365e-03,
         -6.4825e-03, -1.6495e-02],
        ...,
        [-9.6037e-03, -6.2241e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02],
        [-9.6037e-03, -6.2241e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02],
        [-9.6037e-03, -6.2241e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-115992.8594, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.7275, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(75.3673, device='cuda:0')



h[100].sum tensor(23.8687, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-89.8371, device='cuda:0')



h[200].sum tensor(-412.0469, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.5596, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(235437.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0897, 0.0000, 0.0034],
        [0.0000, 0.0000, 0.0000,  ..., 0.0246, 0.0000, 0.0073],
        [0.0000, 0.0000, 0.0000,  ..., 0.0711, 0.0000, 0.0020],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2410, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2410, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2410, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2023687.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2265.2646, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5428.1582, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10414.0654, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1106.8864, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.2767],
        [  1.3918],
        [  0.3794],
        ...,
        [-13.0954],
        [-13.0874],
        [-13.0877]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-983225.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3027],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(447.5969, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3027],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(447.5969, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0259e-02, -5.5588e-03, -2.4247e-08,  ..., -8.9935e-04,
         -6.2636e-03, -1.5655e-02],
        [-1.0328e-02, -5.4900e-03, -2.6774e-08,  ..., -9.4078e-04,
         -6.2905e-03, -1.5758e-02],
        [-9.6037e-03, -6.2196e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02],
        ...,
        [-9.6037e-03, -6.2196e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02],
        [-9.6037e-03, -6.2196e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02],
        [-9.6037e-03, -6.2196e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4664e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-117659.7422, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-983.2139, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(55.2126, device='cuda:0')



h[100].sum tensor(10.3687, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-65.8129, device='cuda:0')



h[200].sum tensor(-420.4296, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.9919, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(225160.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0413, 0.0000, 0.0123],
        [0.0000, 0.0000, 0.0000,  ..., 0.1229, 0.0000, 0.0037],
        [0.0000, 0.0000, 0.0000,  ..., 0.1711, 0.0000, 0.0014],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2414, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2414, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2414, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1949329.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2314.3157, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5256.6763, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8422.0176, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1230.9336, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.6583],
        [ -1.3478],
        [ -3.0945],
        ...,
        [-13.1000],
        [-13.0924],
        [-13.0930]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1048968., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(668.4331, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(668.4331, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-115683.4844, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.2814, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(82.4535, device='cuda:0')



h[100].sum tensor(21.3580, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-98.2838, device='cuda:0')



h[200].sum tensor(-411.2481, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.8687, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(240937.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2380, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2380, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2372, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2420, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2420, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2420, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2033890.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2041.2203, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5476.2192, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10659.5684, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1276.3383, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.2656],
        [-10.0285],
        [ -9.5655],
        ...,
        [-13.1434],
        [-13.1357],
        [-13.1364]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-934638.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(748.6191, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(748.6191, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-115496.2656, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.0045, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(92.3448, device='cuda:0')



h[100].sum tensor(24.2578, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-110.0740, device='cuda:0')



h[200].sum tensor(-406.8647, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.0920, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(249420.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2374, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.2162, 0.0000, 0.0014],
        [0.0000, 0.0000, 0.0000,  ..., 0.1593, 0.0000, 0.0055],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2431, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2431, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2431, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2137721.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2252.2756, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5493.5732, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13682.4707, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1087.6455, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.1756],
        [ -8.4469],
        [ -5.5919],
        ...,
        [-13.2561],
        [-13.2483],
        [-13.2489]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-965830.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(589.9164, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(589.9164, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-117439.5781, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.5514, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(72.7682, device='cuda:0')



h[100].sum tensor(12.4256, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-86.7390, device='cuda:0')



h[200].sum tensor(-411.6089, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.7126, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(233149.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.7883e-01, 0.0000e+00,
         1.2397e-03],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.2325e-01, 0.0000e+00,
         3.1171e-04],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.3775e-01, 0.0000e+00,
         3.1295e-05],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.4503e-01, 0.0000e+00,
         6.2590e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.4503e-01, 0.0000e+00,
         6.2590e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.4503e-01, 0.0000e+00,
         6.2590e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2013130.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2203.7178, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5435.5181, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10095.0596, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1102.3020, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -4.2953],
        [ -7.0778],
        [ -8.8776],
        ...,
        [-13.4565],
        [-13.4483],
        [-13.4488]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1085604.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(609.8967, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(609.8967, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.2056e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-9.6037e-03, -6.2056e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-1.0741e-02, -5.0344e-03, -3.4859e-08,  ..., -1.1915e-03,
         -6.4533e-03, -1.6383e-02],
        ...,
        [-9.6037e-03, -6.2056e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-9.6037e-03, -6.2056e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-9.6037e-03, -6.2056e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-117242.2422, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.7399, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(75.2329, device='cuda:0')



h[100].sum tensor(13.7293, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-89.6768, device='cuda:0')



h[200].sum tensor(-406.8709, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.5158, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(240005.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1876, 0.0000, 0.0057],
        [0.0000, 0.0000, 0.0000,  ..., 0.0875, 0.0000, 0.0164],
        [0.0000, 0.0000, 0.0000,  ..., 0.0334, 0.0000, 0.0299],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2435, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0000,  ..., 0.2435, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0000,  ..., 0.2435, 0.0000, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2046791.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2276.8032, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5682.7495, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10269.4766, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1241.8887, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -6.1631],
        [ -2.3906],
        [  0.6093],
        ...,
        [-13.4040],
        [-13.3961],
        [-13.3967]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-977286.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6211],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(722.5938, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6211],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(722.5938, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0914e-02, -4.8483e-03, -3.8298e-08,  ..., -1.2963e-03,
         -6.5213e-03, -1.6644e-02],
        [-1.1089e-02, -4.6673e-03, -4.3415e-08,  ..., -1.4025e-03,
         -6.5902e-03, -1.6908e-02],
        [-9.6037e-03, -6.2029e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        ...,
        [-9.6037e-03, -6.2029e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-9.6037e-03, -6.2029e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-9.6037e-03, -6.2029e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-116603.2266, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.7450, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(89.1345, device='cuda:0')



h[100].sum tensor(21.4537, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-106.2473, device='cuda:0')



h[200].sum tensor(-399.8354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.0458, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(246495.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1143],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0803],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0706],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2431, 0.0000, 0.0010],
        [0.0000, 0.0000, 0.0000,  ..., 0.2431, 0.0000, 0.0010],
        [0.0000, 0.0000, 0.0000,  ..., 0.2431, 0.0000, 0.0010]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2101340.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2225.9805, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5402.5732, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11777.1348, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1041.4290, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.6953],
        [  0.9350],
        [  1.0689],
        ...,
        [-13.4562],
        [-13.4484],
        [-13.4491]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-996474.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 580.0 event: 8700 loss: tensor(426.1450, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2590],
        [0.9194],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(706.8558, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2590],
        [0.9194],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(706.8558, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0223e-02, -5.5577e-03, -1.7266e-08,  ..., -8.7740e-04,
         -6.2494e-03, -1.5600e-02],
        [-1.1803e-02, -3.9189e-03, -6.1284e-08,  ..., -1.8352e-03,
         -6.8711e-03, -1.7986e-02],
        [-1.2274e-02, -3.4300e-03, -7.4416e-08,  ..., -2.1210e-03,
         -7.0566e-03, -1.8698e-02],
        ...,
        [-9.6037e-03, -6.2004e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-9.6037e-03, -6.2004e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-9.6037e-03, -6.2004e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-117134.5156, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.6246, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(87.1931, device='cuda:0')



h[100].sum tensor(22.3763, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-103.9333, device='cuda:0')



h[200].sum tensor(-398.1346, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.4132, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(243186.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0787, 0.0000, 0.0412],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0740],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1081],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2430, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.2430, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.2430, 0.0000, 0.0011]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2065433.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2236.7520, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5358.5869, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11044.3359, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1020.5938, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -3.4746],
        [ -1.1070],
        [ -0.0565],
        ...,
        [-13.5326],
        [-13.5259],
        [-13.5274]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1052186.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2805],
        [0.3481],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(730.4985, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2805],
        [0.3481],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(730.4985, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1458e-02, -4.2684e-03, -4.9259e-08,  ..., -1.6259e-03,
         -6.7353e-03, -1.7465e-02],
        [-1.0436e-02, -5.3315e-03, -2.2124e-08,  ..., -1.0066e-03,
         -6.3333e-03, -1.5921e-02],
        [-1.0275e-02, -5.4999e-03, -1.7826e-08,  ..., -9.0856e-04,
         -6.2696e-03, -1.5677e-02],
        ...,
        [-9.6037e-03, -6.1983e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-9.6037e-03, -6.1983e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-9.6037e-03, -6.1983e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-117180.7031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.8726, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(90.1095, device='cuda:0')



h[100].sum tensor(27.1448, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-107.4096, device='cuda:0')



h[200].sum tensor(-394.4888, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.3636, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(250263.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0346, 0.0000, 0.0203],
        [0.0000, 0.0000, 0.0000,  ..., 0.0243, 0.0000, 0.0238],
        [0.0000, 0.0000, 0.0000,  ..., 0.0608, 0.0000, 0.0166],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2414, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0000,  ..., 0.2414, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0000,  ..., 0.2414, 0.0000, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2137745., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2259.3254, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5331.9468, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12835.5156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1000.5530, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.1064],
        [  1.5580],
        [  0.0344],
        ...,
        [-13.4868],
        [-13.4796],
        [-13.4805]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1042298.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(657.0529, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(657.0529, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1061e-02, -4.6747e-03, -3.6913e-08,  ..., -1.3855e-03,
         -6.5792e-03, -1.6865e-02],
        [-9.6037e-03, -6.1963e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-1.0483e-02, -5.2788e-03, -2.2257e-08,  ..., -1.0346e-03,
         -6.3514e-03, -1.5991e-02],
        ...,
        [-9.6037e-03, -6.1963e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-9.6037e-03, -6.1963e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-9.6037e-03, -6.1963e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-117948.5469, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.1665, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(81.0498, device='cuda:0')



h[100].sum tensor(27.1864, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-96.6105, device='cuda:0')



h[200].sum tensor(-392.1541, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.4113, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(243499.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.1820e-03, 0.0000e+00,
         3.7341e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         3.6124e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.1820e-03, 0.0000e+00,
         4.4274e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.3725e-01, 0.0000e+00,
         1.3695e-04],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.3725e-01, 0.0000e+00,
         1.3695e-04],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.3725e-01, 0.0000e+00,
         1.3695e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2053292., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2128.2449, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5545.0562, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10557.1562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1176.8210, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.3368],
        [  2.3316],
        [  2.2075],
        ...,
        [-13.2405],
        [-13.2330],
        [-13.2329]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-857512.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(741.5101, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(741.5101, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-117198.3125, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.9918, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(91.4678, device='cuda:0')



h[100].sum tensor(38.0990, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-109.0287, device='cuda:0')



h[200].sum tensor(-386.9030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.8062, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(255434.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2190, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.2365, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2360, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2375, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2375, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2375, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2126144.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2041.5060, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5772.3516, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13483.6211, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1193.8168, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -8.7064],
        [ -9.8432],
        [ -9.9520],
        ...,
        [-13.3598],
        [-13.3537],
        [-13.3550]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-924774.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(644.2640, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(644.2640, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-118612.1016, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.0391, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(79.4722, device='cuda:0')



h[100].sum tensor(35.0137, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-94.7300, device='cuda:0')



h[200].sum tensor(-385.2034, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.8972, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(243871.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1701, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.1114, 0.0000, 0.0122],
        [0.0000, 0.0000, 0.0000,  ..., 0.0640, 0.0000, 0.0176],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2372, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2372, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2372, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2054860.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2136.4751, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5531.1973, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10740.2461, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1099.6276, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -4.4206],
        [ -2.2068],
        [ -0.3436],
        ...,
        [-13.3968],
        [-13.3913],
        [-13.3928]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-941908.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(834.2002, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(834.2002, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0220e-02, -5.5437e-03, -1.3499e-08,  ..., -8.7524e-04,
         -6.2480e-03, -1.5594e-02],
        [-1.0220e-02, -5.5437e-03, -1.3499e-08,  ..., -8.7524e-04,
         -6.2480e-03, -1.5594e-02],
        [-9.6037e-03, -6.1915e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        ...,
        [-9.6037e-03, -6.1915e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-9.6037e-03, -6.1915e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-9.6037e-03, -6.1915e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-117159.0781, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.8121, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(102.9015, device='cuda:0')



h[100].sum tensor(49.1171, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-122.6575, device='cuda:0')



h[200].sum tensor(-369.9431, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.5320, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(258402.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0845e-01, 0.0000e+00,
         3.9657e-04],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 9.2119e-02, 0.0000e+00,
         1.8122e-03],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.2151e-01, 0.0000e+00,
         1.7517e-03],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.3728e-01, 0.0000e+00,
         1.9589e-04],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.3728e-01, 0.0000e+00,
         1.9589e-04],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.3728e-01, 0.0000e+00,
         1.9589e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2174973.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2255.9160, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5395.3926, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13153.0449, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-952.2552, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -1.5000],
        [ -1.0185],
        [ -2.3841],
        ...,
        [-13.4444],
        [-13.4389],
        [-13.4404]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-842901.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4688],
        [0.3921],
        [0.3428],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(525.5649, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4688],
        [0.3921],
        [0.3428],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(525.5649, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2964e-02, -2.6496e-03, -7.0154e-08,  ..., -2.5397e-03,
         -7.3284e-03, -1.9740e-02],
        [-1.2363e-02, -3.2829e-03, -5.7604e-08,  ..., -2.1751e-03,
         -7.0917e-03, -1.8832e-02],
        [-1.1608e-02, -4.0789e-03, -4.1833e-08,  ..., -1.7170e-03,
         -6.7943e-03, -1.7690e-02],
        ...,
        [-9.6037e-03, -6.1902e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-9.6037e-03, -6.1902e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-9.6037e-03, -6.1902e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-120750.2031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-983.9028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(64.8303, device='cuda:0')



h[100].sum tensor(31.8686, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-77.2770, device='cuda:0')



h[200].sum tensor(-381.2711, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.1259, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(230753.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1383],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1431],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1049],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2411, 0.0000, 0.0007],
        [0.0000, 0.0000, 0.0000,  ..., 0.2411, 0.0000, 0.0007],
        [0.0000, 0.0000, 0.0000,  ..., 0.2411, 0.0000, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1994676.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2205.0195, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5311.8472, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8574.5020, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-965.3145, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.3126],
        [  0.2939],
        [  0.2007],
        ...,
        [-13.7554],
        [-13.7490],
        [-13.7502]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1100577.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(769.0875, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(769.0875, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0304e-02, -5.4497e-03, -1.3930e-08,  ..., -9.2662e-04,
         -6.2813e-03, -1.5721e-02],
        [-9.6037e-03, -6.1891e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-9.6037e-03, -6.1891e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        ...,
        [-9.6037e-03, -6.1891e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-9.6037e-03, -6.1891e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-9.6037e-03, -6.1891e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-118650.2344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.2175, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(94.8696, device='cuda:0')



h[100].sum tensor(51.1512, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-113.0836, device='cuda:0')



h[200].sum tensor(-365.5606, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.9147, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(250084.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1427, 0.0000, 0.0039],
        [0.0000, 0.0000, 0.0000,  ..., 0.1838, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.2103, 0.0000, 0.0009],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2410, 0.0000, 0.0015],
        [0.0000, 0.0000, 0.0000,  ..., 0.2410, 0.0000, 0.0015],
        [0.0000, 0.0000, 0.0000,  ..., 0.2410, 0.0000, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2097861.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2216.6838, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5329.9302, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10481.8496, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-916.1373, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -3.9351],
        [ -5.5506],
        [ -5.9062],
        ...,
        [-13.7628],
        [-13.7567],
        [-13.7581]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1028466.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3242],
        [0.3210],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(652.1185, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3242],
        [0.3210],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(652.1185, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1861e-02, -3.8023e-03, -4.2734e-08,  ..., -1.8706e-03,
         -6.8941e-03, -1.8073e-02],
        [-1.0866e-02, -4.8541e-03, -2.3895e-08,  ..., -1.2671e-03,
         -6.5024e-03, -1.6569e-02],
        [-1.0874e-02, -4.8460e-03, -2.4039e-08,  ..., -1.2718e-03,
         -6.5053e-03, -1.6581e-02],
        ...,
        [-9.6037e-03, -6.1881e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-9.6037e-03, -6.1881e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-9.6037e-03, -6.1881e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-120091.7344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.1289, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(80.4411, device='cuda:0')



h[100].sum tensor(48.2986, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-95.8849, device='cuda:0')



h[200].sum tensor(-368.3733, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.2130, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(243112.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0619],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0835],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1363],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2399, 0.0000, 0.0023],
        [0.0000, 0.0000, 0.0000,  ..., 0.2399, 0.0000, 0.0023],
        [0.0000, 0.0000, 0.0000,  ..., 0.2399, 0.0000, 0.0023]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2070930.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2176.1790, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5321.9585, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9278.1514, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-966.1565, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.6159],
        [  0.4484],
        [  0.2456],
        ...,
        [-13.6858],
        [-13.6801],
        [-13.6816]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-951474.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5098],
        [0.5044],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(665.3451, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5098],
        [0.5044],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(665.3451, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2816e-02, -2.7875e-03, -5.7898e-08,  ..., -2.4498e-03,
         -7.2701e-03, -1.9515e-02],
        [-1.1899e-02, -3.7582e-03, -4.1367e-08,  ..., -1.8936e-03,
         -6.9090e-03, -1.8129e-02],
        [-1.0823e-02, -4.8970e-03, -2.1973e-08,  ..., -1.2410e-03,
         -6.4854e-03, -1.6504e-02],
        ...,
        [-9.6037e-03, -6.1872e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-9.6037e-03, -6.1872e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-9.6037e-03, -6.1872e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-120491.1641, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.1853, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(82.0726, device='cuda:0')



h[100].sum tensor(45.4973, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-97.8297, device='cuda:0')



h[200].sum tensor(-372.6580, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.7446, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(241788.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1163],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1100],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1154],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2414, 0.0000, 0.0018],
        [0.0000, 0.0000, 0.0000,  ..., 0.2414, 0.0000, 0.0018],
        [0.0000, 0.0000, 0.0000,  ..., 0.2414, 0.0000, 0.0018]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2057011.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2196.3896, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5269.8325, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9962.1943, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-961.5977, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.2854],
        [  0.3292],
        [  0.3550],
        ...,
        [-13.7958],
        [-13.7892],
        [-13.7905]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1120393.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 590.0 event: 8850 loss: tensor(446.2607, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(603.7852, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(603.7852, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.1864e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-9.6037e-03, -6.1864e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-1.0854e-02, -4.8612e-03, -2.1456e-08,  ..., -1.2602e-03,
         -6.4978e-03, -1.6552e-02],
        ...,
        [-9.6037e-03, -6.1864e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-9.6037e-03, -6.1864e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-9.6037e-03, -6.1864e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-121325.5938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.6409, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(74.4790, device='cuda:0')



h[100].sum tensor(48.7033, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-88.7782, device='cuda:0')



h[200].sum tensor(-376.7136, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.2701, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(234367.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1868, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.1269, 0.0000, 0.0176],
        [0.0000, 0.0000, 0.0000,  ..., 0.0708, 0.0000, 0.0423],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2391, 0.0000, 0.0014],
        [0.0000, 0.0000, 0.0000,  ..., 0.2391, 0.0000, 0.0014],
        [0.0000, 0.0000, 0.0000,  ..., 0.2391, 0.0000, 0.0014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1992300.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2187.2290, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5237.1255, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8459.7861, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1016.5840, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -3.1148],
        [ -2.5203],
        [ -1.3804],
        ...,
        [-13.6608],
        [-13.6547],
        [-13.6561]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1021020.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(535.5525, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(535.5525, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-122141.3438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.0284, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(66.0623, device='cuda:0')



h[100].sum tensor(54.3586, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-78.7455, device='cuda:0')



h[200].sum tensor(-378.1533, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.5274, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(231320.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1588, 0.0000, 0.0052],
        [0.0000, 0.0000, 0.0000,  ..., 0.2099, 0.0000, 0.0019],
        [0.0000, 0.0000, 0.0000,  ..., 0.2267, 0.0000, 0.0010],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2352, 0.0000, 0.0015],
        [0.0000, 0.0000, 0.0000,  ..., 0.2352, 0.0000, 0.0015],
        [0.0000, 0.0000, 0.0000,  ..., 0.2352, 0.0000, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1976838.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2162.0791, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5186.2676, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7398.4229, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1092.3524, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -3.2755],
        [ -6.2843],
        [ -8.4372],
        ...,
        [-13.3972],
        [-13.3916],
        [-13.3931]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-839972.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6694],
        [0.5161],
        [0.5010],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(640.5792, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6694],
        [0.5161],
        [0.5010],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(640.5792, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4179e-02, -1.3271e-03, -7.1077e-08,  ..., -3.2765e-03,
         -7.8067e-03, -2.1572e-02],
        [-1.4730e-02, -7.4208e-04, -7.9637e-08,  ..., -3.6107e-03,
         -8.0236e-03, -2.2404e-02],
        [-1.3289e-02, -2.2722e-03, -5.7249e-08,  ..., -2.7367e-03,
         -7.4562e-03, -2.0228e-02],
        ...,
        [-9.6037e-03, -6.1851e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-9.6037e-03, -6.1851e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-9.6037e-03, -6.1851e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-121435., device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.9591, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(79.0177, device='cuda:0')



h[100].sum tensor(61.6040, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-94.1882, device='cuda:0')



h[200].sum tensor(-378.2785, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.7491, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(237651.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.2253],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.2268],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1841],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2361, 0.0000, 0.0013],
        [0.0000, 0.0000, 0.0000,  ..., 0.2361, 0.0000, 0.0013],
        [0.0000, 0.0000, 0.0000,  ..., 0.2361, 0.0000, 0.0013]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2017047., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2201.4170, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5174.4878, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9622.9609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1060.6875, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.2038],
        [  0.2471],
        [  0.4443],
        ...,
        [-13.5050],
        [-13.4993],
        [-13.5007]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-952444.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(539.8663, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(539.8663, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0922e-02, -4.7832e-03, -1.9486e-08,  ..., -1.3014e-03,
         -6.5246e-03, -1.6654e-02],
        [-9.6037e-03, -6.1845e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-9.6037e-03, -6.1845e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        ...,
        [-9.6037e-03, -6.1845e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-9.6037e-03, -6.1845e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-9.6037e-03, -6.1845e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-122548.6875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.0610, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(66.5944, device='cuda:0')



h[100].sum tensor(55.9327, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-79.3798, device='cuda:0')



h[200].sum tensor(-381.9124, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.7008, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(229640.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0519, 0.0000, 0.0254],
        [0.0000, 0.0000, 0.0000,  ..., 0.0774, 0.0000, 0.0220],
        [0.0000, 0.0000, 0.0000,  ..., 0.1556, 0.0000, 0.0112],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2369, 0.0000, 0.0021],
        [0.0000, 0.0000, 0.0000,  ..., 0.2369, 0.0000, 0.0021],
        [0.0000, 0.0000, 0.0000,  ..., 0.2369, 0.0000, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1975030.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2194.6497, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5131.3921, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8624.1934, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1092.8679, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.9148],
        [  0.9306],
        [ -0.5604],
        ...,
        [-13.5900],
        [-13.5847],
        [-13.5868]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1003425.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(584.4961, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(584.4961, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0062,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-122160.0469, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.4812, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(72.0996, device='cuda:0')



h[100].sum tensor(62.2237, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-85.9420, device='cuda:0')



h[200].sum tensor(-373.4268, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.4948, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(233985.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2352, 0.0000, 0.0035],
        [0.0000, 0.0000, 0.0000,  ..., 0.2352, 0.0000, 0.0035],
        [0.0000, 0.0000, 0.0000,  ..., 0.2185, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2362, 0.0000, 0.0035],
        [0.0000, 0.0000, 0.0000,  ..., 0.2362, 0.0000, 0.0035],
        [0.0000, 0.0000, 0.0000,  ..., 0.2362, 0.0000, 0.0035]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2013346.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2187.0583, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5131.1064, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8685.2344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1140.9053, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.0709],
        [ -9.0553],
        [ -7.0953],
        ...,
        [-13.5677],
        [-13.5619],
        [-13.5633]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-885338.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(960.0989, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(960.0989, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0078e-02, -5.6789e-03, -6.3366e-09,  ..., -7.8923e-04,
         -6.1921e-03, -1.5378e-02],
        [-9.6037e-03, -6.1835e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-9.6037e-03, -6.1835e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        ...,
        [-9.6037e-03, -6.1835e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-9.6037e-03, -6.1835e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-9.6037e-03, -6.1835e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-118646.2266, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-987.9037, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(118.4315, device='cuda:0')



h[100].sum tensor(82.2442, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-141.1691, device='cuda:0')



h[200].sum tensor(-353.9252, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.5927, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(274015.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1664, 0.0000, 0.0026],
        [0.0000, 0.0000, 0.0000,  ..., 0.1758, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.2049, 0.0000, 0.0050],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2379, 0.0000, 0.0053],
        [0.0000, 0.0000, 0.0000,  ..., 0.2379, 0.0000, 0.0053],
        [0.0000, 0.0000, 0.0000,  ..., 0.2379, 0.0000, 0.0053]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2313738.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2217.4897, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5139.0254, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15735.5352, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-978.9766, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -4.2420],
        [ -5.0565],
        [ -6.2917],
        ...,
        [-13.7420],
        [-13.7360],
        [-13.7375]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-889501.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2830],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(562.7744, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2830],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(562.7744, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -6.1141e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-1.0280e-02, -5.3245e-03, -8.5987e-09,  ..., -9.1210e-04,
         -6.2719e-03, -1.5684e-02],
        [-9.6037e-03, -6.1141e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        ...,
        [-9.6037e-03, -6.1141e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-9.6037e-03, -6.1141e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-9.6037e-03, -6.1141e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-122078.4922, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.2393, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(69.4202, device='cuda:0')



h[100].sum tensor(61.7287, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-82.7481, device='cuda:0')



h[200].sum tensor(-357.1562, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.6216, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(233457.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0383, 0.0000, 0.0146],
        [0.0000, 0.0000, 0.0000,  ..., 0.1009, 0.0000, 0.0085],
        [0.0000, 0.0000, 0.0000,  ..., 0.1431, 0.0000, 0.0050],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2390, 0.0000, 0.0021],
        [0.0000, 0.0000, 0.0000,  ..., 0.2390, 0.0000, 0.0021],
        [0.0000, 0.0000, 0.0000,  ..., 0.2390, 0.0000, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1989154.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2167.2087, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4829.1152, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11235.5957, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1082.6498, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.5989],
        [  0.4894],
        [ -0.8083],
        ...,
        [-13.9845],
        [-13.9776],
        [-13.9787]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1165933.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(605.7002, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(605.7002, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0061,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0061,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0061,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0061,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0061,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0061,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-121939.0156, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.6501, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(74.7152, device='cuda:0')



h[100].sum tensor(73.5580, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-89.0598, device='cuda:0')



h[200].sum tensor(-330.2563, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.3471, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(232950.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.1183e-01, 0.0000e+00,
         1.1492e-04],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.2269e-01, 0.0000e+00,
         1.1492e-04],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.3014e-01, 0.0000e+00,
         2.2984e-04],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.3590e-01, 0.0000e+00,
         2.2984e-04],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.3590e-01, 0.0000e+00,
         2.2984e-04],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.3590e-01, 0.0000e+00,
         2.2984e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1984073.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2396.2278, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4922.0806, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12536.8047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1277.4917, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -9.1236],
        [ -9.7074],
        [-10.2563],
        ...,
        [-13.8953],
        [-13.8893],
        [-13.8908]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1023249.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2983],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(678.4443, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2983],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(678.4443, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -5.9953e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-1.0317e-02, -5.0374e-03, -8.1895e-09,  ..., -9.3441e-04,
         -6.2864e-03, -1.5740e-02],
        [-9.6037e-03, -5.9953e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        ...,
        [-9.6037e-03, -5.9953e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-9.6037e-03, -5.9953e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02],
        [-9.6037e-03, -5.9953e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4663e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-121532.8125, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.4185, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(83.6885, device='cuda:0')



h[100].sum tensor(80.9222, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-99.7558, device='cuda:0')



h[200].sum tensor(-294.6028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.2712, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(244022.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1442, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1807, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1912, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2327, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2327, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2327, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1995941.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2443.7498, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4592.6279, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12615.9512, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1367.7700, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -8.5503],
        [ -9.0387],
        [ -9.7091],
        ...,
        [-13.7312],
        [-13.7269],
        [-13.7289]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-794565.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(735.0464, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(735.0464, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0191e-02, -5.1143e-03, -6.4025e-09,  ..., -8.5774e-04,
         -6.2366e-03, -1.5549e-02],
        [-1.0238e-02, -5.0471e-03, -6.9199e-09,  ..., -8.8652e-04,
         -6.2553e-03, -1.5620e-02],
        [-1.0825e-02, -4.2156e-03, -1.3322e-08,  ..., -1.2426e-03,
         -6.4864e-03, -1.6506e-02],
        ...,
        [-9.6037e-03, -5.9458e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -5.9458e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -5.9458e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-121826.2344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.8858, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(90.6705, device='cuda:0')



h[100].sum tensor(46.0416, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-108.0783, device='cuda:0')



h[200].sum tensor(-287.3538, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.5464, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(246200.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0891, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0361, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0665, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2393, 0.0000, 0.0017],
        [0.0000, 0.0000, 0.0000,  ..., 0.2393, 0.0000, 0.0017],
        [0.0000, 0.0000, 0.0000,  ..., 0.2393, 0.0000, 0.0017]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2080136.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2294.1064, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5285.8027, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(22543.4941, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-647.3820, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -1.7423],
        [ -0.3555],
        [ -0.3670],
        ...,
        [-14.1187],
        [-14.0855],
        [-14.1128]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1271704.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 600.0 event: 9000 loss: tensor(476.1150, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(593.8207, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(593.8207, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0059,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0059,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0059,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0059,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0059,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0059,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-124607.4219, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.5087, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(73.2498, device='cuda:0')



h[100].sum tensor(2.6568, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-87.3130, device='cuda:0')



h[200].sum tensor(-280.0287, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.8696, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(233617.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.4080e-01, 0.0000e+00,
         1.1058e-04],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.4080e-01, 0.0000e+00,
         1.1058e-04],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.3783e-01, 0.0000e+00,
         5.5291e-05],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.4180e-01, 0.0000e+00,
         1.1058e-04],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.4180e-01, 0.0000e+00,
         1.1058e-04],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.4180e-01, 0.0000e+00,
         1.1058e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2033469., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2035.7814, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5648.5762, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(23426.6797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-5.8589, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-12.8071],
        [-12.4648],
        [-11.8058],
        ...,
        [-14.6461],
        [-14.6411],
        [-14.6431]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1493161.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(698.7106, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(698.7106, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -5.8611e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -5.8611e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-1.0296e-02, -4.7928e-03, -6.8130e-09,  ..., -9.2166e-04,
         -6.2781e-03, -1.5708e-02],
        ...,
        [-9.6037e-03, -5.8611e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -5.8611e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -5.8611e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-125196.8047, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.5181, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(86.1884, device='cuda:0')



h[100].sum tensor(-15.8824, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-102.7356, device='cuda:0')



h[200].sum tensor(-268.8844, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.0858, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(245839.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2264, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1925, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1692, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2414, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2414, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2414, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2141423.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1982.6021, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5903.3906, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(27356.6055, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(468.8317, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -9.6593],
        [ -8.0250],
        [ -6.2502],
        ...,
        [-14.5089],
        [-14.5057],
        [-14.5083]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1271159.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2598],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(750.8954, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2598],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(750.8954, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0257e-02, -4.7826e-03, -6.1013e-09,  ..., -8.9779e-04,
         -6.2626e-03, -1.5648e-02],
        [-1.0880e-02, -3.7886e-03, -1.1918e-08,  ..., -1.2754e-03,
         -6.5077e-03, -1.6588e-02],
        [-1.0258e-02, -4.7803e-03, -6.1147e-09,  ..., -8.9866e-04,
         -6.2632e-03, -1.5650e-02],
        ...,
        [-9.6037e-03, -5.8252e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -5.8252e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -5.8252e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-127046.0781, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.0151, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(92.6256, device='cuda:0')



h[100].sum tensor(-35.9929, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-110.4087, device='cuda:0')



h[200].sum tensor(-295.1631, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.1834, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(250331.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0088, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0271, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0362, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2407, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2407, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2407, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2193774.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1941.3271, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6039.7378, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(29338.6250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(882.8299, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.5800],
        [  0.2833],
        [ -0.6599],
        ...,
        [-14.1881],
        [-14.1765],
        [-14.1718]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1115228.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(585.5122, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(585.5122, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -5.7962e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -5.7962e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-1.0210e-02, -4.8002e-03, -5.3798e-09,  ..., -8.6961e-04,
         -6.2443e-03, -1.5578e-02],
        ...,
        [-9.6037e-03, -5.7962e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -5.7962e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -5.7962e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-131751.5312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.4512, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(72.2250, device='cuda:0')



h[100].sum tensor(-72.7822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-86.0914, device='cuda:0')



h[200].sum tensor(-358.4225, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.5356, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(234619.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1823, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1415, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0890, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2452, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2452, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2452, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2149483., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2011.8582, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6273.9272, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(34409.0859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1455.1848, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -4.2367],
        [ -2.6633],
        [ -0.9593],
        ...,
        [-14.2961],
        [-14.2889],
        [-14.2900]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1504523.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2844],
        [0.5166],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(663.1322, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2844],
        [0.5166],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(663.1322, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1920e-02, -1.8722e-03, -1.9496e-08,  ..., -1.9065e-03,
         -6.9173e-03, -1.8158e-02],
        [-1.1371e-02, -2.7958e-03, -1.4876e-08,  ..., -1.5736e-03,
         -6.7013e-03, -1.7329e-02],
        [-1.1231e-02, -3.0318e-03, -1.3696e-08,  ..., -1.4886e-03,
         -6.6461e-03, -1.7118e-02],
        ...,
        [-9.6037e-03, -5.7701e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -5.7701e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -5.7701e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-133368.8438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.2122, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(81.7997, device='cuda:0')



h[100].sum tensor(-86.3892, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-97.5043, device='cuda:0')



h[200].sum tensor(-419.0648, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.6557, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(242587.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2428, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2428, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2428, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2205032.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2095.7004, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6689.7539, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(40166.3008, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1934.3435, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.9135],
        [  0.5655],
        [  0.2922],
        ...,
        [-13.6884],
        [-13.6826],
        [-13.6841]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1255545.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6611],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(654.4628, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6611],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(654.4628, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2253e-02, -1.1926e-03, -2.1155e-08,  ..., -2.1082e-03,
         -7.0483e-03, -1.8659e-02],
        [-9.6037e-03, -5.7470e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-1.1185e-02, -3.0286e-03, -1.2627e-08,  ..., -1.4606e-03,
         -6.6279e-03, -1.7048e-02],
        ...,
        [-1.0770e-02, -3.7423e-03, -9.3118e-09,  ..., -1.2088e-03,
         -6.4645e-03, -1.6422e-02],
        [-9.6037e-03, -5.7470e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -5.7470e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-135593.8594, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.1505, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(80.7303, device='cuda:0')



h[100].sum tensor(-102.6835, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-96.2296, device='cuda:0')



h[200].sum tensor(-487.6602, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.3072, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(250545.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0569, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1415, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1628, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2145, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2278243.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2122.9868, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7041.4194, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(44853.8750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2338.0486, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5948],
        [ 0.1814],
        [ 0.2469],
        ...,
        [-8.0719],
        [-8.9742],
        [-9.5463]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-900678., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4663],
        [0.4819],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(714.9203, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4663],
        [0.4819],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(714.9203, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3907e-02,  1.8096e-03, -3.2603e-08,  ..., -3.1116e-03,
         -7.6996e-03, -2.1155e-02],
        [-1.1615e-02, -2.2054e-03, -1.5235e-08,  ..., -1.7213e-03,
         -6.7971e-03, -1.7696e-02],
        [-1.3688e-02,  1.4258e-03, -3.0943e-08,  ..., -2.9786e-03,
         -7.6133e-03, -2.0824e-02],
        ...,
        [-9.6037e-03, -5.7273e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -5.7273e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -5.7273e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-138056.0312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.6788, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(88.1879, device='cuda:0')



h[100].sum tensor(-114.4298, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-105.1190, device='cuda:0')



h[200].sum tensor(-556.6984, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.7374, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0029, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(250339.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2397, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2397, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2397, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2322859.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2053.4861, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7025.8833, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(52784.0547, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2682.1096, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -3.5677],
        [ -2.8915],
        [ -2.0819],
        ...,
        [-12.4467],
        [-12.4433],
        [-12.4458]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-946805.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(562.5480, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(562.5480, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-142345.9062, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.2649, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(69.3923, device='cuda:0')



h[100].sum tensor(-138.7184, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-82.7148, device='cuda:0')



h[200].sum tensor(-630.1675, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.6125, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(238257.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2330, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2380, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2397, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2417, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2417, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2417, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2305821., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1884.1801, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6908.9678, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(55958.0078, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2905.3040, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -7.5757],
        [ -8.4677],
        [ -9.0932],
        ...,
        [-12.1071],
        [-12.1045],
        [-12.1067]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-938792.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2452],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(721.4838, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2452],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(721.4838, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1654e-02, -2.0620e-03, -1.4734e-08,  ..., -1.7453e-03,
         -6.8127e-03, -1.7756e-02],
        [-1.0901e-02, -3.4025e-03, -9.3205e-09,  ..., -1.2884e-03,
         -6.5161e-03, -1.6619e-02],
        [-1.1710e-02, -1.9626e-03, -1.5135e-08,  ..., -1.7791e-03,
         -6.8347e-03, -1.7840e-02],
        ...,
        [-9.6037e-03, -5.7107e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -5.7107e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -5.7107e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-140830.5625, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.7511, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(88.9975, device='cuda:0')



h[100].sum tensor(-127.8690, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-106.0841, device='cuda:0')



h[200].sum tensor(-623.4106, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.0012, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(250353.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2417, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2417, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2417, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2381741.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1912.5093, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6973.7573, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(59363.5078, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2945.0825, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.6980],
        [  0.4999],
        [  0.4518],
        ...,
        [-12.1071],
        [-12.1045],
        [-12.1067]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-906671.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(787.6448, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(787.6448, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-142968.9062, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.3703, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(97.1587, device='cuda:0')



h[100].sum tensor(-135.8184, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-115.8122, device='cuda:0')



h[200].sum tensor(-685.1532, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.6606, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(253808.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1777, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1846, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2029, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2430, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2430, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2430, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2411793., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1848.2274, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7003.0254, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(61978.2812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3249.8496, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -2.2343],
        [ -3.2915],
        [ -4.3970],
        ...,
        [-11.7595],
        [-11.7583],
        [-11.7608]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-740720.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 610.0 event: 9150 loss: tensor(462.2220, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(653.9934, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(653.9934, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -5.6855e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -5.6855e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-1.0260e-02, -4.4884e-03, -4.2365e-09,  ..., -8.9953e-04,
         -6.2637e-03, -1.5652e-02],
        ...,
        [-9.6037e-03, -5.6855e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -5.6855e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -5.6855e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-146937.1562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.1074, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(80.6724, device='cuda:0')



h[100].sum tensor(-161.7048, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-96.1606, device='cuda:0')



h[200].sum tensor(-747.9930, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.2883, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(248098.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0645, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0671, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0402, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2428, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2428, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2428, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2400945., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1730.1074, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6944.7227, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(62665.3789, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3413.4829, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.2047],
        [  1.9588],
        [  1.5860],
        ...,
        [-10.5294],
        [-11.2358],
        [-11.4513]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-637121.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(609.0004, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(609.0004, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0885e-02, -3.3126e-03, -7.8402e-09,  ..., -1.2785e-03,
         -6.5098e-03, -1.6594e-02],
        [-9.6037e-03, -5.6749e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-1.1728e-02, -1.7576e-03, -1.3001e-08,  ..., -1.7899e-03,
         -6.8417e-03, -1.7866e-02],
        ...,
        [-9.6037e-03, -5.6749e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -5.6749e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -5.6749e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-150523.3750, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.7052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(75.1223, device='cuda:0')



h[100].sum tensor(-178.3879, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-89.5450, device='cuda:0')



h[200].sum tensor(-801.8477, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.4797, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(242430.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0314, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2498, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2498, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2498, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2383881.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1934.8269, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7061.4604, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(70352.9219, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3621.6133, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.9413],
        [  1.5036],
        [  0.7823],
        ...,
        [-11.8301],
        [-11.8281],
        [-11.8304]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1095528.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6382],
        [0.7036],
        [0.6753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(551.8673, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6382],
        [0.7036],
        [0.6753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(551.8673, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2911e-02,  4.8892e-04, -1.9178e-08,  ..., -2.5073e-03,
         -7.3073e-03, -1.9650e-02],
        [-1.2745e-02,  1.8031e-04, -1.8217e-08,  ..., -2.4067e-03,
         -7.2421e-03, -1.9400e-02],
        [-1.2760e-02,  2.0856e-04, -1.8305e-08,  ..., -2.4159e-03,
         -7.2480e-03, -1.9423e-02],
        ...,
        [-9.6037e-03, -5.6660e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -5.6660e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -5.6660e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-153417.2188, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.1531, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(68.0748, device='cuda:0')



h[100].sum tensor(-194.4996, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-81.1444, device='cuda:0')



h[200].sum tensor(-850.0344, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.1832, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(240522.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2447, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2447, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2447, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2411808.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1891.3043, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6887.5273, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(69023.6094, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3717.5522, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -0.1958],
        [ -0.3694],
        [ -0.1658],
        ...,
        [-11.4838],
        [-11.4832],
        [-11.4858]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-809508.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(614.5498, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(614.5498, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-154823.4688, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.7341, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(75.8069, device='cuda:0')



h[100].sum tensor(-202.3640, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-90.3610, device='cuda:0')



h[200].sum tensor(-888.1678, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.7028, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(250157.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2389, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2389, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2384, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2399, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2399, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2399, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2499117., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1797.4733, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6630.3921, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(67083.5938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3788.8716, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -8.9390],
        [ -9.0154],
        [ -9.0328],
        ...,
        [-11.1189],
        [-11.1201],
        [-11.1233]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-567882.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(668.0437, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(668.0437, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-156388.5312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.2920, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(82.4055, device='cuda:0')



h[100].sum tensor(-208.5800, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-98.2265, device='cuda:0')



h[200].sum tensor(-923.1652, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.8531, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(254973.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2396, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2396, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2392, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2406, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2406, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2406, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2570504., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1847.5901, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6494.2754, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(69451.9375, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3936.3789, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -8.9065],
        [ -8.4299],
        [ -7.5236],
        ...,
        [-11.2299],
        [-11.2306],
        [-11.2337]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-651709.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(599.5914, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(599.5914, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -5.6493e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -5.6493e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-1.0138e-02, -4.6353e-03, -2.6296e-09,  ..., -8.2551e-04,
         -6.2157e-03, -1.5467e-02],
        ...,
        [-9.6037e-03, -5.6493e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -5.6493e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -5.6493e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-159215.7969, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.5812, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(73.9617, device='cuda:0')



h[100].sum tensor(-223.0005, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-88.1616, device='cuda:0')



h[200].sum tensor(-960.6904, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.1015, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(246728.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2036, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1808, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1651, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2450, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2450, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2450, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2541751.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1898.7118, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6288.5376, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(68717.2188, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4045.1606, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -4.4084],
        [ -4.7147],
        [ -4.3856],
        ...,
        [-11.5956],
        [-11.5945],
        [-11.5970]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-895890.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(601.5392, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(601.5392, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0222e-02, -4.4683e-03, -2.8822e-09,  ..., -8.7669e-04,
         -6.2489e-03, -1.5594e-02],
        [-9.6037e-03, -5.6476e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-1.0106e-02, -4.6902e-03, -2.3400e-09,  ..., -8.0614e-04,
         -6.2031e-03, -1.5419e-02],
        ...,
        [-9.6037e-03, -5.6476e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -5.6476e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -5.6476e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-160891.8438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.6331, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(74.2020, device='cuda:0')



h[100].sum tensor(-231.4507, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-88.4479, device='cuda:0')



h[200].sum tensor(-990.9460, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.1798, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(247176.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1539, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0880, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2476, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2476, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2476, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2573444.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1891.6604, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6114.7915, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(68119.1562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4114.1255, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -1.5678],
        [ -0.3700],
        [  0.5313],
        ...,
        [-11.8036],
        [-11.8014],
        [-11.8035]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-965953., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(598.3425, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(598.3425, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -5.6468e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -5.6468e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-1.0306e-02, -4.3023e-03, -3.0980e-09,  ..., -9.2768e-04,
         -6.2820e-03, -1.5721e-02],
        ...,
        [-9.6037e-03, -5.6468e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -5.6468e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -5.6468e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-162272.1875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.5874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(73.8076, device='cuda:0')



h[100].sum tensor(-239.6989, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-87.9779, device='cuda:0')



h[200].sum tensor(-1018.2725, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.0513, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(247682.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2301, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2038, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1842, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2202, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1987, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1946, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2582972.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1820.8906, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5914.3359, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(64990.3281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4130.4009, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.3193],
        [-6.3826],
        [-4.9615],
        ...,
        [-6.7963],
        [-5.4596],
        [-5.2861]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-854146.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2512],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(704.8788, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2512],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(704.8788, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -5.6462e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-1.0205e-02, -4.4926e-03, -2.5067e-09,  ..., -8.6607e-04,
         -6.2420e-03, -1.5568e-02],
        [-9.6037e-03, -5.6462e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        ...,
        [-9.6037e-03, -5.6462e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -5.6462e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -5.6462e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-162385.5938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.6500, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(86.9492, device='cuda:0')



h[100].sum tensor(-238.0777, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-103.6426, device='cuda:0')



h[200].sum tensor(-1038.6121, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.3337, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(257039.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1905, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1846, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1538, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2435, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2125, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1411, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2663625.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1791.1560, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5832.4048, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(65682.6719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4188.0225, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.3745],
        [-4.4665],
        [-4.6985],
        ...,
        [-9.8532],
        [-7.4344],
        [-4.0907]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-738690.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3191],
        [0.6729],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(568.2058, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3191],
        [0.6729],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(568.2058, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1879e-02, -1.2653e-03, -8.9755e-09,  ..., -1.8812e-03,
         -6.9010e-03, -1.8091e-02],
        [-1.0367e-02, -4.1760e-03, -3.0110e-09,  ..., -9.6450e-04,
         -6.3059e-03, -1.5812e-02],
        [-1.1213e-02, -2.5470e-03, -6.3492e-09,  ..., -1.4776e-03,
         -6.6390e-03, -1.7088e-02],
        ...,
        [-9.6037e-03, -5.6455e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -5.6455e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -5.6455e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-165020.8438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.3148, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(70.0902, device='cuda:0')



h[100].sum tensor(-252.3348, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-83.5467, device='cuda:0')



h[200].sum tensor(-1065.5203, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.8399, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(244127.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0667, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2488, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2488, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2488, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2593187., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1791.5023, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5638.0977, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(63826.3086, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4215.0762, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.6950],
        [  1.0273],
        [ -0.4396],
        ...,
        [-12.0185],
        [-12.0151],
        [-12.0168]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1009355.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 620.0 event: 9300 loss: tensor(456.4777, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(649.3215, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(649.3215, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -5.6450e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -5.6450e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-1.1554e-02, -1.8804e-03, -7.2745e-09,  ..., -1.6844e-03,
         -6.7732e-03, -1.7601e-02],
        ...,
        [-9.6037e-03, -5.6450e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -5.6450e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02],
        [-9.6037e-03, -5.6450e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4662e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-165155.1250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.1229, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(80.0961, device='cuda:0')



h[100].sum tensor(-249.5632, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-95.4737, device='cuda:0')



h[200].sum tensor(-1081.7751, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.1005, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(251679.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0599, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0516, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0584, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2472, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2472, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2472, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2658206.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1829.0973, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5599.0903, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(66513.5000, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4290.4834, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.3367],
        [  1.2662],
        [  0.9513],
        ...,
        [-11.9529],
        [-11.9497],
        [-11.9515]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-992525.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(572.2815, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(572.2815, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0056,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0056,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-166536.6250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.3667, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(70.5929, device='cuda:0')



h[100].sum tensor(-258.4015, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-84.1460, device='cuda:0')



h[200].sum tensor(-1101.9171, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.0038, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(247084.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0281, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0372, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0393, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2418, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2418, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2418, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2627992., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1803.3281, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5523.0151, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(63839.2422, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4279.6133, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.7844],
        [  0.9757],
        [  1.0470],
        ...,
        [-11.5668],
        [-11.5685],
        [-11.5742]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-782931.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(780.9530, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(780.9530, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0056,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0056,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-165031.8750, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.2832, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(96.3333, device='cuda:0')



h[100].sum tensor(-246.4791, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-114.8282, device='cuda:0')



h[200].sum tensor(-1110.2603, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.3917, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(272516.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2367, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2367, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2358, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2377, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2377, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2377, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2836967.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1643.7966, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5535.1108, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(70353.6875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4483.5684, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -9.1572],
        [ -8.8668],
        [ -8.3287],
        ...,
        [-11.2913],
        [-11.2910],
        [-11.2937]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-647546., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(665.0172, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(665.0172, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0196e-02, -4.5002e-03, -1.8619e-09,  ..., -8.6058e-04,
         -6.2385e-03, -1.5553e-02],
        [-9.6037e-03, -5.6480e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02],
        [-9.6037e-03, -5.6480e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02],
        ...,
        [-9.6037e-03, -5.6480e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02],
        [-9.6037e-03, -5.6480e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02],
        [-9.6037e-03, -5.6480e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-166835.1094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.2838, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(82.0322, device='cuda:0')



h[100].sum tensor(-254.2714, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-97.7815, device='cuda:0')



h[200].sum tensor(-1127.9927, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.7314, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(257594.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0710, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1372, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1983, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2399, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2399, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2399, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2695654., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1707.6036, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5622.2231, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(65110.9023, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4257.0078, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -0.9192],
        [ -2.8427],
        [ -5.0375],
        ...,
        [-11.0431],
        [-10.9329],
        [-10.9253]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-763025.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.9604],
        [0.8120],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(761.0405, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.9604],
        [0.8120],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(761.0405, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1546e-02, -1.8871e-03, -5.7696e-09,  ..., -1.6794e-03,
         -6.7700e-03, -1.7588e-02],
        [-1.1901e-02, -1.1987e-03, -6.8243e-09,  ..., -1.8947e-03,
         -6.9097e-03, -1.8123e-02],
        [-1.3692e-02,  2.2750e-03, -1.2146e-08,  ..., -2.9811e-03,
         -7.6149e-03, -2.0823e-02],
        ...,
        [-9.6037e-03, -5.6529e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02],
        [-9.6037e-03, -5.6529e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02],
        [-9.6037e-03, -5.6529e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-166847.5938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.1118, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(93.8770, device='cuda:0')



h[100].sum tensor(-247.3880, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-111.9004, device='cuda:0')



h[200].sum tensor(-1137.7351, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.5912, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0011, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(267669.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2435, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2435, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2435, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2761564., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1659.2019, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5664.0586, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(67928.3359, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4256.6250, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.1234],
        [  0.9923],
        [  0.9679],
        ...,
        [-11.6516],
        [-11.6665],
        [-11.6696]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1010342.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(559.9310, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(559.9310, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-169708.7031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.2114, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(69.0694, device='cuda:0')



h[100].sum tensor(-261.2428, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-82.3300, device='cuda:0')



h[200].sum tensor(-1156.5546, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.5073, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(247555.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2329, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2098, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1801, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2426, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2426, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2426, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2619800.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1676.6047, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5465.3760, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(60651.4688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4287.7588, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -5.7253],
        [ -4.4619],
        [ -2.8771],
        ...,
        [-11.4902],
        [-11.4883],
        [-11.4905]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-897201.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(779.1466, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(779.1466, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-168450.9375, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.3005, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(96.1104, device='cuda:0')



h[100].sum tensor(-244.3983, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-114.5626, device='cuda:0')



h[200].sum tensor(-1159.6057, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.3191, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(263859.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2152, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1829, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1103, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2411, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2411, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2411, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2744524., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1813.1383, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5420.7983, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(64434.9492, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4476.9961, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -5.8862],
        [ -4.1756],
        [ -2.5298],
        ...,
        [-11.2704],
        [-11.2694],
        [-11.2719]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-714954., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.9287],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(737.1028, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.9287],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(737.1028, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -5.6781e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02],
        [-1.1825e-02, -1.3989e-03, -5.5448e-09,  ..., -1.8487e-03,
         -6.8798e-03, -1.8008e-02],
        [-9.6037e-03, -5.6781e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02],
        ...,
        [-9.6037e-03, -5.6781e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02],
        [-9.6037e-03, -5.6781e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02],
        [-9.6037e-03, -5.6781e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-169558.1406, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.9872, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(90.9242, device='cuda:0')



h[100].sum tensor(-244.6483, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-108.3807, device='cuda:0')



h[200].sum tensor(-1170.7581, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.6290, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(258632.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2348, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2376, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2416, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2718785.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1868.3093, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5381.5352, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(63879.1250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4546.3589, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.7918],
        [  1.6561],
        [  1.0297],
        ...,
        [-10.1633],
        [-10.2983],
        [-10.6044]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-749371.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3777],
        [0.3833],
        [0.3801],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(735.7069, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3777],
        [0.3833],
        [0.3801],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(735.7069, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0520e-02, -3.9205e-03, -2.1580e-09,  ..., -1.0576e-03,
         -6.3664e-03, -1.6042e-02],
        [-1.1416e-02, -2.1969e-03, -4.2666e-09,  ..., -1.6008e-03,
         -6.7189e-03, -1.7392e-02],
        [-1.1427e-02, -2.1767e-03, -4.2913e-09,  ..., -1.6072e-03,
         -6.7231e-03, -1.7408e-02],
        ...,
        [-9.6037e-03, -5.6846e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02],
        [-9.6037e-03, -5.6846e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02],
        [-9.6037e-03, -5.6846e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-170471.1250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.8866, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(90.7520, device='cuda:0')



h[100].sum tensor(-242.5536, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-108.1754, device='cuda:0')



h[200].sum tensor(-1180.8975, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.5729, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(249578.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0124, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2468, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2468, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2468, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2639707., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1867.2832, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5418.8447, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(61773.7344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4557.9878, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.8602],
        [  1.4186],
        [  1.1797],
        ...,
        [-11.4495],
        [-11.4225],
        [-11.3913]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-891683.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(643.6998, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(643.6998, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-171293.8750, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.0574, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(79.4026, device='cuda:0')



h[100].sum tensor(-248.9023, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-94.6471, device='cuda:0')



h[200].sum tensor(-1183.8708, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.8745, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(243364.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0340, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0620, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0558, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2468, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2468, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2468, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2610881., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1850.8953, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5415.0684, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(60895.7578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4534.2163, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.0068],
        [  1.2166],
        [  1.4088],
        ...,
        [-11.4936],
        [-11.4950],
        [-11.5020]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-875298.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 630.0 event: 9450 loss: tensor(419.4495, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(622.9886, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(622.9886, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-172163.4531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.8312, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(76.8478, device='cuda:0')



h[100].sum tensor(-248.3635, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-91.6018, device='cuda:0')



h[200].sum tensor(-1193.2715, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.0420, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(239038.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2481, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2481, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2477, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2490, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2490, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2490, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2573528.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1841.7830, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5384.5889, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(59051.7656, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4537.1992, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -9.9062],
        [ -9.7584],
        [ -9.4897],
        ...,
        [-11.6004],
        [-11.5973],
        [-11.5990]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-947437., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(618.4819, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(618.4819, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-172657.9844, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.7822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(76.2919, device='cuda:0')



h[100].sum tensor(-246.9638, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-90.9391, device='cuda:0')



h[200].sum tensor(-1201.1818, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.8609, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(238148.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2475, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2475, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2470, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2484, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2484, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2484, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2562255.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1821.4894, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5367.7563, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(57826.7695, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4522.0615, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -9.8124],
        [ -9.8118],
        [ -9.7595],
        ...,
        [-11.4950],
        [-11.4922],
        [-11.4941]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-864982.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(696.3260, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(696.3260, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-172310.1250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.5143, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(85.8942, device='cuda:0')



h[100].sum tensor(-238.8321, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-102.3850, device='cuda:0')



h[200].sum tensor(-1205.5375, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.9899, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(244072.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2470, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2470, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2465, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2479, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2479, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2479, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2599487.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1837.7124, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5369.6875, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(59090.5078, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4540.8013, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -9.7503],
        [ -9.6712],
        [ -9.5119],
        ...,
        [-11.4136],
        [-11.4070],
        [-11.4050]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-819893.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3047],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(674.4069, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3047],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(674.4069, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0916e-02, -3.1801e-03, -2.4340e-09,  ..., -1.2975e-03,
         -6.5221e-03, -1.6638e-02],
        [-9.6037e-03, -5.7004e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02],
        [-1.0332e-02, -4.3008e-03, -1.3516e-09,  ..., -9.4361e-04,
         -6.2924e-03, -1.5759e-02],
        ...,
        [-9.6037e-03, -5.7004e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02],
        [-9.6037e-03, -5.7004e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02],
        [-9.6037e-03, -5.7004e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-172903.1875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.3230, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(83.1904, device='cuda:0')



h[100].sum tensor(-238.3590, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-99.1621, device='cuda:0')



h[200].sum tensor(-1212.4990, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.1089, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(240368.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2488, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2488, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2488, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2576873., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1873.9703, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5365.2007, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(59955.9258, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4564.3760, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.8262],
        [  1.9418],
        [  1.8403],
        ...,
        [-11.3515],
        [-11.4607],
        [-11.4825]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-941949.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1025.1824, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1025.1824, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -5.7012e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02],
        [-9.6037e-03, -5.7012e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02],
        [-1.0279e-02, -4.4031e-03, -1.1795e-09,  ..., -9.1139e-04,
         -6.2714e-03, -1.5679e-02],
        ...,
        [-9.6037e-03, -5.7012e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02],
        [-9.6037e-03, -5.7012e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02],
        [-9.6037e-03, -5.7012e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-170021.2344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-988.4777, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(126.4598, device='cuda:0')



h[100].sum tensor(-212.6095, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-150.7388, device='cuda:0')



h[200].sum tensor(-1207.0002, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(41.2089, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(267335.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2036, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1331, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0690, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2356, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1970, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1093, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2744353., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1951.2974, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5351.9253, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(66572.0391, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4713.9868, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.4293],
        [-1.3960],
        [ 0.3544],
        ...,
        [-9.5037],
        [-7.4378],
        [-4.6802]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-889933.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(775.4427, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(775.4427, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-172376.5312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.2129, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(95.6536, device='cuda:0')



h[100].sum tensor(-228.7977, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-114.0180, device='cuda:0')



h[200].sum tensor(-1220.3030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.1702, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(253109.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2426, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2426, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2421, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2435, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2435, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2435, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2657570., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1858.9465, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5182.9116, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(62822., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4660.6987, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -9.4211],
        [ -9.3488],
        [ -9.2099],
        ...,
        [-11.0983],
        [-11.0972],
        [-11.0995]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-750968.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(540.0774, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(540.0774, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-174766.8750, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.0872, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(66.6204, device='cuda:0')



h[100].sum tensor(-243.7672, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-79.4108, device='cuda:0')



h[200].sum tensor(-1232.8381, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.7093, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(230942.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2431, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2431, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2426, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2440, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2440, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2440, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2487760.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1761.9030, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5058., device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(56632.4766, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4538.8916, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -9.4903],
        [ -9.4293],
        [ -9.3167],
        ...,
        [-11.1101],
        [-11.1162],
        [-11.1231]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-752624.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(485.4911, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(485.4911, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0137e-02, -4.6584e-03, -7.7419e-10,  ..., -8.2494e-04,
         -6.2153e-03, -1.5463e-02],
        [-1.0137e-02, -4.6584e-03, -7.7419e-10,  ..., -8.2494e-04,
         -6.2153e-03, -1.5463e-02],
        [-9.6037e-03, -5.6914e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02],
        ...,
        [-9.6037e-03, -5.6914e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02],
        [-9.6037e-03, -5.6914e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02],
        [-9.6037e-03, -5.6914e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-175757.7500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-983.5461, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(59.8870, device='cuda:0')



h[100].sum tensor(-246.2935, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-71.3847, device='cuda:0')



h[200].sum tensor(-1239.3319, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.5151, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(225344.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1666, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1731, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1882, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2488, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2488, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2488, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2456220., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1789.6147, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4947.3477, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(56322.7227, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4499.7578, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -4.9238],
        [ -5.2120],
        [ -5.7526],
        ...,
        [-11.5063],
        [-11.5031],
        [-11.5047]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1048518.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6245],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(646.3888, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6245],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(646.3888, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0857e-02, -3.2541e-03, -1.7101e-09,  ..., -1.2615e-03,
         -6.4987e-03, -1.6547e-02],
        [-1.3631e-02,  2.1363e-03, -5.4967e-09,  ..., -2.9438e-03,
         -7.5907e-03, -2.0725e-02],
        [-1.2051e-02, -9.3264e-04, -3.3409e-09,  ..., -1.9860e-03,
         -6.9690e-03, -1.8346e-02],
        ...,
        [-9.6037e-03, -5.6886e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02],
        [-9.6037e-03, -5.6886e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02],
        [-9.6037e-03, -5.6886e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-174470.3906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.0555, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(79.7343, device='cuda:0')



h[100].sum tensor(-232.9327, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-95.0424, device='cuda:0')



h[200].sum tensor(-1238.3197, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.9826, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0021, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0025, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(234290.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2492, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2492, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2492, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2506757.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1823.5500, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4963.0508, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(59140.5820, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4541.2979, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.1346],
        [  0.9141],
        [  0.7863],
        ...,
        [-11.5529],
        [-11.5495],
        [-11.5510]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-993480.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(697.0507, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(697.0507, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1524e-02, -1.9468e-03, -2.4627e-09,  ..., -1.6666e-03,
         -6.7616e-03, -1.7553e-02],
        [-1.0490e-02, -3.9612e-03, -1.1363e-09,  ..., -1.0392e-03,
         -6.3544e-03, -1.5995e-02],
        [-1.0736e-02, -3.4812e-03, -1.4523e-09,  ..., -1.1886e-03,
         -6.4514e-03, -1.6366e-02],
        ...,
        [-9.6037e-03, -5.6868e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02],
        [-9.6037e-03, -5.6868e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02],
        [-9.6037e-03, -5.6868e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-174033.6562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.5190, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(85.9836, device='cuda:0')



h[100].sum tensor(-228.0250, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-102.4916, device='cuda:0')



h[200].sum tensor(-1240.4335, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.0191, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(237843.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0235, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2461, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2461, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2461, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2510695.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1775.1034, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4909.3398, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(58744.5508, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4555.3418, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.9261],
        [  2.1493],
        [  2.1855],
        ...,
        [-11.3232],
        [-11.3211],
        [-11.3230]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-779027.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 640.0 event: 9600 loss: tensor(475.0337, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(640.7275, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(640.7275, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-174947.1875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.9966, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(79.0360, device='cuda:0')



h[100].sum tensor(-231.0095, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-94.2100, device='cuda:0')



h[200].sum tensor(-1245.3374, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.7551, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(233673.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0609, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1137, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1739, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2453, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2453, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2453, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2477880.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1740.3687, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4814.1279, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(57381.1406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4533.3086, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.3457],
        [  1.0662],
        [  0.8948],
        ...,
        [-11.2751],
        [-11.2737],
        [-11.2764]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-690520.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2678],
        [0.4895],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(691.1582, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2678],
        [0.4895],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(691.1582, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0244e-02, -4.4383e-03, -7.7104e-10,  ..., -8.9014e-04,
         -6.2576e-03, -1.5625e-02],
        [-1.1361e-02, -2.2608e-03, -2.1157e-09,  ..., -1.5676e-03,
         -6.6974e-03, -1.7307e-02],
        [-1.0831e-02, -3.2943e-03, -1.4775e-09,  ..., -1.2460e-03,
         -6.4887e-03, -1.6509e-02],
        ...,
        [-9.6037e-03, -5.6869e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02],
        [-9.6037e-03, -5.6869e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02],
        [-9.6037e-03, -5.6869e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-174493.7812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.4527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(85.2568, device='cuda:0')



h[100].sum tensor(-227.4843, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-101.6252, device='cuda:0')



h[200].sum tensor(-1243.7803, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.7822, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(236404.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2453, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2453, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2453, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2494514.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1744.1594, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4828.2891, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(57914.3438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4543.6401, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.3622],
        [  2.3411],
        [  2.2361],
        ...,
        [-11.2798],
        [-11.2780],
        [-11.2801]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-683908.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(650.9526, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(650.9526, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-175373.5781, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.1019, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(80.2973, device='cuda:0')



h[100].sum tensor(-228.7090, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-95.7135, device='cuda:0')



h[200].sum tensor(-1247.7075, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.1661, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(236209.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2483, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2483, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2478, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2492, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2492, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2492, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2512341.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1791.7881, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4696.0977, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(59311.3789, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4522.4541, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.1039],
        [ -9.9812],
        [ -9.7578],
        ...,
        [-11.6001],
        [-11.5969],
        [-11.5986]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-909698., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5044],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(579.1711, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5044],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(579.1711, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0387e-02, -4.1601e-03, -8.2967e-10,  ..., -9.7654e-04,
         -6.3137e-03, -1.5839e-02],
        [-1.0810e-02, -3.3339e-03, -1.2782e-09,  ..., -1.2333e-03,
         -6.4804e-03, -1.6477e-02],
        [-1.0913e-02, -3.1333e-03, -1.3871e-09,  ..., -1.2956e-03,
         -6.5208e-03, -1.6631e-02],
        ...,
        [-9.6037e-03, -5.6883e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02],
        [-9.6037e-03, -5.6883e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02],
        [-9.6037e-03, -5.6883e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-176493.5312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.4009, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(71.4428, device='cuda:0')



h[100].sum tensor(-233.0620, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-85.1590, device='cuda:0')



h[200].sum tensor(-1252.4600, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.2807, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(224370.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0602, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0169, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2522, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2522, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2522, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2418369., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1786.3805, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4601.2324, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(56726.5156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4450.7437, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.2271],
        [  1.5306],
        [  2.0181],
        ...,
        [-11.8622],
        [-11.8579],
        [-11.8593]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1017516.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6533],
        [0.6255],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(640.5796, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6533],
        [0.6255],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(640.5796, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1166e-02, -2.6386e-03, -1.5522e-09,  ..., -1.4493e-03,
         -6.6206e-03, -1.7013e-02],
        [-1.1100e-02, -2.7686e-03, -1.4861e-09,  ..., -1.4089e-03,
         -6.5944e-03, -1.6912e-02],
        [-1.1759e-02, -1.4807e-03, -2.1414e-09,  ..., -1.8089e-03,
         -6.8540e-03, -1.7905e-02],
        ...,
        [-9.6037e-03, -5.6893e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02],
        [-9.6037e-03, -5.6893e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02],
        [-9.6037e-03, -5.6893e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4661e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-175910.7500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.9753, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(79.0177, device='cuda:0')



h[100].sum tensor(-228.2665, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-94.1883, device='cuda:0')



h[200].sum tensor(-1252.6140, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.7491, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(230679.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2518, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2518, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2518, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2430887.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1733.1122, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4753.5615, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(56662., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4419.6445, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.1635],
        [  2.0894],
        [  1.9753],
        ...,
        [-11.8397],
        [-11.8357],
        [-11.8371]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-937426.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6323],
        [0.0000],
        [0.5942],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(544.2288, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6323],
        [0.0000],
        [0.5942],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(544.2288, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -5.6890e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02],
        [-1.2537e-02,  4.3578e-05, -2.7307e-09,  ..., -2.2807e-03,
         -7.1602e-03, -1.9076e-02],
        [-9.6037e-03, -5.6890e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02],
        ...,
        [-9.6037e-03, -5.6890e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02],
        [-9.6037e-03, -5.6890e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02],
        [-9.6037e-03, -5.6890e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-176718.9688, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.1086, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(67.1325, device='cuda:0')



h[100].sum tensor(-235.1060, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-80.0212, device='cuda:0')



h[200].sum tensor(-1257.5276, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.8762, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 4.3578e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 4.3578e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(225389.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2512, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2512, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2512, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2361382., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1632.0775, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4846.2881, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(52814.3320, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4319.3633, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.3405],
        [  2.3270],
        [  2.2724],
        ...,
        [-11.8070],
        [-11.8034],
        [-11.8049]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-821488.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(511.0820, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(511.0820, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-177558.6562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-983.7744, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(63.0437, device='cuda:0')



h[100].sum tensor(-236.9875, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-75.1475, device='cuda:0')



h[200].sum tensor(-1260.4316, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.5438, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(220417.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2537, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2537, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2533, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2546, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2546, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2546, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2346156., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1641.1991, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4545.6543, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(52818.0625, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4311.7231, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -9.3412],
        [ -9.9644],
        [-10.3064],
        ...,
        [-12.1060],
        [-12.1012],
        [-12.1023]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-962262.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(585.2054, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(585.2054, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-176810.6562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.5258, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(72.1871, device='cuda:0')



h[100].sum tensor(-231.1980, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-86.0463, device='cuda:0')



h[200].sum tensor(-1257.8894, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.5233, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(226816., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2509, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2300, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1830, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2546, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2546, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2546, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2383319., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1658.2659, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4571.0879, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(54219.5547, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4341.1455, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -8.6798],
        [ -7.0557],
        [ -4.9058],
        ...,
        [-12.1060],
        [-12.1012],
        [-12.1023]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-954825.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(950.9340, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(950.9340, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-174176.3281, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-987.8198, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(117.3010, device='cuda:0')



h[100].sum tensor(-204.5955, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-139.8216, device='cuda:0')



h[200].sum tensor(-1248.3379, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.2243, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(248956.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2561, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2520, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2300, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2570, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2570, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2570, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2552720., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1808.7805, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4374.8418, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(61823.9922, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4454.9048, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.0299],
        [ -8.8865],
        [ -6.8811],
        ...,
        [-12.3188],
        [-12.2489],
        [-11.9154]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-982038., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(589.1174, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(589.1174, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0975e-02, -3.0040e-03, -1.0467e-09,  ..., -1.3331e-03,
         -6.5452e-03, -1.6724e-02],
        [-9.6037e-03, -5.6885e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02],
        [-1.1052e-02, -2.8530e-03, -1.1055e-09,  ..., -1.3799e-03,
         -6.5755e-03, -1.6840e-02],
        ...,
        [-9.6037e-03, -5.6885e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02],
        [-9.6037e-03, -5.6885e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02],
        [-9.6037e-03, -5.6885e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-177932.1875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.5427, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(72.6697, device='cuda:0')



h[100].sum tensor(-228.6474, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-86.6215, device='cuda:0')



h[200].sum tensor(-1260.8179, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.6805, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(221576.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2572, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2572, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2572, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2349327.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1719.0004, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4187.4355, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(53741.0078, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4293.6860, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.1114],
        [  2.0511],
        [  1.9200],
        ...,
        [-12.4070],
        [-12.4013],
        [-12.4022]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-975538.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 650.0 event: 9750 loss: tensor(356.1895, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(664.8849, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(664.8849, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -5.6887e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02],
        [-9.6037e-03, -5.6887e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02],
        [-9.6037e-03, -5.6887e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02],
        ...,
        [-1.0241e-02, -4.4408e-03, -4.5471e-10,  ..., -8.8802e-04,
         -6.2563e-03, -1.5619e-02],
        [-9.6037e-03, -5.6887e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02],
        [-9.6037e-03, -5.6887e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-177598.2656, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.2303, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(82.0159, device='cuda:0')



h[100].sum tensor(-222.2480, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-97.7620, device='cuda:0')



h[200].sum tensor(-1259.7404, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.7261, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(226748.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2550, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2298, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1769, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2022, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2135, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2427, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2377597.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1714.6224, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4214.5269, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(54967.5156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4297.9185, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -9.5272],
        [ -7.5478],
        [ -4.6489],
        ...,
        [ -9.7319],
        [-10.3765],
        [-11.1890]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-844513.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(631.4736, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(631.4736, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-178235.0469, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.9283, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(77.8945, device='cuda:0')



h[100].sum tensor(-222.9970, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-92.8494, device='cuda:0')



h[200].sum tensor(-1261.9587, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.3831, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(224868.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2551, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2551, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2542, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2559, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2559, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2559, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2353665., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1711.5950, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4152.5234, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(54124.0586, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4266.7163, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.1556],
        [ -9.9241],
        [ -9.2716],
        ...,
        [-12.4289],
        [-12.4236],
        [-12.4246]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-861356.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4404],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(685.9840, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4404],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(685.9840, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0477e-02, -3.9755e-03, -5.8222e-10,  ..., -1.0311e-03,
         -6.3491e-03, -1.5974e-02],
        [-1.0657e-02, -3.6217e-03, -7.0256e-10,  ..., -1.1405e-03,
         -6.4201e-03, -1.6245e-02],
        [-1.0818e-02, -3.3057e-03, -8.1005e-10,  ..., -1.2382e-03,
         -6.4836e-03, -1.6488e-02],
        ...,
        [-9.6037e-03, -5.6874e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02],
        [-9.6037e-03, -5.6874e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02],
        [-9.6037e-03, -5.6874e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-177744.5312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.4208, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(84.6185, device='cuda:0')



h[100].sum tensor(-219.2042, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-100.8644, device='cuda:0')



h[200].sum tensor(-1260.3005, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.5742, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(225491.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2559, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2559, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2559, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2341885.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1710.7294, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4168.6436, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(53826.5898, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4268.9297, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.5492],
        [  2.5474],
        [  2.5314],
        ...,
        [-12.4289],
        [-12.4236],
        [-12.4246]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-824012.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(591.6161, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(591.6161, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -5.6860e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02],
        [-9.6037e-03, -5.6860e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02],
        [-1.2343e-02, -3.0801e-04, -1.7062e-09,  ..., -2.1629e-03,
         -7.0838e-03, -1.8782e-02],
        ...,
        [-9.6037e-03, -5.6860e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02],
        [-9.6037e-03, -5.6860e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02],
        [-9.6037e-03, -5.6860e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-178979.0469, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.5228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(72.9779, device='cuda:0')



h[100].sum tensor(-223.9320, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-86.9889, device='cuda:0')



h[200].sum tensor(-1264.3848, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.7810, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(218890.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2577, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2577, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2577, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2289592., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1705.5724, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4027.8584, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(52217.1562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4214.5347, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.8870],
        [  1.7664],
        [  1.6646],
        ...,
        [-12.6303],
        [-12.6236],
        [-12.6240]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-966679.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4307],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(611.0690, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4307],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(611.0690, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -5.6850e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02],
        [-1.0634e-02, -3.6612e-03, -5.9868e-10,  ..., -1.1263e-03,
         -6.4110e-03, -1.6210e-02],
        [-9.6037e-03, -5.6850e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02],
        ...,
        [-9.6037e-03, -5.6850e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02],
        [-9.6037e-03, -5.6850e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02],
        [-9.6037e-03, -5.6850e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-178997.8750, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.7325, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(75.3775, device='cuda:0')



h[100].sum tensor(-220.7175, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-89.8492, device='cuda:0')



h[200].sum tensor(-1264.5886, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.5629, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(220345.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1475, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0912, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2580, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2580, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2580, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2296441.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1724.7361, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4025.1528, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(53603.5195, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4208.3154, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -4.3527],
        [ -1.6476],
        [  0.6351],
        ...,
        [-12.7541],
        [-12.7477],
        [-12.7485]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-948509.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(672.0842, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(672.0842, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0304e-02, -4.3076e-03, -3.7949e-10,  ..., -9.2626e-04,
         -6.2811e-03, -1.5713e-02],
        [-9.6037e-03, -5.6842e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02],
        [-9.6037e-03, -5.6842e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02],
        ...,
        [-9.6037e-03, -5.6842e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02],
        [-9.6037e-03, -5.6842e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02],
        [-9.6037e-03, -5.6842e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-178577.9375, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.2699, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(82.9039, device='cuda:0')



h[100].sum tensor(-215.9532, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-98.8206, device='cuda:0')



h[200].sum tensor(-1263.5891, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.0155, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(224585.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0951, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1681, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2313, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2568, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2568, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2568, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2295067.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1724.9192, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3990.5757, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(53164.5781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4206.8350, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -1.4877],
        [ -4.3684],
        [ -7.1550],
        ...,
        [-12.5577],
        [-12.6355],
        [-12.6706]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-884009.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2878],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(611.0099, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2878],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(611.0099, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0164e-02, -4.5840e-03, -2.8322e-10,  ..., -8.4170e-04,
         -6.2262e-03, -1.5503e-02],
        [-1.0292e-02, -4.3329e-03, -3.4776e-10,  ..., -9.1918e-04,
         -6.2765e-03, -1.5696e-02],
        [-9.6037e-03, -5.6859e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02],
        ...,
        [-9.6037e-03, -5.6859e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02],
        [-9.6037e-03, -5.6859e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02],
        [-9.6037e-03, -5.6859e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-179194.2812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.7030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(75.3702, device='cuda:0')



h[100].sum tensor(-220.1516, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-89.8405, device='cuda:0')



h[200].sum tensor(-1266.1132, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.5605, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(219271.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0665, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1354, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2550, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2550, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2550, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2247038., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1700.2490, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3989.6877, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(51545.9297, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4169.5444, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.7880],
        [  0.2249],
        [ -1.8265],
        ...,
        [-11.1226],
        [-12.1737],
        [-12.5218]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-786562.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(778.6202, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(778.6202, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-177779.6250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.2401, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(96.0455, device='cuda:0')



h[100].sum tensor(-207.1372, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-114.4852, device='cuda:0')



h[200].sum tensor(-1261.4806, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.2979, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(235835.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0419, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1163, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2560, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2560, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2560, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2382951.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1786.6179, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4028.3293, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(58387.4883, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4242.2725, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.8386],
        [  0.5488],
        [ -1.6231],
        ...,
        [-12.7646],
        [-12.7498],
        [-12.6805]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-842935.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(558.0760, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(558.0760, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-179923.3750, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.2124, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(68.8406, device='cuda:0')



h[100].sum tensor(-221.4180, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-82.0573, device='cuda:0')



h[200].sum tensor(-1268.8363, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.4328, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(216899.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2537, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2559, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2566, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2579, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2579, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2579, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2229326.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1733.6259, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3836.6316, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(52908.2188, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4126.6406, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.2593],
        [-10.5759],
        [-10.8898],
        ...,
        [-13.0244],
        [-13.0172],
        [-13.0175]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1007403.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(568.2886, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(568.2886, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-179789.3594, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.3147, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(70.1004, device='cuda:0')



h[100].sum tensor(-220.1294, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-83.5589, device='cuda:0')



h[200].sum tensor(-1268.9670, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.8433, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(217021.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2271, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2487, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2536, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2571, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2571, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2571, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2213440.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1738.3766, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3855.6799, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(52966.2969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4113.2300, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -6.3824],
        [ -8.2004],
        [ -9.2202],
        ...,
        [-13.0414],
        [-13.0345],
        [-13.0353]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-973657.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 660.0 event: 9900 loss: tensor(406.5546, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(584.2970, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(584.2970, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-179428.2344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.4608, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(72.0751, device='cuda:0')



h[100].sum tensor(-218.8434, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-85.9127, device='cuda:0')



h[200].sum tensor(-1268.8989, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.4868, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(219372.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1565, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1118, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0426, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2524, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2524, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2524, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2214548.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1813.3096, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3912.3525, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(53369.6719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4185.1123, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.8618],
        [  1.1742],
        [  1.6568],
        ...,
        [-12.7701],
        [-12.7648],
        [-12.7660]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-816908.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3469],
        [0.2426],
        [0.3691],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(654.0195, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3469],
        [0.2426],
        [0.3691],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(654.0195, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1931e-02, -1.1158e-03, -8.1681e-10,  ..., -1.9131e-03,
         -6.9216e-03, -1.8160e-02],
        [-1.2543e-02,  8.7452e-05, -1.0317e-09,  ..., -2.2844e-03,
         -7.1627e-03, -1.9081e-02],
        [-1.1450e-02, -2.0601e-03, -6.4816e-10,  ..., -1.6217e-03,
         -6.7325e-03, -1.7437e-02],
        ...,
        [-9.6037e-03, -5.6892e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02],
        [-9.6037e-03, -5.6892e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02],
        [-9.6037e-03, -5.6892e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-178694.4688, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.1038, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(80.6756, device='cuda:0')



h[100].sum tensor(-212.8646, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-96.1644, device='cuda:0')



h[200].sum tensor(-1267.1212, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.2894, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 8.7452e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 7.6441e-04, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 8.5186e-04, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(224864.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2513, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2513, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2513, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2241784., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1859.8112, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3939.5000, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(55443.2578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4213.0547, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.8534],
        [  0.7479],
        [  0.7354],
        ...,
        [-12.7620],
        [-12.7569],
        [-12.7582]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-801829.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3547],
        [0.3411],
        [0.3718],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(585.3971, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3547],
        [0.3411],
        [0.3718],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(585.3971, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1335e-02, -2.2872e-03, -5.6364e-10,  ..., -1.5519e-03,
         -6.6872e-03, -1.7264e-02],
        [-1.1341e-02, -2.2757e-03, -5.6554e-10,  ..., -1.5555e-03,
         -6.6895e-03, -1.7273e-02],
        [-1.0419e-02, -4.0872e-03, -2.6548e-10,  ..., -9.9637e-04,
         -6.3266e-03, -1.5886e-02],
        ...,
        [-9.6037e-03, -5.6899e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02],
        [-9.6037e-03, -5.6899e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02],
        [-9.6037e-03, -5.6899e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-179359.2188, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.5076, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(72.2108, device='cuda:0')



h[100].sum tensor(-215.7399, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-86.0745, device='cuda:0')



h[200].sum tensor(-1269.4463, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.5310, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(219615.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2537, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2537, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2537, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2197184., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1815.6554, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3838.2471, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(54619.5469, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4119.9927, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.7506],
        [  2.0708],
        [  2.2726],
        ...,
        [-13.0485],
        [-13.0421],
        [-13.0430]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1011988.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4182],
        [0.6533],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(749.0883, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4182],
        [0.6533],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(749.0883, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4683e-02,  4.2909e-03, -1.5316e-09,  ..., -3.5821e-03,
         -8.0050e-03, -2.2298e-02],
        [-1.1809e-02, -1.3566e-03, -6.6497e-10,  ..., -1.8391e-03,
         -6.8736e-03, -1.7976e-02],
        [-1.2670e-02,  3.3558e-04, -9.2463e-10,  ..., -2.3614e-03,
         -7.2126e-03, -1.9271e-02],
        ...,
        [-9.6037e-03, -5.6901e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02],
        [-9.6037e-03, -5.6901e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02],
        [-9.6037e-03, -5.6901e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-177729.5312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.9922, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(92.4026, device='cuda:0')



h[100].sum tensor(-203.3348, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-110.1430, device='cuda:0')



h[200].sum tensor(-1264.7346, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.1108, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0030, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(234752.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2522, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2522, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2522, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2291044.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1901.8539, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3863.8682, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(59141.3945, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4198.6328, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.4150],
        [  0.3902],
        [  0.3733],
        ...,
        [-13.0292],
        [-13.0230],
        [-13.0240]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1021313.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(695.8319, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(695.8319, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-177914.8438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.5056, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(85.8333, device='cuda:0')



h[100].sum tensor(-206.7225, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-102.3124, device='cuda:0')



h[200].sum tensor(-1266.6058, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.9701, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(228014.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2035, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2402, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2456, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2476, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2476, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2476, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2218182., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1957.0522, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3915.4834, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(56623.6562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4214.9414, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -5.3417],
        [ -7.6986],
        [ -9.1875],
        ...,
        [-12.7738],
        [-12.7690],
        [-12.7704]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-857039.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.8315],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(755.0613, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.8315],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(755.0613, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -5.6902e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02],
        [-1.1592e-02, -1.7818e-03, -5.1306e-10,  ..., -1.7077e-03,
         -6.7884e-03, -1.7650e-02],
        [-9.6037e-03, -5.6902e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02],
        ...,
        [-9.6037e-03, -5.6902e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02],
        [-9.6037e-03, -5.6902e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02],
        [-9.6037e-03, -5.6902e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4660e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-177131.2812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.0549, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(93.1394, device='cuda:0')



h[100].sum tensor(-202.5938, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-111.0212, device='cuda:0')



h[200].sum tensor(-1264.9678, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.3509, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(235500.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0520, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0688, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2446, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2446, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2446, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2258106.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2020.9031, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3959.0725, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(58137.7344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4269.9346, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.6400],
        [  1.9169],
        [  1.8988],
        ...,
        [-12.6349],
        [-12.6309],
        [-12.6326]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-770988.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(568.8191, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(568.8191, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-178798.2188, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.3356, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(70.1658, device='cuda:0')



h[100].sum tensor(-215.0167, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-83.6369, device='cuda:0')



h[200].sum tensor(-1270.8636, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.8646, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(219427.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2446, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2446, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2441, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2455, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2455, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2455, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2144249.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1963.9261, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3906.3882, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(54102.9219, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4137.7881, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.8345],
        [-10.8141],
        [-10.6771],
        ...,
        [-12.7785],
        [-12.7969],
        [-12.8000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-862244.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(613.4459, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(613.4459, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0893e-02, -3.1600e-03, -2.8340e-10,  ..., -1.2839e-03,
         -6.5132e-03, -1.6598e-02],
        [-9.6037e-03, -5.6924e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02],
        [-9.6037e-03, -5.6924e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02],
        ...,
        [-9.6037e-03, -5.6924e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02],
        [-9.6037e-03, -5.6924e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02],
        [-9.6037e-03, -5.6924e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-178380.2188, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.7655, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(75.6707, device='cuda:0')



h[100].sum tensor(-210.8888, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-90.1987, device='cuda:0')



h[200].sum tensor(-1269.5359, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.6584, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(222538.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0351, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0826, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1863, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2479, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2479, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2479, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2165389.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1955.6670, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3897.4480, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(55758.9453, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4075.1101, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.1750],
        [ -2.2192],
        [ -5.4189],
        ...,
        [-13.1009],
        [-13.0950],
        [-13.0961]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1057085.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4358],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(640.0647, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4358],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(640.0647, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0099e-02, -4.7220e-03, -1.0016e-10,  ..., -8.0181e-04,
         -6.2003e-03, -1.5403e-02],
        [-1.1141e-02, -2.6765e-03, -3.1112e-10,  ..., -1.4339e-03,
         -6.6106e-03, -1.6970e-02],
        [-9.6037e-03, -5.6933e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02],
        ...,
        [-9.6037e-03, -5.6933e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02],
        [-9.6037e-03, -5.6933e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02],
        [-9.6037e-03, -5.6933e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-177858.7812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.9827, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(78.9542, device='cuda:0')



h[100].sum tensor(-209.1031, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-94.1126, device='cuda:0')



h[200].sum tensor(-1268.9049, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.7284, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(225836.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0296, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0725, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2435, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2435, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2435, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2193729.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1953.2072, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3962.9556, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(54314.2422, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4007.7554, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.8646],
        [  0.8104],
        [ -1.2198],
        ...,
        [-12.9674],
        [-12.9621],
        [-12.9634]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-932670.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(640.6439, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(640.6439, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-177540.0312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.0024, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(79.0257, device='cuda:0')



h[100].sum tensor(-208.7966, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-94.1977, device='cuda:0')



h[200].sum tensor(-1268.9185, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.7517, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(223413.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2390, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2390, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2385, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2400, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2400, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2400, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2160129.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1944.2404, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4034.9939, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(51304.4570, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3903.2920, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.9373],
        [-10.9245],
        [-10.8709],
        ...,
        [-12.9220],
        [-12.9173],
        [-12.9188]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-847021.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 670.0 event: 10050 loss: tensor(438.4288, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(627.0079, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(627.0079, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-177536.6406, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.8970, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(77.3436, device='cuda:0')



h[100].sum tensor(-208.3279, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-92.1928, device='cuda:0')



h[200].sum tensor(-1269.3859, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.2036, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(223381., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2397, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2397, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2393, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2407, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2407, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2407, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2175420.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1903.7727, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4046.4768, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(50881.9883, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3759.4514, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-11.2557],
        [-11.2290],
        [-11.1359],
        ...,
        [-13.1950],
        [-13.1897],
        [-13.1911]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1011381.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(782.6793, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(782.6793, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-175930.7500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.3853, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(96.5462, device='cuda:0')



h[100].sum tensor(-195.9059, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-115.0821, device='cuda:0')



h[200].sum tensor(-1264.4990, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.4611, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(237583.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2216, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1914, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0887, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2406, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2406, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2406, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2281691.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1942.3049, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4101.7354, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(54217.6719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3725.9937, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -8.8391],
        [ -6.4586],
        [ -3.2063],
        ...,
        [-13.3694],
        [-13.3648],
        [-13.3665]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1073793.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(630.2289, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(630.2289, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -5.6906e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02],
        [-9.6037e-03, -5.6906e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02],
        [-1.0209e-02, -4.5020e-03, -8.6874e-11,  ..., -8.6854e-04,
         -6.2436e-03, -1.5568e-02],
        ...,
        [-9.6037e-03, -5.6906e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02],
        [-9.6037e-03, -5.6906e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02],
        [-9.6037e-03, -5.6906e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-177153.7188, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.8944, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(77.7409, device='cuda:0')



h[100].sum tensor(-206.8385, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-92.6664, device='cuda:0')



h[200].sum tensor(-1269.6287, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.3331, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(223544.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1902, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1167, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0524, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2371, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2371, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2371, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2175718., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1914.4285, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4144.1040, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(48849.7969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3605.8193, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -5.4286],
        [ -2.7445],
        [ -0.4127],
        ...,
        [-13.2418],
        [-13.2381],
        [-13.2401]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-980338.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2595],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(655.8695, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2595],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(655.8695, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0974e-02, -2.9954e-03, -1.7991e-10,  ..., -1.3329e-03,
         -6.5450e-03, -1.6719e-02],
        [-1.1078e-02, -2.7911e-03, -1.9354e-10,  ..., -1.3959e-03,
         -6.5859e-03, -1.6875e-02],
        [-1.0169e-02, -4.5783e-03, -7.4216e-11,  ..., -8.4460e-04,
         -6.2281e-03, -1.5509e-02],
        ...,
        [-9.6037e-03, -5.6898e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02],
        [-9.6037e-03, -5.6898e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02],
        [-9.6037e-03, -5.6898e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-176759.6875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.1234, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(80.9038, device='cuda:0')



h[100].sum tensor(-204.5152, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-96.4365, device='cuda:0')



h[200].sum tensor(-1268.9500, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.3637, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(223138.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2348, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2348, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2348, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2164573.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1925.3163, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4203.5830, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(47408.5000, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3556.7168, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.0114],
        [  2.2016],
        [  2.1354],
        ...,
        [-13.1737],
        [-13.1705],
        [-13.1726]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-897656.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(497.1412, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(497.1412, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1537e-02, -1.8887e-03, -2.3146e-10,  ..., -1.6741e-03,
         -6.7665e-03, -1.7564e-02],
        [-1.1400e-02, -2.1587e-03, -2.1502e-10,  ..., -1.5908e-03,
         -6.7124e-03, -1.7358e-02],
        [-1.2628e-02,  2.5632e-04, -3.6209e-10,  ..., -2.3357e-03,
         -7.1960e-03, -1.9203e-02],
        ...,
        [-9.6037e-03, -5.6896e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02],
        [-9.6037e-03, -5.6896e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02],
        [-9.6037e-03, -5.6896e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-178291.2969, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-983.6400, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(61.3241, device='cuda:0')



h[100].sum tensor(-214.8898, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-73.0977, device='cuda:0')



h[200].sum tensor(-1273.9663, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.9834, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(213200.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2360, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2360, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2360, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2105839.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1853.6370, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4136.4072, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(43798.3594, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3411.9004, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.6366],
        [  1.3393],
        [  1.0316],
        ...,
        [-13.3328],
        [-13.3293],
        [-13.3314]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1020761.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(636.2589, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(636.2589, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-177112.5000, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.9249, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(78.4847, device='cuda:0')



h[100].sum tensor(-203.7840, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-93.5530, device='cuda:0')



h[200].sum tensor(-1269.6954, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.5755, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(218655.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1866, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1611, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0856, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2377, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2377, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2377, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2130309.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1886.3495, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4206.0205, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(44994.8281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3383.1729, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -6.6815],
        [ -4.5946],
        [ -1.9605],
        ...,
        [-13.5101],
        [-13.5058],
        [-13.5078]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1019669.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2920],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(622.7213, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2920],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(622.7213, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -5.6895e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02],
        [-1.0302e-02, -4.3163e-03, -6.9119e-11,  ..., -9.2520e-04,
         -6.2804e-03, -1.5708e-02],
        [-1.0100e-02, -4.7136e-03, -4.9121e-11,  ..., -8.0267e-04,
         -6.2009e-03, -1.5405e-02],
        ...,
        [-9.6037e-03, -5.6895e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02],
        [-9.6037e-03, -5.6895e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02],
        [-9.6037e-03, -5.6895e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-177359.6875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.8163, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(76.8148, device='cuda:0')



h[100].sum tensor(-203.5461, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-91.5625, device='cuda:0')



h[200].sum tensor(-1270.0785, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.0313, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(218222.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1517, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0895, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0225, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2391, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2391, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2391, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2128835.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1882.7802, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4167.7749, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(44336.8906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3326.2217, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -4.6467],
        [ -1.9353],
        [ -0.0644],
        ...,
        [-13.6462],
        [-13.6413],
        [-13.6430]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1109128., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2930],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(543.0565, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2930],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(543.0565, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0304e-02, -4.3122e-03, -6.2824e-11,  ..., -9.2662e-04,
         -6.2813e-03, -1.5712e-02],
        [-1.0127e-02, -4.6606e-03, -4.6935e-11,  ..., -8.1915e-04,
         -6.2116e-03, -1.5445e-02],
        [-1.0828e-02, -3.2829e-03, -1.0976e-10,  ..., -1.2441e-03,
         -6.4874e-03, -1.6498e-02],
        ...,
        [-9.6037e-03, -5.6899e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02],
        [-9.6037e-03, -5.6899e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02],
        [-9.6037e-03, -5.6899e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-178182.2188, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.0961, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(66.9879, device='cuda:0')



h[100].sum tensor(-208.3611, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-79.8489, device='cuda:0')



h[200].sum tensor(-1272.4824, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.8290, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(212759.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0334, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2397, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2397, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2397, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2093804., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1851.7634, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4140.6597, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(42316.7422, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3245.1833, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.5542],
        [  2.2244],
        [  0.9310],
        ...,
        [-13.6839],
        [-13.6784],
        [-13.6799]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1101188.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.9639],
        [0.3210],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(686.3151, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.9639],
        [0.3210],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(686.3151, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1909e-02, -1.1570e-03, -1.8672e-10,  ..., -1.8997e-03,
         -6.9129e-03, -1.8122e-02],
        [-1.0371e-02, -4.1801e-03, -6.2193e-11,  ..., -9.6734e-04,
         -6.3078e-03, -1.5812e-02],
        [-1.4660e-02,  4.2531e-03, -4.0958e-10,  ..., -3.5682e-03,
         -7.9960e-03, -2.2255e-02],
        ...,
        [-9.6037e-03, -5.6899e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02],
        [-9.6037e-03, -5.6899e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02],
        [-9.6037e-03, -5.6899e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-176887.5469, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.4579, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(84.6593, device='cuda:0')



h[100].sum tensor(-197.2966, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-100.9131, device='cuda:0')



h[200].sum tensor(-1267.9080, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.5875, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0012, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(222503.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2397, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2397, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2397, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2143236., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1886.2944, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4174.4473, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(44047.1641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3253.4226, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.9459],
        [  1.0721],
        [  0.9626],
        ...,
        [-13.6734],
        [-13.6669],
        [-13.6680]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1003638.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2976],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(813.2690, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2976],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(813.2690, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -5.6897e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02],
        [-1.0857e-02, -3.2242e-03, -9.1469e-11,  ..., -1.2621e-03,
         -6.4991e-03, -1.6542e-02],
        [-1.0146e-02, -4.6238e-03, -3.9544e-11,  ..., -8.3043e-04,
         -6.2189e-03, -1.5473e-02],
        ...,
        [-9.6037e-03, -5.6897e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02],
        [-9.6037e-03, -5.6897e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02],
        [-9.6037e-03, -5.6897e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-175818.5625, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.6180, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(100.3196, device='cuda:0')



h[100].sum tensor(-186.6738, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-119.5799, device='cuda:0')



h[200].sum tensor(-1264.0339, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.6907, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(227853.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0790, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0338, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2418, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2418, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2418, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2168402.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1918.5940, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4232.6064, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(45653.1641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3254.6023, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -1.4962],
        [  0.8896],
        [  2.2636],
        ...,
        [-13.8165],
        [-13.8093],
        [-13.8101]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1024617.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 680.0 event: 10200 loss: tensor(436.5881, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(549.4364, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(549.4364, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-178314.7969, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.1257, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(67.7749, device='cuda:0')



h[100].sum tensor(-204.3267, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-80.7870, device='cuda:0')



h[200].sum tensor(-1272.3831, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.0855, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(210503.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2420, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2420, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2416, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2430, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2430, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2430, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2060634., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1854.9087, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4155.3799, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(41321.7383, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3132.9033, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-11.6319],
        [-11.1394],
        [-10.2645],
        ...,
        [-13.8944],
        [-13.8863],
        [-13.8869]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1081537., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6211],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(630.9174, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6211],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(630.9174, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -5.6880e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02],
        [-1.1089e-02, -2.7656e-03, -8.6974e-11,  ..., -1.4025e-03,
         -6.5902e-03, -1.6890e-02],
        [-9.6037e-03, -5.6880e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02],
        ...,
        [-9.6037e-03, -5.6880e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02],
        [-9.6037e-03, -5.6880e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02],
        [-9.6037e-03, -5.6880e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-177261.7812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.9171, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(77.8259, device='cuda:0')



h[100].sum tensor(-196.8593, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-92.7676, device='cuda:0')



h[200].sum tensor(-1269.7268, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.3607, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(222101.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0852, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0616, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2433, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2433, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2433, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2130605.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1890.1930, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4281.0430, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(44059.6797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3038.0005, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -0.0574],
        [  0.8552],
        [  1.6033],
        ...,
        [-13.8858],
        [-13.8773],
        [-13.8778]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1061608.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3457],
        [0.3203],
        [0.3201],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(617.3879, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3457],
        [0.3203],
        [0.3201],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(617.3879, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2277e-02, -4.2748e-04, -1.3936e-10,  ..., -2.1228e-03,
         -7.0578e-03, -1.8673e-02],
        [-1.3206e-02,  1.4005e-03, -1.8779e-10,  ..., -2.6862e-03,
         -7.4235e-03, -2.0068e-02],
        [-1.2971e-02,  9.3798e-04, -1.7554e-10,  ..., -2.5437e-03,
         -7.3310e-03, -1.9715e-02],
        ...,
        [-9.6037e-03, -5.6873e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02],
        [-9.6037e-03, -5.6873e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02],
        [-9.6037e-03, -5.6873e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-177083.1562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.8032, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(76.1569, device='cuda:0')



h[100].sum tensor(-196.5985, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-90.7783, device='cuda:0')



h[200].sum tensor(-1270.0747, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.8169, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0023, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(220970.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2429, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2429, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2429, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2087493.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1884.3258, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4429.9839, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(42837.5625, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2884.1292, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.5973],
        [  0.6171],
        [  0.6764],
        ...,
        [-13.8242],
        [-13.8157],
        [-13.8161]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1010935.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4817],
        [0.3882],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(603.3099, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4817],
        [0.3882],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(603.3099, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0532e-02, -3.8594e-03, -4.2883e-11,  ..., -1.0647e-03,
         -6.3710e-03, -1.6053e-02],
        [-1.0756e-02, -3.4193e-03, -5.3212e-11,  ..., -1.2003e-03,
         -6.4590e-03, -1.6389e-02],
        [-1.0532e-02, -3.8594e-03, -4.2883e-11,  ..., -1.0647e-03,
         -6.3710e-03, -1.6053e-02],
        ...,
        [-9.6037e-03, -5.6864e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02],
        [-9.6037e-03, -5.6864e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02],
        [-9.6037e-03, -5.6864e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-177400.8281, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.6250, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(74.4204, device='cuda:0')



h[100].sum tensor(-196.3041, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-88.7083, device='cuda:0')



h[200].sum tensor(-1270.6409, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.2510, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(218640.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0688, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2439, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2439, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2439, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2086286.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1900.4135, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4321.7588, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(43579.5586, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2994.5120, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.5361],
        [  1.2022],
        [ -1.4205],
        ...,
        [-13.6343],
        [-13.7192],
        [-13.7813]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1075695.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(771.6254, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(771.6254, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-175828.6562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.1471, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(95.1827, device='cuda:0')



h[100].sum tensor(-183.7220, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-113.4567, device='cuda:0')



h[200].sum tensor(-1265.4895, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.0167, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(231225.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2388, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2269, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1911, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2434, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2434, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2434, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2163451., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1959.6123, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4303.8662, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(46761.1875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3101.5095, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-11.2032],
        [-10.4878],
        [ -9.2769],
        ...,
        [-13.7775],
        [-13.7687],
        [-13.7691]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1040006.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4028],
        [0.5376],
        [0.4321],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(692.6974, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4028],
        [0.5376],
        [0.4321],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(692.6974, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2908e-02,  8.5146e-04, -1.1773e-10,  ..., -2.5058e-03,
         -7.3064e-03, -1.9620e-02],
        [-1.1600e-02, -1.7328e-03, -7.1140e-11,  ..., -1.7127e-03,
         -6.7916e-03, -1.7657e-02],
        [-1.1981e-02, -9.8168e-04, -8.4682e-11,  ..., -1.9432e-03,
         -6.9412e-03, -1.8228e-02],
        ...,
        [-9.6037e-03, -5.6787e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02],
        [-9.6037e-03, -5.6787e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02],
        [-9.6037e-03, -5.6787e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-176388.9219, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.4447, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(85.4466, device='cuda:0')



h[100].sum tensor(-189.7398, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-101.8515, device='cuda:0')



h[200].sum tensor(-1268.0325, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.8441, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(223202.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2418, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2418, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2418, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2105112., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1928.0468, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4325.6548, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(44615.9609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3060.0205, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.8790],
        [  0.9222],
        [  1.2066],
        ...,
        [-13.6214],
        [-13.6130],
        [-13.6134]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-935361.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2467],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(753.2385, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2467],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(753.2385, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0219e-02, -4.4502e-03, -1.9047e-11,  ..., -8.7495e-04,
         -6.2478e-03, -1.5583e-02],
        [-1.0194e-02, -4.5007e-03, -1.8259e-11,  ..., -8.5952e-04,
         -6.2378e-03, -1.5544e-02],
        [-9.6037e-03, -5.6712e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02],
        ...,
        [-9.6037e-03, -5.6712e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02],
        [-9.6037e-03, -5.6712e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02],
        [-9.6037e-03, -5.6712e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4659e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-175660.4531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.0333, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(92.9146, device='cuda:0')



h[100].sum tensor(-186.2097, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-110.7532, device='cuda:0')



h[200].sum tensor(-1266.2264, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.2776, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(230681.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0526, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0792, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2414, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2414, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2414, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2149670.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1965.3289, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4310.3970, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(46007.3867, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3095.5847, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.4243],
        [  2.2785],
        [  2.1664],
        ...,
        [-13.5873],
        [-13.5789],
        [-13.5794]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-922379.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.7236],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(771.8608, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.7236],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(771.8608, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -5.6682e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02],
        [-1.1334e-02, -2.2281e-03, -4.6102e-11,  ..., -1.5512e-03,
         -6.6868e-03, -1.7257e-02],
        [-1.0226e-02, -4.4306e-03, -1.6586e-11,  ..., -8.7929e-04,
         -6.2506e-03, -1.5593e-02],
        ...,
        [-9.6037e-03, -5.6682e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02],
        [-9.6037e-03, -5.6682e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02],
        [-9.6037e-03, -5.6682e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-175455.4062, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.1677, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(95.2117, device='cuda:0')



h[100].sum tensor(-186.2158, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-113.4914, device='cuda:0')



h[200].sum tensor(-1265.7970, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.0262, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(230151.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2433, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2433, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2433, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2158991.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1992.7661, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4353.3994, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(47472.2539, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3066.0762, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.0714],
        [  1.1860],
        [  1.2534],
        ...,
        [-13.7475],
        [-13.7418],
        [-13.7421]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1012620.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2510],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(670.0121, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2510],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(670.0121, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0204e-02, -4.4709e-03, -1.3611e-11,  ..., -8.6571e-04,
         -6.2418e-03, -1.5559e-02],
        [-1.0912e-02, -3.0614e-03, -2.9664e-11,  ..., -1.2950e-03,
         -6.5205e-03, -1.6622e-02],
        [-1.3999e-02,  3.0864e-03, -9.9681e-11,  ..., -3.1676e-03,
         -7.7360e-03, -2.1257e-02],
        ...,
        [-9.6037e-03, -5.6660e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02],
        [-9.6037e-03, -5.6660e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02],
        [-9.6037e-03, -5.6660e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-176227.7500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.2926, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(82.6483, device='cuda:0')



h[100].sum tensor(-194.2462, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-98.5159, device='cuda:0')



h[200].sum tensor(-1268.7288, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.9322, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(225426.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0139, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2444, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2444, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2444, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2113485., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1992.5156, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4303.5332, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(45829.3672, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3007.2913, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.0576],
        [  0.8096],
        [  0.6742],
        ...,
        [-13.8186],
        [-13.8093],
        [-13.8095]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1118703.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(744.3934, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(744.3934, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -5.6607e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02],
        [-9.6037e-03, -5.6607e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02],
        [-9.6037e-03, -5.6607e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02],
        ...,
        [-9.6037e-03, -5.6607e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02],
        [-9.6037e-03, -5.6607e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02],
        [-1.0700e-02, -3.4715e-03, -2.0874e-11,  ..., -1.1667e-03,
         -6.4372e-03, -1.6304e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-175456.0312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.9232, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(91.8235, device='cuda:0')



h[100].sum tensor(-190.5998, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-109.4527, device='cuda:0')



h[200].sum tensor(-1266.6776, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.9221, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(228741.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2149, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2390, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2416, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2073, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1226, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0637, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2127907., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2019.1492, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4294.3994, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(46512.7578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3075.6853, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -7.2864],
        [ -9.2688],
        [-10.4005],
        ...,
        [ -9.8902],
        [ -6.6435],
        [ -3.1139]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1023410.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 690.0 event: 10350 loss: tensor(434.1703, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6284],
        [0.0000],
        [0.4326],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(546.9941, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6284],
        [0.0000],
        [0.4326],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(546.9941, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2397e-02, -6.2231e-05, -4.3859e-11,  ..., -2.1959e-03,
         -7.1052e-03, -1.8852e-02],
        [-1.3688e-02,  2.5220e-03, -6.4125e-11,  ..., -2.9788e-03,
         -7.6134e-03, -2.0789e-02],
        [-1.2117e-02, -6.2409e-04, -3.9452e-11,  ..., -2.0257e-03,
         -6.9947e-03, -1.8430e-02],
        ...,
        [-9.6037e-03, -5.6549e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02],
        [-9.6037e-03, -5.6549e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02],
        [-9.6037e-03, -5.6549e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-176897.7812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.1233, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(67.4736, device='cuda:0')



h[100].sum tensor(-205.5361, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-80.4279, device='cuda:0')



h[200].sum tensor(-1272.7581, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.9873, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(218497.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2409, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2409, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2409, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2040530.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1967.4951, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4150.6294, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(42239.8281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3134.6326, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.6873],
        [  0.6836],
        [  0.7219],
        ...,
        [-13.5031],
        [-13.4924],
        [-13.4915]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-989778.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(705.9219, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(705.9219, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-175470.5781, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.6069, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(87.0779, device='cuda:0')



h[100].sum tensor(-194.0245, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-103.7960, device='cuda:0')



h[200].sum tensor(-1267.8025, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.3757, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(227403.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2384, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2384, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2379, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2394, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2394, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2394, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2114040., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2025.9086, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4344.3662, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(46284.5078, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3089.0278, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-11.2196],
        [-10.9429],
        [-10.4191],
        ...,
        [-13.4166],
        [-13.4087],
        [-13.4092]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-881439.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3293],
        [0.2595],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(539.0309, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3293],
        [0.2595],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(539.0309, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0391e-02, -4.0651e-03, -7.7620e-12,  ..., -9.7938e-04,
         -6.3156e-03, -1.5840e-02],
        [-1.0224e-02, -4.4006e-03, -6.1164e-12,  ..., -8.7810e-04,
         -6.2498e-03, -1.5590e-02],
        [-1.0983e-02, -2.8761e-03, -1.3594e-11,  ..., -1.3382e-03,
         -6.5485e-03, -1.6728e-02],
        ...,
        [-9.6037e-03, -5.6477e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02],
        [-9.6037e-03, -5.6477e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02],
        [-9.6037e-03, -5.6477e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-177230.7500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.0431, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(66.4913, device='cuda:0')



h[100].sum tensor(-205.7374, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-79.2570, device='cuda:0')



h[200].sum tensor(-1273.0281, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.6672, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(214152.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0895, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2400, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2400, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2400, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2026535.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1995.2931, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4289.4854, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(42958.5859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3019.8472, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -3.2411],
        [ -0.3177],
        [  1.5670],
        ...,
        [-13.4586],
        [-13.4504],
        [-13.4509]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-988795., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(532.5537, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(532.5537, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -5.6443e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02],
        [-9.6037e-03, -5.6443e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02],
        [-1.0182e-02, -4.4795e-03, -4.2310e-12,  ..., -8.5269e-04,
         -6.2333e-03, -1.5527e-02],
        ...,
        [-9.6037e-03, -5.6443e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02],
        [-9.6037e-03, -5.6443e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02],
        [-9.6037e-03, -5.6443e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-177262.4375, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-983.9834, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(65.6924, device='cuda:0')



h[100].sum tensor(-206.4043, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-78.3046, device='cuda:0')



h[200].sum tensor(-1273.2108, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.4069, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(218507.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2116, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1570, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1373, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1893, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2363, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2398, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2043318.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1871.5986, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4482.5518, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(44542., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3063.6294, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -8.2807],
        [ -6.6588],
        [ -5.4547],
        ...,
        [ -6.8849],
        [-10.2365],
        [-12.2649]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1035172.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(784.9929, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(784.9929, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -5.6414e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02],
        [-9.6037e-03, -5.6414e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02],
        [-1.1427e-02, -1.9663e-03, -9.1103e-12,  ..., -1.6076e-03,
         -6.7233e-03, -1.7394e-02],
        ...,
        [-9.6037e-03, -5.6414e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02],
        [-9.6037e-03, -5.6414e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02],
        [-9.6037e-03, -5.6414e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-175268.4375, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.3408, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(96.8316, device='cuda:0')



h[100].sum tensor(-188.6722, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-115.4223, device='cuda:0')



h[200].sum tensor(-1265.2927, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.5541, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(234550.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1353, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0530, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0087, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1864, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2358, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2395, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2165999.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2094.1912, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4349.1748, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(47330.9453, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3064.4502, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -3.7923],
        [ -0.8582],
        [  1.1099],
        ...,
        [ -6.6080],
        [-10.0571],
        [-12.1627]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-933508.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(816.4719, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(816.4719, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0056,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0056,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056,  0.0000,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056,  0.0000,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-175351.8438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.6344, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(100.7146, device='cuda:0')



h[100].sum tensor(-185.6064, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-120.0508, device='cuda:0')



h[200].sum tensor(-1264.3033, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.8194, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(236992.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2393, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2393, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2304, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2403, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2403, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2403, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2181043.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2109.5244, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4327.7012, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(46872.9648, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3045.7202, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.7996],
        [ -9.9269],
        [ -8.3434],
        ...,
        [-13.5263],
        [-13.5177],
        [-13.5180]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-988962.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(566.4194, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(566.4194, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0254e-02, -4.3148e-03, -6.4662e-13,  ..., -8.9606e-04,
         -6.2615e-03, -1.5634e-02],
        [-1.0909e-02, -2.9879e-03, -1.2984e-12,  ..., -1.2936e-03,
         -6.5195e-03, -1.6617e-02],
        [-1.0259e-02, -4.3041e-03, -6.5183e-13,  ..., -8.9924e-04,
         -6.2635e-03, -1.5642e-02],
        ...,
        [-9.6037e-03, -5.6310e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02],
        [-9.6037e-03, -5.6310e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02],
        [-9.6037e-03, -5.6310e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-177980.2188, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.3129, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(69.8698, device='cuda:0')



h[100].sum tensor(-201.7542, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-83.2841, device='cuda:0')



h[200].sum tensor(-1272.0891, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.7681, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(216654.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0322, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2422, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2422, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2422, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2048944.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2061.6399, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4259.0366, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(41016.8594, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2923.7258, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.5193],
        [  2.3523],
        [  1.0406],
        ...,
        [-13.7168],
        [-13.7075],
        [-13.7077]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1116533.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.5933]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(748.2438, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.5933]], device='cuda:0') 
g.ndata[nfet].sum tensor(748.2438, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0113e-02, -4.5919e-03,  3.6772e-13,  ..., -8.1077e-04,
         -6.2061e-03, -1.5423e-02],
        [-1.0099e-02, -4.6209e-03,  3.5740e-13,  ..., -8.0210e-04,
         -6.2005e-03, -1.5401e-02],
        [-1.0706e-02, -3.3898e-03,  7.9529e-13,  ..., -1.1701e-03,
         -6.4394e-03, -1.6312e-02],
        ...,
        [-9.6037e-03, -5.6257e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02],
        [-1.1022e-02, -2.7475e-03,  1.0237e-12,  ..., -1.3621e-03,
         -6.5640e-03, -1.6787e-02],
        [-9.6037e-03, -5.6257e-03,  0.0000e+00,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-176386.7031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.0918, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(92.2985, device='cuda:0')



h[100].sum tensor(-186.2538, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-110.0188, device='cuda:0')



h[200].sum tensor(-1266.1366, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.0769, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 1.7475e-12,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.1126e-12,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.4446e-12,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 1.0237e-12,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 8.3588e-13,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 3.7192e-12,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(231878.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0245, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0123, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0305, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1236, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0972, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2147476., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2129.6143, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4297.1592, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(44312.5469, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2974.9531, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 2.6158],
        [ 2.6012],
        [ 2.5525],
        ...,
        [-7.8170],
        [-5.7857],
        [-4.6557]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1127091.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(591.1495, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(591.1495, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0143e-02, -4.5401e-03,  2.5695e-06,  ..., -8.2898e-04,
         -6.2179e-03, -1.5467e-02],
        [-1.0143e-02, -4.5401e-03,  2.5695e-06,  ..., -8.2898e-04,
         -6.2179e-03, -1.5467e-02],
        [-9.6037e-03, -5.6317e-03,  2.5698e-06,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02],
        ...,
        [-9.6037e-03, -5.6317e-03,  2.5698e-06,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02],
        [-9.6037e-03, -5.6317e-03,  2.5698e-06,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02],
        [-9.6037e-03, -5.6317e-03,  2.5698e-06,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-178068.2812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.5431, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(72.9203, device='cuda:0')



h[100].sum tensor(-195.9885, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-86.9203, device='cuda:0')



h[200].sum tensor(-1270.9138, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.7622, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 1.0279e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.0279e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.0279e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 1.0279e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.0279e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.0279e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(219358.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1302, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1184, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1303, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2408, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2408, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2408, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2072153.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2071.9207, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4329.9961, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(41236.7812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2901.3394, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -3.3728],
        [ -2.8430],
        [ -3.3938],
        ...,
        [-13.6490],
        [-13.6475],
        [-13.6525]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1053602.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(544.6637, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(544.6637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0056,  0.0022,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056,  0.0022,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056,  0.0022,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0056,  0.0022,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056,  0.0022,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056,  0.0022,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-178501.5469, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.1015, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(67.1862, device='cuda:0')



h[100].sum tensor(-197.1573, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-80.0852, device='cuda:0')



h[200].sum tensor(-1272.0266, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.8936, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0092,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0088,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0088,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0088,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0088,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0088,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(219838.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0908, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1891, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2146, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2405, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2405, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2405, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2056482.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1831.4226, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4135.0176, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(40381.5352, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2761.0427, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -1.5163],
        [ -3.5423],
        [ -4.4972],
        ...,
        [-13.6591],
        [-13.6502],
        [-13.6505]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1092381.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 700.0 event: 10500 loss: tensor(435.9706, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(662.4711, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(662.4711, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6037e-03, -5.6425e-03,  1.5398e-06,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02],
        [-9.6037e-03, -5.6425e-03,  1.5398e-06,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02],
        [-9.6037e-03, -5.6425e-03,  1.5398e-06,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02],
        ...,
        [-9.6037e-03, -5.6425e-03,  1.5398e-06,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02],
        [-9.6037e-03, -5.6425e-03,  1.5398e-06,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02],
        [-9.6037e-03, -5.6425e-03,  1.5398e-06,  ..., -5.0170e-04,
         -6.0055e-03, -1.4658e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-177843.3750, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.1770, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(81.7181, device='cuda:0')



h[100].sum tensor(-187.2159, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-97.4071, device='cuda:0')



h[200].sum tensor(-1268.0889, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.6291, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 6.1593e-06,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 6.1593e-06,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 6.1593e-06,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 6.1593e-06,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 6.1593e-06,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 6.1593e-06,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(229497.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2388, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2388, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2383, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2277, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2318, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2378, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2157305.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2082.3467, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4330.8633, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(44112.5625, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2928.9875, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-11.3713],
        [-11.4389],
        [-11.4584],
        ...,
        [-10.7105],
        [-10.9230],
        [-11.6193]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1065805.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(583.9957, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(583.9957, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0056, -0.0018,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0018,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0018,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0056, -0.0018,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0018,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0018,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-178875.7344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.4847, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(72.0379, device='cuda:0')



h[100].sum tensor(-191.3548, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-85.8684, device='cuda:0')



h[200].sum tensor(-1270.0972, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.4746, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(221439.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1622, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1924, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2165, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2388, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2388, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2388, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2070307., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2034.5302, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4325.8740, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(40325.7773, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2873.6953, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -5.3459],
        [ -6.2245],
        [ -7.3275],
        ...,
        [-13.5378],
        [-13.5295],
        [-13.5300]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1015635.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(545.8679, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(545.8679, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0056, -0.0035,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0035,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0035,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0056, -0.0035,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0035,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0035,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-179569.4219, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.1252, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(67.3347, device='cuda:0')



h[100].sum tensor(-193.1654, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-80.2623, device='cuda:0')



h[200].sum tensor(-1271.0801, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.9420, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(217845., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1763, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2230, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2375, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2390, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2390, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2390, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2050758.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2023.2317, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4350.1802, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(39620.7227, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2841.5439, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -5.3278],
        [ -8.0929],
        [ -9.8057],
        ...,
        [-13.5594],
        [-13.5511],
        [-13.5516]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-944440.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(599.2334, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(599.2334, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0056, -0.0050,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0050,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0050,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0056, -0.0050,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0050,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0050,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-179577.4375, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.6000, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(73.9175, device='cuda:0')



h[100].sum tensor(-188.0654, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-88.1089, device='cuda:0')



h[200].sum tensor(-1269.2869, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.0872, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(220827.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2420, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2420, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2411, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2222, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2377, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2430, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2064595.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2040.4086, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4303.6680, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(40260.5117, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2843.8547, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.5661],
        [ -9.7292],
        [ -8.4980],
        ...,
        [ -9.3904],
        [-10.7519],
        [-12.1225]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1144756.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(768.8475, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(768.8475, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0056, -0.0063,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0063,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0063,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0056, -0.0063,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0063,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0063,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-178334.5938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.1833, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(94.8400, device='cuda:0')



h[100].sum tensor(-174.9920, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-113.0483, device='cuda:0')



h[200].sum tensor(-1263.8167, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.9051, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(232329.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2437, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2428, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2400, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2447, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2447, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2447, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2143676.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2114.0068, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4366.2715, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(43511.3906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2902.3721, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.8514],
        [-10.1422],
        [ -9.1290],
        ...,
        [-13.9542],
        [-13.9446],
        [-13.9448]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1079671.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(649.3375, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(649.3375, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0056, -0.0076,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0076,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0076,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0056, -0.0076,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0076,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0076,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-179719.7344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.0525, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(80.0980, device='cuda:0')



h[100].sum tensor(-184.0318, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-95.4760, device='cuda:0')



h[200].sum tensor(-1267.4614, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.1012, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(224110.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1438, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2199, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2397, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2443, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2443, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2443, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2077839., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2092.5740, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4281.8467, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(39396.0625, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2870.8909, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -4.0407],
        [ -7.0609],
        [ -9.0896],
        ...,
        [-13.7911],
        [-13.5118],
        [-12.5535]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-988622., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3572],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(673.1945, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3572],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(673.1945, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0109, -0.0031, -0.0098,  ..., -0.0013, -0.0065, -0.0166],
        [-0.0105, -0.0039, -0.0094,  ..., -0.0010, -0.0063, -0.0159],
        [-0.0096, -0.0056, -0.0087,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0056, -0.0087,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0087,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0087,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-179775.4375, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.3147, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(83.0409, device='cuda:0')



h[100].sum tensor(-181.3280, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-98.9838, device='cuda:0')



h[200].sum tensor(-1266.4457, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.0601, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(223789.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0551, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1195, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2465, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2465, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2465, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2071178.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2122.9319, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4268.1821, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(38767.2812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2872.2014, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.7550],
        [ -0.3146],
        [ -3.3850],
        ...,
        [-14.0190],
        [-14.0092],
        [-14.0093]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1022973.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.8540],
        [0.8237],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(608.6814, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.8540],
        [0.8237],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(608.6814, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0116, -0.0017, -0.0116,  ..., -0.0017, -0.0068, -0.0176],
        [-0.0116, -0.0016, -0.0117,  ..., -0.0017, -0.0068, -0.0177],
        [-0.0116, -0.0017, -0.0116,  ..., -0.0017, -0.0068, -0.0176],
        ...,
        [-0.0096, -0.0056, -0.0096,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0096,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0096,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-180696.6406, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.6982, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(75.0830, device='cuda:0')



h[100].sum tensor(-184.3533, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-89.4981, device='cuda:0')



h[200].sum tensor(-1268.3940, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.4669, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0009, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(224005.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0284, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2495, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2495, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2495, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2089866., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2162.9080, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4186.9766, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(39495.0312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2881.0525, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.7950],
        [  0.5615],
        [ -1.2849],
        ...,
        [-14.2524],
        [-14.2419],
        [-14.2418]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1131032.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(670.9254, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(670.9254, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0056, -0.0105,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0105,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0109, -0.0031, -0.0120,  ..., -0.0013, -0.0065, -0.0166],
        ...,
        [-0.0096, -0.0056, -0.0105,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0105,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0105,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-180297.2812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.3227, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(82.7610, device='cuda:0')



h[100].sum tensor(-177.5289, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-98.6502, device='cuda:0')



h[200].sum tensor(-1266.0381, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.9689, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(226172.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1898, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0876, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0446, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2510, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2510, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2510, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2087508.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2202.0103, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4206.7471, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(39832.2344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2900.1396, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -5.5485],
        [ -2.0707],
        [  0.3473],
        ...,
        [-14.3691],
        [-14.3584],
        [-14.3582]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1167103.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(638.9379, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(638.9379, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057, -0.0113,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0113,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0113,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057, -0.0113,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0113,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0113,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-180654.3125, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.9869, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(78.8152, device='cuda:0')



h[100].sum tensor(-178.6087, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-93.9469, device='cuda:0')



h[200].sum tensor(-1266.8851, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.6831, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(221533.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2487, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2487, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2482, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1450, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1607, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1956, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2044543.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2199.4307, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4248.2129, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(38039.9297, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2893.3000, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-12.1373],
        [-12.1526],
        [-12.1037],
        ...,
        [ -5.0832],
        [ -5.2796],
        [ -7.1425]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1038585., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 710.0 event: 10650 loss: tensor(421.4385, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(527.3337, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(527.3337, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057, -0.0120,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0120,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0120,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057, -0.0120,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0120,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0120,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-181936.5000, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-983.9368, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(65.0484, device='cuda:0')



h[100].sum tensor(-184.3857, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-77.5371, device='cuda:0')



h[200].sum tensor(-1270.1650, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.1970, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(215753.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2476, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2476, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2403, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2486, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2486, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2486, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2008854.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2191.3999, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4196.8950, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(36261.9688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2867.7793, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-11.7302],
        [-11.1178],
        [ -9.9364],
        ...,
        [-14.1617],
        [-14.1519],
        [-14.1520]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1065308.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(551.9369, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(551.9369, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057, -0.0127,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0127,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0127,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057, -0.0127,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0127,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0127,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-181829.6562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.1950, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(68.0833, device='cuda:0')



h[100].sum tensor(-180.1269, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-81.1546, device='cuda:0')



h[200].sum tensor(-1269.0736, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.1860, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(217762.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2456, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2456, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2451, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2466, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2466, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2466, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2019462.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2216.2820, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4249.0879, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(37001.3008, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2878.0457, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-11.4951],
        [-11.6821],
        [-11.7668],
        ...,
        [-14.0276],
        [-14.0184],
        [-14.0187]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1015186.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(725.2535, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(725.2535, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057, -0.0133,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0133,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0133,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057, -0.0133,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0133,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0133,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-180369.7500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.8079, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(89.4625, device='cuda:0')



h[100].sum tensor(-166.1750, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-106.6384, device='cuda:0')



h[200].sum tensor(-1263.4324, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.1527, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(231680.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1783, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2241, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2431, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2446, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2446, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2446, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2109080.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2277.5190, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4335.4819, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(40834.8594, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2935.1995, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -5.3217],
        [ -8.0237],
        [ -9.7290],
        ...,
        [-13.9019],
        [-13.8933],
        [-13.8937]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-952840., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(571.7766, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(571.7766, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057, -0.0138,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0138,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0138,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057, -0.0138,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0138,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0138,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-181949.5312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.3595, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(70.5306, device='cuda:0')



h[100].sum tensor(-176.3845, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-84.0718, device='cuda:0')



h[200].sum tensor(-1268.1306, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.9835, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(219590.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2388, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2420, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2436, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2436, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2436, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2022810.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2245.1699, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4281.4517, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(37304.4922, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2867.3818, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -6.2106],
        [ -8.2852],
        [ -9.2393],
        ...,
        [-13.8803],
        [-13.8719],
        [-13.8724]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-997433.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(569.9995, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(569.9995, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0101, -0.0046, -0.0146,  ..., -0.0008, -0.0062, -0.0155],
        [-0.0096, -0.0057, -0.0138,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0105, -0.0039, -0.0151,  ..., -0.0010, -0.0064, -0.0160],
        ...,
        [-0.0096, -0.0057, -0.0138,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0138,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0138,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-181940.5625, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.3685, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(70.3114, device='cuda:0')



h[100].sum tensor(-176.3148, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-83.8105, device='cuda:0')



h[200].sum tensor(-1268.1005, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.9120, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(221177.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0761, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0275, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2436, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2436, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2436, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2046326.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2251.0737, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4305.3164, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(38463.0898, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2874.8271, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.8309],
        [  2.3770],
        [  2.6161],
        ...,
        [-13.8797],
        [-13.8717],
        [-13.8724]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-990318.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(570.7578, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(570.7578, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057, -0.0143,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0143,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0143,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057, -0.0143,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0143,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0143,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-181883.0938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.3309, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(70.4050, device='cuda:0')



h[100].sum tensor(-176.3846, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-83.9220, device='cuda:0')



h[200].sum tensor(-1268.0536, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.9425, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(222106.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2420, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2412, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2386, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2430, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2430, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2430, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2023337.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2537.5605, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4304.8818, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(37189.8438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3034.1440, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-11.0966],
        [-10.5387],
        [ -9.6064],
        ...,
        [-13.9138],
        [-13.9055],
        [-13.9060]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1045586.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.8413],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(591.1602, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.8413],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(591.1602, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0116, -0.0017, -0.0178,  ..., -0.0017, -0.0068, -0.0177],
        [-0.0096, -0.0057, -0.0147,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0116, -0.0017, -0.0178,  ..., -0.0017, -0.0068, -0.0177],
        ...,
        [-0.0096, -0.0057, -0.0147,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0147,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0147,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-181919.6562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.5204, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(72.9217, device='cuda:0')



h[100].sum tensor(-175.5899, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-86.9219, device='cuda:0')



h[200].sum tensor(-1267.3872, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.7626, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0001, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(223394.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2410, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2410, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2410, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2051101.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2271.6919, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4304.3340, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(38032.6758, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2840.1438, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.8335],
        [  0.7101],
        [  0.7193],
        ...,
        [-13.7791],
        [-13.7837],
        [-13.7957]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-990064.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2717],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(539.1897, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2717],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(539.1897, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0105, -0.0039, -0.0165,  ..., -0.0010, -0.0064, -0.0160],
        [-0.0103, -0.0044, -0.0161,  ..., -0.0009, -0.0063, -0.0156],
        [-0.0096, -0.0057, -0.0151,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057, -0.0151,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0151,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0151,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-182737.1250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.0177, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(66.5109, device='cuda:0')



h[100].sum tensor(-180.0314, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-79.2803, device='cuda:0')



h[200].sum tensor(-1269.0540, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.6736, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(219216.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0277, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0860, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1546, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2405, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2405, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2405, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2018470.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2261.0444, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4268.2725, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(36616.9492, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2802.2935, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.2638],
        [ -1.4166],
        [ -3.5927],
        ...,
        [-13.4753],
        [-13.7702],
        [-13.8410]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1021983.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(573.3307, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(573.3307, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057, -0.0151,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0151,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0151,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057, -0.0151,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0151,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0151,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-182396.9688, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.3583, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(70.7223, device='cuda:0')



h[100].sum tensor(-177.3940, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-84.3003, device='cuda:0')



h[200].sum tensor(-1267.9054, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.0459, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(222098.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1190, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1800, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1431, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2405, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2405, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2405, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2038235.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2266.5110, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4268.6406, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(37334.1953, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2815.6885, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -2.2221],
        [ -3.1563],
        [ -2.2616],
        ...,
        [-13.8537],
        [-13.8460],
        [-13.8467]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1021022.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(501.5388, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(501.5388, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0057, -0.0151,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0151,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0151,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057, -0.0151,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0151,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0151,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-183037.8906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-983.7166, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(61.8666, device='cuda:0')



h[100].sum tensor(-182.3637, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-73.7443, device='cuda:0')



h[200].sum tensor(-1270.0695, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.1602, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(217754.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1259, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0897, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1290, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2405, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2405, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2405, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2013111., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2255.8093, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4259.3711, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(36354.9141, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2795.4141, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -3.5574],
        [ -2.5649],
        [ -2.8809],
        ...,
        [-13.0926],
        [-13.6582],
        [-13.8059]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1026258., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 720.0 event: 10800 loss: tensor(289.2746, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4016],
        [0.5322],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(667.4298, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4016],
        [0.5322],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(667.4298, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0106, -0.0037, -0.0170,  ..., -0.0011, -0.0064, -0.0161],
        [-0.0115, -0.0018, -0.0185,  ..., -0.0017, -0.0068, -0.0175],
        [-0.0122, -0.0005, -0.0196,  ..., -0.0021, -0.0070, -0.0185],
        ...,
        [-0.0096, -0.0057, -0.0154,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0154,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0154,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-181863.1875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.2437, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(82.3298, device='cuda:0')



h[100].sum tensor(-170.6778, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-98.1362, device='cuda:0')



h[200].sum tensor(-1264.8912, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.8284, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0019, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(227439.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0402, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2408, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2408, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2408, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2064820.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2296.2710, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4295.7314, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(38602.6172, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2818.7886, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -0.6510],
        [  0.4257],
        [  0.6271],
        ...,
        [-13.9368],
        [-13.9289],
        [-13.9296]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1047684.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(619.5704, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(619.5704, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0101, -0.0046, -0.0166,  ..., -0.0008, -0.0062, -0.0154],
        [-0.0096, -0.0057, -0.0158,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0158,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057, -0.0158,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0158,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0158,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-182624.4844, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.7994, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(76.4262, device='cuda:0')



h[100].sum tensor(-173.7069, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-91.0992, device='cuda:0')



h[200].sum tensor(-1266.3583, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.9046, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(224278., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0896, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1325, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1991, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2410, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2410, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2410, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2049536.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2265.4333, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4272.9419, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(36950.6484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2786.1277, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -1.6735],
        [ -3.5451],
        [ -6.1000],
        ...,
        [-13.9887],
        [-13.9806],
        [-13.9812]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1043362.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(531.4753, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(531.4753, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0115, -0.0018, -0.0192,  ..., -0.0017, -0.0068, -0.0175],
        [-0.0102, -0.0044, -0.0171,  ..., -0.0009, -0.0063, -0.0156],
        [-0.0102, -0.0044, -0.0171,  ..., -0.0009, -0.0063, -0.0156],
        ...,
        [-0.0096, -0.0057, -0.0160,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0160,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0160,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-183743.8594, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-983.9734, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(65.5593, device='cuda:0')



h[100].sum tensor(-180.3515, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-78.1460, device='cuda:0')



h[200].sum tensor(-1269.1252, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.3635, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(217152.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0644, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2415, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2415, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2415, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1992120.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2224.2781, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4195.0664, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(33263.8984, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2729.7202, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 2.0102e+00],
        [ 1.6363e+00],
        [-7.3278e-03],
        ...,
        [-1.4061e+01],
        [-1.4053e+01],
        [-1.4053e+01]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1078050.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(710.2693, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(710.2693, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0056, -0.0163,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0163,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0163,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0056, -0.0163,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0163,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0163,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-182385.0312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.6157, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(87.6142, device='cuda:0')



h[100].sum tensor(-167.7189, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-104.4352, device='cuda:0')



h[200].sum tensor(-1263.5557, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.5504, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(232758.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2118, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2341, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2389, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2349, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2104, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1402, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2110000.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2286.7568, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4260.7012, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(37683.5234, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2787.2166, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -7.2464],
        [ -9.2335],
        [-10.3992],
        ...,
        [-12.6190],
        [-10.8609],
        [ -8.5349]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1011668.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(771.9618, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(771.9618, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0056, -0.0165,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0165,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0109, -0.0031, -0.0187,  ..., -0.0013, -0.0065, -0.0166],
        ...,
        [-0.0096, -0.0056, -0.0165,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0165,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0165,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-182040.0469, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.2233, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(95.2242, device='cuda:0')



h[100].sum tensor(-162.6663, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-113.5062, device='cuda:0')



h[200].sum tensor(-1261.5188, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.0302, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(234516.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1505, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0697, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0437, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2437, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2437, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2437, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2103525., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2289.9226, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4255.6475, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(36887.4609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2782.0698, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -1.4872],
        [  0.4012],
        [  1.6366],
        ...,
        [-14.2737],
        [-14.2737],
        [-14.2794]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1070238.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(560.8965, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(560.8965, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0056, -0.0167,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0167,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0167,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0056, -0.0167,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0167,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0167,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-184207.9688, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.2473, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(69.1885, device='cuda:0')



h[100].sum tensor(-177.4066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-82.4720, device='cuda:0')



h[200].sum tensor(-1268.3387, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.5461, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(222254.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2109, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2162, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2115, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2439, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2439, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2439, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2042711.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2231.8413, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4208.7036, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(33796.0312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2720.1860, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -6.3943],
        [ -6.3269],
        [ -5.8144],
        ...,
        [-14.3296],
        [-14.3200],
        [-14.3203]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1086822.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(578.2606, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(578.2606, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0056, -0.0169,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0169,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0169,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0056, -0.0169,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0169,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0169,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-184250.8125, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.3758, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(71.3305, device='cuda:0')



h[100].sum tensor(-175.6530, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-85.0252, device='cuda:0')



h[200].sum tensor(-1268.0068, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.2441, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(221474.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2329, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2122, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1580, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2432, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2432, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2432, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2030432.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2217.5991, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4233.6255, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(33119.4766, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2717.9485, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -8.3196],
        [ -6.0254],
        [ -3.0928],
        ...,
        [-14.2845],
        [-14.2768],
        [-14.2806]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1056749.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(591.9412, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(591.9412, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0056, -0.0171,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0171,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0171,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0056, -0.0171,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0171,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0171,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-184227.7500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.5541, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(73.0180, device='cuda:0')



h[100].sum tensor(-173.6255, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-87.0367, device='cuda:0')



h[200].sum tensor(-1267.4919, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.7940, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(224573.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2224, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2350, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2406, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2421, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2421, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2421, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2046179.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2216.3391, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4259.3960, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(33354.8672, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2732.0845, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -7.7357],
        [ -9.1641],
        [-10.0793],
        ...,
        [-14.2206],
        [-14.2113],
        [-14.2113]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1015874.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(602.4976, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(602.4976, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0113, -0.0021, -0.0204,  ..., -0.0016, -0.0067, -0.0173],
        [-0.0132,  0.0015, -0.0237,  ..., -0.0027, -0.0074, -0.0200],
        [-0.0114, -0.0020, -0.0205,  ..., -0.0016, -0.0067, -0.0174],
        ...,
        [-0.0096, -0.0056, -0.0172,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0172,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0172,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-184315.8906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.6215, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(74.3202, device='cuda:0')



h[100].sum tensor(-172.3269, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-88.5889, device='cuda:0')



h[200].sum tensor(-1267.3513, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.2184, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0018, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(227772.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2416, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2416, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2416, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2074147.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2217.6770, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4254.9697, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(34329.5859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2748.1597, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.0706],
        [  0.9998],
        [  0.5079],
        ...,
        [-14.2047],
        [-14.1958],
        [-14.1962]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1039089.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(726.4243, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(726.4243, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0056, -0.0174,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0174,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0174,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0056, -0.0174,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0174,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0174,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-183261.4688, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.8397, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(89.6070, device='cuda:0')



h[100].sum tensor(-162.1886, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-106.8106, device='cuda:0')



h[200].sum tensor(-1263.3716, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.1998, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(236496.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0626, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1401, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1987, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2409, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2409, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2409, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2133580.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2251.5593, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4350.1934, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(37343.0625, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2777.2722, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.0202],
        [ -1.8108],
        [ -4.2437],
        ...,
        [-14.1874],
        [-14.1790],
        [-14.1800]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1005920.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 730.0 event: 10950 loss: tensor(422.0044, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(493.9133, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(493.9133, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0056, -0.0175,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0175,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0175,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0056, -0.0175,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0175,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0175,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-185593.8125, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-983.6483, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(60.9259, device='cuda:0')



h[100].sum tensor(-179.2269, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-72.6231, device='cuda:0')



h[200].sum tensor(-1270.8549, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.8536, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(218993.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2335, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2384, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2387, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2403, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2403, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2403, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2013780.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2166.3760, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4289.8652, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(32011.8672, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2675.9653, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -9.3736],
        [-10.5393],
        [-11.1096],
        ...,
        [-14.1636],
        [-14.1611],
        [-14.1672]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1065405.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(623.8190, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(623.8190, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0056, -0.0176,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0176,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0176,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0056, -0.0176,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0176,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0176,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-184806.3750, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.8374, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(76.9502, device='cuda:0')



h[100].sum tensor(-170.2911, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-91.7239, device='cuda:0')



h[200].sum tensor(-1266.9191, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.0754, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(226313.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2386, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2386, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2380, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2396, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2396, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2396, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2052455.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2184.8425, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4338.3540, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(33419.7500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2688.2937, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-11.6341],
        [-11.6159],
        [-11.4576],
        ...,
        [-14.1437],
        [-14.1352],
        [-14.1358]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1012102.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5293],
        [0.2495],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(660.7526, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5293],
        [0.2495],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(660.7526, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0119, -0.0010, -0.0219,  ..., -0.0019, -0.0069, -0.0180],
        [-0.0117, -0.0013, -0.0217,  ..., -0.0018, -0.0068, -0.0178],
        [-0.0134,  0.0021, -0.0247,  ..., -0.0028, -0.0075, -0.0203],
        ...,
        [-0.0096, -0.0056, -0.0177,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0177,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0177,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-184910.1719, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.1747, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(81.5061, device='cuda:0')



h[100].sum tensor(-167.3577, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-97.1544, device='cuda:0')



h[200].sum tensor(-1265.8452, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.5600, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0026, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(232094.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2406, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2406, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2406, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2110427.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2199.2432, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4306.7056, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(35711.2930, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2697.0503, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.3371],
        [  0.9189],
        [  0.7132],
        ...,
        [-14.2465],
        [-14.2376],
        [-14.2380]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1129085.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(588.3140, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(588.3140, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0056, -0.0178,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0178,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0178,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0056, -0.0178,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0178,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0178,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-185901.4219, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.5384, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(72.5706, device='cuda:0')



h[100].sum tensor(-171.5869, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-86.5034, device='cuda:0')



h[200].sum tensor(-1267.9821, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.6482, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(223758.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2397, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2397, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2392, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2408, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2408, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2408, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2041220.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2169.4004, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4322.2554, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(33286.2891, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2651.3359, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-11.9758],
        [-11.8369],
        [-11.6070],
        ...,
        [-14.2605],
        [-14.2515],
        [-14.2520]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1129453.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5752],
        [0.5659],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(703.6985, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5752],
        [0.5659],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(703.6985, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3249e-02,  1.8031e-03, -2.4692e-02,  ..., -2.7127e-03,
         -7.4407e-03, -2.0113e-02],
        [-1.2130e-02, -4.7096e-04, -2.2597e-02,  ..., -2.0337e-03,
         -6.9999e-03, -1.8437e-02],
        [-1.2350e-02, -2.3373e-05, -2.3009e-02,  ..., -2.1674e-03,
         -7.0867e-03, -1.8767e-02],
        ...,
        [-9.6037e-03, -5.6019e-03, -1.7869e-02,  ..., -5.0170e-04,
         -6.0055e-03, -1.4656e-02],
        [-9.6037e-03, -5.6019e-03, -1.7869e-02,  ..., -5.0170e-04,
         -6.0055e-03, -1.4656e-02],
        [-9.6037e-03, -5.6019e-03, -1.7869e-02,  ..., -5.0170e-04,
         -6.0055e-03, -1.4656e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-185098.4688, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.6084, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(86.8036, device='cuda:0')



h[100].sum tensor(-162.8689, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-103.4690, device='cuda:0')



h[200].sum tensor(-1264.3706, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.2863, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0027, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0010, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(229959.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2403, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2403, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2403, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2066586.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2192.3030, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4348.4160, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(34158.6133, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2681.8635, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.6025],
        [  0.6550],
        [  0.7745],
        ...,
        [-14.1892],
        [-14.1803],
        [-14.1808]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1068989.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3313],
        [0.3879],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(829.2258, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3313],
        [0.3879],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(829.2258, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0119, -0.0010, -0.0222,  ..., -0.0019, -0.0069, -0.0181],
        [-0.0104, -0.0040, -0.0194,  ..., -0.0010, -0.0063, -0.0158],
        [-0.0118, -0.0011, -0.0221,  ..., -0.0019, -0.0069, -0.0180],
        ...,
        [-0.0096, -0.0056, -0.0179,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0179,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0179,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-184103.1562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.8174, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(102.2879, device='cuda:0')



h[100].sum tensor(-153.3542, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-121.9261, device='cuda:0')



h[200].sum tensor(-1260.3037, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.3321, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(240850.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2401, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2401, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2401, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2147431.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2235.8503, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4383.3198, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(37285.6719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2732.5020, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.3493],
        [  2.3899],
        [  2.2148],
        ...,
        [-14.1768],
        [-14.1680],
        [-14.1684]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1049959.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(596.9695, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(596.9695, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0056, -0.0180,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0180,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0180,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0056, -0.0180,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0180,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0180,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-186473.2344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.5956, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(73.6383, device='cuda:0')



h[100].sum tensor(-170.2661, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-87.7760, device='cuda:0')



h[200].sum tensor(-1267.9900, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.9961, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(223184.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2205, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2397, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2391, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2407, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2407, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2407, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2022277.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2173.5222, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4321.0400, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(32235.9648, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2652.5527, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -8.3610],
        [-10.2245],
        [-11.2132],
        ...,
        [-14.2497],
        [-14.2407],
        [-14.2412]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1092446.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3210],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(742.3564, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3210],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(742.3564, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0104, -0.0040, -0.0195,  ..., -0.0010, -0.0063, -0.0158],
        [-0.0104, -0.0040, -0.0195,  ..., -0.0010, -0.0063, -0.0158],
        [-0.0109, -0.0029, -0.0205,  ..., -0.0013, -0.0065, -0.0166],
        ...,
        [-0.0096, -0.0056, -0.0181,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0181,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0181,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-185030.6875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.0107, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(91.5722, device='cuda:0')



h[100].sum tensor(-159.8541, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-109.1532, device='cuda:0')



h[200].sum tensor(-1263.4097, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.8402, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 6.6227e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(236961.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2420, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2420, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2420, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2093177., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2140.1956, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4518.9834, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(35181.6484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2803.9036, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.8381],
        [  1.5523],
        [  1.2759],
        ...,
        [-14.3646],
        [-14.3566],
        [-14.3577]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1094836.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(561.7534, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(561.7534, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0056, -0.0181,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0181,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0181,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0056, -0.0181,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0181,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0181,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-187127.1094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.2609, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(69.2942, device='cuda:0')



h[100].sum tensor(-173.7977, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-82.5980, device='cuda:0')



h[200].sum tensor(-1269.3071, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.5806, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(222836.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2413, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2387, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2348, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2423, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2423, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2423, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2025642.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2161.5127, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4276.9453, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(31979.5605, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2648.9324, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-11.9124],
        [-11.6598],
        [-11.2578],
        ...,
        [-14.3875],
        [-14.3777],
        [-14.3776]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1144066.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(666.1725, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(666.1725, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0056, -0.0182,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0182,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0182,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0056, -0.0182,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0182,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0182,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-185910.7969, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.2703, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(82.1747, device='cuda:0')



h[100].sum tensor(-165.0919, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-97.9514, device='cuda:0')



h[200].sum tensor(-1265.9233, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.7779, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(233631.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2399, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2353, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2171, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2427, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2427, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2427, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2074913., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2119.8464, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4446.3628, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(33010.9414, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2757.5835, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.4699],
        [ -9.2323],
        [ -7.2430],
        ...,
        [-14.3702],
        [-14.3608],
        [-14.3611]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1035223.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 740.0 event: 11100 loss: tensor(436.9486, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(562.9053, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(562.9053, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0056, -0.0182,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0182,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0182,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0056, -0.0182,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0182,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0056, -0.0182,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-187228.6562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.2487, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(69.4363, device='cuda:0')



h[100].sum tensor(-172.3967, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-82.7674, device='cuda:0')



h[200].sum tensor(-1267.2988, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.6269, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(221632.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2417, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2417, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2411, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2427, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2427, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2427, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2006700., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2158.1521, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4278.2559, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(31237.8203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2657.3250, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-11.9360],
        [-11.8249],
        [-11.6342],
        ...,
        [-14.3256],
        [-14.3161],
        [-14.3163]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1065410.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2690],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(516.3767, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2690],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(516.3767, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0118, -0.0013, -0.0225,  ..., -0.0019, -0.0069, -0.0180],
        [-0.0111, -0.0027, -0.0212,  ..., -0.0014, -0.0066, -0.0170],
        [-0.0096, -0.0057, -0.0182,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0057, -0.0182,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0182,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0057, -0.0182,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-187858.2500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-983.8299, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(63.6969, device='cuda:0')



h[100].sum tensor(-175.2114, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-75.9260, device='cuda:0')



h[200].sum tensor(-1266.8623, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.7566, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(219525.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2417, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2417, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2417, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2000784.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2157.2810, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4280.2603, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(31553.8555, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2673.2026, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.4237],
        [  1.5250],
        [  1.7025],
        ...,
        [-14.1989],
        [-14.1890],
        [-14.1891]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1036491., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(706.1835, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(706.1835, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0058, -0.0183,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0058, -0.0183,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0058, -0.0183,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0058, -0.0183,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0058, -0.0183,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0058, -0.0183,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-186224.5781, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.6558, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(87.1102, device='cuda:0')



h[100].sum tensor(-161.8829, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-103.8344, device='cuda:0')



h[200].sum tensor(-1258.9800, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.3862, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(235838.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2376, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2330, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2197, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2404, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2404, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2404, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2102991.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2208.7717, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4294.7402, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(35609.8359, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2746.6592, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -9.9160],
        [ -9.0704],
        [ -7.4970],
        ...,
        [-14.0476],
        [-14.0383],
        [-14.0386]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-966860.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5923],
        [0.5537],
        [0.5459],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(774.2386, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5923],
        [0.5537],
        [0.5459],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(774.2386, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0132,  0.0008, -0.0251,  ..., -0.0027, -0.0074, -0.0200],
        [-0.0145,  0.0034, -0.0277,  ..., -0.0035, -0.0079, -0.0220],
        [-0.0143,  0.0029, -0.0273,  ..., -0.0033, -0.0078, -0.0216],
        ...,
        [-0.0096, -0.0058, -0.0183,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0058, -0.0183,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0058, -0.0183,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-185899.9219, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.2257, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(95.5050, device='cuda:0')



h[100].sum tensor(-159.5666, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-113.8410, device='cuda:0')



h[200].sum tensor(-1255.8289, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.1218, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0136, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0139, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0155, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(236372.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2408, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2408, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2408, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2093085.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2207.9182, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4283.0815, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(35045.3438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2720.1704, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -1.3033],
        [ -1.3135],
        [ -1.2620],
        ...,
        [-14.0316],
        [-14.0207],
        [-14.0196]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-937442.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(713.9181, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(713.9181, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0058, -0.0183,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0058, -0.0183,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0104, -0.0043, -0.0199,  ..., -0.0010, -0.0063, -0.0159],
        ...,
        [-0.0096, -0.0058, -0.0183,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0058, -0.0183,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0106, -0.0040, -0.0203,  ..., -0.0011, -0.0064, -0.0162]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-186986.8281, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.6732, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(88.0643, device='cuda:0')



h[100].sum tensor(-166.0662, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-104.9717, device='cuda:0')



h[200].sum tensor(-1256.7125, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.6971, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(231714.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2259, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1863, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1593, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2222, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1712, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1326, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2073643.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2186.5210, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4224.2393, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(34393.4297, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2671.3062, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -9.2514],
        [ -7.4028],
        [ -4.7881],
        ...,
        [-11.5740],
        [ -9.0578],
        [ -5.8859]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1097025.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(771.9030, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(771.9030, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0059, -0.0184,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0059, -0.0184,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0059, -0.0184,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0059, -0.0184,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0059, -0.0184,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0059, -0.0184,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-186902.3750, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.2439, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(95.2169, device='cuda:0')



h[100].sum tensor(-164.3416, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-113.4976, device='cuda:0')



h[200].sum tensor(-1254.1465, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.0279, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(237775., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2382, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2316, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2160, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2458, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2321, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2130446.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2206.7900, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4209.2437, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(36893.0547, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2658.2761, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -9.7462],
        [ -8.3887],
        [ -6.4899],
        ...,
        [-12.9929],
        [-11.2577],
        [ -9.1832]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1211711.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(554.1577, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(554.1577, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0059, -0.0184,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0059, -0.0184,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0059, -0.0184,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0059, -0.0184,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0059, -0.0184,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0059, -0.0184,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-189216.9375, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.2002, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(68.3573, device='cuda:0')



h[100].sum tensor(-183.5542, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-81.4812, device='cuda:0')



h[200].sum tensor(-1260.7384, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.2753, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(221231.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2447, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2447, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2442, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2457, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2457, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2457, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2002677.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2123.0447, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4116.9697, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(29922.9531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2570.3320, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-12.2296],
        [-12.2493],
        [-12.2147],
        ...,
        [-14.6572],
        [-14.6454],
        [-14.6449]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1171111.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(733.4567, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(733.4567, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0059, -0.0184,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0059, -0.0184,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0059, -0.0184,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0059, -0.0184,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0059, -0.0184,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0059, -0.0184,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-188075.1250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.8396, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(90.4744, device='cuda:0')



h[100].sum tensor(-172.2530, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-107.8446, device='cuda:0')



h[200].sum tensor(-1254.7500, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.4825, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(233470.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2443, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2462, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2465, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2481, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2481, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2481, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2083478., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2176.0132, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4078.5713, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(33134.8672, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2592.4358, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.6023],
        [-11.3716],
        [-11.8021],
        ...,
        [-14.8640],
        [-14.8513],
        [-14.8506]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1256473.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5073],
        [0.4807],
        [0.4868],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(622.8016, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5073],
        [0.4807],
        [0.4868],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(622.8016, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0108, -0.0039, -0.0207,  ..., -0.0012, -0.0065, -0.0164],
        [-0.0120, -0.0018, -0.0230,  ..., -0.0019, -0.0069, -0.0182],
        [-0.0119, -0.0020, -0.0228,  ..., -0.0019, -0.0069, -0.0181],
        ...,
        [-0.0096, -0.0059, -0.0184,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0059, -0.0184,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0059, -0.0184,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-189510.2188, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.8452, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(76.8247, device='cuda:0')



h[100].sum tensor(-180.3705, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-91.5743, device='cuda:0')



h[200].sum tensor(-1258.0729, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.0345, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(228015.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2474, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2474, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2474, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2039250.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2161.7053, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4057.7019, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(30274.7617, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2567.2666, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.9916],
        [  1.5680],
        [  1.2477],
        ...,
        [-14.7097],
        [-14.6977],
        [-14.6972]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1142279.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(794.5571, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(794.5571, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0059, -0.0184,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0059, -0.0184,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0059, -0.0184,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0059, -0.0184,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0059, -0.0184,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0059, -0.0184,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-188460.7812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.4956, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(98.0114, device='cuda:0')



h[100].sum tensor(-166.5755, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-116.8285, device='cuda:0')



h[200].sum tensor(-1252.7358, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.9385, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(241917.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2478, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2478, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2473, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2488, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2488, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2488, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2126908., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2232.5281, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4040.9282, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(33769.7891, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2615.6365, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-12.4264],
        [-12.4772],
        [-12.4701],
        ...,
        [-14.7885],
        [-14.7760],
        [-14.7753]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1171333.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 750.0 event: 11250 loss: tensor(451.1211, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(829.9396, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(829.9396, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0060, -0.0185,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0060, -0.0185,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0060, -0.0185,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0060, -0.0185,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0060, -0.0185,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0060, -0.0185,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-188529.4531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.7960, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(102.3759, device='cuda:0')



h[100].sum tensor(-162.6637, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-122.0310, device='cuda:0')



h[200].sum tensor(-1252.1221, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.3608, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(240835.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2310, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2431, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2463, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2478, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2409, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2192, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2122903.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2249.5474, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4156.0312, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(33493.5703, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2591.0369, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.3720],
        [-11.3141],
        [-11.7941],
        ...,
        [-11.1951],
        [ -9.5047],
        [ -7.1686]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1033928.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(865.9576, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(865.9576, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0102, -0.0049, -0.0197,  ..., -0.0009, -0.0062, -0.0156],
        [-0.0096, -0.0060, -0.0185,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0111, -0.0034, -0.0214,  ..., -0.0014, -0.0066, -0.0169],
        ...,
        [-0.0096, -0.0060, -0.0185,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0060, -0.0185,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0060, -0.0185,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-188591.6875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-987.0725, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(106.8189, device='cuda:0')



h[100].sum tensor(-158.9629, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-127.3270, device='cuda:0')



h[200].sum tensor(-1251.6025, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.8086, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(246569.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0480, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0361, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0364, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2470, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2470, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2470, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2144685.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2253.5447, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4075.8997, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(32940.5781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2603.3418, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.6409],
        [  2.6088],
        [  2.5104],
        ...,
        [-14.4837],
        [-14.4723],
        [-14.4720]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1037476.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(802.2465, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(802.2465, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0107, -0.0042, -0.0205,  ..., -0.0011, -0.0064, -0.0162],
        [-0.0096, -0.0060, -0.0185,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0060, -0.0185,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0060, -0.0185,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0060, -0.0185,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0060, -0.0185,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-189433.2188, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.4956, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(98.9599, device='cuda:0')



h[100].sum tensor(-161.7814, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-117.9591, device='cuda:0')



h[200].sum tensor(-1254.1329, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.2476, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(239294.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0436, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0130, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0332, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2463, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2463, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2463, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2072590., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2236.6362, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4102.6709, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(29628.5176, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2538.2239, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.1625],
        [  2.6647],
        [  2.7269],
        ...,
        [-14.1267],
        [-14.3321],
        [-14.3818]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-950211.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2568],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(826.0275, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2568],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(826.0275, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0060, -0.0185,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0110, -0.0036, -0.0213,  ..., -0.0014, -0.0066, -0.0168],
        [-0.0104, -0.0046, -0.0201,  ..., -0.0010, -0.0063, -0.0159],
        ...,
        [-0.0096, -0.0060, -0.0185,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0060, -0.0185,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0060, -0.0185,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-189468.7656, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-986.7311, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(101.8934, device='cuda:0')



h[100].sum tensor(-158.9354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-121.4558, device='cuda:0')



h[200].sum tensor(-1254.0631, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.2035, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(239717.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0919, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0499, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2463, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2463, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2463, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2068373., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2245.3574, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4142.7329, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(29264.7148, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2495.1321, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -0.5105],
        [  1.2356],
        [  1.7417],
        ...,
        [-14.4097],
        [-14.3986],
        [-14.3983]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-928615.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(523.8041, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(523.8041, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0060, -0.0185,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0060, -0.0185,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0060, -0.0185,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0060, -0.0185,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0060, -0.0185,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0060, -0.0185,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-192559.8281, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-983.8889, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(64.6131, device='cuda:0')



h[100].sum tensor(-179.6317, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-77.0181, device='cuda:0')



h[200].sum tensor(-1264.4241, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.0551, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(218811.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2453, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2453, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2448, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2464, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2464, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2464, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1929571.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2154.1934, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4030.3784, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(22798.2109, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2372.1719, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-11.7546],
        [-11.8781],
        [-11.8525],
        ...,
        [-14.4389],
        [-14.4277],
        [-14.4274]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1017769.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6377],
        [0.3000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(465.3870, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6377],
        [0.3000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(465.3870, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0136,  0.0007, -0.0262,  ..., -0.0029, -0.0076, -0.0206],
        [-0.0121, -0.0018, -0.0233,  ..., -0.0020, -0.0070, -0.0184],
        [-0.0117, -0.0025, -0.0225,  ..., -0.0018, -0.0068, -0.0178],
        ...,
        [-0.0096, -0.0060, -0.0185,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0060, -0.0185,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0060, -0.0185,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-193384.4531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-983.3358, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(57.4071, device='cuda:0')



h[100].sum tensor(-183.4573, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-68.4287, device='cuda:0')



h[200].sum tensor(-1266.8838, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.7070, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0023, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0012, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(216190.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2458, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2458, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2458, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1908572., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2134.9976, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4003.2236, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(21366.8848, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2327.0178, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.2484],
        [  0.3591],
        [  0.5396],
        ...,
        [-14.4150],
        [-14.4035],
        [-14.4027]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1007317.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6177],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(646.6195, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6177],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(646.6195, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0110, -0.0037, -0.0212,  ..., -0.0013, -0.0065, -0.0167],
        [-0.0111, -0.0035, -0.0214,  ..., -0.0014, -0.0066, -0.0169],
        [-0.0096, -0.0060, -0.0185,  ..., -0.0005, -0.0060, -0.0147],
        ...,
        [-0.0096, -0.0060, -0.0185,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0060, -0.0185,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0060, -0.0185,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-191956.6406, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-985.0300, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(79.7628, device='cuda:0')



h[100].sum tensor(-170.4516, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-95.0764, device='cuda:0')



h[200].sum tensor(-1261.5939, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.9919, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(226099.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0214, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0299, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2445, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2445, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2445, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1954250.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2188.4363, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4092.9312, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(23313.3418, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2346.4753, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.3574],
        [  2.4821],
        [  2.5049],
        ...,
        [-14.3488],
        [-14.3380],
        [-14.3378]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-896521.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3005],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(573.7198, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3005],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(573.7198, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0096, -0.0060, -0.0185,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0109, -0.0039, -0.0210,  ..., -0.0013, -0.0065, -0.0165],
        [-0.0102, -0.0051, -0.0196,  ..., -0.0008, -0.0062, -0.0155],
        ...,
        [-0.0096, -0.0060, -0.0185,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0060, -0.0185,  ..., -0.0005, -0.0060, -0.0147],
        [-0.0096, -0.0060, -0.0185,  ..., -0.0005, -0.0060, -0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-192631.1250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-984.3448, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(70.7703, device='cuda:0')



h[100].sum tensor(-175.6946, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-84.3575, device='cuda:0')



h[200].sum tensor(-1263.9302, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.0616, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(222490.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1032, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0535, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.2445, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2445, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2445, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1936748.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2172.4163, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4063.9106, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(22518.0215, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2331.3840, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -1.7913],
        [  0.8099],
        [  2.3876],
        ...,
        [-14.3488],
        [-14.3380],
        [-14.3378]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-915209.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/./TrainingBha.py", line 79, in <module>
    outi = net(batcheddglgraph, featbatch).reshape(BatchSize, 6796)#.to('cpu')#.type(torch.LongTensor)  # Perform a single forward pass.
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/ModelBha.py", line 57, in forward
    h3 = self.conv3(g, h2)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/nn/pytorch/conv/graphconv.py", line 450, in forward
    rst = rst + self.bias
RuntimeError: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 31.75 GiB total capacity; 27.19 GiB already allocated; 11.75 MiB free; 27.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

real	2m10.531s
user	0m50.995s
sys	0m25.862s
