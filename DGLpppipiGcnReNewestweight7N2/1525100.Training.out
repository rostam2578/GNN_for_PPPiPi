0: gpu016.ihep.ac.cn
GPU 0: Tesla V100-SXM2-32GB (UUID: GPU-2135d612-642f-4ad0-ea96-14ef624f2286)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1127.8.2.el7.x86_64/extra/nvidia.ko.xz
alias:          char-major-195-*
version:        450.36.06
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.8
srcversion:     BB5CB243542347D4EB0C79C
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        
vermagic:       3.10.0-1127.8.2.el7.x86_64 SMP mod_unload modversions 
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_MapRegistersEarly:int
parm:           NVreg_RegisterForACPIEvents:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_EnableBacklightHandler:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_AssignGpus:charp

nvidia-smi:
Wed Aug  3 02:41:03 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:B3:00.0 Off |                    0 |
| N/A   34C    P0    45W / 300W |      0MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: Tesla V100-SXM2-32GB

 CUDA Device Total Memory [GB]: 34.089730048

 Device capability: (7, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2b81401b0fa0> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m8.533s
user	0m2.706s
sys	0m1.205s
[02:41:13] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch




 Training ... 






 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[ 0.8277],
        [ 0.6875],
        [ 1.0213],
        ...,
        [-0.6321],
        [ 0.3311],
        [ 1.3521]], device='cuda:0', requires_grad=True) 
node features sum: tensor(13.1418, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14





 Loading data ... 


shape (80000, 6796) (80000, 6796)
sum 5574226 8401300
shape torch.Size([80000, 6796]) torch.Size([80000, 6796])
Model name: DGLpppipiGcnReNewestweight7N2
net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[-0.0194, -0.1361, -0.0579, -0.1308, -0.0162, -0.0434,  0.0379, -0.0533,
         -0.0669,  0.0547, -0.0357,  0.0558, -0.0524, -0.0395,  0.1518,  0.0601,
         -0.0359,  0.0529,  0.0644,  0.1221,  0.0409, -0.0434, -0.0848, -0.0965,
          0.1171,  0.0583,  0.1266, -0.0428, -0.0202, -0.1042, -0.0178,  0.0703,
          0.1224, -0.1086, -0.1036, -0.0266,  0.0586,  0.1324,  0.0268, -0.0244,
          0.0614,  0.0525,  0.0465,  0.0125, -0.0249,  0.0853,  0.0887, -0.1338,
          0.0106, -0.1368,  0.0241,  0.0824,  0.1322,  0.0541, -0.0797,  0.0234,
         -0.1399,  0.1324,  0.0872,  0.0759, -0.0045,  0.0749,  0.0860,  0.0189,
         -0.1451, -0.1422, -0.1153,  0.0590,  0.1240,  0.0047, -0.0591, -0.0806,
         -0.0365,  0.1147,  0.0992, -0.1527,  0.0889, -0.1297, -0.0266, -0.0515,
          0.0460,  0.0823, -0.0274,  0.0033, -0.0670,  0.0796, -0.1454, -0.0546,
         -0.0554, -0.0374,  0.0678, -0.0021,  0.0901,  0.1375,  0.1476, -0.0450,
          0.0587,  0.0536, -0.0564,  0.0774,  0.0171, -0.1172,  0.1527,  0.0675,
         -0.0734,  0.1163,  0.1243,  0.0061, -0.1184,  0.1352, -0.0605, -0.1233,
         -0.1168,  0.1523,  0.0411, -0.0619,  0.0596, -0.1040, -0.0821,  0.1104,
         -0.1283, -0.0117, -0.1306,  0.0431, -0.1117, -0.0423,  0.0283,  0.1302,
          0.0824,  0.0552,  0.0354, -0.1277, -0.0306,  0.0178, -0.0582,  0.0589,
         -0.1066,  0.1034,  0.1094, -0.1037,  0.0886, -0.1362,  0.0516, -0.1121,
          0.0667, -0.1249,  0.0009,  0.0238,  0.0351,  0.0203,  0.0340,  0.0277,
         -0.0131,  0.0002, -0.0449, -0.1327,  0.0739,  0.0850, -0.0820, -0.0370,
         -0.0523,  0.1366, -0.0280,  0.0241,  0.0723,  0.0038, -0.0279,  0.0095,
         -0.1113, -0.1452, -0.0994,  0.1077,  0.0866, -0.0097,  0.0336,  0.0065,
          0.0774,  0.1250, -0.0857, -0.0617,  0.1104,  0.0291, -0.1455,  0.0540,
         -0.1175, -0.0801,  0.0184, -0.1303,  0.0013, -0.0693,  0.1102,  0.0763,
          0.0389, -0.1399,  0.0961,  0.1282,  0.0008,  0.0787, -0.0185, -0.0133,
          0.1492,  0.0445, -0.1294,  0.1519,  0.0476,  0.1112, -0.0493,  0.1029,
         -0.1524, -0.0865,  0.0538,  0.0615,  0.0137,  0.0993, -0.1151, -0.1508,
          0.0457, -0.0160,  0.0584,  0.0994, -0.1064,  0.1053, -0.1380,  0.0840,
          0.1171, -0.0191,  0.1002,  0.1105,  0.1470,  0.0172, -0.0890, -0.0234,
         -0.0324, -0.0143,  0.0350, -0.0151,  0.0612, -0.1311, -0.0669, -0.0856,
         -0.0141,  0.0499, -0.1380,  0.0093,  0.1046,  0.0696,  0.1203,  0.0870,
          0.1158,  0.0511, -0.0964,  0.0444, -0.0505,  0.1100, -0.1413, -0.0565]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0194, -0.1361, -0.0579, -0.1308, -0.0162, -0.0434,  0.0379, -0.0533,
         -0.0669,  0.0547, -0.0357,  0.0558, -0.0524, -0.0395,  0.1518,  0.0601,
         -0.0359,  0.0529,  0.0644,  0.1221,  0.0409, -0.0434, -0.0848, -0.0965,
          0.1171,  0.0583,  0.1266, -0.0428, -0.0202, -0.1042, -0.0178,  0.0703,
          0.1224, -0.1086, -0.1036, -0.0266,  0.0586,  0.1324,  0.0268, -0.0244,
          0.0614,  0.0525,  0.0465,  0.0125, -0.0249,  0.0853,  0.0887, -0.1338,
          0.0106, -0.1368,  0.0241,  0.0824,  0.1322,  0.0541, -0.0797,  0.0234,
         -0.1399,  0.1324,  0.0872,  0.0759, -0.0045,  0.0749,  0.0860,  0.0189,
         -0.1451, -0.1422, -0.1153,  0.0590,  0.1240,  0.0047, -0.0591, -0.0806,
         -0.0365,  0.1147,  0.0992, -0.1527,  0.0889, -0.1297, -0.0266, -0.0515,
          0.0460,  0.0823, -0.0274,  0.0033, -0.0670,  0.0796, -0.1454, -0.0546,
         -0.0554, -0.0374,  0.0678, -0.0021,  0.0901,  0.1375,  0.1476, -0.0450,
          0.0587,  0.0536, -0.0564,  0.0774,  0.0171, -0.1172,  0.1527,  0.0675,
         -0.0734,  0.1163,  0.1243,  0.0061, -0.1184,  0.1352, -0.0605, -0.1233,
         -0.1168,  0.1523,  0.0411, -0.0619,  0.0596, -0.1040, -0.0821,  0.1104,
         -0.1283, -0.0117, -0.1306,  0.0431, -0.1117, -0.0423,  0.0283,  0.1302,
          0.0824,  0.0552,  0.0354, -0.1277, -0.0306,  0.0178, -0.0582,  0.0589,
         -0.1066,  0.1034,  0.1094, -0.1037,  0.0886, -0.1362,  0.0516, -0.1121,
          0.0667, -0.1249,  0.0009,  0.0238,  0.0351,  0.0203,  0.0340,  0.0277,
         -0.0131,  0.0002, -0.0449, -0.1327,  0.0739,  0.0850, -0.0820, -0.0370,
         -0.0523,  0.1366, -0.0280,  0.0241,  0.0723,  0.0038, -0.0279,  0.0095,
         -0.1113, -0.1452, -0.0994,  0.1077,  0.0866, -0.0097,  0.0336,  0.0065,
          0.0774,  0.1250, -0.0857, -0.0617,  0.1104,  0.0291, -0.1455,  0.0540,
         -0.1175, -0.0801,  0.0184, -0.1303,  0.0013, -0.0693,  0.1102,  0.0763,
          0.0389, -0.1399,  0.0961,  0.1282,  0.0008,  0.0787, -0.0185, -0.0133,
          0.1492,  0.0445, -0.1294,  0.1519,  0.0476,  0.1112, -0.0493,  0.1029,
         -0.1524, -0.0865,  0.0538,  0.0615,  0.0137,  0.0993, -0.1151, -0.1508,
          0.0457, -0.0160,  0.0584,  0.0994, -0.1064,  0.1053, -0.1380,  0.0840,
          0.1171, -0.0191,  0.1002,  0.1105,  0.1470,  0.0172, -0.0890, -0.0234,
         -0.0324, -0.0143,  0.0350, -0.0151,  0.0612, -0.1311, -0.0669, -0.0856,
         -0.0141,  0.0499, -0.1380,  0.0093,  0.1046,  0.0696,  0.1203,  0.0870,
          0.1158,  0.0511, -0.0964,  0.0444, -0.0505,  0.1100, -0.1413, -0.0565]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[ 0.0625,  0.1018,  0.0718,  ..., -0.0273,  0.0194,  0.0277],
        [-0.1027,  0.0175, -0.0880,  ...,  0.0172, -0.0472,  0.0790],
        [ 0.1120, -0.0912,  0.0439,  ...,  0.0465,  0.1085, -0.0254],
        ...,
        [-0.0425, -0.1125, -0.1176,  ..., -0.1137,  0.1233, -0.0996],
        [-0.0592, -0.0806, -0.0118,  ...,  0.1154,  0.1102,  0.0932],
        [ 0.0645,  0.1049, -0.1150,  ..., -0.0299, -0.0930, -0.0362]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0625,  0.1018,  0.0718,  ..., -0.0273,  0.0194,  0.0277],
        [-0.1027,  0.0175, -0.0880,  ...,  0.0172, -0.0472,  0.0790],
        [ 0.1120, -0.0912,  0.0439,  ...,  0.0465,  0.1085, -0.0254],
        ...,
        [-0.0425, -0.1125, -0.1176,  ..., -0.1137,  0.1233, -0.0996],
        [-0.0592, -0.0806, -0.0118,  ...,  0.1154,  0.1102,  0.0932],
        [ 0.0645,  0.1049, -0.1150,  ..., -0.0299, -0.0930, -0.0362]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[-0.0286, -0.1140,  0.1257,  ..., -0.0274, -0.1011, -0.0785],
        [ 0.1758,  0.0879,  0.0653,  ...,  0.0300, -0.1011,  0.1410],
        [-0.0808, -0.1347,  0.0837,  ...,  0.1430,  0.1366, -0.0547],
        ...,
        [-0.1182, -0.0595, -0.1710,  ...,  0.0577, -0.0959,  0.1679],
        [ 0.0763, -0.0354,  0.0874,  ..., -0.1334, -0.0447,  0.1056],
        [-0.0127, -0.0216, -0.0866,  ..., -0.1103, -0.0424,  0.1584]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0286, -0.1140,  0.1257,  ..., -0.0274, -0.1011, -0.0785],
        [ 0.1758,  0.0879,  0.0653,  ...,  0.0300, -0.1011,  0.1410],
        [-0.0808, -0.1347,  0.0837,  ...,  0.1430,  0.1366, -0.0547],
        ...,
        [-0.1182, -0.0595, -0.1710,  ...,  0.0577, -0.0959,  0.1679],
        [ 0.0763, -0.0354,  0.0874,  ..., -0.1334, -0.0447,  0.1056],
        [-0.0127, -0.0216, -0.0866,  ..., -0.1103, -0.0424,  0.1584]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[-0.0202, -0.0467, -0.1105,  ...,  0.1062,  0.0370,  0.2294],
        [-0.0723,  0.0363, -0.1628,  ...,  0.0442, -0.0527,  0.1406],
        [-0.1602, -0.1402,  0.0016,  ...,  0.1856, -0.0710,  0.2486],
        ...,
        [ 0.1599, -0.2238,  0.1709,  ..., -0.1278,  0.2372, -0.0833],
        [-0.1124,  0.1685, -0.1886,  ..., -0.0761,  0.0615,  0.2450],
        [-0.2015, -0.0704, -0.0374,  ..., -0.0188, -0.1827, -0.0198]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0202, -0.0467, -0.1105,  ...,  0.1062,  0.0370,  0.2294],
        [-0.0723,  0.0363, -0.1628,  ...,  0.0442, -0.0527,  0.1406],
        [-0.1602, -0.1402,  0.0016,  ...,  0.1856, -0.0710,  0.2486],
        ...,
        [ 0.1599, -0.2238,  0.1709,  ..., -0.1278,  0.2372, -0.0833],
        [-0.1124,  0.1685, -0.1886,  ..., -0.0761,  0.0615,  0.2450],
        [-0.2015, -0.0704, -0.0374,  ..., -0.0188, -0.1827, -0.0198]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[ 0.0536],
        [-0.2333],
        [-0.3783],
        [-0.3911],
        [-0.4090],
        [ 0.3648],
        [ 0.2297],
        [-0.2986],
        [-0.0102],
        [-0.3481],
        [-0.0868],
        [ 0.1450],
        [-0.0962],
        [-0.1444],
        [ 0.1351],
        [ 0.0669],
        [-0.0295],
        [ 0.3493],
        [ 0.0409],
        [-0.1259],
        [-0.1938],
        [-0.2744],
        [ 0.1648],
        [ 0.1078],
        [-0.1498],
        [ 0.0622],
        [ 0.1270],
        [-0.3172],
        [-0.0899],
        [ 0.1252],
        [ 0.3985],
        [ 0.2351]], device='cuda:0') 
 Parameter containing:
tensor([[ 0.0536],
        [-0.2333],
        [-0.3783],
        [-0.3911],
        [-0.4090],
        [ 0.3648],
        [ 0.2297],
        [-0.2986],
        [-0.0102],
        [-0.3481],
        [-0.0868],
        [ 0.1450],
        [-0.0962],
        [-0.1444],
        [ 0.1351],
        [ 0.0669],
        [-0.0295],
        [ 0.3493],
        [ 0.0409],
        [-0.1259],
        [-0.1938],
        [-0.2744],
        [ 0.1648],
        [ 0.1078],
        [-0.1498],
        [ 0.0622],
        [ 0.1270],
        [-0.3172],
        [-0.0899],
        [ 0.1252],
        [ 0.3985],
        [ 0.2351]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[-8.8776e-02,  9.0528e-02, -1.5108e-01,  7.0932e-02,  1.4921e-01,
          7.4819e-02,  3.0836e-02,  2.1799e-02, -1.2203e-01,  6.3389e-02,
         -8.1567e-02,  1.4566e-01,  1.1137e-01,  6.9064e-02,  7.6316e-02,
          8.2065e-02,  8.5771e-02, -1.4879e-01,  1.1679e-01, -8.4468e-02,
          1.4641e-01, -1.5396e-02, -9.6128e-02, -1.1798e-01, -3.8089e-02,
          2.4453e-02, -6.4307e-02,  5.2524e-02,  9.2654e-02,  7.5940e-03,
         -4.3496e-02, -4.5783e-02, -7.6161e-02,  9.9027e-02,  1.4569e-01,
          6.3198e-02, -1.1257e-01, -6.4695e-02,  1.1024e-01, -1.0297e-01,
         -8.9563e-02, -1.1947e-01, -1.0354e-01,  4.1079e-02,  6.7968e-02,
         -1.2882e-01,  1.1039e-01,  4.0843e-02, -6.1480e-02, -5.3151e-02,
         -1.1916e-01, -4.7050e-02,  5.4517e-02, -3.8659e-02, -4.4901e-02,
         -2.5329e-02, -1.2982e-01,  1.4758e-01, -1.0682e-01,  1.4951e-01,
          8.4151e-03, -6.4497e-02, -1.4737e-01,  2.9777e-02, -1.3991e-01,
          1.3132e-01,  7.3542e-02, -3.5879e-02,  1.3966e-01, -1.2878e-01,
         -8.8229e-02, -1.4940e-01, -7.7733e-02, -8.8102e-02,  9.3143e-02,
          1.3747e-01,  1.4252e-01,  1.1245e-01,  1.4281e-01, -1.4483e-01,
          1.3419e-01, -1.1143e-01,  1.1144e-01, -7.6873e-02, -1.5015e-02,
         -2.6923e-02, -3.9304e-02, -3.2522e-02, -5.7045e-02,  1.0970e-01,
          4.2424e-02, -3.5454e-02,  7.8008e-02, -8.2933e-02, -3.1832e-02,
          2.4476e-02, -1.1154e-01, -3.2080e-02,  1.4932e-01,  1.0149e-01,
         -2.4510e-02, -7.7192e-02,  7.3682e-02,  1.0845e-01, -9.0683e-02,
          2.7114e-02,  2.7487e-02,  2.0966e-02,  1.1242e-01,  1.4225e-01,
          1.1732e-01, -1.2018e-01,  1.1798e-01,  1.2808e-01,  2.6209e-02,
          1.1602e-01, -8.1405e-02,  2.0945e-02,  6.5684e-02, -9.5949e-02,
          1.0332e-01, -1.0749e-01,  8.3213e-02,  6.2422e-02, -6.6479e-02,
         -8.4989e-02, -1.5012e-01,  4.2207e-02, -1.4331e-01,  1.2927e-01,
          1.3339e-01, -9.6800e-02,  2.4894e-02, -1.1346e-01, -1.0956e-01,
          1.1262e-01,  1.3796e-01, -1.0395e-01,  8.8735e-02,  5.7740e-06,
         -1.0210e-01,  4.7120e-02, -1.0229e-01, -3.4101e-02, -1.3946e-01,
          1.4641e-01,  1.3649e-01, -7.3379e-02, -7.1094e-02, -7.4142e-02,
         -7.4012e-02,  7.5714e-02, -4.5687e-02,  6.2762e-02,  4.5543e-02,
          1.0642e-01, -1.1787e-01,  8.8881e-03, -1.0437e-01, -1.2184e-01,
          4.8176e-02,  3.7750e-02, -1.6936e-02,  8.6734e-02,  1.3410e-01,
         -2.4605e-02,  1.1648e-01,  1.0660e-01, -4.5761e-03,  1.3095e-01,
         -7.4148e-02, -1.2985e-01,  2.1411e-04,  1.0564e-01,  1.3944e-01,
         -9.2960e-02, -4.4222e-02, -4.6985e-02,  8.2351e-02, -5.3936e-02,
         -4.4058e-02,  1.3155e-01,  2.3548e-02, -9.9846e-02, -1.6894e-02,
         -8.2417e-02, -1.4341e-01,  6.5798e-02,  4.3312e-02, -1.5021e-01,
          1.1304e-01, -4.3058e-03, -7.1963e-03, -9.6273e-02,  7.4521e-02,
         -1.2326e-01,  9.9507e-02, -7.9588e-02, -1.3088e-01,  7.9667e-04,
          1.1596e-01, -9.1664e-02, -5.8904e-02,  5.5821e-02, -1.1541e-01,
          9.2221e-02,  1.4362e-01,  6.4388e-03, -1.5061e-01,  1.0514e-01,
         -7.4782e-02, -1.1127e-01, -7.7710e-02, -3.5245e-03,  1.0850e-01,
          5.4248e-02,  6.0139e-02,  1.0919e-01,  8.9826e-03,  1.0991e-01,
         -1.4539e-01,  3.1060e-04,  3.8766e-02, -3.9552e-02,  1.5146e-01,
         -5.6272e-02,  9.8709e-02, -1.3422e-01,  6.5936e-02, -1.4973e-01,
         -7.4211e-02, -1.4203e-01, -1.2788e-01,  6.0734e-02, -3.8334e-02,
         -1.0986e-01,  6.2155e-02,  2.4926e-02,  7.4830e-02,  7.5497e-02,
         -9.6808e-02,  4.2932e-02,  8.6683e-03,  1.2048e-01,  1.9071e-02,
         -6.1805e-03,  1.3801e-01, -1.2582e-01, -1.4953e-02,  2.1446e-02,
          8.5319e-02,  1.4926e-01,  5.3721e-02, -1.1186e-01,  1.0863e-01,
          6.3334e-02]], device='cuda:0') 
 Parameter containing:
tensor([[-8.8776e-02,  9.0528e-02, -1.5108e-01,  7.0932e-02,  1.4921e-01,
          7.4819e-02,  3.0836e-02,  2.1799e-02, -1.2203e-01,  6.3389e-02,
         -8.1567e-02,  1.4566e-01,  1.1137e-01,  6.9064e-02,  7.6316e-02,
          8.2065e-02,  8.5771e-02, -1.4879e-01,  1.1679e-01, -8.4468e-02,
          1.4641e-01, -1.5396e-02, -9.6128e-02, -1.1798e-01, -3.8089e-02,
          2.4453e-02, -6.4307e-02,  5.2524e-02,  9.2654e-02,  7.5940e-03,
         -4.3496e-02, -4.5783e-02, -7.6161e-02,  9.9027e-02,  1.4569e-01,
          6.3198e-02, -1.1257e-01, -6.4695e-02,  1.1024e-01, -1.0297e-01,
         -8.9563e-02, -1.1947e-01, -1.0354e-01,  4.1079e-02,  6.7968e-02,
         -1.2882e-01,  1.1039e-01,  4.0843e-02, -6.1480e-02, -5.3151e-02,
         -1.1916e-01, -4.7050e-02,  5.4517e-02, -3.8659e-02, -4.4901e-02,
         -2.5329e-02, -1.2982e-01,  1.4758e-01, -1.0682e-01,  1.4951e-01,
          8.4151e-03, -6.4497e-02, -1.4737e-01,  2.9777e-02, -1.3991e-01,
          1.3132e-01,  7.3542e-02, -3.5879e-02,  1.3966e-01, -1.2878e-01,
         -8.8229e-02, -1.4940e-01, -7.7733e-02, -8.8102e-02,  9.3143e-02,
          1.3747e-01,  1.4252e-01,  1.1245e-01,  1.4281e-01, -1.4483e-01,
          1.3419e-01, -1.1143e-01,  1.1144e-01, -7.6873e-02, -1.5015e-02,
         -2.6923e-02, -3.9304e-02, -3.2522e-02, -5.7045e-02,  1.0970e-01,
          4.2424e-02, -3.5454e-02,  7.8008e-02, -8.2933e-02, -3.1832e-02,
          2.4476e-02, -1.1154e-01, -3.2080e-02,  1.4932e-01,  1.0149e-01,
         -2.4510e-02, -7.7192e-02,  7.3682e-02,  1.0845e-01, -9.0683e-02,
          2.7114e-02,  2.7487e-02,  2.0966e-02,  1.1242e-01,  1.4225e-01,
          1.1732e-01, -1.2018e-01,  1.1798e-01,  1.2808e-01,  2.6209e-02,
          1.1602e-01, -8.1405e-02,  2.0945e-02,  6.5684e-02, -9.5949e-02,
          1.0332e-01, -1.0749e-01,  8.3213e-02,  6.2422e-02, -6.6479e-02,
         -8.4989e-02, -1.5012e-01,  4.2207e-02, -1.4331e-01,  1.2927e-01,
          1.3339e-01, -9.6800e-02,  2.4894e-02, -1.1346e-01, -1.0956e-01,
          1.1262e-01,  1.3796e-01, -1.0395e-01,  8.8735e-02,  5.7740e-06,
         -1.0210e-01,  4.7120e-02, -1.0229e-01, -3.4101e-02, -1.3946e-01,
          1.4641e-01,  1.3649e-01, -7.3379e-02, -7.1094e-02, -7.4142e-02,
         -7.4012e-02,  7.5714e-02, -4.5687e-02,  6.2762e-02,  4.5543e-02,
          1.0642e-01, -1.1787e-01,  8.8881e-03, -1.0437e-01, -1.2184e-01,
          4.8176e-02,  3.7750e-02, -1.6936e-02,  8.6734e-02,  1.3410e-01,
         -2.4605e-02,  1.1648e-01,  1.0660e-01, -4.5761e-03,  1.3095e-01,
         -7.4148e-02, -1.2985e-01,  2.1411e-04,  1.0564e-01,  1.3944e-01,
         -9.2960e-02, -4.4222e-02, -4.6985e-02,  8.2351e-02, -5.3936e-02,
         -4.4058e-02,  1.3155e-01,  2.3548e-02, -9.9846e-02, -1.6894e-02,
         -8.2417e-02, -1.4341e-01,  6.5798e-02,  4.3312e-02, -1.5021e-01,
          1.1304e-01, -4.3058e-03, -7.1963e-03, -9.6273e-02,  7.4521e-02,
         -1.2326e-01,  9.9507e-02, -7.9588e-02, -1.3088e-01,  7.9667e-04,
          1.1596e-01, -9.1664e-02, -5.8904e-02,  5.5821e-02, -1.1541e-01,
          9.2221e-02,  1.4362e-01,  6.4388e-03, -1.5061e-01,  1.0514e-01,
         -7.4782e-02, -1.1127e-01, -7.7710e-02, -3.5245e-03,  1.0850e-01,
          5.4248e-02,  6.0139e-02,  1.0919e-01,  8.9826e-03,  1.0991e-01,
         -1.4539e-01,  3.1060e-04,  3.8766e-02, -3.9552e-02,  1.5146e-01,
         -5.6272e-02,  9.8709e-02, -1.3422e-01,  6.5936e-02, -1.4973e-01,
         -7.4211e-02, -1.4203e-01, -1.2788e-01,  6.0734e-02, -3.8334e-02,
         -1.0986e-01,  6.2155e-02,  2.4926e-02,  7.4830e-02,  7.5497e-02,
         -9.6808e-02,  4.2932e-02,  8.6683e-03,  1.2048e-01,  1.9071e-02,
         -6.1805e-03,  1.3801e-01, -1.2582e-01, -1.4953e-02,  2.1446e-02,
          8.5319e-02,  1.4926e-01,  5.3721e-02, -1.1186e-01,  1.0863e-01,
          6.3334e-02]], device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[ 0.1041, -0.0864,  0.0169,  ...,  0.0566,  0.0239,  0.0295],
        [ 0.0469, -0.1150,  0.0767,  ...,  0.0730,  0.1167,  0.0088],
        [-0.0890, -0.1029, -0.0911,  ...,  0.1117, -0.1109,  0.0623],
        ...,
        [ 0.0725,  0.0725,  0.0035,  ...,  0.0563,  0.0380,  0.0770],
        [-0.0763,  0.1194, -0.1139,  ...,  0.1038, -0.0315,  0.0505],
        [ 0.0595,  0.0333, -0.0469,  ...,  0.1025, -0.0880,  0.0341]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.1041, -0.0864,  0.0169,  ...,  0.0566,  0.0239,  0.0295],
        [ 0.0469, -0.1150,  0.0767,  ...,  0.0730,  0.1167,  0.0088],
        [-0.0890, -0.1029, -0.0911,  ...,  0.1117, -0.1109,  0.0623],
        ...,
        [ 0.0725,  0.0725,  0.0035,  ...,  0.0563,  0.0380,  0.0770],
        [-0.0763,  0.1194, -0.1139,  ...,  0.1038, -0.0315,  0.0505],
        [ 0.0595,  0.0333, -0.0469,  ...,  0.1025, -0.0880,  0.0341]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[ 0.1395, -0.0911, -0.0225,  ...,  0.1478,  0.0744, -0.0355],
        [-0.1589,  0.0361,  0.0018,  ..., -0.0116,  0.0283, -0.0775],
        [ 0.0677, -0.0750,  0.1476,  ...,  0.0256, -0.1002, -0.1591],
        ...,
        [ 0.0981,  0.1121,  0.0162,  ...,  0.0306, -0.1157,  0.1689],
        [-0.1534,  0.0138,  0.0378,  ...,  0.0101, -0.1669, -0.0721],
        [ 0.0901, -0.0541, -0.0489,  ..., -0.0539,  0.1257, -0.1257]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.1395, -0.0911, -0.0225,  ...,  0.1478,  0.0744, -0.0355],
        [-0.1589,  0.0361,  0.0018,  ..., -0.0116,  0.0283, -0.0775],
        [ 0.0677, -0.0750,  0.1476,  ...,  0.0256, -0.1002, -0.1591],
        ...,
        [ 0.0981,  0.1121,  0.0162,  ...,  0.0306, -0.1157,  0.1689],
        [-0.1534,  0.0138,  0.0378,  ...,  0.0101, -0.1669, -0.0721],
        [ 0.0901, -0.0541, -0.0489,  ..., -0.0539,  0.1257, -0.1257]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[ 0.1785, -0.0770,  0.0172,  ...,  0.0815, -0.0655,  0.2356],
        [ 0.1521, -0.1654, -0.2006,  ...,  0.1930, -0.0979,  0.0560],
        [ 0.0851,  0.0629,  0.0799,  ..., -0.1769,  0.0735, -0.0664],
        ...,
        [ 0.1098, -0.2359,  0.1075,  ...,  0.0844, -0.0472,  0.2479],
        [ 0.1469,  0.1373, -0.0523,  ..., -0.2412, -0.1337, -0.0911],
        [-0.0919, -0.0543,  0.1323,  ..., -0.0884, -0.1107, -0.0047]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.1785, -0.0770,  0.0172,  ...,  0.0815, -0.0655,  0.2356],
        [ 0.1521, -0.1654, -0.2006,  ...,  0.1930, -0.0979,  0.0560],
        [ 0.0851,  0.0629,  0.0799,  ..., -0.1769,  0.0735, -0.0664],
        ...,
        [ 0.1098, -0.2359,  0.1075,  ...,  0.0844, -0.0472,  0.2479],
        [ 0.1469,  0.1373, -0.0523,  ..., -0.2412, -0.1337, -0.0911],
        [-0.0919, -0.0543,  0.1323,  ..., -0.0884, -0.1107, -0.0047]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[-0.4038],
        [-0.1826],
        [-0.3912],
        [ 0.0948],
        [-0.1145],
        [-0.3666],
        [-0.0308],
        [-0.1659],
        [ 0.1904],
        [-0.2545],
        [ 0.0357],
        [ 0.4104],
        [ 0.2538],
        [ 0.0634],
        [-0.1629],
        [-0.0624],
        [ 0.2472],
        [-0.0793],
        [-0.0043],
        [-0.3840],
        [ 0.0307],
        [ 0.0480],
        [ 0.1899],
        [-0.2999],
        [-0.1548],
        [-0.0322],
        [-0.1031],
        [ 0.2796],
        [ 0.0986],
        [-0.1139],
        [-0.3688],
        [-0.0341]], device='cuda:0') 
 Parameter containing:
tensor([[-0.4038],
        [-0.1826],
        [-0.3912],
        [ 0.0948],
        [-0.1145],
        [-0.3666],
        [-0.0308],
        [-0.1659],
        [ 0.1904],
        [-0.2545],
        [ 0.0357],
        [ 0.4104],
        [ 0.2538],
        [ 0.0634],
        [-0.1629],
        [-0.0624],
        [ 0.2472],
        [-0.0793],
        [-0.0043],
        [-0.3840],
        [ 0.0307],
        [ 0.0480],
        [ 0.1899],
        [-0.2999],
        [-0.1548],
        [-0.0322],
        [-0.1031],
        [ 0.2796],
        [ 0.0986],
        [-0.1139],
        [-0.3688],
        [-0.0341]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(-10.8461, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(4.6308, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(4.7527, device='cuda:0')



h[100].sum tensor(-0.8206, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8422, device='cuda:0')



h[200].sum tensor(-1.1839, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2151, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(2904.5095, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0012, 0.0012, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0064, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(14633.8457, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(250.0390, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(20.0024, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-9.2535, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-10.3499, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0719],
        [-0.0880],
        [-0.1269],
        ...,
        [-0.0203],
        [-0.0203],
        [-0.0161]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(-1814.0530, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 20 from the network before training 
result1: tensor([[-0.0719],
        [-0.0880],
        [-0.1269],
        ...,
        [-0.0203],
        [-0.0203],
        [-0.0161]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')



input graph: 
g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(70.0222, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(10.4066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(10.3222, device='cuda:0')



h[100].sum tensor(-3.2652, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.2388, device='cuda:0')



h[200].sum tensor(15.1630, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.0402, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(15303.3486, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0013, 0.0543, 0.0000,  ..., 0.0029, 0.0067, 0.0074],
        [0.0003, 0.0114, 0.0000,  ..., 0.0006, 0.0014, 0.0016],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([13592, 128]) 
h2.sum tensor(61084.1953, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(104.6352, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(7.3616, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-55.8435, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(909.3811, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(63.9792, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0233],
        [-0.0143],
        [-0.0087],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([13592, 1]) 
h5.sum tensor(-1367.3046, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[-0.0719],
        [-0.0880],
        [-0.1269],
        ...,
        [-0.0203],
        [-0.0203],
        [-0.0161]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]
=> loading checkpoint from /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/saved_checkpoint.pth.tar



load_model True 
TraEvN 1998 
BatchSize 5 
EpochNum 1 
epoch_save 5 
LrVal 0.0001 
weight_decay 5e-05 






optimizer.param_groups [{'params': [Parameter containing:
tensor([[-0.1068,  0.0208,  0.0446,  0.0847, -0.0522, -0.0267, -0.1487,  0.0695,
          0.0646, -0.0553,  0.0840, -0.1316,  0.1198,  0.1460,  0.0819,  0.0230,
          0.0355, -0.1238,  0.0835,  0.0722, -0.0746,  0.0558, -0.1136,  0.0365,
          0.1440,  0.1022, -0.0807, -0.1027, -0.0195,  0.0432, -0.1148, -0.1134,
         -0.0292,  0.0198,  0.1282, -0.0880,  0.0379,  0.1189,  0.1426,  0.0663,
          0.1447, -0.0264,  0.0977, -0.0291, -0.0715, -0.1263,  0.0095,  0.0186,
         -0.1429,  0.0555,  0.1095, -0.0509, -0.0549, -0.0441,  0.0541, -0.1385,
         -0.0739, -0.1172, -0.1400,  0.0793,  0.0168,  0.0673,  0.0085,  0.0737,
          0.0342, -0.0704, -0.0719,  0.0957, -0.0406, -0.0952, -0.0780, -0.1213,
          0.1229,  0.1273, -0.0948, -0.0151,  0.0674,  0.0503, -0.0776, -0.0572,
          0.0595, -0.0960, -0.0067, -0.1315, -0.1209, -0.0883,  0.0455,  0.0091,
         -0.0929,  0.1173,  0.0923,  0.0221,  0.0838,  0.1350,  0.1012,  0.0610,
          0.0170, -0.0350, -0.0474, -0.0101, -0.0825, -0.1375,  0.0396, -0.0217,
          0.0240, -0.0291, -0.0427, -0.0357,  0.0593, -0.1147, -0.0441, -0.0101,
          0.0202,  0.0216, -0.0873,  0.1109, -0.0627, -0.0149, -0.1067,  0.1436,
          0.1219,  0.1106,  0.0612, -0.0483, -0.0208, -0.0364, -0.1031, -0.0139,
          0.0311, -0.0016, -0.0172,  0.0740,  0.0255, -0.0004,  0.1166,  0.1251,
          0.0387,  0.0558,  0.0963, -0.1268, -0.0033,  0.0319, -0.1432,  0.1133,
          0.0706, -0.0714, -0.0170, -0.0692,  0.1230,  0.1370,  0.0338,  0.1022,
          0.0599, -0.1512,  0.0513,  0.0155,  0.1302,  0.0952, -0.1512, -0.1015,
         -0.1037, -0.1361, -0.1238, -0.1063, -0.0767,  0.1171,  0.0050, -0.0717,
          0.0097, -0.1308, -0.0745,  0.1045,  0.0859,  0.1293,  0.0836,  0.0669,
         -0.1363, -0.0891,  0.0357, -0.1412,  0.1073,  0.0584, -0.0962,  0.0719,
         -0.1361, -0.1311,  0.1210, -0.0002,  0.0230,  0.0801, -0.0059,  0.0316,
          0.0989,  0.0657,  0.0318,  0.0542,  0.0514, -0.0010, -0.0530,  0.1166,
          0.0328,  0.1223, -0.0136,  0.0930,  0.0148, -0.1384,  0.0199,  0.0894,
          0.0489, -0.1200,  0.0554,  0.0010, -0.0068,  0.0445, -0.0970, -0.0315,
          0.0523, -0.0256, -0.1224,  0.0907, -0.1505, -0.1167, -0.0025, -0.0810,
         -0.0845, -0.1478, -0.0024,  0.0361,  0.0695, -0.0674,  0.0708, -0.0738,
         -0.0141, -0.1172, -0.1033, -0.0992, -0.0991, -0.0988, -0.0884,  0.0766,
         -0.0349,  0.0690, -0.0503,  0.1263,  0.1420,  0.0718, -0.0421, -0.0844,
          0.0570,  0.1409, -0.0144, -0.1475,  0.0439,  0.1048, -0.0388,  0.0205]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.1174,  0.0798,  0.0334,  ..., -0.0524,  0.1002, -0.0610],
        [-0.1024, -0.1167,  0.1123,  ..., -0.0395, -0.0586, -0.0088],
        [-0.1112,  0.0204,  0.0044,  ...,  0.0458,  0.0896, -0.0189],
        ...,
        [-0.0651, -0.1064,  0.0184,  ..., -0.0161,  0.0353,  0.0673],
        [-0.0939, -0.1232, -0.0158,  ..., -0.0989,  0.0908,  0.0032],
        [ 0.0428,  0.0804, -0.0451,  ...,  0.0935,  0.0911,  0.0729]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1292, -0.0207,  0.1281,  ..., -0.0015, -0.0412,  0.0309],
        [-0.0631,  0.0664, -0.0385,  ...,  0.0778,  0.0433,  0.1550],
        [-0.1714, -0.0287, -0.1566,  ...,  0.0929, -0.0976,  0.1641],
        ...,
        [-0.0386, -0.1545,  0.0366,  ...,  0.1610, -0.0593,  0.1637],
        [-0.0971, -0.1548,  0.0953,  ..., -0.0310, -0.1065, -0.1128],
        [-0.1226,  0.1233,  0.0840,  ...,  0.0547, -0.0174, -0.0168]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0386, -0.0390, -0.2430,  ..., -0.1133, -0.2021, -0.1594],
        [ 0.2006, -0.0410,  0.1047,  ...,  0.0418,  0.0362, -0.0249],
        [-0.0495, -0.0757,  0.2471,  ...,  0.0404, -0.0929, -0.0589],
        ...,
        [ 0.2498,  0.2005, -0.0756,  ...,  0.1793,  0.1087,  0.0388],
        [ 0.0603,  0.0085,  0.1234,  ..., -0.1437,  0.0603, -0.1105],
        [ 0.0205,  0.2229, -0.1634,  ...,  0.2466,  0.0102,  0.0889]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0821],
        [-0.3211],
        [ 0.0283],
        [ 0.2173],
        [-0.3312],
        [ 0.3452],
        [ 0.1505],
        [-0.0883],
        [-0.4118],
        [ 0.0067],
        [-0.0954],
        [ 0.0045],
        [-0.4149],
        [ 0.0560],
        [-0.3055],
        [-0.0626],
        [ 0.3292],
        [-0.1358],
        [ 0.0152],
        [-0.2833],
        [ 0.1052],
        [-0.2713],
        [ 0.1017],
        [-0.1225],
        [-0.0651],
        [ 0.1492],
        [ 0.0018],
        [-0.2966],
        [-0.1362],
        [ 0.3926],
        [-0.2520],
        [-0.0504]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



optimizer.param_groups [{'params': [Parameter containing:
tensor([[-0.1068,  0.0208,  0.0446,  0.0847, -0.0522, -0.0267, -0.1487,  0.0695,
          0.0646, -0.0553,  0.0840, -0.1316,  0.1198,  0.1460,  0.0819,  0.0230,
          0.0355, -0.1238,  0.0835,  0.0722, -0.0746,  0.0558, -0.1136,  0.0365,
          0.1440,  0.1022, -0.0807, -0.1027, -0.0195,  0.0432, -0.1148, -0.1134,
         -0.0292,  0.0198,  0.1282, -0.0880,  0.0379,  0.1189,  0.1426,  0.0663,
          0.1447, -0.0264,  0.0977, -0.0291, -0.0715, -0.1263,  0.0095,  0.0186,
         -0.1429,  0.0555,  0.1095, -0.0509, -0.0549, -0.0441,  0.0541, -0.1385,
         -0.0739, -0.1172, -0.1400,  0.0793,  0.0168,  0.0673,  0.0085,  0.0737,
          0.0342, -0.0704, -0.0719,  0.0957, -0.0406, -0.0952, -0.0780, -0.1213,
          0.1229,  0.1273, -0.0948, -0.0151,  0.0674,  0.0503, -0.0776, -0.0572,
          0.0595, -0.0960, -0.0067, -0.1315, -0.1209, -0.0883,  0.0455,  0.0091,
         -0.0929,  0.1173,  0.0923,  0.0221,  0.0838,  0.1350,  0.1012,  0.0610,
          0.0170, -0.0350, -0.0474, -0.0101, -0.0825, -0.1375,  0.0396, -0.0217,
          0.0240, -0.0291, -0.0427, -0.0357,  0.0593, -0.1147, -0.0441, -0.0101,
          0.0202,  0.0216, -0.0873,  0.1109, -0.0627, -0.0149, -0.1067,  0.1436,
          0.1219,  0.1106,  0.0612, -0.0483, -0.0208, -0.0364, -0.1031, -0.0139,
          0.0311, -0.0016, -0.0172,  0.0740,  0.0255, -0.0004,  0.1166,  0.1251,
          0.0387,  0.0558,  0.0963, -0.1268, -0.0033,  0.0319, -0.1432,  0.1133,
          0.0706, -0.0714, -0.0170, -0.0692,  0.1230,  0.1370,  0.0338,  0.1022,
          0.0599, -0.1512,  0.0513,  0.0155,  0.1302,  0.0952, -0.1512, -0.1015,
         -0.1037, -0.1361, -0.1238, -0.1063, -0.0767,  0.1171,  0.0050, -0.0717,
          0.0097, -0.1308, -0.0745,  0.1045,  0.0859,  0.1293,  0.0836,  0.0669,
         -0.1363, -0.0891,  0.0357, -0.1412,  0.1073,  0.0584, -0.0962,  0.0719,
         -0.1361, -0.1311,  0.1210, -0.0002,  0.0230,  0.0801, -0.0059,  0.0316,
          0.0989,  0.0657,  0.0318,  0.0542,  0.0514, -0.0010, -0.0530,  0.1166,
          0.0328,  0.1223, -0.0136,  0.0930,  0.0148, -0.1384,  0.0199,  0.0894,
          0.0489, -0.1200,  0.0554,  0.0010, -0.0068,  0.0445, -0.0970, -0.0315,
          0.0523, -0.0256, -0.1224,  0.0907, -0.1505, -0.1167, -0.0025, -0.0810,
         -0.0845, -0.1478, -0.0024,  0.0361,  0.0695, -0.0674,  0.0708, -0.0738,
         -0.0141, -0.1172, -0.1033, -0.0992, -0.0991, -0.0988, -0.0884,  0.0766,
         -0.0349,  0.0690, -0.0503,  0.1263,  0.1420,  0.0718, -0.0421, -0.0844,
          0.0570,  0.1409, -0.0144, -0.1475,  0.0439,  0.1048, -0.0388,  0.0205]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.1174,  0.0798,  0.0334,  ..., -0.0524,  0.1002, -0.0610],
        [-0.1024, -0.1167,  0.1123,  ..., -0.0395, -0.0586, -0.0088],
        [-0.1112,  0.0204,  0.0044,  ...,  0.0458,  0.0896, -0.0189],
        ...,
        [-0.0651, -0.1064,  0.0184,  ..., -0.0161,  0.0353,  0.0673],
        [-0.0939, -0.1232, -0.0158,  ..., -0.0989,  0.0908,  0.0032],
        [ 0.0428,  0.0804, -0.0451,  ...,  0.0935,  0.0911,  0.0729]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1292, -0.0207,  0.1281,  ..., -0.0015, -0.0412,  0.0309],
        [-0.0631,  0.0664, -0.0385,  ...,  0.0778,  0.0433,  0.1550],
        [-0.1714, -0.0287, -0.1566,  ...,  0.0929, -0.0976,  0.1641],
        ...,
        [-0.0386, -0.1545,  0.0366,  ...,  0.1610, -0.0593,  0.1637],
        [-0.0971, -0.1548,  0.0953,  ..., -0.0310, -0.1065, -0.1128],
        [-0.1226,  0.1233,  0.0840,  ...,  0.0547, -0.0174, -0.0168]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0386, -0.0390, -0.2430,  ..., -0.1133, -0.2021, -0.1594],
        [ 0.2006, -0.0410,  0.1047,  ...,  0.0418,  0.0362, -0.0249],
        [-0.0495, -0.0757,  0.2471,  ...,  0.0404, -0.0929, -0.0589],
        ...,
        [ 0.2498,  0.2005, -0.0756,  ...,  0.1793,  0.1087,  0.0388],
        [ 0.0603,  0.0085,  0.1234,  ..., -0.1437,  0.0603, -0.1105],
        [ 0.0205,  0.2229, -0.1634,  ...,  0.2466,  0.0102,  0.0889]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0821],
        [-0.3211],
        [ 0.0283],
        [ 0.2173],
        [-0.3312],
        [ 0.3452],
        [ 0.1505],
        [-0.0883],
        [-0.4118],
        [ 0.0067],
        [-0.0954],
        [ 0.0045],
        [-0.4149],
        [ 0.0560],
        [-0.3055],
        [-0.0626],
        [ 0.3292],
        [-0.1358],
        [ 0.0152],
        [-0.2833],
        [ 0.1052],
        [-0.2713],
        [ 0.1017],
        [-0.1225],
        [-0.0651],
        [ 0.1492],
        [ 0.0018],
        [-0.2966],
        [-0.1362],
        [ 0.3926],
        [-0.2520],
        [-0.0504]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}, {'params': [tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(300.4070, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365930., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(300.4070, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0157,  0.0031,  0.0065,  ...,  0.0154, -0.0057,  0.0030],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-75.0013, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-31.2577, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.9078, device='cuda:0')



h[100].sum tensor(-24.1244, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.2035, device='cuda:0')



h[200].sum tensor(9.5999, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-32.9424, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0115, 0.0247,  ..., 0.0581, 0.0000, 0.0113],
        [0.0000, 0.0095, 0.0204,  ..., 0.0478, 0.0000, 0.0093],
        [0.0000, 0.0022, 0.0048,  ..., 0.0112, 0.0000, 0.0022],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(28342.8828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2367, 0.0000, 0.0845,  ..., 0.1652, 0.0000, 0.0000],
        [0.2028, 0.0000, 0.0724,  ..., 0.1416, 0.0000, 0.0000],
        [0.1629, 0.0000, 0.0581,  ..., 0.1137, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(130357.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3297.2788, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(113.8949, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-167.4165, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2242.9678, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(286.5923, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5770e+00],
        [-1.7064e+00],
        [-1.8830e+00],
        ...,
        [-1.8946e-06],
        [-2.4892e-07],
        [ 0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-29391.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365930., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 0.0 event: 0 loss: tensor(83.6427, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.3519, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [0.9999],
        [0.9999],
        ...,
        [1.0001],
        [1.0001],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365912.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.3519, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.0000e-04,  1.0000e-04,  ...,  1.0000e-04,
          0.0000e+00,  1.0000e-04],
        [ 0.0000e+00, -1.0000e-04,  1.0000e-04,  ...,  1.0000e-04,
          0.0000e+00,  1.0000e-04],
        [ 0.0000e+00, -1.0000e-04,  1.0000e-04,  ...,  1.0000e-04,
          0.0000e+00,  1.0000e-04],
        ...,
        [ 0.0000e+00, -1.0000e-04,  1.0000e-04,  ...,  1.0000e-04,
          0.0000e+00,  1.0000e-04],
        [ 0.0000e+00, -1.0000e-04,  1.0000e-04,  ...,  1.0000e-04,
          0.0000e+00,  1.0000e-04],
        [ 0.0000e+00, -1.0000e-04,  1.0000e-04,  ...,  1.0000e-04,
          0.0000e+00,  1.0000e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-65.8304, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.8047, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.9934, device='cuda:0')



h[100].sum tensor(-22.2251, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.7067, device='cuda:0')



h[200].sum tensor(5.4298, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.3045, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0004,  ..., 0.0004, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0004,  ..., 0.0004, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0004,  ..., 0.0004, 0.0000, 0.0004],
        ...,
        [0.0000, 0.0000, 0.0004,  ..., 0.0004, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0004,  ..., 0.0004, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0004,  ..., 0.0004, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(28974.9805, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0027, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0027, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0030, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0027, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0027, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0027, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(138874.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4057.4209, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(89.4231, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-180.0302, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2253.9380, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(284.4913, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0192],
        [0.0148],
        [0.0046],
        ...,
        [0.0109],
        [0.0108],
        [0.0108]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-9024.9883, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [0.9999],
        [0.9999],
        ...,
        [1.0001],
        [1.0001],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365912.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.3872, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [0.9998],
        [0.9998],
        ...,
        [1.0001],
        [1.0001],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365887.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.3872, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.0008e-04,  1.9325e-04,  ...,  3.1675e-05,
          0.0000e+00,  2.0014e-04],
        [ 0.0000e+00, -2.0008e-04,  1.9325e-04,  ...,  3.1675e-05,
          0.0000e+00,  2.0014e-04],
        [ 0.0000e+00, -2.0008e-04,  1.9325e-04,  ...,  3.1675e-05,
          0.0000e+00,  2.0014e-04],
        ...,
        [ 0.0000e+00, -2.0008e-04,  1.9325e-04,  ...,  3.1675e-05,
          0.0000e+00,  2.0014e-04],
        [ 0.0000e+00, -2.0008e-04,  1.9325e-04,  ...,  3.1675e-05,
          0.0000e+00,  2.0014e-04],
        [ 0.0000e+00, -2.0008e-04,  1.9325e-04,  ...,  3.1675e-05,
          0.0000e+00,  2.0014e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-112.5258, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.4584, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.6294, device='cuda:0')



h[100].sum tensor(-15.7809, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.0555, device='cuda:0')



h[200].sum tensor(-0.2257, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.5357, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0096, 0.0229,  ..., 0.0519, 0.0000, 0.0110],
        [0.0000, 0.0063, 0.0154,  ..., 0.0345, 0.0000, 0.0076],
        [0.0000, 0.0052, 0.0129,  ..., 0.0286, 0.0000, 0.0064],
        ...,
        [0.0000, 0.0000, 0.0008,  ..., 0.0001, 0.0000, 0.0008],
        [0.0000, 0.0000, 0.0008,  ..., 0.0001, 0.0000, 0.0008],
        [0.0000, 0.0000, 0.0008,  ..., 0.0001, 0.0000, 0.0008]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(23293.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2095, 0.0000, 0.0681,  ..., 0.1322, 0.0000, 0.0000],
        [0.1690, 0.0000, 0.0541,  ..., 0.1059, 0.0000, 0.0000],
        [0.1296, 0.0000, 0.0403,  ..., 0.0803, 0.0000, 0.0000],
        ...,
        [0.0043, 0.0000, 0.0000,  ..., 0.0000, 0.0005, 0.0000],
        [0.0043, 0.0000, 0.0000,  ..., 0.0000, 0.0005, 0.0000],
        [0.0043, 0.0000, 0.0000,  ..., 0.0000, 0.0005, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(118775.1484, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3805.1045, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(62.8914, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-151.1248, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1674.4802, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(215.3693, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3191],
        [-0.2800],
        [-0.2152],
        ...,
        [ 0.0051],
        [ 0.0051],
        [ 0.0051]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(2306.0825, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [0.9998],
        [0.9998],
        ...,
        [1.0001],
        [1.0001],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365887.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3103],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.8990, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [0.9998],
        [0.9997],
        ...,
        [1.0000],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365865.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3103],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.8990, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.8038e-03,  8.1663e-04,  2.7297e-03,  ...,  5.6935e-03,
         -2.0978e-03,  1.4152e-03],
        [-1.4069e-02,  2.4064e-03,  6.2125e-03,  ...,  1.3847e-02,
         -5.0856e-03,  3.0247e-03],
        [-1.0295e-02,  1.6804e-03,  4.6220e-03,  ...,  1.0123e-02,
         -3.7211e-03,  2.2897e-03],
        ...,
        [ 0.0000e+00, -2.9961e-04,  2.8434e-04,  ..., -3.1359e-05,
          0.0000e+00,  2.8504e-04],
        [ 0.0000e+00, -2.9961e-04,  2.8434e-04,  ..., -3.1359e-05,
          0.0000e+00,  2.8504e-04],
        [ 0.0000e+00, -2.9961e-04,  2.8434e-04,  ..., -3.1359e-05,
          0.0000e+00,  2.8504e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-185.1451, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.4420, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.1823, device='cuda:0')



h[100].sum tensor(-17.3061, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.4584, device='cuda:0')



h[200].sum tensor(-2.4664, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.6753, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0085, 0.0223,  ..., 0.0494, 0.0000, 0.0109],
        [0.0000, 0.0050, 0.0148,  ..., 0.0318, 0.0000, 0.0074],
        [0.0000, 0.0043, 0.0126,  ..., 0.0267, 0.0000, 0.0064],
        ...,
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0011]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(24628.7559, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1478, 0.0000, 0.0439,  ..., 0.0887, 0.0000, 0.0000],
        [0.1330, 0.0000, 0.0384,  ..., 0.0792, 0.0000, 0.0000],
        [0.1152, 0.0000, 0.0327,  ..., 0.0682, 0.0000, 0.0000],
        ...,
        [0.0055, 0.0000, 0.0000,  ..., 0.0010, 0.0003, 0.0000],
        [0.0055, 0.0000, 0.0000,  ..., 0.0010, 0.0003, 0.0000],
        [0.0055, 0.0000, 0.0000,  ..., 0.0010, 0.0003, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(122756.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4101.0913, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(64.8134, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-161.6707, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1536.1503, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(224.9768, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1493],
        [-0.1333],
        [-0.1026],
        ...,
        [-0.0030],
        [-0.0030],
        [-0.0030]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(5457.1475, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [0.9998],
        [0.9997],
        ...,
        [1.0000],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365865.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.0282, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [0.9997],
        [0.9997],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365847.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.0282, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.9408e-04,  3.7727e-04,  ..., -8.2239e-05,
          0.0000e+00,  3.4588e-04],
        [ 0.0000e+00, -3.9408e-04,  3.7727e-04,  ..., -8.2239e-05,
          0.0000e+00,  3.4588e-04],
        [ 0.0000e+00, -3.9408e-04,  3.7727e-04,  ..., -8.2239e-05,
          0.0000e+00,  3.4588e-04],
        ...,
        [ 0.0000e+00, -3.9408e-04,  3.7727e-04,  ..., -8.2239e-05,
          0.0000e+00,  3.4588e-04],
        [ 0.0000e+00, -3.9408e-04,  3.7727e-04,  ..., -8.2239e-05,
          0.0000e+00,  3.4588e-04],
        [ 0.0000e+00, -3.9408e-04,  3.7727e-04,  ..., -8.2239e-05,
          0.0000e+00,  3.4588e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-248.5810, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.8024, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.9988, device='cuda:0')



h[100].sum tensor(-16.0373, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.1513, device='cuda:0')



h[200].sum tensor(-5.5795, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.0446, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0014],
        [0.0000, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0014],
        [0.0000, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0014],
        ...,
        [0.0000, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0014],
        [0.0000, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0014],
        [0.0000, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(23545.6133, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[6.3641e-03, 0.0000e+00, 0.0000e+00,  ..., 1.9584e-03, 0.0000e+00,
         0.0000e+00],
        [7.9279e-03, 0.0000e+00, 7.1537e-05,  ..., 2.8738e-03, 0.0000e+00,
         0.0000e+00],
        [1.4614e-02, 0.0000e+00, 1.2842e-03,  ..., 6.7103e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.3223e-03, 0.0000e+00, 0.0000e+00,  ..., 1.9406e-03, 0.0000e+00,
         0.0000e+00],
        [6.3222e-03, 0.0000e+00, 0.0000e+00,  ..., 1.9406e-03, 0.0000e+00,
         0.0000e+00],
        [6.3222e-03, 0.0000e+00, 0.0000e+00,  ..., 1.9406e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(119437.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3986.6230, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(60.2996, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-152.1477, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1263.1382, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(207.9913, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0469],
        [ 0.0541],
        [ 0.0660],
        ...,
        [-0.0041],
        [-0.0088],
        [-0.0103]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(4285.0786, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [0.9997],
        [0.9997],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365847.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(288.0699, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9996],
        [0.9996],
        ...,
        [0.9998],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365833.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(288.0699, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0099,  0.0014,  0.0046,  ...,  0.0096, -0.0036,  0.0023],
        [ 0.0000, -0.0005,  0.0005,  ..., -0.0001,  0.0000,  0.0004],
        [ 0.0000, -0.0005,  0.0005,  ..., -0.0001,  0.0000,  0.0004],
        ...,
        [ 0.0000, -0.0005,  0.0005,  ..., -0.0001,  0.0000,  0.0004],
        [ 0.0000, -0.0005,  0.0005,  ..., -0.0001,  0.0000,  0.0004],
        [ 0.0000, -0.0005,  0.0005,  ..., -0.0001,  0.0000,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-328.6721, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.2072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.9260, device='cuda:0')



h[100].sum tensor(-23.2814, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.9487, device='cuda:0')



h[200].sum tensor(-4.9888, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-31.5895, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0061, 0.0174,  ..., 0.0362, 0.0000, 0.0087],
        [0.0000, 0.0014, 0.0060,  ..., 0.0096, 0.0000, 0.0034],
        [0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0015],
        ...,
        [0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0015],
        [0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0015],
        [0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34574.9922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1379, 0.0000, 0.0400,  ..., 0.0801, 0.0000, 0.0000],
        [0.0660, 0.0000, 0.0163,  ..., 0.0373, 0.0000, 0.0000],
        [0.0234, 0.0000, 0.0039,  ..., 0.0122, 0.0000, 0.0000],
        ...,
        [0.0070, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0070, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0070, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(180743.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5856.8867, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(101.1212, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-210.0865, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2184.2856, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(312.3636, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1030],
        [ 0.0900],
        [ 0.0641],
        ...,
        [-0.0190],
        [-0.0189],
        [-0.0188]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(6387.6943, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9996],
        [0.9996],
        ...,
        [0.9998],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365833.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.2333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9996],
        [0.9996],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365822.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.2333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0005,  0.0005,  ..., -0.0002,  0.0000,  0.0004],
        [ 0.0000, -0.0005,  0.0005,  ..., -0.0002,  0.0000,  0.0004],
        [ 0.0000, -0.0005,  0.0005,  ..., -0.0002,  0.0000,  0.0004],
        ...,
        [ 0.0000, -0.0005,  0.0005,  ..., -0.0002,  0.0000,  0.0004],
        [ 0.0000, -0.0005,  0.0005,  ..., -0.0002,  0.0000,  0.0004],
        [ 0.0000, -0.0005,  0.0005,  ..., -0.0002,  0.0000,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-389.9078, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.8540, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.2331, device='cuda:0')



h[100].sum tensor(-20.6912, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.2500, device='cuda:0')



h[200].sum tensor(-7.8879, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.8790, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0014],
        [0.0000, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0014],
        [0.0000, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0014],
        ...,
        [0.0000, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0014],
        [0.0000, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0014],
        [0.0000, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31678.6055, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0306, 0.0000, 0.0052,  ..., 0.0158, 0.0000, 0.0000],
        [0.0125, 0.0000, 0.0005,  ..., 0.0055, 0.0000, 0.0000],
        [0.0077, 0.0000, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        ...,
        [0.0076, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0076, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0076, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(168100.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5354.8237, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(84.5935, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-191.9982, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1847.4944, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(277.1853, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0867],
        [ 0.0566],
        [ 0.0304],
        ...,
        [-0.0268],
        [-0.0267],
        [-0.0266]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-3754.0681, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9996],
        [0.9996],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365822.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.8365, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9996],
        [0.9995],
        ...,
        [0.9997],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365813.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.8365, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0181,  0.0029,  0.0083,  ...,  0.0177, -0.0065,  0.0039],
        [ 0.0000, -0.0006,  0.0005,  ..., -0.0002,  0.0000,  0.0003],
        [ 0.0000, -0.0006,  0.0005,  ..., -0.0002,  0.0000,  0.0003],
        ...,
        [ 0.0000, -0.0006,  0.0005,  ..., -0.0002,  0.0000,  0.0003],
        [ 0.0000, -0.0006,  0.0005,  ..., -0.0002,  0.0000,  0.0003],
        [ 0.0000, -0.0006,  0.0005,  ..., -0.0002,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-436.7339, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.9936, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.0631, device='cuda:0')



h[100].sum tensor(-16.1712, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.1680, device='cuda:0')



h[200].sum tensor(-11.3266, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.1332, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0050, 0.0160,  ..., 0.0318, 0.0000, 0.0078],
        [0.0000, 0.0029, 0.0099,  ..., 0.0177, 0.0000, 0.0049],
        [0.0000, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0013],
        ...,
        [0.0000, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0013],
        [0.0000, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0013],
        [0.0000, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0013]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(26502.4590, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1935, 0.0000, 0.0602,  ..., 0.1106, 0.0000, 0.0000],
        [0.1008, 0.0000, 0.0283,  ..., 0.0567, 0.0000, 0.0000],
        [0.0391, 0.0000, 0.0065,  ..., 0.0206, 0.0000, 0.0000],
        ...,
        [0.0080, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0080, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0080, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(148909.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4884.7148, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(57.9319, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-158.9325, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1534.6780, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(216.9320, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1250],
        [ 0.1260],
        [ 0.1248],
        ...,
        [-0.0354],
        [-0.0352],
        [-0.0351]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-4467.0449, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9996],
        [0.9995],
        ...,
        [0.9997],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365813.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(166.5609, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9995],
        [0.9995],
        ...,
        [0.9996],
        [0.9996],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365809.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(166.5609, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0006,  0.0006,  ..., -0.0003,  0.0000,  0.0003],
        [ 0.0000, -0.0006,  0.0006,  ..., -0.0003,  0.0000,  0.0003],
        [ 0.0000, -0.0006,  0.0006,  ..., -0.0003,  0.0000,  0.0003],
        ...,
        [ 0.0000, -0.0006,  0.0006,  ..., -0.0003,  0.0000,  0.0003],
        [ 0.0000, -0.0006,  0.0006,  ..., -0.0003,  0.0000,  0.0003],
        [ 0.0000, -0.0006,  0.0006,  ..., -0.0003,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-480.2306, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.1073, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.2557, device='cuda:0')



h[100].sum tensor(-13.1739, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.4395, device='cuda:0')



h[200].sum tensor(-13.9967, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.2649, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0036, 0.0132,  ..., 0.0249, 0.0000, 0.0063],
        [0.0000, 0.0018, 0.0078,  ..., 0.0125, 0.0000, 0.0038],
        [0.0000, 0.0036, 0.0118,  ..., 0.0219, 0.0000, 0.0056],
        ...,
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(23479.6504, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0950, 0.0000, 0.0238,  ..., 0.0513, 0.0000, 0.0000],
        [0.0862, 0.0000, 0.0214,  ..., 0.0466, 0.0000, 0.0000],
        [0.1026, 0.0000, 0.0278,  ..., 0.0564, 0.0000, 0.0000],
        ...,
        [0.0083, 0.0000, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0083, 0.0000, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0083, 0.0000, 0.0000,  ..., 0.0031, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(144645.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4463.3799, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(37.1545, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-139.8383, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1439.4641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(180.3512, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1034],
        [ 0.1130],
        [ 0.1250],
        ...,
        [-0.0458],
        [-0.0455],
        [-0.0455]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-15003.7559, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9995],
        [0.9995],
        ...,
        [0.9996],
        [0.9996],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365809.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2717],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(149.5398, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9995],
        [0.9995],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365806.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2717],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(149.5398, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0006,  0.0006,  ..., -0.0003,  0.0000,  0.0003],
        [-0.0072,  0.0007,  0.0037,  ...,  0.0068, -0.0026,  0.0017],
        [ 0.0000, -0.0006,  0.0006,  ..., -0.0003,  0.0000,  0.0003],
        ...,
        [ 0.0000, -0.0006,  0.0006,  ..., -0.0003,  0.0000,  0.0003],
        [ 0.0000, -0.0006,  0.0006,  ..., -0.0003,  0.0000,  0.0003],
        [ 0.0000, -0.0006,  0.0006,  ..., -0.0003,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-518.9738, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.4940, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(11.9011, device='cuda:0')



h[100].sum tensor(-11.9282, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.0880, device='cuda:0')



h[200].sum tensor(-15.7409, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-16.3984, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0007, 0.0054,  ..., 0.0068, 0.0000, 0.0025],
        [0.0000, 0.0013, 0.0067,  ..., 0.0098, 0.0000, 0.0031],
        [0.0000, 0.0040, 0.0172,  ..., 0.0332, 0.0000, 0.0080],
        ...,
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0011]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(22803.7422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0458, 0.0000, 0.0064,  ..., 0.0227, 0.0000, 0.0000],
        [0.0637, 0.0000, 0.0116,  ..., 0.0323, 0.0000, 0.0000],
        [0.0939, 0.0000, 0.0203,  ..., 0.0483, 0.0000, 0.0000],
        ...,
        [0.0085, 0.0000, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0085, 0.0000, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0085, 0.0000, 0.0000,  ..., 0.0032, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(149334.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4531.8599, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(25.0223, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-132.2862, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1594.5000, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(165.6593, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0778],
        [ 0.1047],
        [ 0.1275],
        ...,
        [-0.0573],
        [-0.0570],
        [-0.0569]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-21088.2344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9995],
        [0.9995],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365806.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5518],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.4784, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9995],
        [0.9995],
        ...,
        [0.9995],
        [0.9995],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365808.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5518],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.4784, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0007,  0.0006,  ..., -0.0004,  0.0000,  0.0002],
        [-0.0238,  0.0038,  0.0108,  ...,  0.0233, -0.0085,  0.0050],
        [-0.0241,  0.0039,  0.0109,  ...,  0.0236, -0.0086,  0.0050],
        ...,
        [ 0.0000, -0.0007,  0.0006,  ..., -0.0004,  0.0000,  0.0002],
        [ 0.0000, -0.0007,  0.0006,  ..., -0.0004,  0.0000,  0.0002],
        [ 0.0000, -0.0007,  0.0006,  ..., -0.0004,  0.0000,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-555.8609, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.3666, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.1488, device='cuda:0')



h[100].sum tensor(-17.2143, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.4497, device='cuda:0')



h[200].sum tensor(-14.7519, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.6292, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0046, 0.0158,  ..., 0.0303, 0.0000, 0.0072],
        [0.0000, 0.0062, 0.0210,  ..., 0.0421, 0.0000, 0.0096],
        [0.0000, 0.0146, 0.0414,  ..., 0.0891, 0.0000, 0.0191],
        ...,
        [0.0000, 0.0000, 0.0024,  ..., 0.0000, 0.0000, 0.0009],
        [0.0000, 0.0000, 0.0024,  ..., 0.0000, 0.0000, 0.0009],
        [0.0000, 0.0000, 0.0024,  ..., 0.0000, 0.0000, 0.0009]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(28760.3691, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1400, 0.0000, 0.0396,  ..., 0.0751, 0.0000, 0.0000],
        [0.2133, 0.0000, 0.0660,  ..., 0.1162, 0.0000, 0.0000],
        [0.3181, 0.0000, 0.1039,  ..., 0.1752, 0.0000, 0.0000],
        ...,
        [0.0084, 0.0000, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0084, 0.0000, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0084, 0.0000, 0.0000,  ..., 0.0031, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(178716.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5209.7461, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(42.8617, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-163.5609, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2156.8640, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(217.6819, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2275],
        [ 0.2475],
        [ 0.2692],
        ...,
        [-0.0697],
        [-0.0694],
        [-0.0693]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-26651.0566, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9995],
        [0.9995],
        ...,
        [0.9995],
        [0.9995],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365808.3438, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 10.0 event: 50 loss: tensor(581.3135, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3843],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.8324, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9995],
        [0.9996],
        ...,
        [0.9995],
        [0.9995],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365810.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3843],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.8324, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0084,  0.0009,  0.0042,  ...,  0.0079, -0.0030,  0.0019],
        [-0.0102,  0.0012,  0.0050,  ...,  0.0097, -0.0036,  0.0022],
        [-0.0167,  0.0025,  0.0078,  ...,  0.0162, -0.0060,  0.0035],
        ...,
        [ 0.0000, -0.0007,  0.0006,  ..., -0.0004,  0.0000,  0.0002],
        [ 0.0000, -0.0007,  0.0006,  ..., -0.0004,  0.0000,  0.0002],
        [ 0.0000, -0.0007,  0.0006,  ..., -0.0004,  0.0000,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-582.4944, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-31.5542, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.1008, device='cuda:0')



h[100].sum tensor(-24.2787, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.2536, device='cuda:0')



h[200].sum tensor(-12.9617, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-33.2084, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0056, 0.0212,  ..., 0.0419, 0.0000, 0.0095],
        [0.0000, 0.0062, 0.0226,  ..., 0.0452, 0.0000, 0.0102],
        [0.0000, 0.0034, 0.0164,  ..., 0.0308, 0.0000, 0.0073],
        ...,
        [0.0000, 0.0000, 0.0025,  ..., 0.0000, 0.0000, 0.0007],
        [0.0000, 0.0000, 0.0025,  ..., 0.0000, 0.0000, 0.0007],
        [0.0000, 0.0000, 0.0025,  ..., 0.0000, 0.0000, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36257.8516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1237, 0.0000, 0.0327,  ..., 0.0648, 0.0000, 0.0000],
        [0.1559, 0.0000, 0.0436,  ..., 0.0823, 0.0000, 0.0000],
        [0.1892, 0.0000, 0.0549,  ..., 0.1005, 0.0000, 0.0000],
        ...,
        [0.0081, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0081, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0081, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(209990.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5960.6108, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(68.0075, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-205.6947, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2739.1260, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(285.5862, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1595],
        [ 0.1921],
        [ 0.2243],
        ...,
        [-0.0820],
        [-0.0816],
        [-0.0814]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-31693.0430, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9995],
        [0.9996],
        ...,
        [0.9995],
        [0.9995],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365810.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(421.8401, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9995],
        [0.9996],
        ...,
        [0.9994],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365813.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(421.8401, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0007,  0.0006,  ..., -0.0004,  0.0000,  0.0001],
        [ 0.0000, -0.0007,  0.0006,  ..., -0.0004,  0.0000,  0.0001],
        [ 0.0000, -0.0007,  0.0006,  ..., -0.0004,  0.0000,  0.0001],
        ...,
        [ 0.0000, -0.0007,  0.0006,  ..., -0.0004,  0.0000,  0.0001],
        [ 0.0000, -0.0007,  0.0006,  ..., -0.0004,  0.0000,  0.0001],
        [ 0.0000, -0.0007,  0.0006,  ..., -0.0004,  0.0000,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-601.2271, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-43.4218, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.5720, device='cuda:0')



h[100].sum tensor(-33.4006, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-8.7111, device='cuda:0')



h[200].sum tensor(-10.2690, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-46.2587, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0025,  ..., 0.0000, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0025,  ..., 0.0000, 0.0000, 0.0006],
        [0.0000, 0.0015, 0.0075,  ..., 0.0111, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0025,  ..., 0.0000, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0025,  ..., 0.0000, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0025,  ..., 0.0000, 0.0000, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46517.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0104, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0205, 0.0000, 0.0019,  ..., 0.0098, 0.0000, 0.0000],
        [0.0491, 0.0000, 0.0089,  ..., 0.0250, 0.0000, 0.0000],
        ...,
        [0.0078, 0.0000, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0078, 0.0000, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0078, 0.0000, 0.0000,  ..., 0.0031, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(260345.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7341.8584, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(105.2300, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-265.4962, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3620.4258, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(381.3338, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0568],
        [-0.0149],
        [ 0.0316],
        ...,
        [-0.0933],
        [-0.0929],
        [-0.0927]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-34241.7734, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9995],
        [0.9996],
        ...,
        [0.9994],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365813.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4951],
        [0.5117],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(308.5466, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9996],
        [0.9996],
        ...,
        [0.9994],
        [0.9994],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365814.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4951],
        [0.5117],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(308.5466, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.3813e-02,  3.8451e-03,  1.0826e-02,  ...,  2.3249e-02,
         -8.4649e-03,  4.8622e-03],
        [-3.4622e-02,  5.9039e-03,  1.5465e-02,  ...,  3.4023e-02,
         -1.2307e-02,  7.0282e-03],
        [-2.4750e-02,  4.0234e-03,  1.1227e-02,  ...,  2.4182e-02,
         -8.7977e-03,  5.0499e-03],
        ...,
        [ 0.0000e+00, -6.9095e-04,  6.0300e-04,  ..., -4.8792e-04,
          0.0000e+00,  8.9990e-05],
        [ 0.0000e+00, -6.9095e-04,  6.0300e-04,  ..., -4.8792e-04,
          0.0000e+00,  8.9990e-05],
        [ 0.0000e+00, -6.9095e-04,  6.0300e-04,  ..., -4.8792e-04,
          0.0000e+00,  8.9990e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-594.8233, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-31.6921, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.5556, device='cuda:0')



h[100].sum tensor(-24.3710, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.3716, device='cuda:0')



h[200].sum tensor(-14.5851, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-33.8350, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0176, 0.0482,  ..., 0.1044, 0.0000, 0.0217],
        [0.0000, 0.0210, 0.0559,  ..., 0.1222, 0.0000, 0.0253],
        [0.0000, 0.0169, 0.0467,  ..., 0.1010, 0.0000, 0.0211],
        ...,
        [0.0000, 0.0000, 0.0024,  ..., 0.0000, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0024,  ..., 0.0000, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0024,  ..., 0.0000, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37903.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3443, 0.0000, 0.1225,  ..., 0.1907, 0.0000, 0.0000],
        [0.4170, 0.0000, 0.1506,  ..., 0.2315, 0.0000, 0.0000],
        [0.4038, 0.0000, 0.1447,  ..., 0.2236, 0.0000, 0.0000],
        ...,
        [0.0077, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0077, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0077, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(229549.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6275.7002, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(66.3806, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-219.6308, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3107.4766, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(290.1144, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1619],
        [ 0.1833],
        [ 0.1997],
        ...,
        [-0.1029],
        [-0.1024],
        [-0.1022]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-43241.9883, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9996],
        [0.9996],
        ...,
        [0.9994],
        [0.9994],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365814.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.6185, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [0.9997],
        ...,
        [0.9994],
        [0.9994],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365816.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.6185, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -6.7564e-04,  5.7987e-04,  ..., -5.2309e-04,
          0.0000e+00,  3.2447e-05],
        [ 0.0000e+00, -6.7564e-04,  5.7987e-04,  ..., -5.2309e-04,
          0.0000e+00,  3.2447e-05],
        [ 0.0000e+00, -6.7564e-04,  5.7987e-04,  ..., -5.2309e-04,
          0.0000e+00,  3.2447e-05],
        ...,
        [ 0.0000e+00, -6.7564e-04,  5.7987e-04,  ..., -5.2309e-04,
          0.0000e+00,  3.2447e-05],
        [ 0.0000e+00, -6.7564e-04,  5.7987e-04,  ..., -5.2309e-04,
          0.0000e+00,  3.2447e-05],
        [ 0.0000e+00, -6.7564e-04,  5.7987e-04,  ..., -5.2309e-04,
          0.0000e+00,  3.2447e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-596.2935, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.5303, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.2879, device='cuda:0')



h[100].sum tensor(-23.4710, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.0427, device='cuda:0')



h[200].sum tensor(-15.6576, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-32.0883, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0001],
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0001],
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0001],
        ...,
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0001],
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0001],
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0001]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37838.7109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0079, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0079, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0079, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        ...,
        [0.0078, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0078, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0078, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(237386.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6522.9033, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(61.1411, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-224.0629, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3320.7053, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(287.8503, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1478],
        [-0.1359],
        [-0.1098],
        ...,
        [-0.0868],
        [-0.1049],
        [-0.1099]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-46721.4336, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [0.9997],
        ...,
        [0.9994],
        [0.9994],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365816.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3965],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(265.5134, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [0.9997],
        ...,
        [0.9993],
        [0.9993],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365817.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3965],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(265.5134, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0443e-02,  1.3496e-03,  5.0440e-03,  ...,  9.8622e-03,
         -3.6995e-03,  2.0596e-03],
        [ 0.0000e+00, -6.5380e-04,  5.6250e-04,  ..., -5.5334e-04,
          0.0000e+00, -2.7294e-05],
        [-1.7767e-02,  2.7547e-03,  8.1871e-03,  ...,  1.7167e-02,
         -6.2941e-03,  3.5232e-03],
        ...,
        [ 0.0000e+00, -6.5380e-04,  5.6250e-04,  ..., -5.5334e-04,
          0.0000e+00, -2.7294e-05],
        [ 0.0000e+00, -6.5380e-04,  5.6250e-04,  ..., -5.5334e-04,
          0.0000e+00, -2.7294e-05],
        [ 0.0000e+00, -6.5380e-04,  5.6250e-04,  ..., -5.5334e-04,
          0.0000e+00, -2.7294e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-592.0769, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.3556, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.1308, device='cuda:0')



h[100].sum tensor(-21.0244, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.4829, device='cuda:0')



h[200].sum tensor(-17.1719, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-29.1160, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0010, 0.0059,  ..., 0.0079, 0.0000, 0.0017],
        [0.0000, 0.0061, 0.0217,  ..., 0.0429, 0.0000, 0.0089],
        [0.0000, 0.0029, 0.0131,  ..., 0.0236, 0.0000, 0.0050],
        ...,
        [0.0000, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35500.8828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0626, 0.0000, 0.0161,  ..., 0.0339, 0.0000, 0.0000],
        [0.1195, 0.0000, 0.0360,  ..., 0.0644, 0.0000, 0.0000],
        [0.1230, 0.0000, 0.0361,  ..., 0.0656, 0.0000, 0.0000],
        ...,
        [0.0079, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0079, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0079, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(225132.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6459.6895, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(47.2081, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-214.8577, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3256.5459, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(260.6019, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0186],
        [ 0.0092],
        [ 0.0266],
        ...,
        [-0.1187],
        [-0.1181],
        [-0.1180]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-46625.1797, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [0.9997],
        ...,
        [0.9993],
        [0.9993],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365817.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3511],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.7086, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [0.9997],
        ...,
        [0.9993],
        [0.9993],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365820.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3511],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.7086, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.2380e-03,  1.1376e-03,  4.5337e-03,  ...,  8.6657e-03,
         -3.2671e-03,  1.7663e-03],
        [ 0.0000e+00, -6.4070e-04,  5.6955e-04,  ..., -5.5176e-04,
          0.0000e+00, -7.7207e-05],
        [-9.2380e-03,  1.1376e-03,  4.5337e-03,  ...,  8.6657e-03,
         -3.2671e-03,  1.7663e-03],
        ...,
        [ 0.0000e+00, -6.4070e-04,  5.6955e-04,  ..., -5.5176e-04,
          0.0000e+00, -7.7207e-05],
        [ 0.0000e+00, -6.4070e-04,  5.6955e-04,  ..., -5.5176e-04,
          0.0000e+00, -7.7207e-05],
        [ 0.0000e+00, -6.4070e-04,  5.6955e-04,  ..., -5.5176e-04,
          0.0000e+00, -7.7207e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-578.8080, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.0420, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.2917, device='cuda:0')



h[100].sum tensor(-16.1674, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.2273, device='cuda:0')



h[200].sum tensor(-19.6469, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.4482, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0008, 0.0055,  ..., 0.0070, 0.0000, 0.0014],
        [0.0000, 0.0039, 0.0167,  ..., 0.0313, 0.0000, 0.0064],
        [0.0000, 0.0008, 0.0055,  ..., 0.0070, 0.0000, 0.0014],
        ...,
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29243.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0453, 0.0000, 0.0106,  ..., 0.0253, 0.0000, 0.0000],
        [0.0721, 0.0000, 0.0200,  ..., 0.0396, 0.0000, 0.0000],
        [0.0454, 0.0000, 0.0106,  ..., 0.0253, 0.0000, 0.0000],
        ...,
        [0.0079, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0079, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0079, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(201672.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5295.9316, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(20.3315, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-185.0146, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2594.3079, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(192.8468, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1007],
        [-0.0866],
        [-0.0899],
        ...,
        [-0.1245],
        [-0.1239],
        [-0.1237]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-57660.8906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [0.9997],
        ...,
        [0.9993],
        [0.9993],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365820.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4473],
        [0.3840],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(246.0198, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [0.9998],
        ...,
        [0.9992],
        [0.9992],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365823.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4473],
        [0.3840],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(246.0198, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0200,  0.0032,  0.0092,  ...,  0.0194, -0.0071,  0.0039],
        [-0.0185,  0.0029,  0.0085,  ...,  0.0179, -0.0065,  0.0036],
        [-0.0285,  0.0049,  0.0128,  ...,  0.0279, -0.0101,  0.0056],
        ...,
        [ 0.0000, -0.0006,  0.0006,  ..., -0.0005,  0.0000, -0.0001],
        [ 0.0000, -0.0006,  0.0006,  ..., -0.0005,  0.0000, -0.0001],
        [ 0.0000, -0.0006,  0.0006,  ..., -0.0005,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-567.7322, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.1670, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.5794, device='cuda:0')



h[100].sum tensor(-19.3313, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.0804, device='cuda:0')



h[200].sum tensor(-18.9340, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.9783, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0129, 0.0366,  ..., 0.0774, 0.0000, 0.0154],
        [0.0000, 0.0174, 0.0467,  ..., 0.1010, 0.0000, 0.0201],
        [0.0000, 0.0215, 0.0558,  ..., 0.1223, 0.0000, 0.0244],
        ...,
        [0.0000, 0.0000, 0.0024,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0024,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0024,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32124.1016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3332, 0.0000, 0.1289,  ..., 0.1894, 0.0000, 0.0000],
        [0.3965, 0.0000, 0.1563,  ..., 0.2263, 0.0000, 0.0000],
        [0.4692, 0.0000, 0.1876,  ..., 0.2686, 0.0000, 0.0000],
        ...,
        [0.0078, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0078, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0078, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(209901.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5666.9756, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(30.5255, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-208.2876, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2689.4104, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(216.0029, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0438],
        [-0.0536],
        [-0.0621],
        ...,
        [-0.1298],
        [-0.1292],
        [-0.1290]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-56397.4844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [0.9998],
        ...,
        [0.9992],
        [0.9992],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365823.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2505],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(384.1675, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [0.9998],
        ...,
        [0.9992],
        [0.9992],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365827.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2505],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(384.1675, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0070,  0.0007,  0.0036,  ...,  0.0065, -0.0025,  0.0012],
        [-0.0066,  0.0006,  0.0034,  ...,  0.0061, -0.0023,  0.0012],
        [ 0.0000, -0.0006,  0.0006,  ..., -0.0005,  0.0000, -0.0002],
        ...,
        [ 0.0000, -0.0006,  0.0006,  ..., -0.0005,  0.0000, -0.0002],
        [ 0.0000, -0.0006,  0.0006,  ..., -0.0005,  0.0000, -0.0002],
        [ 0.0000, -0.0006,  0.0006,  ..., -0.0005,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-556.3745, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-39.6578, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.5738, device='cuda:0')



h[100].sum tensor(-30.4534, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-7.9332, device='cuda:0')



h[200].sum tensor(-15.0755, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-42.1275, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0080, 0.0258,  ..., 0.0524, 0.0000, 0.0102],
        [0.0000, 0.0011, 0.0078,  ..., 0.0114, 0.0000, 0.0022],
        [0.0000, 0.0033, 0.0140,  ..., 0.0253, 0.0000, 0.0049],
        ...,
        [0.0000, 0.0000, 0.0025,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0025,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0025,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47639.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1938, 0.0000, 0.0709,  ..., 0.1102, 0.0000, 0.0000],
        [0.1191, 0.0000, 0.0398,  ..., 0.0674, 0.0000, 0.0000],
        [0.1306, 0.0000, 0.0440,  ..., 0.0737, 0.0000, 0.0000],
        ...,
        [0.0078, 0.0004, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0078, 0.0004, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0078, 0.0004, 0.0000,  ..., 0.0065, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(284545.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8362.4736, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(90.6178, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-304.0111, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4057.1494, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(365.9816, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0034],
        [ 0.0111],
        [ 0.0159],
        ...,
        [-0.1213],
        [-0.1206],
        [-0.1231]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-48836.4609, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [0.9998],
        ...,
        [0.9992],
        [0.9992],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365827.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.2115, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [0.9998],
        ...,
        [0.9992],
        [0.9992],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365827.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.2115, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0006,  0.0006,  ..., -0.0005,  0.0000, -0.0002],
        [ 0.0000, -0.0006,  0.0006,  ..., -0.0005,  0.0000, -0.0002],
        [ 0.0000, -0.0006,  0.0006,  ..., -0.0005,  0.0000, -0.0002],
        ...,
        [ 0.0000, -0.0006,  0.0006,  ..., -0.0005,  0.0000, -0.0002],
        [ 0.0000, -0.0006,  0.0006,  ..., -0.0005,  0.0000, -0.0002],
        [ 0.0000, -0.0006,  0.0006,  ..., -0.0005,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-552.7895, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.8567, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.0168, device='cuda:0')



h[100].sum tensor(-22.9271, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.9723, device='cuda:0')



h[200].sum tensor(-18.0644, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-31.7147, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0025,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0025,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0025,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0025,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0025,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0025,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37054.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0078, 0.0004, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0078, 0.0004, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0078, 0.0004, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        ...,
        [0.0078, 0.0004, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0078, 0.0004, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0078, 0.0004, 0.0000,  ..., 0.0065, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(233417.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6827.5005, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(49.0349, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-242.2509, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3123.3599, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(258.8425, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1930],
        [-0.1974],
        [-0.1951],
        ...,
        [-0.1344],
        [-0.1315],
        [-0.1251]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-45992.0547, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [0.9998],
        ...,
        [0.9992],
        [0.9992],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365827.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3511],
        [0.3350],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(341.3232, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [0.9999],
        ...,
        [0.9992],
        [0.9991],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365831.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3511],
        [0.3350],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(341.3232, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0088,  0.0011,  0.0044,  ...,  0.0083, -0.0031,  0.0016],
        [-0.0092,  0.0012,  0.0046,  ...,  0.0087, -0.0032,  0.0016],
        [-0.0088,  0.0011,  0.0044,  ...,  0.0083, -0.0031,  0.0016],
        ...,
        [ 0.0000, -0.0006,  0.0006,  ..., -0.0005,  0.0000, -0.0002],
        [ 0.0000, -0.0006,  0.0006,  ..., -0.0005,  0.0000, -0.0002],
        [ 0.0000, -0.0006,  0.0006,  ..., -0.0005,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-533.6997, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-34.9425, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.1641, device='cuda:0')



h[100].sum tensor(-26.8249, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-7.0484, device='cuda:0')



h[200].sum tensor(-17.0700, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-37.4293, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0053, 0.0200,  ..., 0.0388, 0.0000, 0.0073],
        [0.0000, 0.0051, 0.0195,  ..., 0.0376, 0.0000, 0.0071],
        [0.0000, 0.0019, 0.0096,  ..., 0.0155, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43249.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1159, 0.0000, 0.0382,  ..., 0.0660, 0.0000, 0.0000],
        [0.1172, 0.0000, 0.0384,  ..., 0.0666, 0.0000, 0.0000],
        [0.0932, 0.0000, 0.0290,  ..., 0.0532, 0.0000, 0.0000],
        ...,
        [0.0076, 0.0009, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0076, 0.0009, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0076, 0.0009, 0.0000,  ..., 0.0069, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(265624.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7649.7256, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(74.2575, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-284.1980, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3457.5088, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(315.1328, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0219],
        [ 0.0269],
        [ 0.0302],
        ...,
        [-0.1344],
        [-0.1332],
        [-0.1335]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-49116.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [0.9999],
        ...,
        [0.9992],
        [0.9991],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365831.5000, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 20.0 event: 100 loss: tensor(527.3118, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.2831, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [0.9999],
        ...,
        [0.9992],
        [0.9991],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365836.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.2831, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0066,  0.0006,  0.0035,  ...,  0.0062, -0.0023,  0.0011],
        [ 0.0000, -0.0006,  0.0007,  ..., -0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0006,  0.0007,  ..., -0.0004,  0.0000, -0.0002],
        ...,
        [ 0.0000, -0.0006,  0.0007,  ..., -0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0006,  0.0007,  ..., -0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0006,  0.0007,  ..., -0.0004,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-508.7826, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.0995, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.8841, device='cuda:0')



h[100].sum tensor(-18.4956, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.9000, device='cuda:0')



h[200].sum tensor(-20.9958, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.0203, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0037, 0.0137,  ..., 0.0248, 0.0000, 0.0047],
        [0.0000, 0.0006, 0.0055,  ..., 0.0062, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32240.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1358, 0.0000, 0.0490,  ..., 0.0791, 0.0000, 0.0000],
        [0.0724, 0.0000, 0.0237,  ..., 0.0433, 0.0000, 0.0000],
        [0.0282, 0.0005, 0.0074,  ..., 0.0187, 0.0000, 0.0000],
        ...,
        [0.0074, 0.0011, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0074, 0.0011, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0074, 0.0011, 0.0000,  ..., 0.0070, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(215000.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5757.6377, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(32.5769, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-225.2572, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2187.0244, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(198.5746, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0084],
        [ 0.0042],
        [-0.0168],
        ...,
        [-0.1380],
        [-0.1373],
        [-0.1371]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-52892.5508, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [0.9999],
        ...,
        [0.9992],
        [0.9991],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365836.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.5594, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [0.9999],
        ...,
        [0.9992],
        [0.9991],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365836.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.5594, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0006,  0.0007,  ..., -0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0006,  0.0007,  ..., -0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0006,  0.0007,  ..., -0.0004,  0.0000, -0.0002],
        ...,
        [ 0.0000, -0.0006,  0.0007,  ..., -0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0006,  0.0007,  ..., -0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0006,  0.0007,  ..., -0.0004,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-508.3121, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.7225, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.1206, device='cuda:0')



h[100].sum tensor(-15.9038, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.1829, device='cuda:0')



h[200].sum tensor(-22.0271, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.2125, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0025, 0.0095,  ..., 0.0156, 0.0000, 0.0030],
        ...,
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(28716.2383, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0139, 0.0003, 0.0014,  ..., 0.0105, 0.0000, 0.0000],
        [0.0275, 0.0003, 0.0071,  ..., 0.0183, 0.0000, 0.0000],
        [0.0696, 0.0000, 0.0240,  ..., 0.0424, 0.0000, 0.0000],
        ...,
        [0.0074, 0.0011, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0074, 0.0011, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0074, 0.0011, 0.0000,  ..., 0.0070, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(199407.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4981.8901, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(19.3898, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-204.6986, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1829.7289, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(164.1184, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0008],
        [-0.0050],
        [-0.0067],
        ...,
        [-0.1380],
        [-0.1373],
        [-0.1371]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-60715.7227, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [0.9999],
        ...,
        [0.9992],
        [0.9991],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365836.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.4166, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [0.9999],
        ...,
        [0.9992],
        [0.9990],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365842.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.4166, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0007,  0.0007,  ..., -0.0003,  0.0000, -0.0002],
        [ 0.0000, -0.0007,  0.0007,  ..., -0.0003,  0.0000, -0.0002],
        [ 0.0000, -0.0007,  0.0007,  ..., -0.0003,  0.0000, -0.0002],
        ...,
        [ 0.0000, -0.0007,  0.0007,  ..., -0.0003,  0.0000, -0.0002],
        [ 0.0000, -0.0007,  0.0007,  ..., -0.0003,  0.0000, -0.0002],
        [ 0.0000, -0.0007,  0.0007,  ..., -0.0003,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-484.7770, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.2954, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.4310, device='cuda:0')



h[100].sum tensor(-23.2440, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.0798, device='cuda:0')



h[200].sum tensor(-19.7422, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-32.2855, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39135.7617, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0386, 0.0002, 0.0112,  ..., 0.0249, 0.0000, 0.0000],
        [0.0158, 0.0005, 0.0027,  ..., 0.0121, 0.0000, 0.0000],
        [0.0094, 0.0008, 0.0005,  ..., 0.0084, 0.0000, 0.0000],
        ...,
        [0.0074, 0.0010, 0.0000,  ..., 0.0074, 0.0000, 0.0000],
        [0.0074, 0.0010, 0.0000,  ..., 0.0074, 0.0000, 0.0000],
        [0.0074, 0.0010, 0.0000,  ..., 0.0074, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(247217.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6961.8564, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(59.0024, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-271.3493, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2527.0635, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(260.4037, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0163],
        [-0.0305],
        [-0.0286],
        ...,
        [-0.1369],
        [-0.1363],
        [-0.1360]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-43304.0742, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [0.9999],
        ...,
        [0.9992],
        [0.9990],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365842.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(264.4037, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [0.9999],
        ...,
        [0.9992],
        [0.9990],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365849.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(264.4037, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0057,  0.0004,  0.0031,  ...,  0.0054, -0.0020,  0.0009],
        [-0.0073,  0.0007,  0.0038,  ...,  0.0070, -0.0025,  0.0012],
        [ 0.0000, -0.0007,  0.0007,  ..., -0.0003,  0.0000, -0.0002],
        ...,
        [-0.0094,  0.0012,  0.0047,  ...,  0.0091, -0.0033,  0.0016],
        [ 0.0000, -0.0007,  0.0007,  ..., -0.0003,  0.0000, -0.0002],
        [ 0.0000, -0.0007,  0.0007,  ..., -0.0003,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-460.2358, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.9859, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.0425, device='cuda:0')



h[100].sum tensor(-20.6989, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.4600, device='cuda:0')



h[200].sum tensor(-21.3304, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.9943, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0079, 0.0260,  ..., 0.0529, 0.0000, 0.0097],
        [0.0000, 0.0029, 0.0121,  ..., 0.0211, 0.0000, 0.0038],
        [0.0000, 0.0007, 0.0059,  ..., 0.0070, 0.0000, 0.0012],
        ...,
        [0.0000, 0.0112, 0.0319,  ..., 0.0671, 0.0000, 0.0127],
        [0.0000, 0.0097, 0.0286,  ..., 0.0593, 0.0000, 0.0112],
        [0.0000, 0.0071, 0.0213,  ..., 0.0426, 0.0000, 0.0081]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34916.8672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1796, 0.0000, 0.0658,  ..., 0.1044, 0.0000, 0.0000],
        [0.1221, 0.0000, 0.0429,  ..., 0.0718, 0.0000, 0.0000],
        [0.0657, 0.0000, 0.0209,  ..., 0.0401, 0.0000, 0.0000],
        ...,
        [0.4025, 0.0000, 0.1659,  ..., 0.2359, 0.0000, 0.0000],
        [0.3250, 0.0000, 0.1320,  ..., 0.1906, 0.0000, 0.0000],
        [0.2286, 0.0000, 0.0909,  ..., 0.1347, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(228911.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6013.8335, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(43.7754, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-252.1331, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1814.0001, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(212.0153, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0436],
        [ 0.0439],
        [ 0.0368],
        ...,
        [-0.1715],
        [-0.1242],
        [-0.0775]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-50529.8203, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [0.9999],
        ...,
        [0.9992],
        [0.9990],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365849.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.6886, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [1.0000],
        ...,
        [0.9992],
        [0.9989],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365856.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.6886, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0007,  0.0007,  ..., -0.0002,  0.0000, -0.0003],
        [-0.0111,  0.0015,  0.0054,  ...,  0.0109, -0.0039,  0.0019],
        [-0.0111,  0.0015,  0.0054,  ...,  0.0109, -0.0039,  0.0019],
        ...,
        [ 0.0000, -0.0007,  0.0007,  ..., -0.0002,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0007,  ..., -0.0002,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0007,  ..., -0.0002,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-437.6451, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.2738, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.1205, device='cuda:0')



h[100].sum tensor(-17.8465, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.7018, device='cuda:0')



h[200].sum tensor(-23.0007, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.9682, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0026, 0.0114,  ..., 0.0197, 0.0000, 0.0035],
        [0.0000, 0.0036, 0.0153,  ..., 0.0284, 0.0000, 0.0050],
        [0.0000, 0.0062, 0.0209,  ..., 0.0417, 0.0000, 0.0076],
        ...,
        [0.0000, 0.0000, 0.0028,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0028,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0028,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32610.2559, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1420, 0.0000, 0.0520,  ..., 0.0837, 0.0000, 0.0000],
        [0.1992, 0.0000, 0.0759,  ..., 0.1167, 0.0000, 0.0000],
        [0.2693, 0.0000, 0.1064,  ..., 0.1575, 0.0000, 0.0000],
        ...,
        [0.0076, 0.0009, 0.0000,  ..., 0.0078, 0.0000, 0.0000],
        [0.0076, 0.0009, 0.0000,  ..., 0.0078, 0.0000, 0.0000],
        [0.0076, 0.0009, 0.0000,  ..., 0.0078, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(223428.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5589.2910, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(36.5377, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-243.4135, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1445.2637, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(184.3990, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1116],
        [-0.1544],
        [-0.1955],
        ...,
        [-0.1372],
        [-0.1365],
        [-0.1363]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-55727.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [1.0000],
        ...,
        [0.9992],
        [0.9989],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365856.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.0996, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [1.0000],
        ...,
        [0.9991],
        [0.9989],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365864.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.0996, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0007,  0.0007,  ..., -0.0002,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0007,  ..., -0.0002,  0.0000, -0.0003],
        [-0.0069,  0.0007,  0.0037,  ...,  0.0067, -0.0024,  0.0011],
        ...,
        [ 0.0000, -0.0007,  0.0007,  ..., -0.0002,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0007,  ..., -0.0002,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0007,  ..., -0.0002,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-415.3986, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.1361, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.0287, device='cuda:0')



h[100].sum tensor(-18.5024, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.9375, device='cuda:0')



h[200].sum tensor(-23.2447, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.2195, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0007, 0.0057,  ..., 0.0067, 0.0000, 0.0011],
        [0.0000, 0.0017, 0.0095,  ..., 0.0155, 0.0000, 0.0026],
        ...,
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34941.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0243, 0.0001, 0.0043,  ..., 0.0165, 0.0000, 0.0000],
        [0.0608, 0.0000, 0.0177,  ..., 0.0368, 0.0000, 0.0000],
        [0.1101, 0.0000, 0.0366,  ..., 0.0644, 0.0000, 0.0000],
        ...,
        [0.0077, 0.0007, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0077, 0.0007, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0077, 0.0007, 0.0000,  ..., 0.0077, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(236181., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6351.3647, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(47.9784, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-258.9397, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1619.9492, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(203.9601, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0504],
        [ 0.0475],
        [ 0.0371],
        ...,
        [-0.1370],
        [-0.1364],
        [-0.1362]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-39738.0273, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [1.0000],
        ...,
        [0.9991],
        [0.9989],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365864.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3613],
        [0.3167],
        [0.4028],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.6964, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [1.0000],
        ...,
        [0.9991],
        [0.9988],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365871.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3613],
        [0.3167],
        [0.4028],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.6964, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0193,  0.0031,  0.0090,  ...,  0.0192, -0.0067,  0.0035],
        [-0.0199,  0.0032,  0.0093,  ...,  0.0199, -0.0069,  0.0037],
        [-0.0168,  0.0026,  0.0079,  ...,  0.0168, -0.0059,  0.0031],
        ...,
        [ 0.0000, -0.0007,  0.0007,  ..., -0.0001,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0007,  ..., -0.0001,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0007,  ..., -0.0001,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-399.1519, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.7793, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.2111, device='cuda:0')



h[100].sum tensor(-15.9246, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.2064, device='cuda:0')



h[200].sum tensor(-24.6898, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.3372, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0114, 0.0339,  ..., 0.0723, 0.0000, 0.0133],
        [0.0000, 0.0125, 0.0363,  ..., 0.0779, 0.0000, 0.0144],
        [0.0000, 0.0160, 0.0441,  ..., 0.0962, 0.0000, 0.0180],
        ...,
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30784.1914, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2788, 0.0000, 0.1087,  ..., 0.1613, 0.0000, 0.0000],
        [0.3033, 0.0000, 0.1189,  ..., 0.1753, 0.0000, 0.0000],
        [0.3295, 0.0000, 0.1302,  ..., 0.1905, 0.0000, 0.0000],
        ...,
        [0.0080, 0.0007, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0080, 0.0007, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0080, 0.0007, 0.0000,  ..., 0.0077, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(218916., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5513.3320, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(32.8792, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-238.7086, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1147.4310, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(162.9529, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0274],
        [ 0.0239],
        [ 0.0260],
        ...,
        [-0.1403],
        [-0.1396],
        [-0.1394]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-47727.4609, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [1.0000],
        ...,
        [0.9991],
        [0.9988],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365871.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2839],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.0151, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [1.0000],
        ...,
        [0.9990],
        [0.9987],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365879.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2839],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.0151, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -7.1003e-04,  6.5639e-04,  ..., -7.2391e-05,
          0.0000e+00, -3.0291e-04],
        [-7.4007e-03,  7.4011e-04,  3.8458e-03,  ...,  7.3694e-03,
         -2.5724e-03,  1.1714e-03],
        [ 0.0000e+00, -7.1003e-04,  6.5639e-04,  ..., -7.2391e-05,
          0.0000e+00, -3.0291e-04],
        ...,
        [ 0.0000e+00, -7.1003e-04,  6.5639e-04,  ..., -7.2391e-05,
          0.0000e+00, -3.0291e-04],
        [ 0.0000e+00, -7.1003e-04,  6.5639e-04,  ..., -7.2391e-05,
          0.0000e+00, -3.0291e-04],
        [ 0.0000e+00, -7.1003e-04,  6.5639e-04,  ..., -7.2391e-05,
          0.0000e+00, -3.0291e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-384.8898, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.9623, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.7036, device='cuda:0')



h[100].sum tensor(-18.3586, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.8531, device='cuda:0')



h[200].sum tensor(-23.9693, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.7716, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0034, 0.0165,  ..., 0.0320, 0.0000, 0.0052],
        [0.0000, 0.0005, 0.0052,  ..., 0.0060, 0.0000, 0.0009],
        [0.0000, 0.0007, 0.0058,  ..., 0.0074, 0.0000, 0.0012],
        ...,
        [0.0000, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34224.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.2064e-01, 0.0000e+00, 3.9549e-02,  ..., 6.9122e-02, 0.0000e+00,
         0.0000e+00],
        [7.6003e-02, 0.0000e+00, 2.3288e-02,  ..., 4.4536e-02, 0.0000e+00,
         0.0000e+00],
        [4.8160e-02, 0.0000e+00, 1.2976e-02,  ..., 2.9138e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [8.0526e-03, 7.7669e-04, 1.0180e-04,  ..., 7.5009e-03, 0.0000e+00,
         0.0000e+00],
        [8.0514e-03, 7.7647e-04, 1.0192e-04,  ..., 7.5002e-03, 0.0000e+00,
         0.0000e+00],
        [8.0507e-03, 7.7631e-04, 1.0197e-04,  ..., 7.4997e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(235323.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6147.1963, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(45.8672, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-262.4621, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1341.2683, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(198.2659, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0768],
        [ 0.0809],
        [ 0.0854],
        ...,
        [-0.1452],
        [-0.1447],
        [-0.1445]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-45133.1992, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [1.0000],
        ...,
        [0.9990],
        [0.9987],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365879.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.6747, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [1.0000],
        ...,
        [0.9990],
        [0.9987],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365887.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.6747, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -7.1625e-04,  6.5111e-04,  ..., -2.4961e-05,
          0.0000e+00, -3.1306e-04],
        [ 0.0000e+00, -7.1625e-04,  6.5111e-04,  ..., -2.4961e-05,
          0.0000e+00, -3.1306e-04],
        [ 0.0000e+00, -7.1625e-04,  6.5111e-04,  ..., -2.4961e-05,
          0.0000e+00, -3.1306e-04],
        ...,
        [ 0.0000e+00, -7.1625e-04,  6.5111e-04,  ..., -2.4961e-05,
          0.0000e+00, -3.1306e-04],
        [ 0.0000e+00, -7.1625e-04,  6.5111e-04,  ..., -2.4961e-05,
          0.0000e+00, -3.1306e-04],
        [ 0.0000e+00, -7.1625e-04,  6.5111e-04,  ..., -2.4961e-05,
          0.0000e+00, -3.1306e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-370.9624, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.3795, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.1782, device='cuda:0')



h[100].sum tensor(-21.7366, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.7547, device='cuda:0')



h[200].sum tensor(-22.7875, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.5593, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0011, 0.0082,  ..., 0.0129, 0.0000, 0.0019],
        ...,
        [0.0000, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0011, 0.0066,  ..., 0.0093, 0.0000, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38427.7578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0140, 0.0005, 0.0019,  ..., 0.0104, 0.0000, 0.0000],
        [0.0415, 0.0001, 0.0120,  ..., 0.0256, 0.0000, 0.0000],
        [0.1233, 0.0000, 0.0443,  ..., 0.0717, 0.0000, 0.0000],
        ...,
        [0.0102, 0.0008, 0.0010,  ..., 0.0084, 0.0000, 0.0000],
        [0.0321, 0.0005, 0.0093,  ..., 0.0206, 0.0000, 0.0000],
        [0.0790, 0.0000, 0.0268,  ..., 0.0467, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(254224.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6880.1401, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(64.9956, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-290.0863, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1597.6907, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(243.2833, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0068],
        [-0.0421],
        [-0.1078],
        ...,
        [-0.0663],
        [-0.0140],
        [ 0.0190]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-38444.7656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [1.0000],
        ...,
        [0.9990],
        [0.9987],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365887.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(189.6604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [1.0000],
        ...,
        [0.9989],
        [0.9987],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365894.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(189.6604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -7.1678e-04,  6.5301e-04,  ...,  1.8678e-05,
          0.0000e+00, -3.2116e-04],
        [ 0.0000e+00, -7.1678e-04,  6.5301e-04,  ...,  1.8678e-05,
          0.0000e+00, -3.2116e-04],
        [ 0.0000e+00, -7.1678e-04,  6.5301e-04,  ...,  1.8678e-05,
          0.0000e+00, -3.2116e-04],
        ...,
        [ 0.0000e+00, -7.1678e-04,  6.5301e-04,  ...,  1.8678e-05,
          0.0000e+00, -3.2116e-04],
        [ 0.0000e+00, -7.1678e-04,  6.5301e-04,  ...,  1.8678e-05,
          0.0000e+00, -3.2116e-04],
        [ 0.0000e+00, -7.1678e-04,  6.5301e-04,  ...,  1.8678e-05,
          0.0000e+00, -3.2116e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-370.7468, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.3548, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.0941, device='cuda:0')



h[100].sum tensor(-14.8201, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.9165, device='cuda:0')



h[200].sum tensor(-25.4729, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-20.7980, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 2.6102e-03,  ..., 7.4659e-05, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.6107e-03,  ..., 7.4672e-05, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.6114e-03,  ..., 7.4693e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 2.6151e-03,  ..., 7.4798e-05, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.6149e-03,  ..., 7.4793e-05, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.6148e-03,  ..., 7.4791e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29916.8516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0096, 0.0010, 0.0012,  ..., 0.0081, 0.0000, 0.0000],
        [0.0078, 0.0012, 0.0005,  ..., 0.0071, 0.0000, 0.0000],
        [0.0078, 0.0012, 0.0005,  ..., 0.0071, 0.0000, 0.0000],
        ...,
        [0.0077, 0.0012, 0.0006,  ..., 0.0070, 0.0000, 0.0000],
        [0.0077, 0.0012, 0.0006,  ..., 0.0070, 0.0000, 0.0000],
        [0.0077, 0.0012, 0.0006,  ..., 0.0070, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(221097.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5283.9951, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(37.7348, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-243.3903, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(942.8903, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(166.9730, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0370],
        [-0.1029],
        [-0.1600],
        ...,
        [-0.1625],
        [-0.1617],
        [-0.1615]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-58485.1055, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [1.0000],
        ...,
        [0.9989],
        [0.9987],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365894.5312, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 30.0 event: 150 loss: tensor(492.9530, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(238.5350, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [1.0000],
        ...,
        [0.9989],
        [0.9986],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365901.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(238.5350, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -7.1226e-04,  6.5634e-04,  ...,  4.0836e-05,
          0.0000e+00, -3.2865e-04],
        [ 0.0000e+00, -7.1226e-04,  6.5634e-04,  ...,  4.0836e-05,
          0.0000e+00, -3.2865e-04],
        [ 0.0000e+00, -7.1226e-04,  6.5634e-04,  ...,  4.0836e-05,
          0.0000e+00, -3.2865e-04],
        ...,
        [ 0.0000e+00, -7.1226e-04,  6.5634e-04,  ...,  4.0836e-05,
          0.0000e+00, -3.2865e-04],
        [ 0.0000e+00, -7.1226e-04,  6.5634e-04,  ...,  4.0836e-05,
          0.0000e+00, -3.2865e-04],
        [ 0.0000e+00, -7.1226e-04,  6.5634e-04,  ...,  4.0836e-05,
          0.0000e+00, -3.2865e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-363.2375, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.1860, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.9837, device='cuda:0')



h[100].sum tensor(-18.5140, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.9258, device='cuda:0')



h[200].sum tensor(-23.8498, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.1576, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0026,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0026,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0026,  ..., 0.0002, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0026,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0026,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0026,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34817.0234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0092, 0.0013, 0.0013,  ..., 0.0076, 0.0000, 0.0000],
        [0.0080, 0.0014, 0.0012,  ..., 0.0071, 0.0000, 0.0000],
        [0.0075, 0.0015, 0.0011,  ..., 0.0069, 0.0000, 0.0000],
        ...,
        [0.0074, 0.0015, 0.0012,  ..., 0.0068, 0.0000, 0.0000],
        [0.0074, 0.0015, 0.0012,  ..., 0.0068, 0.0000, 0.0000],
        [0.0074, 0.0015, 0.0012,  ..., 0.0068, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(242983.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6152.8921, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(60.6926, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13.1982, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-274.3919, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1274.6384, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(221.3673, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0148],
        [-0.0223],
        [-0.0648],
        ...,
        [-0.1725],
        [-0.1715],
        [-0.1712]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-54137.6211, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [1.0000],
        ...,
        [0.9989],
        [0.9986],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365901.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(265.2435, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [1.0000],
        ...,
        [0.9989],
        [0.9986],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365909.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(265.2435, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -7.0798e-04,  6.6692e-04,  ...,  4.4103e-05,
          0.0000e+00, -3.3450e-04],
        [ 0.0000e+00, -7.0798e-04,  6.6692e-04,  ...,  4.4103e-05,
          0.0000e+00, -3.3450e-04],
        [-8.0572e-03,  8.7887e-04,  4.1494e-03,  ...,  8.1751e-03,
         -2.7809e-03,  1.2748e-03],
        ...,
        [ 0.0000e+00, -7.0798e-04,  6.6692e-04,  ...,  4.4103e-05,
          0.0000e+00, -3.3450e-04],
        [ 0.0000e+00, -7.0798e-04,  6.6692e-04,  ...,  4.4103e-05,
          0.0000e+00, -3.3450e-04],
        [ 0.0000e+00, -7.0798e-04,  6.6692e-04,  ...,  4.4103e-05,
          0.0000e+00, -3.3450e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-357.7149, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.5620, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.1093, device='cuda:0')



h[100].sum tensor(-21.0921, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.4774, device='cuda:0')



h[200].sum tensor(-22.6103, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-29.0864, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0027,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0014, 0.0088,  ..., 0.0146, 0.0000, 0.0022],
        [0.0000, 0.0049, 0.0181,  ..., 0.0362, 0.0000, 0.0061],
        ...,
        [0.0000, 0.0000, 0.0027,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0027,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0027,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38952.2383, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0463, 0.0004, 0.0153,  ..., 0.0281, 0.0000, 0.0000],
        [0.1095, 0.0000, 0.0400,  ..., 0.0636, 0.0000, 0.0000],
        [0.1982, 0.0000, 0.0772,  ..., 0.1143, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0020, 0.0018,  ..., 0.0067, 0.0000, 0.0000],
        [0.0071, 0.0020, 0.0018,  ..., 0.0067, 0.0000, 0.0000],
        [0.0071, 0.0020, 0.0018,  ..., 0.0067, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(268541.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6747.7178, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(82.0449, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(38.9012, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-300.8576, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1671.8984, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(269.3023, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0452],
        [ 0.0439],
        [ 0.0254],
        ...,
        [-0.1681],
        [-0.1812],
        [-0.1836]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-62912.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [1.0000],
        ...,
        [0.9989],
        [0.9986],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365909.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(188.6642, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [1.0000],
        ...,
        [0.9989],
        [0.9986],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365917., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(188.6642, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -7.0138e-04,  6.7038e-04,  ...,  6.0971e-05,
          0.0000e+00, -3.3901e-04],
        [ 0.0000e+00, -7.0138e-04,  6.7038e-04,  ...,  6.0971e-05,
          0.0000e+00, -3.3901e-04],
        [ 0.0000e+00, -7.0138e-04,  6.7038e-04,  ...,  6.0971e-05,
          0.0000e+00, -3.3901e-04],
        ...,
        [ 0.0000e+00, -7.0138e-04,  6.7038e-04,  ...,  6.0971e-05,
          0.0000e+00, -3.3901e-04],
        [ 0.0000e+00, -7.0138e-04,  6.7038e-04,  ...,  6.0971e-05,
          0.0000e+00, -3.3901e-04],
        [ 0.0000e+00, -7.0138e-04,  6.7038e-04,  ...,  6.0971e-05,
          0.0000e+00, -3.3901e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-362.3780, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.3127, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.0148, device='cuda:0')



h[100].sum tensor(-14.7750, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.8960, device='cuda:0')



h[200].sum tensor(-24.8830, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-20.6888, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0027,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0027,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0027,  ..., 0.0002, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0027,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0027,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0027,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30405.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0069, 0.0023, 0.0026,  ..., 0.0067, 0.0000, 0.0000],
        [0.0069, 0.0023, 0.0026,  ..., 0.0067, 0.0000, 0.0000],
        [0.0070, 0.0023, 0.0026,  ..., 0.0067, 0.0000, 0.0000],
        ...,
        [0.0069, 0.0022, 0.0027,  ..., 0.0066, 0.0000, 0.0000],
        [0.0069, 0.0022, 0.0027,  ..., 0.0066, 0.0000, 0.0000],
        [0.0069, 0.0022, 0.0027,  ..., 0.0066, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(230161.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5330.4697, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(53.0734, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(70.2549, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-252.5164, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(897.6706, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(188.8783, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2699],
        [-0.2843],
        [-0.2956],
        ...,
        [-0.1963],
        [-0.1954],
        [-0.1951]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-70380.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [1.0000],
        ...,
        [0.9989],
        [0.9986],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365917., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2788],
        [0.3132],
        [0.2900],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(335.2019, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [0.9999],
        ...,
        [0.9989],
        [0.9986],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365924.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2788],
        [0.3132],
        [0.2900],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(335.2019, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.9706e-02,  5.1675e-03,  1.3527e-02,  ...,  3.0125e-02,
         -1.0217e-02,  5.6007e-03],
        [-3.1703e-02,  5.5617e-03,  1.4391e-02,  ...,  3.2144e-02,
         -1.0903e-02,  6.0002e-03],
        [-2.5321e-02,  4.3018e-03,  1.1628e-02,  ...,  2.5691e-02,
         -8.7085e-03,  4.7234e-03],
        ...,
        [ 0.0000e+00, -6.9723e-04,  6.6570e-04,  ...,  8.9988e-05,
          0.0000e+00, -3.4296e-04],
        [ 0.0000e+00, -6.9723e-04,  6.6570e-04,  ...,  8.9988e-05,
          0.0000e+00, -3.4296e-04],
        [ 0.0000e+00, -6.9723e-04,  6.6570e-04,  ...,  8.9988e-05,
          0.0000e+00, -3.4296e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-337.8604, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-34.5767, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.6769, device='cuda:0')



h[100].sum tensor(-26.4449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.9220, device='cuda:0')



h[200].sum tensor(-19.9017, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-36.7580, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0216, 0.0562,  ..., 0.1254, 0.0000, 0.0234],
        [0.0000, 0.0224, 0.0578,  ..., 0.1292, 0.0000, 0.0241],
        [0.0000, 0.0248, 0.0631,  ..., 0.1416, 0.0000, 0.0266],
        ...,
        [0.0000, 0.0000, 0.0027,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0027,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0027,  ..., 0.0004, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47023.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.5077, 0.0000, 0.2217,  ..., 0.2961, 0.0000, 0.0000],
        [0.5520, 0.0000, 0.2424,  ..., 0.3222, 0.0000, 0.0000],
        [0.5800, 0.0000, 0.2557,  ..., 0.3388, 0.0000, 0.0000],
        ...,
        [0.0068, 0.0023, 0.0035,  ..., 0.0066, 0.0000, 0.0000],
        [0.0068, 0.0023, 0.0035,  ..., 0.0066, 0.0000, 0.0000],
        [0.0068, 0.0023, 0.0035,  ..., 0.0066, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(316872.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8112.8916, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(121.9859, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(112.9303, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-351.5735, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2461.3760, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(361.6271, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1810],
        [-0.2112],
        [-0.2176],
        ...,
        [-0.2063],
        [-0.2054],
        [-0.2051]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-75695.7578, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [0.9999],
        ...,
        [0.9989],
        [0.9986],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365924.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(194.3772, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [0.9999],
        ...,
        [0.9989],
        [0.9986],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365931.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(194.3772, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0078,  0.0009,  0.0041,  ...,  0.0080, -0.0027,  0.0012],
        [ 0.0000, -0.0007,  0.0007,  ...,  0.0001,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0007,  ...,  0.0001,  0.0000, -0.0003],
        ...,
        [ 0.0000, -0.0007,  0.0007,  ...,  0.0001,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0007,  ...,  0.0001,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0007,  ...,  0.0001,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-351.4822, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.6748, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.4694, device='cuda:0')



h[100].sum tensor(-15.0433, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.0139, device='cuda:0')



h[200].sum tensor(-24.2465, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.3153, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0006, 0.0054,  ..., 0.0069, 0.0000, 0.0009],
        [0.0000, 0.0009, 0.0060,  ..., 0.0084, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0026,  ..., 0.0004, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0026,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0026,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0026,  ..., 0.0004, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31186.7168, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0407, 0.0000, 0.0158,  ..., 0.0250, 0.0000, 0.0000],
        [0.0331, 0.0000, 0.0133,  ..., 0.0209, 0.0000, 0.0000],
        [0.0151, 0.0011, 0.0071,  ..., 0.0110, 0.0000, 0.0000],
        ...,
        [0.0067, 0.0022, 0.0042,  ..., 0.0065, 0.0000, 0.0000],
        [0.0067, 0.0022, 0.0042,  ..., 0.0065, 0.0000, 0.0000],
        [0.0067, 0.0022, 0.0042,  ..., 0.0065, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(239151.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5457.6772, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(64.7797, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(162.0485, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-260.8015, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(953.7565, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(207.1037, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1638],
        [-0.1754],
        [-0.1759],
        ...,
        [-0.2164],
        [-0.2155],
        [-0.2152]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-78814.8594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [0.9999],
        ...,
        [0.9989],
        [0.9986],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365931.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.8457, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [0.9999],
        ...,
        [0.9989],
        [0.9985],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365939.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.8457, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0143,  0.0021,  0.0068,  ...,  0.0147, -0.0049,  0.0025],
        [-0.0164,  0.0025,  0.0077,  ...,  0.0168, -0.0056,  0.0029],
        [-0.0459,  0.0084,  0.0205,  ...,  0.0466, -0.0157,  0.0089],
        ...,
        [ 0.0000, -0.0007,  0.0006,  ...,  0.0002,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0006,  ...,  0.0002,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0006,  ...,  0.0002,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-326.7383, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.8677, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.0673, device='cuda:0')



h[100].sum tensor(-22.8301, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.9854, device='cuda:0')



h[200].sum tensor(-20.9558, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-31.7843, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0193, 0.0509,  ..., 0.1139, 0.0000, 0.0211],
        [0.0000, 0.0223, 0.0574,  ..., 0.1290, 0.0000, 0.0241],
        [0.0000, 0.0125, 0.0359,  ..., 0.0788, 0.0000, 0.0141],
        ...,
        [0.0000, 0.0000, 0.0025,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0025,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0025,  ..., 0.0007, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42902.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.4679, 0.0000, 0.2041,  ..., 0.2705, 0.0000, 0.0000],
        [0.5005, 0.0000, 0.2193,  ..., 0.2896, 0.0000, 0.0000],
        [0.4179, 0.0000, 0.1812,  ..., 0.2414, 0.0000, 0.0000],
        ...,
        [0.0072, 0.0018, 0.0047,  ..., 0.0061, 0.0000, 0.0000],
        [0.0072, 0.0018, 0.0047,  ..., 0.0061, 0.0000, 0.0000],
        [0.0072, 0.0018, 0.0047,  ..., 0.0061, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(298176.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7540.4634, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(113.8575, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(245.0690, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-331.3919, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2035.2399, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(333.8735, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1528],
        [-0.1581],
        [-0.1387],
        ...,
        [-0.2229],
        [-0.2220],
        [-0.2217]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-77698.7734, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [0.9999],
        ...,
        [0.9989],
        [0.9985],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365939.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(282.1343, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9996],
        [0.9999],
        ...,
        [0.9989],
        [0.9985],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365947.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(282.1343, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0007,  0.0006,  ...,  0.0002,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0006,  ...,  0.0002,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0006,  ...,  0.0002,  0.0000, -0.0003],
        ...,
        [ 0.0000, -0.0007,  0.0006,  ...,  0.0002,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0006,  ...,  0.0002,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0006,  ...,  0.0002,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-314.0491, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.3716, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.4536, device='cuda:0')



h[100].sum tensor(-21.6802, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.8262, device='cuda:0')



h[200].sum tensor(-21.2852, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.9386, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0023,  ..., 0.0009, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0023,  ..., 0.0009, 0.0000, 0.0000],
        [0.0000, 0.0024, 0.0107,  ..., 0.0206, 0.0000, 0.0032],
        ...,
        [0.0000, 0.0000, 0.0023,  ..., 0.0009, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0023,  ..., 0.0009, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0023,  ..., 0.0009, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40564.7227, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0189, 0.0005, 0.0092,  ..., 0.0118, 0.0000, 0.0000],
        [0.0308, 0.0003, 0.0129,  ..., 0.0182, 0.0000, 0.0000],
        [0.0795, 0.0000, 0.0305,  ..., 0.0449, 0.0000, 0.0000],
        ...,
        [0.0077, 0.0012, 0.0049,  ..., 0.0057, 0.0000, 0.0000],
        [0.0077, 0.0012, 0.0049,  ..., 0.0057, 0.0000, 0.0000],
        [0.0077, 0.0012, 0.0049,  ..., 0.0057, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(284593.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7374.4258, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(108.7022, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(314.4515, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-317.5191, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1822.9807, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(320.2275, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0099],
        [ 0.0217],
        [ 0.0617],
        ...,
        [-0.2283],
        [-0.2273],
        [-0.2270]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-71060., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9996],
        [0.9999],
        ...,
        [0.9989],
        [0.9985],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365947.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3115],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.4121, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9996],
        [0.9999],
        ...,
        [0.9989],
        [0.9985],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365955.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3115],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.4121, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0080,  0.0009,  0.0040,  ...,  0.0085, -0.0027,  0.0013],
        [-0.0052,  0.0003,  0.0028,  ...,  0.0055, -0.0018,  0.0007],
        [-0.0132,  0.0019,  0.0063,  ...,  0.0137, -0.0045,  0.0023],
        ...,
        [ 0.0000, -0.0007,  0.0005,  ...,  0.0003,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0005,  ...,  0.0003,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0005,  ...,  0.0003,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-312.5167, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.5711, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.3926, device='cuda:0')



h[100].sum tensor(-14.9509, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.9940, device='cuda:0')



h[200].sum tensor(-23.8362, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.2094, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0018, 0.0090,  ..., 0.0173, 0.0000, 0.0025],
        [0.0000, 0.0059, 0.0211,  ..., 0.0457, 0.0000, 0.0075],
        [0.0000, 0.0031, 0.0136,  ..., 0.0280, 0.0000, 0.0043],
        ...,
        [0.0000, 0.0000, 0.0021,  ..., 0.0011, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0021,  ..., 0.0011, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0021,  ..., 0.0011, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32186.8691, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1029, 0.0000, 0.0398,  ..., 0.0568, 0.0000, 0.0000],
        [0.1486, 0.0000, 0.0576,  ..., 0.0821, 0.0000, 0.0000],
        [0.1354, 0.0000, 0.0520,  ..., 0.0746, 0.0000, 0.0000],
        ...,
        [0.0084, 0.0006, 0.0052,  ..., 0.0051, 0.0000, 0.0000],
        [0.0084, 0.0006, 0.0052,  ..., 0.0051, 0.0000, 0.0000],
        [0.0084, 0.0006, 0.0052,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(249416.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5975.5615, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(78.9356, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(476.0788, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-269.4886, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1230.3130, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(249.2046, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0645],
        [ 0.0656],
        [ 0.0658],
        ...,
        [-0.2156],
        [-0.2021],
        [-0.1913]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-88666.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9996],
        [0.9999],
        ...,
        [0.9989],
        [0.9985],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365955.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(377.8278, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9996],
        [0.9999],
        ...,
        [0.9989],
        [0.9985],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365964.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(377.8278, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0007,  0.0005,  ...,  0.0003,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0005,  ...,  0.0003,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0005,  ...,  0.0003,  0.0000, -0.0003],
        ...,
        [ 0.0000, -0.0007,  0.0005,  ...,  0.0003,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0005,  ...,  0.0003,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0005,  ...,  0.0003,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-257.9803, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-38.6001, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.0693, device='cuda:0')



h[100].sum tensor(-29.4791, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-7.8023, device='cuda:0')



h[200].sum tensor(-17.8038, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-41.4323, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0018,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0018,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0018,  ..., 0.0014, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0018,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0018,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0001, 0.0037,  ..., 0.0057, 0.0000, 0.0005]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48900.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[9.1651e-03, 2.7869e-05, 5.2508e-03,  ..., 4.4887e-03, 0.0000e+00,
         0.0000e+00],
        [9.1691e-03, 2.8086e-05, 5.2519e-03,  ..., 4.4901e-03, 0.0000e+00,
         0.0000e+00],
        [1.2056e-02, 3.6731e-05, 6.3517e-03,  ..., 6.0626e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.4126e-02, 7.3188e-06, 6.5598e-03,  ..., 7.0765e-03, 0.0000e+00,
         0.0000e+00],
        [2.1436e-02, 0.0000e+00, 8.4537e-03,  ..., 1.0867e-02, 0.0000e+00,
         0.0000e+00],
        [3.3382e-02, 0.0000e+00, 1.1549e-02,  ..., 1.7061e-02, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(318078., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8568.3301, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(148.0003, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(527.1084, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-372.4759, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2621.1582, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(426.5246, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2696],
        [-0.2084],
        [-0.1170],
        ...,
        [-0.1700],
        [-0.1159],
        [-0.0665]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-76476.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9996],
        [0.9999],
        ...,
        [0.9989],
        [0.9985],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365964.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.1949, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9996],
        [0.9999],
        ...,
        [0.9988],
        [0.9984],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365972.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.1949, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0007,  0.0004,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0004,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0004,  ...,  0.0004,  0.0000, -0.0003],
        ...,
        [ 0.0000, -0.0007,  0.0004,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0004,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0004,  ...,  0.0004,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-270.0782, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.7757, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.6279, device='cuda:0')



h[100].sum tensor(-19.6793, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.3525, device='cuda:0')



h[200].sum tensor(-21.6030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.4231, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0024, 0.0084,  ..., 0.0175, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0016,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0016,  ..., 0.0016, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0016,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0016,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0016,  ..., 0.0016, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37800.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0912, 0.0000, 0.0384,  ..., 0.0490, 0.0000, 0.0000],
        [0.0533, 0.0000, 0.0231,  ..., 0.0280, 0.0000, 0.0000],
        [0.0348, 0.0000, 0.0152,  ..., 0.0177, 0.0000, 0.0000],
        ...,
        [0.0098, 0.0000, 0.0058,  ..., 0.0041, 0.0000, 0.0000],
        [0.0098, 0.0000, 0.0058,  ..., 0.0041, 0.0000, 0.0000],
        [0.0098, 0.0000, 0.0058,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(274015.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7245.9053, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(106.4964, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(650.8255, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-308.2167, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1959.4583, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(325.9573, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0668],
        [ 0.0700],
        [ 0.0745],
        ...,
        [-0.2408],
        [-0.2398],
        [-0.2394]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-79939.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9996],
        [0.9999],
        ...,
        [0.9988],
        [0.9984],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365972.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 40.0 event: 200 loss: tensor(572.0004, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.3210, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9996],
        [0.9999],
        ...,
        [0.9988],
        [0.9984],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365979.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.3210, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0054,  0.0004,  0.0027,  ...,  0.0059, -0.0018,  0.0008],
        [-0.0054,  0.0004,  0.0027,  ...,  0.0059, -0.0018,  0.0008],
        [ 0.0000, -0.0007,  0.0004,  ...,  0.0004,  0.0000, -0.0003],
        ...,
        [ 0.0000, -0.0007,  0.0004,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0004,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0004,  ...,  0.0004,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-267.3951, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.2667, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.4096, device='cuda:0')



h[100].sum tensor(-17.7585, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.7768, device='cuda:0')



h[200].sum tensor(-22.2035, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.3665, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0005, 0.0057,  ..., 0.0117, 0.0000, 0.0013],
        [0.0000, 0.0005, 0.0057,  ..., 0.0117, 0.0000, 0.0013],
        [0.0000, 0.0005, 0.0057,  ..., 0.0117, 0.0000, 0.0013],
        ...,
        [0.0000, 0.0000, 0.0015,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0015,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0015,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35448.8203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0702, 0.0000, 0.0235,  ..., 0.0346, 0.0000, 0.0000],
        [0.0562, 0.0000, 0.0191,  ..., 0.0273, 0.0000, 0.0000],
        [0.0444, 0.0000, 0.0159,  ..., 0.0212, 0.0000, 0.0000],
        ...,
        [0.0104, 0.0000, 0.0065,  ..., 0.0037, 0.0000, 0.0000],
        [0.0104, 0.0000, 0.0065,  ..., 0.0037, 0.0000, 0.0000],
        [0.0104, 0.0000, 0.0065,  ..., 0.0037, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(263097.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7041.9019, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(96.8588, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(746.4429, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-300.0250, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1918.1194, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(311.0322, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0807],
        [ 0.0391],
        [-0.0415],
        ...,
        [-0.2441],
        [-0.2431],
        [-0.2427]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-74591.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9996],
        [0.9999],
        ...,
        [0.9988],
        [0.9984],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365979.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9526],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.7308, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9996],
        [0.9999],
        ...,
        [0.9988],
        [0.9984],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365988.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9526],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.7308, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0007,  0.0003,  ...,  0.0004,  0.0000, -0.0003],
        [-0.0245,  0.0042,  0.0110,  ...,  0.0254, -0.0083,  0.0046],
        [ 0.0000, -0.0007,  0.0003,  ...,  0.0004,  0.0000, -0.0003],
        ...,
        [ 0.0000, -0.0007,  0.0003,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0003,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0003,  ...,  0.0004,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-242.1451, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-32.3214, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.1273, device='cuda:0')



h[100].sum tensor(-24.6623, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.5199, device='cuda:0')



h[200].sum tensor(-19.1848, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-34.6228, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0062, 0.0180,  ..., 0.0409, 0.0000, 0.0071],
        [0.0000, 0.0081, 0.0222,  ..., 0.0506, 0.0000, 0.0091],
        [0.0000, 0.0176, 0.0461,  ..., 0.1068, 0.0000, 0.0196],
        ...,
        [0.0000, 0.0000, 0.0013,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44856.6055, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1613, 0.0000, 0.0695,  ..., 0.0865, 0.0000, 0.0000],
        [0.2113, 0.0000, 0.0908,  ..., 0.1144, 0.0000, 0.0000],
        [0.2565, 0.0000, 0.1109,  ..., 0.1397, 0.0000, 0.0000],
        ...,
        [0.0108, 0.0000, 0.0071,  ..., 0.0034, 0.0000, 0.0000],
        [0.0108, 0.0000, 0.0071,  ..., 0.0034, 0.0000, 0.0000],
        [0.0108, 0.0000, 0.0071,  ..., 0.0034, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(310065.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8738.4697, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(134.3530, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(884.3832, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-361.2787, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2836.0493, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(411.9384, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0274],
        [ 0.0229],
        [ 0.0201],
        ...,
        [-0.2486],
        [-0.2475],
        [-0.2472]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-74895.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9996],
        [0.9999],
        ...,
        [0.9988],
        [0.9984],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365988.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4209],
        [0.3872],
        [0.4094],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(270.7292, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9996],
        [0.9999],
        ...,
        [0.9988],
        [0.9984],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365996.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4209],
        [0.3872],
        [0.4094],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(270.7292, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0399,  0.0072,  0.0177,  ...,  0.0412, -0.0135,  0.0078],
        [-0.0378,  0.0068,  0.0168,  ...,  0.0390, -0.0128,  0.0073],
        [-0.0382,  0.0069,  0.0169,  ...,  0.0395, -0.0129,  0.0074],
        ...,
        [ 0.0000, -0.0007,  0.0003,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0003,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0003,  ...,  0.0004,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-246.6225, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.2754, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.5459, device='cuda:0')



h[100].sum tensor(-20.8059, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.5906, device='cuda:0')



h[200].sum tensor(-20.5273, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-29.6880, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0271, 0.0669,  ..., 0.1557, 0.0000, 0.0293],
        [0.0000, 0.0298, 0.0727,  ..., 0.1694, 0.0000, 0.0321],
        [0.0000, 0.0292, 0.0713,  ..., 0.1662, 0.0000, 0.0314],
        ...,
        [0.0000, 0.0000, 0.0012,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0018, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39669.6953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.6543, 0.0000, 0.2886,  ..., 0.3622, 0.0000, 0.0000],
        [0.6753, 0.0000, 0.2985,  ..., 0.3742, 0.0000, 0.0000],
        [0.6412, 0.0000, 0.2824,  ..., 0.3547, 0.0000, 0.0000],
        ...,
        [0.0112, 0.0000, 0.0077,  ..., 0.0031, 0.0000, 0.0000],
        [0.0112, 0.0000, 0.0077,  ..., 0.0031, 0.0000, 0.0000],
        [0.0112, 0.0000, 0.0077,  ..., 0.0031, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(287907.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8134.7949, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(114.4737, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1025.3812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-336.2646, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2511.7588, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(364.2716, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1498],
        [-0.1490],
        [-0.1354],
        ...,
        [-0.2538],
        [-0.2527],
        [-0.2524]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-73439., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9996],
        [0.9999],
        ...,
        [0.9988],
        [0.9984],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365996.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(311.7811, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9996],
        [1.0000],
        ...,
        [0.9988],
        [0.9984],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366005.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(311.7811, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0007,  0.0003,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0003,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0003,  ...,  0.0004,  0.0000, -0.0003],
        ...,
        [ 0.0000, -0.0007,  0.0003,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0003,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0003,  ...,  0.0004,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-228.0030, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-31.0034, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.8130, device='cuda:0')



h[100].sum tensor(-23.6427, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.4384, device='cuda:0')



h[200].sum tensor(-19.0910, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-34.1897, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0010,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0010,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0010,  ..., 0.0018, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0010,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0010,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0010,  ..., 0.0018, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43095.5195, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0338, 0.0000, 0.0155,  ..., 0.0144, 0.0000, 0.0000],
        [0.0163, 0.0000, 0.0091,  ..., 0.0051, 0.0000, 0.0000],
        [0.0138, 0.0000, 0.0083,  ..., 0.0038, 0.0000, 0.0000],
        ...,
        [0.0114, 0.0000, 0.0081,  ..., 0.0028, 0.0000, 0.0000],
        [0.0114, 0.0000, 0.0081,  ..., 0.0028, 0.0000, 0.0000],
        [0.0114, 0.0000, 0.0081,  ..., 0.0028, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(307706.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8514.2812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(128.9325, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1220.4935, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-360.0100, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2973.0684, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(405.2158, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0335],
        [-0.0215],
        [-0.1039],
        ...,
        [-0.2585],
        [-0.2574],
        [-0.2571]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-86709.3516, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9996],
        [1.0000],
        ...,
        [0.9988],
        [0.9984],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366005.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.8812, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9996],
        [1.0000],
        ...,
        [0.9988],
        [0.9984],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366005.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.8812, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0007,  0.0003,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0003,  ...,  0.0004,  0.0000, -0.0003],
        [-0.0107,  0.0014,  0.0049,  ...,  0.0114, -0.0036,  0.0019],
        ...,
        [ 0.0000, -0.0007,  0.0003,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0003,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0003,  ...,  0.0004,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-251.4312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.1673, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.7829, device='cuda:0')



h[100].sum tensor(-16.1419, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.3548, device='cuda:0')



h[200].sum tensor(-22.1432, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.1251, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0030, 0.0091,  ..., 0.0208, 0.0000, 0.0035],
        [0.0000, 0.0014, 0.0057,  ..., 0.0127, 0.0000, 0.0019],
        [0.0000, 0.0049, 0.0147,  ..., 0.0339, 0.0000, 0.0058],
        ...,
        [0.0000, 0.0000, 0.0010,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0010,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0010,  ..., 0.0018, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34007.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1332, 0.0000, 0.0568,  ..., 0.0690, 0.0000, 0.0000],
        [0.1260, 0.0000, 0.0535,  ..., 0.0649, 0.0000, 0.0000],
        [0.1706, 0.0000, 0.0728,  ..., 0.0897, 0.0000, 0.0000],
        ...,
        [0.0114, 0.0000, 0.0081,  ..., 0.0028, 0.0000, 0.0000],
        [0.0114, 0.0000, 0.0081,  ..., 0.0028, 0.0000, 0.0000],
        [0.0114, 0.0000, 0.0081,  ..., 0.0028, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(269004.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7194.4082, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(93.2136, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1240.8945, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-306.7989, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2264.2578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(313.6589, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0429],
        [ 0.0424],
        [ 0.0380],
        ...,
        [-0.2585],
        [-0.2574],
        [-0.2571]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-87797.5234, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9996],
        [1.0000],
        ...,
        [0.9988],
        [0.9984],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366005.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2500],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(331.7354, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9996],
        [1.0000],
        ...,
        [0.9988],
        [0.9984],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366013.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2500],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(331.7354, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0132,  0.0019,  0.0060,  ...,  0.0140, -0.0045,  0.0024],
        [-0.0064,  0.0006,  0.0030,  ...,  0.0070, -0.0022,  0.0010],
        [ 0.0000, -0.0007,  0.0002,  ...,  0.0004,  0.0000, -0.0003],
        ...,
        [ 0.0000, -0.0007,  0.0002,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0002,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0002,  ...,  0.0004,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-212.1545, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-33.5882, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.4010, device='cuda:0')



h[100].sum tensor(-25.6064, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.8504, device='cuda:0')



h[200].sum tensor(-18.0028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-36.3778, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0085, 0.0256,  ..., 0.0596, 0.0000, 0.0103],
        [0.0000, 0.0054, 0.0157,  ..., 0.0364, 0.0000, 0.0063],
        [0.0000, 0.0006, 0.0037,  ..., 0.0083, 0.0000, 0.0010],
        ...,
        [0.0000, 0.0000, 0.0009,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0009,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0009,  ..., 0.0018, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45322.6797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2488, 0.0000, 0.1036,  ..., 0.1313, 0.0000, 0.0000],
        [0.1870, 0.0000, 0.0784,  ..., 0.0976, 0.0000, 0.0000],
        [0.1222, 0.0000, 0.0509,  ..., 0.0620, 0.0000, 0.0000],
        ...,
        [0.0115, 0.0000, 0.0084,  ..., 0.0025, 0.0000, 0.0000],
        [0.0115, 0.0000, 0.0084,  ..., 0.0025, 0.0000, 0.0000],
        [0.0115, 0.0000, 0.0084,  ..., 0.0025, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(315399.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8990.4570, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(141.8318, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1258.2006, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-376.7997, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3204.5664, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(430.4111, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0610],
        [-0.0273],
        [ 0.0034],
        ...,
        [-0.2649],
        [-0.2637],
        [-0.2634]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-77713.6641, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9996],
        [1.0000],
        ...,
        [0.9988],
        [0.9984],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366013.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(214.0159, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9996],
        [1.0000],
        ...,
        [0.9988],
        [0.9984],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366022.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(214.0159, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0007,  0.0002,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0002,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0002,  ...,  0.0004,  0.0000, -0.0003],
        ...,
        [ 0.0000, -0.0007,  0.0002,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0002,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0002,  ...,  0.0004,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-233.5816, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.5144, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.0324, device='cuda:0')



h[100].sum tensor(-16.3969, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.4195, device='cuda:0')



h[200].sum tensor(-21.4662, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.4688, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0008,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0017, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0008,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33557.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0949, 0.0000, 0.0433,  ..., 0.0478, 0.0000, 0.0000],
        [0.0344, 0.0000, 0.0176,  ..., 0.0146, 0.0000, 0.0000],
        [0.0168, 0.0000, 0.0100,  ..., 0.0048, 0.0000, 0.0000],
        ...,
        [0.0115, 0.0000, 0.0086,  ..., 0.0023, 0.0000, 0.0000],
        [0.0115, 0.0000, 0.0086,  ..., 0.0023, 0.0000, 0.0000],
        [0.0115, 0.0000, 0.0086,  ..., 0.0023, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(266555.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7010.0591, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(97.5372, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1482.5746, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-310.8979, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2349.3708, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(314.6556, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0426],
        [ 0.0528],
        [ 0.0629],
        ...,
        [-0.2713],
        [-0.2703],
        [-0.2701]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-91382.0078, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9996],
        [1.0000],
        ...,
        [0.9988],
        [0.9984],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366022.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.3290, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [1.0000],
        ...,
        [0.9988],
        [0.9983],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366032.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.3290, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0007,  0.0002,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0002,  ...,  0.0004,  0.0000, -0.0003],
        [-0.0075,  0.0008,  0.0035,  ...,  0.0081, -0.0025,  0.0012],
        ...,
        [ 0.0000, -0.0007,  0.0002,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0002,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0002,  ...,  0.0004,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-213.5499, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.3064, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.2407, device='cuda:0')



h[100].sum tensor(-19.2813, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.2520, device='cuda:0')



h[200].sum tensor(-20.0378, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.8895, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0022, 0.0071,  ..., 0.0164, 0.0000, 0.0027],
        [0.0000, 0.0008, 0.0040,  ..., 0.0093, 0.0000, 0.0012],
        [0.0000, 0.0019, 0.0080,  ..., 0.0185, 0.0000, 0.0028],
        ...,
        [0.0000, 0.0000, 0.0008,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0016, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36976.0508, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0783, 0.0000, 0.0335,  ..., 0.0376, 0.0000, 0.0000],
        [0.0815, 0.0000, 0.0334,  ..., 0.0390, 0.0000, 0.0000],
        [0.1137, 0.0000, 0.0450,  ..., 0.0560, 0.0000, 0.0000],
        ...,
        [0.0115, 0.0000, 0.0086,  ..., 0.0020, 0.0000, 0.0000],
        [0.0115, 0.0000, 0.0086,  ..., 0.0020, 0.0000, 0.0000],
        [0.0115, 0.0000, 0.0086,  ..., 0.0020, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(283667.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7542.7866, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(113.0789, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1598.9209, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-332.1983, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2738.0696, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(349.3443, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0770],
        [ 0.0810],
        [ 0.0850],
        ...,
        [-0.2807],
        [-0.2795],
        [-0.2792]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-95633.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [1.0000],
        ...,
        [0.9988],
        [0.9983],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366032.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.3989, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [1.0000],
        ...,
        [0.9988],
        [0.9984],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366042.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.3989, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0007,  0.0002,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0002,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0002,  ...,  0.0004,  0.0000, -0.0003],
        ...,
        [ 0.0000, -0.0007,  0.0002,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0002,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0002,  ...,  0.0004,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-208.6929, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.6058, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.0179, device='cuda:0')



h[100].sum tensor(-17.2186, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.6752, device='cuda:0')



h[200].sum tensor(-20.6263, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.8267, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0007,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0007,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0016, 0.0057,  ..., 0.0131, 0.0000, 0.0020],
        ...,
        [0.0000, 0.0000, 0.0007,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0007,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0007,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34603.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0183, 0.0000, 0.0104,  ..., 0.0053, 0.0000, 0.0000],
        [0.0281, 0.0000, 0.0146,  ..., 0.0106, 0.0000, 0.0000],
        [0.0633, 0.0000, 0.0285,  ..., 0.0295, 0.0000, 0.0000],
        ...,
        [0.0115, 0.0000, 0.0085,  ..., 0.0018, 0.0000, 0.0000],
        [0.0115, 0.0000, 0.0085,  ..., 0.0018, 0.0000, 0.0000],
        [0.0115, 0.0000, 0.0085,  ..., 0.0018, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(276932.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7294.1689, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(106.1840, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1692.8967, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-320.3819, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2673.6392, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(324.4178, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0257],
        [-0.0613],
        [-0.0596],
        ...,
        [-0.2900],
        [-0.2888],
        [-0.2884]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-93539.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9996],
        [1.0000],
        ...,
        [0.9988],
        [0.9984],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366042.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5879],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.6108, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9997],
        [1.0001],
        ...,
        [0.9988],
        [0.9983],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366052.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5879],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.6108, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0007,  0.0002,  ...,  0.0004,  0.0000, -0.0003],
        [-0.0150,  0.0023,  0.0068,  ...,  0.0158, -0.0050,  0.0028],
        [ 0.0000, -0.0007,  0.0002,  ...,  0.0004,  0.0000, -0.0003],
        ...,
        [ 0.0000, -0.0007,  0.0002,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0002,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0002,  ...,  0.0004,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-196.3689, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.0167, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.6368, device='cuda:0')



h[100].sum tensor(-16.7649, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.5763, device='cuda:0')



h[200].sum tensor(-20.5377, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.3017, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0029, 0.0101,  ..., 0.0234, 0.0000, 0.0038],
        [0.0000, 0.0018, 0.0061,  ..., 0.0140, 0.0000, 0.0022],
        [0.0000, 0.0114, 0.0315,  ..., 0.0738, 0.0000, 0.0132],
        ...,
        [0.0000, 0.0000, 0.0007,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0007,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0007,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35492.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1093, 0.0000, 0.0446,  ..., 0.0537, 0.0000, 0.0000],
        [0.1101, 0.0000, 0.0466,  ..., 0.0545, 0.0000, 0.0000],
        [0.1868, 0.0000, 0.0791,  ..., 0.0964, 0.0000, 0.0000],
        ...,
        [0.0115, 0.0000, 0.0083,  ..., 0.0017, 0.0000, 0.0000],
        [0.0115, 0.0000, 0.0083,  ..., 0.0017, 0.0000, 0.0000],
        [0.0115, 0.0000, 0.0083,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(288684.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7665.1475, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(111.0393, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1775.4839, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-326.1258, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2958.0996, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(330.5958, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0827],
        [ 0.0738],
        [ 0.0629],
        ...,
        [-0.2995],
        [-0.2983],
        [-0.2979]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-90993.3359, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9997],
        [1.0001],
        ...,
        [0.9988],
        [0.9983],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366052.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(527.6202, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9997],
        [1.0001],
        ...,
        [0.9988],
        [0.9983],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366052.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(527.6202, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0007,  0.0002,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0002,  ...,  0.0004,  0.0000, -0.0003],
        [-0.0092,  0.0012,  0.0042,  ...,  0.0098, -0.0031,  0.0016],
        ...,
        [ 0.0000, -0.0007,  0.0002,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0002,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0007,  0.0002,  ...,  0.0004,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-103.1854, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-53.5044, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(41.9905, device='cuda:0')



h[100].sum tensor(-40.7417, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.8955, device='cuda:0')



h[200].sum tensor(-10.7270, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-57.8584, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0007,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0015, 0.0069,  ..., 0.0161, 0.0000, 0.0023],
        [0.0000, 0.0046, 0.0152,  ..., 0.0355, 0.0000, 0.0059],
        ...,
        [0.0000, 0.0000, 0.0007,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0007,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0007,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66494.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0470, 0.0000, 0.0203,  ..., 0.0203, 0.0000, 0.0000],
        [0.1029, 0.0000, 0.0413,  ..., 0.0502, 0.0000, 0.0000],
        [0.1774, 0.0000, 0.0711,  ..., 0.0903, 0.0000, 0.0000],
        ...,
        [0.0115, 0.0000, 0.0083,  ..., 0.0017, 0.0000, 0.0000],
        [0.0115, 0.0000, 0.0083,  ..., 0.0017, 0.0000, 0.0000],
        [0.0115, 0.0000, 0.0083,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(429507.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12744.2344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(234.7640, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1577.6886, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-508.7402, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5486.3896, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(641.6790, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0714],
        [ 0.0734],
        [ 0.0717],
        ...,
        [-0.2996],
        [-0.2983],
        [-0.2979]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-76127.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9997],
        [1.0001],
        ...,
        [0.9988],
        [0.9983],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366052.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.2900, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9997],
        [1.0001],
        ...,
        [0.9988],
        [0.9983],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366062.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.2900, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0006,  0.0002,  ...,  0.0003,  0.0000, -0.0003],
        [ 0.0000, -0.0006,  0.0002,  ...,  0.0003,  0.0000, -0.0003],
        [ 0.0000, -0.0006,  0.0002,  ...,  0.0003,  0.0000, -0.0003],
        ...,
        [ 0.0000, -0.0006,  0.0002,  ...,  0.0003,  0.0000, -0.0003],
        [ 0.0000, -0.0006,  0.0002,  ...,  0.0003,  0.0000, -0.0003],
        [ 0.0000, -0.0006,  0.0002,  ...,  0.0003,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-158.2710, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.8273, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.9885, device='cuda:0')



h[100].sum tensor(-21.1832, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.7055, device='cuda:0')



h[200].sum tensor(-18.3926, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.2978, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0006,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0006,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0006,  ..., 0.0014, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0006,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0006,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0006,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43239.5703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0279, 0.0000, 0.0132,  ..., 0.0101, 0.0000, 0.0000],
        [0.0236, 0.0000, 0.0119,  ..., 0.0078, 0.0000, 0.0000],
        [0.0155, 0.0000, 0.0095,  ..., 0.0036, 0.0000, 0.0000],
        ...,
        [0.0117, 0.0000, 0.0085,  ..., 0.0017, 0.0000, 0.0000],
        [0.0117, 0.0000, 0.0085,  ..., 0.0017, 0.0000, 0.0000],
        [0.0117, 0.0000, 0.0085,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(335935.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9055.6113, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(139.8591, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1917.3252, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-371.6057, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3855.0603, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(408.4226, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0297],
        [ 0.0004],
        [-0.0475],
        ...,
        [-0.3067],
        [-0.3054],
        [-0.3050]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-107526.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9997],
        [1.0001],
        ...,
        [0.9988],
        [0.9983],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366062.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3650],
        [0.3645],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(257.5333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9997],
        [1.0001],
        ...,
        [0.9988],
        [0.9983],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366072.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3650],
        [0.3645],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(257.5333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0185,  0.0031,  0.0083,  ...,  0.0194, -0.0062,  0.0035],
        [-0.0093,  0.0012,  0.0042,  ...,  0.0099, -0.0031,  0.0016],
        [-0.0093,  0.0012,  0.0042,  ...,  0.0099, -0.0031,  0.0016],
        ...,
        [ 0.0000, -0.0006,  0.0001,  ...,  0.0003,  0.0000, -0.0003],
        [ 0.0000, -0.0006,  0.0001,  ...,  0.0003,  0.0000, -0.0003],
        [ 0.0000, -0.0006,  0.0001,  ...,  0.0003,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-145.7589, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.6611, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.4957, device='cuda:0')



h[100].sum tensor(-19.5284, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.3181, device='cuda:0')



h[200].sum tensor(-18.6955, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.2409, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0072, 0.0220,  ..., 0.0517, 0.0000, 0.0089],
        [0.0000, 0.0075, 0.0228,  ..., 0.0534, 0.0000, 0.0092],
        [0.0000, 0.0021, 0.0080,  ..., 0.0187, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0005,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0005,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0005,  ..., 0.0013, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39522.7422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.7524e-01, 0.0000e+00, 7.1677e-02,  ..., 8.9241e-02, 0.0000e+00,
         0.0000e+00],
        [1.5861e-01, 0.0000e+00, 6.5011e-02,  ..., 8.0315e-02, 0.0000e+00,
         0.0000e+00],
        [1.0255e-01, 0.0000e+00, 4.1783e-02,  ..., 4.9924e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.1798e-02, 3.9291e-05, 8.6768e-03,  ..., 1.6723e-03, 0.0000e+00,
         0.0000e+00],
        [1.1795e-02, 3.9176e-05, 8.6756e-03,  ..., 1.6719e-03, 0.0000e+00,
         0.0000e+00],
        [1.1794e-02, 3.9060e-05, 8.6750e-03,  ..., 1.6717e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(309541., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8410.2803, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(126.9344, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1952.3477, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-352.2787, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3468.2764, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(369.1213, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0883],
        [ 0.0895],
        [ 0.0888],
        ...,
        [-0.3125],
        [-0.3115],
        [-0.3116]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-91365.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9997],
        [1.0001],
        ...,
        [0.9988],
        [0.9983],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366072.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.0093, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9997],
        [1.0002],
        ...,
        [0.9988],
        [0.9983],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366082.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.0093, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0190,  0.0032,  0.0085,  ...,  0.0199, -0.0063,  0.0036],
        [-0.0132,  0.0020,  0.0059,  ...,  0.0139, -0.0044,  0.0024],
        [-0.0186,  0.0031,  0.0083,  ...,  0.0195, -0.0062,  0.0035],
        ...,
        [ 0.0000, -0.0006,  0.0001,  ...,  0.0003,  0.0000, -0.0003],
        [ 0.0000, -0.0006,  0.0001,  ...,  0.0003,  0.0000, -0.0003],
        [ 0.0000, -0.0006,  0.0001,  ...,  0.0003,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-125.8747, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.8074, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.9765, device='cuda:0')



h[100].sum tensor(-18.8731, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.1834, device='cuda:0')



h[200].sum tensor(-18.5477, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.5255, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0069, 0.0212,  ..., 0.0499, 0.0000, 0.0086],
        [0.0000, 0.0081, 0.0237,  ..., 0.0558, 0.0000, 0.0097],
        [0.0000, 0.0047, 0.0150,  ..., 0.0353, 0.0000, 0.0059],
        ...,
        [0.0000, 0.0000, 0.0005,  ..., 0.0012, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0005,  ..., 0.0012, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0005,  ..., 0.0012, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38104.4883, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.3443e-01, 0.0000e+00, 9.4329e-02,  ..., 1.2104e-01, 0.0000e+00,
         0.0000e+00],
        [2.2832e-01, 0.0000e+00, 9.2611e-02,  ..., 1.1789e-01, 0.0000e+00,
         0.0000e+00],
        [1.9201e-01, 0.0000e+00, 7.7637e-02,  ..., 9.8118e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.2002e-02, 1.3109e-04, 8.8170e-03,  ..., 1.6658e-03, 0.0000e+00,
         0.0000e+00],
        [1.1999e-02, 1.3097e-04, 8.8157e-03,  ..., 1.6655e-03, 0.0000e+00,
         0.0000e+00],
        [1.1997e-02, 1.3084e-04, 8.8151e-03,  ..., 1.6653e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(302188.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8129.7974, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(121.1823, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2069.0752, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-344.9289, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3390.7129, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(354.1006, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0732],
        [ 0.0774],
        [ 0.0807],
        ...,
        [-0.3206],
        [-0.3193],
        [-0.3189]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-96940.2734, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9997],
        [1.0002],
        ...,
        [0.9988],
        [0.9983],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366082.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.4366, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9997],
        [1.0002],
        ...,
        [0.9988],
        [0.9983],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366082.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.4366, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0006,  0.0001,  ...,  0.0003,  0.0000, -0.0003],
        [ 0.0000, -0.0006,  0.0001,  ...,  0.0003,  0.0000, -0.0003],
        [ 0.0000, -0.0006,  0.0001,  ...,  0.0003,  0.0000, -0.0003],
        ...,
        [ 0.0000, -0.0006,  0.0001,  ...,  0.0003,  0.0000, -0.0003],
        [ 0.0000, -0.0006,  0.0001,  ...,  0.0003,  0.0000, -0.0003],
        [ 0.0000, -0.0006,  0.0001,  ...,  0.0003,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-136.3155, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.6510, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.1455, device='cuda:0')



h[100].sum tensor(-16.4718, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.4488, device='cuda:0')



h[200].sum tensor(-19.5341, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.6246, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0005,  ..., 0.0012, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0005,  ..., 0.0012, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0005,  ..., 0.0012, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0005,  ..., 0.0012, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0005,  ..., 0.0012, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0005,  ..., 0.0012, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35042.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0120, 0.0001, 0.0088,  ..., 0.0017, 0.0000, 0.0000],
        [0.0120, 0.0001, 0.0088,  ..., 0.0017, 0.0000, 0.0000],
        [0.0121, 0.0002, 0.0088,  ..., 0.0017, 0.0000, 0.0000],
        ...,
        [0.0120, 0.0001, 0.0088,  ..., 0.0017, 0.0000, 0.0000],
        [0.0120, 0.0001, 0.0088,  ..., 0.0017, 0.0000, 0.0000],
        [0.0120, 0.0001, 0.0088,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(291557.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7542.6758, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(107.7710, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2161.4260, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-326.3296, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3182.8691, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(324.1310, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3530],
        [-0.3295],
        [-0.2802],
        ...,
        [-0.3205],
        [-0.3192],
        [-0.3187]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-108232.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9997],
        [1.0002],
        ...,
        [0.9988],
        [0.9983],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366082.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(264.7843, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9997],
        [1.0002],
        ...,
        [0.9988],
        [0.9983],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366092.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(264.7843, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -6.2118e-04,  8.8655e-05,  ...,  3.0509e-04,
          0.0000e+00, -2.8178e-04],
        [ 0.0000e+00, -6.2118e-04,  8.8655e-05,  ...,  3.0509e-04,
          0.0000e+00, -2.8178e-04],
        [ 0.0000e+00, -6.2118e-04,  8.8655e-05,  ...,  3.0509e-04,
          0.0000e+00, -2.8178e-04],
        ...,
        [ 0.0000e+00, -6.2118e-04,  8.8655e-05,  ...,  3.0509e-04,
          0.0000e+00, -2.8178e-04],
        [ 0.0000e+00, -6.2118e-04,  8.8655e-05,  ...,  3.0509e-04,
          0.0000e+00, -2.8178e-04],
        [ 0.0000e+00, -6.2118e-04,  8.8655e-05,  ...,  3.0509e-04,
          0.0000e+00, -2.8178e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-92.6792, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.2347, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.0728, device='cuda:0')



h[100].sum tensor(-19.9531, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.4679, device='cuda:0')



h[200].sum tensor(-17.7187, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-29.0360, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0004,  ..., 0.0012, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0012, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0012, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0004,  ..., 0.0012, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0012, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0012, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37966.9180, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0123, 0.0002, 0.0089,  ..., 0.0017, 0.0000, 0.0000],
        [0.0123, 0.0002, 0.0089,  ..., 0.0017, 0.0000, 0.0000],
        [0.0252, 0.0001, 0.0140,  ..., 0.0086, 0.0000, 0.0000],
        ...,
        [0.0123, 0.0002, 0.0089,  ..., 0.0017, 0.0000, 0.0000],
        [0.0123, 0.0002, 0.0089,  ..., 0.0017, 0.0000, 0.0000],
        [0.0123, 0.0002, 0.0089,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(299276.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7757.0308, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(118.1081, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2275.9844, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-343.6773, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3390.1453, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(353.1176, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0254],
        [-0.0641],
        [-0.0649],
        ...,
        [-0.3256],
        [-0.3242],
        [-0.3238]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-111635.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9997],
        [1.0002],
        ...,
        [0.9988],
        [0.9983],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366092.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(358.1553, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9997],
        [1.0002],
        ...,
        [0.9988],
        [0.9983],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366102.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(358.1553, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -6.1431e-04,  6.6821e-05,  ...,  3.1359e-04,
          0.0000e+00, -2.8090e-04],
        [ 0.0000e+00, -6.1431e-04,  6.6821e-05,  ...,  3.1359e-04,
          0.0000e+00, -2.8090e-04],
        [ 0.0000e+00, -6.1431e-04,  6.6821e-05,  ...,  3.1359e-04,
          0.0000e+00, -2.8090e-04],
        ...,
        [ 0.0000e+00, -6.1431e-04,  6.6821e-05,  ...,  3.1359e-04,
          0.0000e+00, -2.8090e-04],
        [ 0.0000e+00, -6.1431e-04,  6.6821e-05,  ...,  3.1359e-04,
          0.0000e+00, -2.8090e-04],
        [ 0.0000e+00, -6.1431e-04,  6.6821e-05,  ...,  3.1359e-04,
          0.0000e+00, -2.8090e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-25.3750, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-35.7706, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.5037, device='cuda:0')



h[100].sum tensor(-27.1977, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-7.3960, device='cuda:0')



h[200].sum tensor(-14.4047, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-39.2750, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0003,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0013, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0003,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0013, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49141.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0437, 0.0000, 0.0188,  ..., 0.0178, 0.0000, 0.0000],
        [0.0333, 0.0000, 0.0152,  ..., 0.0123, 0.0000, 0.0000],
        [0.0262, 0.0000, 0.0132,  ..., 0.0086, 0.0000, 0.0000],
        ...,
        [0.0128, 0.0002, 0.0090,  ..., 0.0016, 0.0000, 0.0000],
        [0.0128, 0.0002, 0.0090,  ..., 0.0016, 0.0000, 0.0000],
        [0.0128, 0.0002, 0.0090,  ..., 0.0016, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(351719.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10081.8242, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(163.6261, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2074.4802, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-409.9643, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4475.6494, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(461.0577, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0773],
        [ 0.0596],
        [ 0.0255],
        ...,
        [-0.3293],
        [-0.3280],
        [-0.3275]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-89418.7031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9997],
        [1.0002],
        ...,
        [0.9988],
        [0.9983],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366102.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6011],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(454.4949, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9997],
        [1.0002],
        ...,
        [0.9988],
        [0.9983],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366113.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6011],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(454.4949, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.5615e-02,  4.5318e-03,  1.1342e-02,  ...,  2.6824e-02,
         -8.4804e-03,  5.0022e-03],
        [-7.1539e-03,  8.3150e-04,  3.1999e-03,  ...,  7.7179e-03,
         -2.3684e-03,  1.1941e-03],
        [-2.2420e-02,  3.8914e-03,  9.9328e-03,  ...,  2.3517e-02,
         -7.4226e-03,  4.3432e-03],
        ...,
        [ 0.0000e+00, -6.0239e-04,  4.4805e-05,  ...,  3.1410e-04,
          0.0000e+00, -2.8153e-04],
        [ 0.0000e+00, -6.0239e-04,  4.4805e-05,  ...,  3.1410e-04,
          0.0000e+00, -2.8153e-04],
        [ 0.0000e+00, -6.0239e-04,  4.4805e-05,  ...,  3.1410e-04,
          0.0000e+00, -2.8153e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38.5481, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-45.5812, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(36.1708, device='cuda:0')



h[100].sum tensor(-34.6466, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.3855, device='cuda:0')



h[200].sum tensor(-10.9289, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-49.8396, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0058, 0.0170,  ..., 0.0407, 0.0000, 0.0070],
        [0.0000, 0.0164, 0.0416,  ..., 0.0984, 0.0000, 0.0182],
        [0.0000, 0.0096, 0.0265,  ..., 0.0630, 0.0000, 0.0112],
        ...,
        [0.0000, 0.0000, 0.0002,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0013, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62016.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.4310e-01, 0.0000e+00, 1.0137e-01,  ..., 1.2631e-01, 0.0000e+00,
         0.0000e+00],
        [3.5995e-01, 0.0000e+00, 1.5239e-01,  ..., 1.9039e-01, 0.0000e+00,
         0.0000e+00],
        [3.7122e-01, 0.0000e+00, 1.5670e-01,  ..., 1.9638e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.3045e-02, 4.6180e-04, 9.1748e-03,  ..., 1.6967e-03, 0.0000e+00,
         0.0000e+00],
        [2.1962e-02, 2.3088e-04, 1.2660e-02,  ..., 6.5029e-03, 0.0000e+00,
         0.0000e+00],
        [4.8425e-02, 3.7451e-05, 2.3470e-02,  ..., 2.0867e-02, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(429829.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12644.5664, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(211.2787, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2145.4448, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-485.0397, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5885.1484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(591.4851, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0470],
        [ 0.0345],
        [ 0.0266],
        ...,
        [-0.2050],
        [-0.0969],
        [-0.0170]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-87581.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9997],
        [1.0002],
        ...,
        [0.9988],
        [0.9983],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366113.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(331.5157, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9997],
        [1.0003],
        ...,
        [0.9988],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366123.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(331.5157, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -5.8962e-04,  2.4106e-05,  ...,  3.1821e-04,
          0.0000e+00, -2.8423e-04],
        [ 0.0000e+00, -5.8962e-04,  2.4106e-05,  ...,  3.1821e-04,
          0.0000e+00, -2.8423e-04],
        [ 0.0000e+00, -5.8962e-04,  2.4106e-05,  ...,  3.1821e-04,
          0.0000e+00, -2.8423e-04],
        ...,
        [ 0.0000e+00, -5.8962e-04,  2.4106e-05,  ...,  3.1821e-04,
          0.0000e+00, -2.8423e-04],
        [ 0.0000e+00, -5.8962e-04,  2.4106e-05,  ...,  3.1821e-04,
          0.0000e+00, -2.8423e-04],
        [ 0.0000e+00, -5.8962e-04,  2.4106e-05,  ...,  3.1821e-04,
          0.0000e+00, -2.8423e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(20.5845, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-33.1440, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.3836, device='cuda:0')



h[100].sum tensor(-25.1855, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.8459, device='cuda:0')



h[200].sum tensor(-14.3860, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-36.3538, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 9.6351e-05,  ..., 1.2719e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 9.6376e-05,  ..., 1.2722e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 9.6418e-05,  ..., 1.2728e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 9.6850e-05,  ..., 1.2785e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 9.6839e-05,  ..., 1.2783e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 9.6834e-05,  ..., 1.2783e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46664.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0168, 0.0003, 0.0100,  ..., 0.0034, 0.0000, 0.0000],
        [0.0139, 0.0007, 0.0093,  ..., 0.0020, 0.0000, 0.0000],
        [0.0134, 0.0008, 0.0093,  ..., 0.0017, 0.0000, 0.0000],
        ...,
        [0.0134, 0.0008, 0.0093,  ..., 0.0017, 0.0000, 0.0000],
        [0.0134, 0.0008, 0.0093,  ..., 0.0017, 0.0000, 0.0000],
        [0.0134, 0.0008, 0.0093,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(346685.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9906.3887, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(148.0481, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2260.1675, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-393.7946, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4518.5078, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(435.6649, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1699],
        [-0.2771],
        [-0.3628],
        ...,
        [-0.3365],
        [-0.3351],
        [-0.3347]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-85043.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9997],
        [1.0003],
        ...,
        [0.9988],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366123.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.7849, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9998],
        [1.0003],
        ...,
        [0.9988],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366134.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.7849, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -5.7226e-04,  2.7138e-06,  ...,  3.1487e-04,
          0.0000e+00, -2.8770e-04],
        [ 0.0000e+00, -5.7226e-04,  2.7138e-06,  ...,  3.1487e-04,
          0.0000e+00, -2.8770e-04],
        [ 0.0000e+00, -5.7226e-04,  2.7138e-06,  ...,  3.1487e-04,
          0.0000e+00, -2.8770e-04],
        ...,
        [ 0.0000e+00, -5.7226e-04,  2.7138e-06,  ...,  3.1487e-04,
          0.0000e+00, -2.8770e-04],
        [ 0.0000e+00, -5.7226e-04,  2.7138e-06,  ...,  3.1487e-04,
          0.0000e+00, -2.8770e-04],
        [ 0.0000e+00, -5.7226e-04,  2.7138e-06,  ...,  3.1487e-04,
          0.0000e+00, -2.8770e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(8.2961, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.4257, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.8444, device='cuda:0')



h[100].sum tensor(-17.7955, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.8897, device='cuda:0')



h[200].sum tensor(-16.9057, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.9657, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 1.0847e-05,  ..., 1.2585e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.0850e-05,  ..., 1.2589e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.0855e-05,  ..., 1.2594e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 1.0905e-05,  ..., 1.2652e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.0904e-05,  ..., 1.2651e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.0903e-05,  ..., 1.2650e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38531.9414, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0135, 0.0012, 0.0095,  ..., 0.0017, 0.0000, 0.0000],
        [0.0135, 0.0012, 0.0095,  ..., 0.0017, 0.0000, 0.0000],
        [0.0136, 0.0012, 0.0095,  ..., 0.0018, 0.0000, 0.0000],
        ...,
        [0.0136, 0.0012, 0.0095,  ..., 0.0017, 0.0000, 0.0000],
        [0.0136, 0.0012, 0.0095,  ..., 0.0017, 0.0000, 0.0000],
        [0.0136, 0.0012, 0.0095,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(312533.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8563.3828, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(114.5788, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2422.4321, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-347.0139, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3926.3887, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(352.8273, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4582],
        [-0.4376],
        [-0.3990],
        ...,
        [-0.3404],
        [-0.3390],
        [-0.3386]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-95582.1797, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9998],
        [1.0003],
        ...,
        [0.9988],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366134.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(284.6670, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9998],
        [1.0003],
        ...,
        [0.9988],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366134.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(284.6670, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -5.7226e-04,  2.7138e-06,  ...,  3.1487e-04,
          0.0000e+00, -2.8770e-04],
        [ 0.0000e+00, -5.7226e-04,  2.7138e-06,  ...,  3.1487e-04,
          0.0000e+00, -2.8770e-04],
        [ 0.0000e+00, -5.7226e-04,  2.7138e-06,  ...,  3.1487e-04,
          0.0000e+00, -2.8770e-04],
        ...,
        [ 0.0000e+00, -5.7226e-04,  2.7138e-06,  ...,  3.1487e-04,
          0.0000e+00, -2.8770e-04],
        [ 0.0000e+00, -5.7226e-04,  2.7138e-06,  ...,  3.1487e-04,
          0.0000e+00, -2.8770e-04],
        [ 0.0000e+00, -5.7226e-04,  2.7138e-06,  ...,  3.1487e-04,
          0.0000e+00, -2.8770e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(27.1059, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.2612, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.6551, device='cuda:0')



h[100].sum tensor(-21.4688, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.8785, device='cuda:0')



h[200].sum tensor(-15.3862, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-31.2164, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 1.0847e-05,  ..., 1.2585e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.0850e-05,  ..., 1.2589e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 8.7198e-04, 3.1865e-03,  ..., 8.7133e-03, 0.0000e+00,
         1.1978e-03],
        ...,
        [0.0000e+00, 0.0000e+00, 1.0905e-05,  ..., 1.2652e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.0904e-05,  ..., 1.2651e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.0903e-05,  ..., 1.2650e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41618.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0157, 0.0009, 0.0102,  ..., 0.0029, 0.0000, 0.0000],
        [0.0304, 0.0005, 0.0152,  ..., 0.0108, 0.0000, 0.0000],
        [0.0617, 0.0000, 0.0260,  ..., 0.0276, 0.0000, 0.0000],
        ...,
        [0.0136, 0.0012, 0.0095,  ..., 0.0017, 0.0000, 0.0000],
        [0.0136, 0.0012, 0.0095,  ..., 0.0017, 0.0000, 0.0000],
        [0.0136, 0.0012, 0.0095,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(321389.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8785.7314, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(125.6156, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2451.9492, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-364.4872, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4092.8467, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(384.2888, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0986],
        [-0.0079],
        [ 0.0554],
        ...,
        [-0.3404],
        [-0.3390],
        [-0.3386]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-102123.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9998],
        [1.0003],
        ...,
        [0.9988],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366134.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(181.4102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9998],
        [1.0003],
        ...,
        [0.9989],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366145.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(181.4102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.7083e-02,  2.8837e-03,  7.5426e-03,  ...,  1.8033e-02,
         -5.6243e-03,  3.2435e-03],
        [-2.2576e-02,  3.9891e-03,  9.9718e-03,  ...,  2.3735e-02,
         -7.4326e-03,  4.3799e-03],
        [-1.1157e-02,  1.6910e-03,  4.9215e-03,  ...,  1.1880e-02,
         -3.6731e-03,  2.0173e-03],
        ...,
        [ 0.0000e+00, -5.5439e-04, -1.2707e-05,  ...,  2.9793e-04,
          0.0000e+00, -2.9091e-04],
        [ 0.0000e+00, -5.5439e-04, -1.2707e-05,  ...,  2.9793e-04,
          0.0000e+00, -2.9091e-04],
        [ 0.0000e+00, -5.5439e-04, -1.2707e-05,  ...,  2.9793e-04,
          0.0000e+00, -2.9091e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6.3174, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.0366, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.4375, device='cuda:0')



h[100].sum tensor(-13.6975, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.7462, device='cuda:0')



h[200].sum tensor(-18.0422, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.8933, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0183, 0.0451,  ..., 0.1071, 0.0000, 0.0199],
        [0.0000, 0.0149, 0.0376,  ..., 0.0896, 0.0000, 0.0165],
        [0.0000, 0.0126, 0.0314,  ..., 0.0750, 0.0000, 0.0138],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33525.5078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.5241, 0.0000, 0.2297,  ..., 0.2834, 0.0000, 0.0000],
        [0.4761, 0.0000, 0.2082,  ..., 0.2567, 0.0000, 0.0000],
        [0.3780, 0.0000, 0.1656,  ..., 0.2025, 0.0000, 0.0000],
        ...,
        [0.0136, 0.0017, 0.0096,  ..., 0.0018, 0.0000, 0.0000],
        [0.0136, 0.0017, 0.0096,  ..., 0.0018, 0.0000, 0.0000],
        [0.0136, 0.0017, 0.0096,  ..., 0.0018, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(292765., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7860.8350, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(94.5226, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2526.6709, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-317.7389, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3615.3174, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(300.5820, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0691],
        [-0.0624],
        [-0.0443],
        ...,
        [-0.3459],
        [-0.3446],
        [-0.3442]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-94237.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9998],
        [1.0003],
        ...,
        [0.9989],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366145.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3135],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(389.1227, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9998],
        [1.0003],
        ...,
        [0.9989],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366156.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3135],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(389.1227, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.4569e-03,  5.6396e-04,  2.3913e-03,  ...,  5.9348e-03,
         -1.7932e-03,  8.3736e-04],
        [-1.3388e-02,  2.1622e-03,  5.9025e-03,  ...,  1.4177e-02,
         -4.3995e-03,  2.4800e-03],
        [ 0.0000e+00, -5.3573e-04, -2.4507e-05,  ...,  2.6401e-04,
          0.0000e+00, -2.9283e-04],
        ...,
        [ 0.0000e+00, -5.3573e-04, -2.4507e-05,  ...,  2.6401e-04,
          0.0000e+00, -2.9283e-04],
        [ 0.0000e+00, -5.3573e-04, -2.4507e-05,  ...,  2.6401e-04,
          0.0000e+00, -2.9283e-04],
        [ 0.0000e+00, -5.3573e-04, -2.4507e-05,  ...,  2.6401e-04,
          0.0000e+00, -2.9283e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(101.6402, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-38.0797, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.9682, device='cuda:0')



h[100].sum tensor(-28.9102, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-8.0355, device='cuda:0')



h[200].sum tensor(-11.1948, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-42.6709, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0075, 0.0211,  ..., 0.0508, 0.0000, 0.0087],
        [0.0000, 0.0026, 0.0092,  ..., 0.0229, 0.0000, 0.0035],
        [0.0000, 0.0025, 0.0078,  ..., 0.0196, 0.0000, 0.0031],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0011, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0011, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0011, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49297.8320, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1612, 0.0000, 0.0654,  ..., 0.0824, 0.0000, 0.0000],
        [0.1263, 0.0000, 0.0507,  ..., 0.0633, 0.0000, 0.0000],
        [0.0973, 0.0000, 0.0403,  ..., 0.0475, 0.0000, 0.0000],
        ...,
        [0.0134, 0.0022, 0.0095,  ..., 0.0018, 0.0000, 0.0000],
        [0.0134, 0.0022, 0.0095,  ..., 0.0018, 0.0000, 0.0000],
        [0.0134, 0.0022, 0.0095,  ..., 0.0018, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(355406.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9553.5137, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(154.2561, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2728.1594, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-408.8982, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4722.5776, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(457.2709, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0747],
        [ 0.0758],
        [ 0.0702],
        ...,
        [-0.3510],
        [-0.3397],
        [-0.3194]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-119529.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9998],
        [1.0003],
        ...,
        [0.9989],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366156.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(209.7820, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9998],
        [1.0004],
        ...,
        [0.9989],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366167.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(209.7820, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -5.2319e-04, -3.3011e-05,  ...,  2.3247e-04,
          0.0000e+00, -2.8942e-04],
        [ 0.0000e+00, -5.2319e-04, -3.3011e-05,  ...,  2.3247e-04,
          0.0000e+00, -2.8942e-04],
        [ 0.0000e+00, -5.2319e-04, -3.3011e-05,  ...,  2.3247e-04,
          0.0000e+00, -2.8942e-04],
        ...,
        [ 0.0000e+00, -5.2319e-04, -3.3011e-05,  ...,  2.3247e-04,
          0.0000e+00, -2.8942e-04],
        [ 0.0000e+00, -5.2319e-04, -3.3011e-05,  ...,  2.3247e-04,
          0.0000e+00, -2.8942e-04],
        [ 0.0000e+00, -5.2319e-04, -3.3011e-05,  ...,  2.3247e-04,
          0.0000e+00, -2.8942e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41.0553, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.7993, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.6954, device='cuda:0')



h[100].sum tensor(-15.7862, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.3321, device='cuda:0')



h[200].sum tensor(-16.2297, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.0045, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35103.2422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0179, 0.0013, 0.0104,  ..., 0.0043, 0.0000, 0.0000],
        [0.0137, 0.0025, 0.0092,  ..., 0.0021, 0.0000, 0.0000],
        [0.0133, 0.0027, 0.0091,  ..., 0.0018, 0.0000, 0.0000],
        ...,
        [0.0132, 0.0027, 0.0091,  ..., 0.0018, 0.0000, 0.0000],
        [0.0132, 0.0027, 0.0091,  ..., 0.0018, 0.0000, 0.0000],
        [0.0132, 0.0027, 0.0091,  ..., 0.0018, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(301105.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7795.4961, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(99.9226, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2773.0137, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-323.9130, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3728.4231, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(309.4564, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1465],
        [-0.1934],
        [-0.2258],
        ...,
        [-0.3640],
        [-0.3625],
        [-0.3621]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-111046.5391, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9998],
        [1.0004],
        ...,
        [0.9989],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366167.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6270],
        [0.0000],
        [0.6748],
        ...,
        [0.0000],
        [0.4685],
        [0.3289]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.2502, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9998],
        [1.0004],
        ...,
        [0.9989],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366178.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6270],
        [0.0000],
        [0.6748],
        ...,
        [0.0000],
        [0.4685],
        [0.3289]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.2502, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -5.1010e-04, -4.4053e-05,  ...,  2.1299e-04,
          0.0000e+00, -2.8527e-04],
        [-3.2871e-02,  6.1214e-03,  1.4545e-02,  ...,  3.4452e-02,
         -1.0762e-02,  6.5473e-03],
        [-1.5486e-02,  2.6141e-03,  6.8291e-03,  ...,  1.6344e-02,
         -5.0700e-03,  2.9337e-03],
        ...,
        [-1.1830e-02,  1.8766e-03,  5.2066e-03,  ...,  1.2536e-02,
         -3.8731e-03,  2.1738e-03],
        [-8.3040e-03,  1.1652e-03,  3.6415e-03,  ...,  8.8627e-03,
         -2.7187e-03,  1.4408e-03],
        [-1.1830e-02,  1.8766e-03,  5.2066e-03,  ...,  1.2536e-02,
         -3.8731e-03,  2.1738e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76.2017, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.4966, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.7119, device='cuda:0')



h[100].sum tensor(-19.3455, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.3742, device='cuda:0')



h[200].sum tensor(-14.3521, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.5388, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0202, 0.0487,  ..., 0.1155, 0.0000, 0.0217],
        [0.0000, 0.0070, 0.0186,  ..., 0.0449, 0.0000, 0.0079],
        [0.0000, 0.0204, 0.0491,  ..., 0.1166, 0.0000, 0.0220],
        ...,
        [0.0000, 0.0026, 0.0079,  ..., 0.0197, 0.0000, 0.0032],
        [0.0000, 0.0080, 0.0220,  ..., 0.0529, 0.0000, 0.0092],
        [0.0000, 0.0060, 0.0176,  ..., 0.0425, 0.0000, 0.0072]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38194.9453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.4113, 0.0000, 0.1790,  ..., 0.2208, 0.0000, 0.0000],
        [0.3176, 0.0000, 0.1368,  ..., 0.1689, 0.0000, 0.0000],
        [0.4233, 0.0000, 0.1839,  ..., 0.2273, 0.0000, 0.0000],
        ...,
        [0.0957, 0.0000, 0.0405,  ..., 0.0467, 0.0000, 0.0000],
        [0.1474, 0.0000, 0.0614,  ..., 0.0750, 0.0000, 0.0000],
        [0.1606, 0.0000, 0.0665,  ..., 0.0822, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(313128.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7964.5195, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(109.5365, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2888.1060, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-340.4057, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3914.7627, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(338.1393, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0069],
        [-0.0002],
        [-0.0085],
        ...,
        [-0.0210],
        [ 0.0316],
        [ 0.0396]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-122662.8672, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9998],
        [1.0004],
        ...,
        [0.9989],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366178.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.8149, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9998],
        [1.0004],
        ...,
        [0.9989],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366189.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.8149, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -4.9768e-04, -5.4983e-05,  ...,  1.9839e-04,
          0.0000e+00, -2.8166e-04],
        [ 0.0000e+00, -4.9768e-04, -5.4983e-05,  ...,  1.9839e-04,
          0.0000e+00, -2.8166e-04],
        [ 0.0000e+00, -4.9768e-04, -5.4983e-05,  ...,  1.9839e-04,
          0.0000e+00, -2.8166e-04],
        ...,
        [ 0.0000e+00, -4.9768e-04, -5.4983e-05,  ...,  1.9839e-04,
          0.0000e+00, -2.8166e-04],
        [ 0.0000e+00, -4.9768e-04, -5.4983e-05,  ...,  1.9839e-04,
          0.0000e+00, -2.8166e-04],
        [ 0.0000e+00, -4.9768e-04, -5.4983e-05,  ...,  1.9839e-04,
          0.0000e+00, -2.8166e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72.1099, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.9499, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.8572, device='cuda:0')



h[100].sum tensor(-15.8909, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.3740, device='cuda:0')



h[200].sum tensor(-15.3883, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.2275, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35969.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0570, 0.0007, 0.0256,  ..., 0.0257, 0.0000, 0.0000],
        [0.0285, 0.0014, 0.0140,  ..., 0.0102, 0.0000, 0.0000],
        [0.0196, 0.0019, 0.0102,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0130, 0.0034, 0.0085,  ..., 0.0018, 0.0000, 0.0000],
        [0.0130, 0.0034, 0.0085,  ..., 0.0018, 0.0000, 0.0000],
        [0.0130, 0.0034, 0.0085,  ..., 0.0018, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(312241.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7852.7612, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(99.7060, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2905.9954, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-326.5920, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3852.8528, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(311.9490, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0509],
        [ 0.0391],
        [ 0.0366],
        ...,
        [-0.3796],
        [-0.3781],
        [-0.3776]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-126112.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9998],
        [1.0004],
        ...,
        [0.9989],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366189.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.0449, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9998],
        [1.0004],
        ...,
        [0.9989],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366201.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.0449, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -4.8566e-04, -7.0017e-05,  ...,  1.9424e-04,
          0.0000e+00, -2.7881e-04],
        [ 0.0000e+00, -4.8566e-04, -7.0017e-05,  ...,  1.9424e-04,
          0.0000e+00, -2.7881e-04],
        [ 0.0000e+00, -4.8566e-04, -7.0017e-05,  ...,  1.9424e-04,
          0.0000e+00, -2.7881e-04],
        ...,
        [ 0.0000e+00, -4.8566e-04, -7.0017e-05,  ...,  1.9424e-04,
          0.0000e+00, -2.7881e-04],
        [ 0.0000e+00, -4.8566e-04, -7.0017e-05,  ...,  1.9424e-04,
          0.0000e+00, -2.7881e-04],
        [ 0.0000e+00, -4.8566e-04, -7.0017e-05,  ...,  1.9424e-04,
          0.0000e+00, -2.7881e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(107.5413, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.2018, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.5018, device='cuda:0')



h[100].sum tensor(-18.3521, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.0602, device='cuda:0')



h[200].sum tensor(-13.9508, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.8714, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39661.8203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0371, 0.0018, 0.0181,  ..., 0.0149, 0.0000, 0.0000],
        [0.0196, 0.0024, 0.0108,  ..., 0.0054, 0.0000, 0.0000],
        [0.0215, 0.0013, 0.0113,  ..., 0.0064, 0.0000, 0.0000],
        ...,
        [0.0131, 0.0035, 0.0081,  ..., 0.0018, 0.0000, 0.0000],
        [0.0131, 0.0035, 0.0081,  ..., 0.0018, 0.0000, 0.0000],
        [0.0131, 0.0035, 0.0081,  ..., 0.0018, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(326775.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8576.9746, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(114.8112, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2730.6016, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-348.1054, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4085.1631, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(344.5218, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0845],
        [-0.0774],
        [-0.0189],
        ...,
        [-0.3849],
        [-0.3836],
        [-0.3833]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-106103.7266, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9998],
        [1.0004],
        ...,
        [0.9989],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366201.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(169.9961, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9998],
        [1.0005],
        ...,
        [0.9989],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366213., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(169.9961, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -4.6530e-04, -9.0203e-05,  ...,  1.8795e-04,
          0.0000e+00, -2.7585e-04],
        [ 0.0000e+00, -4.6530e-04, -9.0203e-05,  ...,  1.8795e-04,
          0.0000e+00, -2.7585e-04],
        [ 0.0000e+00, -4.6530e-04, -9.0203e-05,  ...,  1.8795e-04,
          0.0000e+00, -2.7585e-04],
        ...,
        [ 0.0000e+00, -4.6530e-04, -9.0203e-05,  ...,  1.8795e-04,
          0.0000e+00, -2.7585e-04],
        [ 0.0000e+00, -4.6530e-04, -9.0203e-05,  ...,  1.8795e-04,
          0.0000e+00, -2.7585e-04],
        [ 0.0000e+00, -4.6530e-04, -9.0203e-05,  ...,  1.8795e-04,
          0.0000e+00, -2.7585e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(93.5043, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.7274, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.5291, device='cuda:0')



h[100].sum tensor(-12.6804, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.5105, device='cuda:0')



h[200].sum tensor(-15.8018, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.6416, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0020,  ..., 0.0056, 0.0000, 0.0007],
        [0.0000, 0.0010, 0.0040,  ..., 0.0105, 0.0000, 0.0014],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31465.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0404, 0.0008, 0.0170,  ..., 0.0166, 0.0000, 0.0000],
        [0.0594, 0.0000, 0.0234,  ..., 0.0269, 0.0000, 0.0000],
        [0.0763, 0.0000, 0.0294,  ..., 0.0361, 0.0000, 0.0000],
        ...,
        [0.0132, 0.0038, 0.0083,  ..., 0.0018, 0.0000, 0.0000],
        [0.0132, 0.0038, 0.0083,  ..., 0.0018, 0.0000, 0.0000],
        [0.0132, 0.0038, 0.0083,  ..., 0.0018, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(294875.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7017.8799, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(79.2838, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3055.6338, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-301.4215, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3466.2170, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(265.0695, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0742],
        [ 0.0952],
        [ 0.1047],
        ...,
        [-0.3911],
        [-0.3895],
        [-0.3890]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-138445.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9998],
        [1.0005],
        ...,
        [0.9989],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366213., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.6708, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9998],
        [1.0005],
        ...,
        [0.9989],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366224.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.6708, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0129,  0.0022,  0.0056,  ...,  0.0136, -0.0042,  0.0024],
        [-0.0053,  0.0006,  0.0023,  ...,  0.0057, -0.0017,  0.0008],
        [-0.0075,  0.0011,  0.0033,  ...,  0.0081, -0.0025,  0.0013],
        ...,
        [ 0.0000, -0.0004, -0.0001,  ...,  0.0002,  0.0000, -0.0003],
        [ 0.0000, -0.0004, -0.0001,  ...,  0.0002,  0.0000, -0.0003],
        [ 0.0000, -0.0004, -0.0001,  ...,  0.0002,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(150.1700, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.3950, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.7108, device='cuda:0')



h[100].sum tensor(-18.4874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.1145, device='cuda:0')



h[200].sum tensor(-12.9261, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.1594, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0030, 0.0091,  ..., 0.0230, 0.0000, 0.0036],
        [0.0000, 0.0079, 0.0209,  ..., 0.0509, 0.0000, 0.0090],
        [0.0000, 0.0034, 0.0093,  ..., 0.0230, 0.0000, 0.0039],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38788.6055, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1260, 0.0000, 0.0494,  ..., 0.0629, 0.0000, 0.0000],
        [0.1609, 0.0000, 0.0640,  ..., 0.0819, 0.0000, 0.0000],
        [0.1327, 0.0000, 0.0534,  ..., 0.0665, 0.0000, 0.0000],
        ...,
        [0.0133, 0.0040, 0.0083,  ..., 0.0017, 0.0000, 0.0000],
        [0.0133, 0.0040, 0.0083,  ..., 0.0017, 0.0000, 0.0000],
        [0.0133, 0.0040, 0.0083,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(326897.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8136.2598, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(107.1988, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3070.6658, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-345.2856, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4006.6196, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(338.7673, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1145],
        [ 0.1156],
        [ 0.1141],
        ...,
        [-0.3959],
        [-0.3946],
        [-0.3944]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-139963.7656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9998],
        [1.0005],
        ...,
        [0.9989],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366224.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.1464, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9999],
        [1.0005],
        ...,
        [0.9989],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366236.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.1464, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0004, -0.0001,  ...,  0.0002,  0.0000, -0.0003],
        [ 0.0000, -0.0004, -0.0001,  ...,  0.0002,  0.0000, -0.0003],
        [ 0.0000, -0.0004, -0.0001,  ...,  0.0002,  0.0000, -0.0003],
        ...,
        [ 0.0000, -0.0004, -0.0001,  ...,  0.0002,  0.0000, -0.0003],
        [ 0.0000, -0.0004, -0.0001,  ...,  0.0002,  0.0000, -0.0003],
        [ 0.0000, -0.0004, -0.0001,  ...,  0.0002,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(166.6836, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.0711, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.7140, device='cuda:0')



h[100].sum tensor(-17.4789, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.8558, device='cuda:0')



h[200].sum tensor(-12.7979, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.7860, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39555.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0339, 0.0016, 0.0155,  ..., 0.0128, 0.0000, 0.0000],
        [0.0189, 0.0028, 0.0104,  ..., 0.0046, 0.0000, 0.0000],
        [0.0136, 0.0041, 0.0087,  ..., 0.0017, 0.0000, 0.0000],
        ...,
        [0.0135, 0.0041, 0.0087,  ..., 0.0017, 0.0000, 0.0000],
        [0.0135, 0.0041, 0.0087,  ..., 0.0017, 0.0000, 0.0000],
        [0.0135, 0.0041, 0.0087,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(331884.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8810.7988, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(113.3302, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2894.5391, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-353.5511, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4105.3555, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(346.7588, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0302],
        [-0.0412],
        [-0.1458],
        ...,
        [-0.4011],
        [-0.3995],
        [-0.3990]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-104048.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9999],
        [1.0005],
        ...,
        [0.9989],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366236.0625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 70.0 event: 350 loss: tensor(1457.8397, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3591],
        [0.3643],
        [0.3508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(326.4241, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9999],
        [1.0006],
        ...,
        [0.9989],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366247.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3591],
        [0.3643],
        [0.3508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(326.4241, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0190,  0.0035,  0.0083,  ...,  0.0201, -0.0061,  0.0037],
        [-0.0253,  0.0047,  0.0112,  ...,  0.0267, -0.0082,  0.0050],
        [-0.0326,  0.0062,  0.0144,  ...,  0.0344, -0.0106,  0.0066],
        ...,
        [ 0.0000, -0.0004, -0.0002,  ...,  0.0002,  0.0000, -0.0003],
        [ 0.0000, -0.0004, -0.0002,  ...,  0.0002,  0.0000, -0.0003],
        [ 0.0000, -0.0004, -0.0002,  ...,  0.0002,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(235.2332, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-32.2175, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.9783, device='cuda:0')



h[100].sum tensor(-24.4009, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.7408, device='cuda:0')



h[200].sum tensor(-9.2789, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-35.7954, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0157, 0.0374,  ..., 0.0900, 0.0000, 0.0168],
        [0.0000, 0.0173, 0.0410,  ..., 0.0984, 0.0000, 0.0185],
        [0.0000, 0.0209, 0.0489,  ..., 0.1169, 0.0000, 0.0222],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46897.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3284, 0.0000, 0.1410,  ..., 0.1736, 0.0000, 0.0000],
        [0.3895, 0.0000, 0.1679,  ..., 0.2072, 0.0000, 0.0000],
        [0.4436, 0.0000, 0.1913,  ..., 0.2369, 0.0000, 0.0000],
        ...,
        [0.0138, 0.0043, 0.0093,  ..., 0.0017, 0.0000, 0.0000],
        [0.0138, 0.0043, 0.0093,  ..., 0.0017, 0.0000, 0.0000],
        [0.0138, 0.0043, 0.0093,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(358266.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9777.1279, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(142.0299, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2936.5303, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-400.8131, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4562.4282, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(425.0525, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0626],
        [ 0.0648],
        [ 0.0697],
        ...,
        [-0.4057],
        [-0.4041],
        [-0.4036]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-104165.0234, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9999],
        [1.0006],
        ...,
        [0.9989],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366247.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(255.7729, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9999],
        [1.0006],
        ...,
        [0.9989],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366258.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(255.7729, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0004, -0.0002,  ...,  0.0002,  0.0000, -0.0003],
        [ 0.0000, -0.0004, -0.0002,  ...,  0.0002,  0.0000, -0.0003],
        [ 0.0000, -0.0004, -0.0002,  ...,  0.0002,  0.0000, -0.0003],
        ...,
        [ 0.0000, -0.0004, -0.0002,  ...,  0.0002,  0.0000, -0.0003],
        [ 0.0000, -0.0004, -0.0002,  ...,  0.0002,  0.0000, -0.0003],
        [ 0.0000, -0.0004, -0.0002,  ...,  0.0002,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(215.4793, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.0548, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.3556, device='cuda:0')



h[100].sum tensor(-18.9703, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.2818, device='cuda:0')



h[200].sum tensor(-10.7585, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.0479, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0026, 0.0070,  ..., 0.0179, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41218.4023, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0934, 0.0000, 0.0389,  ..., 0.0454, 0.0000, 0.0000],
        [0.0496, 0.0010, 0.0225,  ..., 0.0214, 0.0000, 0.0000],
        [0.0287, 0.0020, 0.0150,  ..., 0.0099, 0.0000, 0.0000],
        ...,
        [0.0138, 0.0046, 0.0099,  ..., 0.0017, 0.0000, 0.0000],
        [0.0138, 0.0046, 0.0099,  ..., 0.0017, 0.0000, 0.0000],
        [0.0138, 0.0046, 0.0099,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(339554.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9108.2109, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(120.5294, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3112.3931, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-371.7669, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4251.2847, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(372.4033, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0790],
        [ 0.0318],
        [-0.0434],
        ...,
        [-0.4125],
        [-0.4109],
        [-0.4100]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-106605.1328, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9999],
        [1.0006],
        ...,
        [0.9989],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366258.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4207],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(256.4604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9999],
        [1.0007],
        ...,
        [0.9989],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366269.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4207],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(256.4604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0003, -0.0002,  ...,  0.0002,  0.0000, -0.0003],
        [-0.0105,  0.0018,  0.0045,  ...,  0.0112, -0.0034,  0.0019],
        [-0.0076,  0.0012,  0.0032,  ...,  0.0082, -0.0025,  0.0013],
        ...,
        [ 0.0000, -0.0003, -0.0002,  ...,  0.0002,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0002,  ...,  0.0002,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0002,  ...,  0.0002,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(228.7173, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.0807, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.4103, device='cuda:0')



h[100].sum tensor(-18.9842, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.2960, device='cuda:0')



h[200].sum tensor(-9.9388, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.1233, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0044, 0.0108,  ..., 0.0268, 0.0000, 0.0047],
        [0.0000, 0.0039, 0.0097,  ..., 0.0242, 0.0000, 0.0042],
        [0.0000, 0.0094, 0.0226,  ..., 0.0554, 0.0000, 0.0099],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42436.6641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1215, 0.0000, 0.0539,  ..., 0.0609, 0.0000, 0.0000],
        [0.1300, 0.0000, 0.0566,  ..., 0.0656, 0.0000, 0.0000],
        [0.1811, 0.0000, 0.0775,  ..., 0.0936, 0.0000, 0.0000],
        ...,
        [0.0137, 0.0048, 0.0103,  ..., 0.0017, 0.0000, 0.0000],
        [0.0137, 0.0048, 0.0103,  ..., 0.0017, 0.0000, 0.0000],
        [0.0137, 0.0048, 0.0103,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(349618.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9350.5039, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(126.2886, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3238.5156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-382.8052, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4415.7334, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(389.0915, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0594],
        [ 0.0612],
        [ 0.0639],
        ...,
        [-0.4206],
        [-0.4190],
        [-0.4184]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-113606.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9999],
        [1.0007],
        ...,
        [0.9989],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366269.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(270.9592, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9999],
        [1.0007],
        ...,
        [0.9989],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366281.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(270.9592, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0003, -0.0002,  ...,  0.0002,  0.0000, -0.0003],
        [-0.0050,  0.0007,  0.0020,  ...,  0.0054, -0.0016,  0.0008],
        [-0.0050,  0.0007,  0.0020,  ...,  0.0054, -0.0016,  0.0008],
        ...,
        [ 0.0000, -0.0003, -0.0002,  ...,  0.0002,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0002,  ...,  0.0002,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0002,  ...,  0.0002,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(253.3165, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.2617, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.5642, device='cuda:0')



h[100].sum tensor(-19.8720, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.5954, device='cuda:0')



h[200].sum tensor(-8.8304, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-29.7132, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0013, 0.0036,  ..., 0.0102, 0.0000, 0.0014],
        [0.0000, 0.0013, 0.0036,  ..., 0.0102, 0.0000, 0.0014],
        [0.0000, 0.0013, 0.0036,  ..., 0.0102, 0.0000, 0.0014],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41738., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0454, 0.0000, 0.0205,  ..., 0.0195, 0.0000, 0.0000],
        [0.0636, 0.0000, 0.0273,  ..., 0.0295, 0.0000, 0.0000],
        [0.1040, 0.0000, 0.0446,  ..., 0.0518, 0.0000, 0.0000],
        ...,
        [0.0138, 0.0049, 0.0104,  ..., 0.0017, 0.0000, 0.0000],
        [0.0137, 0.0049, 0.0104,  ..., 0.0017, 0.0000, 0.0000],
        [0.0137, 0.0049, 0.0104,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(342979.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9139.2871, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(123.2158, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3263.8569, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-381.8610, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4341.3838, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(385.7034, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-9.2820e-02],
        [ 3.1339e-04],
        [ 4.2047e-02],
        ...,
        [-4.2707e-01],
        [-4.2541e-01],
        [-4.2489e-01]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-112210.3516, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9999],
        [1.0007],
        ...,
        [0.9989],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366281.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.7188, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [1.0000],
        [1.0008],
        ...,
        [0.9989],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366293.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.7188, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0003, -0.0002,  ...,  0.0002,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0002,  ...,  0.0002,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0002,  ...,  0.0002,  0.0000, -0.0003],
        ...,
        [ 0.0000, -0.0003, -0.0002,  ...,  0.0002,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0002,  ...,  0.0002,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0002,  ...,  0.0002,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(244.3747, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.3742, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.5312, device='cuda:0')



h[100].sum tensor(-15.4123, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.2895, device='cuda:0')



h[200].sum tensor(-10.0460, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.7783, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35774.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0138, 0.0050, 0.0103,  ..., 0.0018, 0.0000, 0.0000],
        [0.0138, 0.0050, 0.0103,  ..., 0.0018, 0.0000, 0.0000],
        [0.0139, 0.0050, 0.0103,  ..., 0.0018, 0.0000, 0.0000],
        ...,
        [0.0138, 0.0050, 0.0104,  ..., 0.0018, 0.0000, 0.0000],
        [0.0138, 0.0050, 0.0104,  ..., 0.0018, 0.0000, 0.0000],
        [0.0138, 0.0050, 0.0104,  ..., 0.0018, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(321316.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7886.9844, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(94.9951, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3540.8574, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-349.4986, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3916.7769, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(329.3878, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5900],
        [-0.5797],
        [-0.5577],
        ...,
        [-0.4318],
        [-0.4301],
        [-0.4296]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-151143.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [1.0000],
        [1.0008],
        ...,
        [0.9989],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366293.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.8715, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [1.0000],
        [1.0008],
        ...,
        [0.9989],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366305., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.8715, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0003, -0.0002,  ...,  0.0002,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0002,  ...,  0.0002,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0002,  ...,  0.0002,  0.0000, -0.0003],
        ...,
        [ 0.0000, -0.0003, -0.0002,  ...,  0.0002,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0002,  ...,  0.0002,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0002,  ...,  0.0002,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(286.0511, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.1290, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.7268, device='cuda:0')



h[100].sum tensor(-18.2472, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.1186, device='cuda:0')



h[200].sum tensor(-8.2745, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.1814, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0036, 0.0086,  ..., 0.0223, 0.0000, 0.0037],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43179.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1062, 0.0000, 0.0460,  ..., 0.0531, 0.0000, 0.0000],
        [0.0492, 0.0012, 0.0230,  ..., 0.0215, 0.0000, 0.0000],
        [0.0263, 0.0024, 0.0141,  ..., 0.0088, 0.0000, 0.0000],
        ...,
        [0.0139, 0.0051, 0.0101,  ..., 0.0018, 0.0000, 0.0000],
        [0.0139, 0.0051, 0.0101,  ..., 0.0018, 0.0000, 0.0000],
        [0.0139, 0.0051, 0.0101,  ..., 0.0018, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(369765., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9717.0205, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(123.8844, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3332.2217, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-396.2472, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4795.1099, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(403.6174, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0308],
        [-0.0668],
        [-0.1946],
        ...,
        [-0.4372],
        [-0.4355],
        [-0.4349]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-129870.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [1.0000],
        [1.0008],
        ...,
        [0.9989],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366305., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.8999, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [1.0000],
        [1.0009],
        ...,
        [0.9990],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366317.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.8999, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0003, -0.0003,  ...,  0.0003,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0003,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0003,  0.0000, -0.0003],
        ...,
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0003,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0003,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0003,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(293.3470, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.4956, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.5802, device='cuda:0')



h[100].sum tensor(-16.2508, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.5616, device='cuda:0')



h[200].sum tensor(-8.7186, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.2237, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0041,  ..., 0.0114, 0.0000, 0.0018],
        [0.0000, 0.0000, 0.0000,  ..., 0.0011, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0011, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0011, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0011, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0011, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39143.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0648, 0.0000, 0.0304,  ..., 0.0299, 0.0000, 0.0000],
        [0.0272, 0.0026, 0.0148,  ..., 0.0090, 0.0000, 0.0000],
        [0.0166, 0.0041, 0.0106,  ..., 0.0031, 0.0000, 0.0000],
        ...,
        [0.0141, 0.0052, 0.0097,  ..., 0.0017, 0.0000, 0.0000],
        [0.0141, 0.0052, 0.0097,  ..., 0.0017, 0.0000, 0.0000],
        [0.0141, 0.0052, 0.0097,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(344343.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8943.8896, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(104.9177, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3194.3857, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-374.9312, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4310.9561, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(360.7340, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0639],
        [-0.2090],
        [-0.3514],
        ...,
        [-0.4407],
        [-0.4390],
        [-0.4384]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-123766.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [1.0000],
        [1.0009],
        ...,
        [0.9990],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366317.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(195.7550, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [1.0000],
        [1.0009],
        ...,
        [0.9990],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366329.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(195.7550, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0003, -0.0003,  ...,  0.0003,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0003,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0003,  0.0000, -0.0003],
        ...,
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0003,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0003,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0003,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(298.3015, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.9970, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.5791, device='cuda:0')



h[100].sum tensor(-14.3574, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.0424, device='cuda:0')



h[200].sum tensor(-9.1171, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.4663, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0000],
        [0.0000, 0.0039, 0.0094,  ..., 0.0246, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36282.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0222, 0.0036, 0.0125,  ..., 0.0063, 0.0000, 0.0000],
        [0.0508, 0.0021, 0.0243,  ..., 0.0222, 0.0000, 0.0000],
        [0.1220, 0.0000, 0.0534,  ..., 0.0616, 0.0000, 0.0000],
        ...,
        [0.0142, 0.0053, 0.0093,  ..., 0.0018, 0.0000, 0.0000],
        [0.0142, 0.0053, 0.0093,  ..., 0.0018, 0.0000, 0.0000],
        [0.0142, 0.0053, 0.0093,  ..., 0.0018, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(331070.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8372.9160, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(89.2182, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3161.7969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-359.8192, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4060.4229, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(330.6990, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2072],
        [-0.0656],
        [ 0.0327],
        ...,
        [-0.4445],
        [-0.4428],
        [-0.4423]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-128590.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [1.0000],
        [1.0009],
        ...,
        [0.9990],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366329.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(194.5568, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [1.0001],
        [1.0010],
        ...,
        [0.9990],
        [0.9984],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366341.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(194.5568, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0064,  0.0010,  0.0026,  ...,  0.0071, -0.0020,  0.0011],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0003,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0003,  0.0000, -0.0003],
        ...,
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0003,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0003,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0003,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(317.8807, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.9914, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.4837, device='cuda:0')



h[100].sum tensor(-14.3488, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.0177, device='cuda:0')



h[200].sum tensor(-8.7657, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.3349, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0037, 0.0089,  ..., 0.0237, 0.0000, 0.0039],
        [0.0000, 0.0010, 0.0026,  ..., 0.0081, 0.0000, 0.0011],
        [0.0000, 0.0065, 0.0152,  ..., 0.0383, 0.0000, 0.0069],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36348.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1565, 0.0000, 0.0674,  ..., 0.0812, 0.0000, 0.0000],
        [0.1313, 0.0000, 0.0577,  ..., 0.0672, 0.0000, 0.0000],
        [0.1872, 0.0000, 0.0830,  ..., 0.0982, 0.0000, 0.0000],
        ...,
        [0.0142, 0.0054, 0.0091,  ..., 0.0019, 0.0000, 0.0000],
        [0.0142, 0.0054, 0.0091,  ..., 0.0019, 0.0000, 0.0000],
        [0.0142, 0.0054, 0.0091,  ..., 0.0019, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(331137.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8348.9336, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(86.7955, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3054.7666, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-361.4623, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4019.9531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(329.6059, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0829],
        [ 0.0782],
        [ 0.0728],
        ...,
        [-0.4469],
        [-0.4453],
        [-0.4450]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-127380.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [1.0001],
        [1.0010],
        ...,
        [0.9990],
        [0.9984],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366341.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(205.2098, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [1.0001],
        [1.0011],
        ...,
        [0.9990],
        [0.9984],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366353.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(205.2098, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0003, -0.0003,  ...,  0.0003,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0003,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0003,  0.0000, -0.0003],
        ...,
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0003,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0003,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0003,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(338.4845, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.9138, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.3316, device='cuda:0')



h[100].sum tensor(-15.0412, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.2376, device='cuda:0')



h[200].sum tensor(-8.0414, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.5032, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37941.2383, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0139, 0.0054, 0.0088,  ..., 0.0019, 0.0000, 0.0000],
        [0.0157, 0.0046, 0.0095,  ..., 0.0029, 0.0000, 0.0000],
        [0.0315, 0.0027, 0.0156,  ..., 0.0117, 0.0000, 0.0000],
        ...,
        [0.0140, 0.0055, 0.0089,  ..., 0.0019, 0.0000, 0.0000],
        [0.0140, 0.0055, 0.0089,  ..., 0.0019, 0.0000, 0.0000],
        [0.0140, 0.0055, 0.0089,  ..., 0.0019, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(343846.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8465.1084, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(89.2878, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3164.3535, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-371.5894, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4214.9473, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(345.8852, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4264],
        [-0.2999],
        [-0.1323],
        ...,
        [-0.4555],
        [-0.4538],
        [-0.4533]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-145137.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [1.0001],
        [1.0011],
        ...,
        [0.9990],
        [0.9984],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366353.1250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 80.0 event: 400 loss: tensor(545.8372, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2673],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.8358, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [1.0001],
        [1.0012],
        ...,
        [0.9990],
        [0.9984],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366365.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2673],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.8358, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0003, -0.0003,  ...,  0.0004,  0.0000, -0.0003],
        [-0.0134,  0.0024,  0.0058,  ...,  0.0146, -0.0043,  0.0026],
        [-0.0068,  0.0011,  0.0028,  ...,  0.0076, -0.0022,  0.0012],
        ...,
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0004,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(386.7256, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.4518, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.8831, device='cuda:0')



h[100].sum tensor(-18.4632, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.1592, device='cuda:0')



h[200].sum tensor(-6.3622, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.3968, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0032, 0.0080,  ..., 0.0216, 0.0000, 0.0035],
        [0.0000, 0.0034, 0.0089,  ..., 0.0244, 0.0000, 0.0038],
        [0.0000, 0.0147, 0.0340,  ..., 0.0842, 0.0000, 0.0155],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39914.4492, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1093, 0.0000, 0.0471,  ..., 0.0554, 0.0000, 0.0000],
        [0.1786, 0.0000, 0.0765,  ..., 0.0943, 0.0000, 0.0000],
        [0.3157, 0.0000, 0.1377,  ..., 0.1709, 0.0000, 0.0000],
        ...,
        [0.0141, 0.0055, 0.0087,  ..., 0.0019, 0.0000, 0.0000],
        [0.0141, 0.0055, 0.0087,  ..., 0.0019, 0.0000, 0.0000],
        [0.0141, 0.0055, 0.0087,  ..., 0.0019, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(344149.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8683.4580, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(98.3118, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2929.7847, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-384.2348, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4186.5684, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(363.8523, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0757],
        [ 0.0986],
        [ 0.0965],
        ...,
        [-0.4601],
        [-0.4583],
        [-0.4578]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-126081.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [1.0001],
        [1.0012],
        ...,
        [0.9990],
        [0.9984],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366365.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.2504, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [1.0002],
        [1.0012],
        ...,
        [0.9990],
        [0.9984],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366377.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.2504, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0003, -0.0003,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0004,  0.0000, -0.0003],
        ...,
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0004,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(432.3816, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.6623, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.4974, device='cuda:0')



h[100].sum tensor(-21.6359, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.0970, device='cuda:0')



h[200].sum tensor(-4.8039, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-32.3769, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47856.2070, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0141, 0.0055, 0.0088,  ..., 0.0020, 0.0000, 0.0000],
        [0.0141, 0.0055, 0.0088,  ..., 0.0020, 0.0000, 0.0000],
        [0.0142, 0.0055, 0.0088,  ..., 0.0020, 0.0000, 0.0000],
        ...,
        [0.0142, 0.0055, 0.0089,  ..., 0.0020, 0.0000, 0.0000],
        [0.0142, 0.0055, 0.0089,  ..., 0.0020, 0.0000, 0.0000],
        [0.0142, 0.0055, 0.0089,  ..., 0.0020, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(387751.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10312.7930, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(130.2744, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2804.5442, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-433.3210, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4965.1187, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(446.9017, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3616],
        [-0.4179],
        [-0.4669],
        ...,
        [-0.4651],
        [-0.4634],
        [-0.4629]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-117917.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [1.0002],
        [1.0012],
        ...,
        [0.9990],
        [0.9984],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366377.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(345.0648, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [1.0002],
        [1.0013],
        ...,
        [0.9990],
        [0.9984],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366388.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(345.0648, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0003, -0.0003,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0004,  0.0000, -0.0003],
        ...,
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0004,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(486.7925, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-33.6403, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.4619, device='cuda:0')



h[100].sum tensor(-25.3858, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-7.1257, device='cuda:0')



h[200].sum tensor(-3.0482, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-37.8396, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0007, 0.0018,  ..., 0.0067, 0.0000, 0.0008],
        [0.0000, 0.0013, 0.0037,  ..., 0.0118, 0.0000, 0.0015],
        [0.0000, 0.0007, 0.0018,  ..., 0.0067, 0.0000, 0.0008],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50454.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0568, 0.0000, 0.0244,  ..., 0.0265, 0.0000, 0.0000],
        [0.0632, 0.0000, 0.0266,  ..., 0.0302, 0.0000, 0.0000],
        [0.0614, 0.0000, 0.0262,  ..., 0.0290, 0.0000, 0.0000],
        ...,
        [0.0146, 0.0056, 0.0094,  ..., 0.0022, 0.0000, 0.0000],
        [0.0146, 0.0056, 0.0094,  ..., 0.0022, 0.0000, 0.0000],
        [0.0146, 0.0056, 0.0094,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(397083.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10533.5361, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(141.3385, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2909.2153, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-453.5980, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5128.7178, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(480.6023, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1139],
        [ 0.1167],
        [ 0.1151],
        ...,
        [-0.4689],
        [-0.4672],
        [-0.4666]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-130338.5547, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [1.0002],
        [1.0013],
        ...,
        [0.9990],
        [0.9984],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366388.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.2078, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [1.0002],
        [1.0013],
        ...,
        [0.9990],
        [0.9984],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366399.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.2078, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0003, -0.0004,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0004,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0004,  ...,  0.0004,  0.0000, -0.0003],
        ...,
        [ 0.0000, -0.0003, -0.0004,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0004,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0004,  ...,  0.0004,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(426.2026, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.3898, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.4802, device='cuda:0')



h[100].sum tensor(-16.8907, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.7952, device='cuda:0')



h[200].sum tensor(-6.5072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.4637, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40537.8203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0851, 0.0006, 0.0396,  ..., 0.0425, 0.0000, 0.0000],
        [0.0572, 0.0013, 0.0276,  ..., 0.0267, 0.0000, 0.0000],
        [0.0453, 0.0006, 0.0228,  ..., 0.0198, 0.0000, 0.0000],
        ...,
        [0.0147, 0.0057, 0.0100,  ..., 0.0025, 0.0000, 0.0000],
        [0.0147, 0.0057, 0.0100,  ..., 0.0025, 0.0000, 0.0000],
        [0.0147, 0.0057, 0.0100,  ..., 0.0025, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(355492.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9191.5879, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(106.2472, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2964.8672, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-399.7491, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4452.0376, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(386.1797, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0359],
        [ 0.0326],
        [ 0.0308],
        ...,
        [-0.4711],
        [-0.4691],
        [-0.4666]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-130403.3984, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [1.0002],
        [1.0013],
        ...,
        [0.9990],
        [0.9984],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366399.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4585],
        [0.5786],
        [0.6382],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(225.6981, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0002],
        [1.0014],
        ...,
        [0.9990],
        [0.9984],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366410.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4585],
        [0.5786],
        [0.6382],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(225.6981, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0378,  0.0075,  0.0168,  ...,  0.0406, -0.0119,  0.0078],
        [-0.0272,  0.0053,  0.0119,  ...,  0.0293, -0.0086,  0.0055],
        [-0.0143,  0.0026,  0.0061,  ...,  0.0156, -0.0045,  0.0028],
        ...,
        [ 0.0000, -0.0003, -0.0004,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0004,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0004,  ...,  0.0004,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(434.3839, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.9865, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.9621, device='cuda:0')



h[100].sum tensor(-16.5814, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.6607, device='cuda:0')



h[200].sum tensor(-6.5191, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.7499, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0226, 0.0510,  ..., 0.1247, 0.0000, 0.0236],
        [0.0000, 0.0207, 0.0467,  ..., 0.1147, 0.0000, 0.0216],
        [0.0000, 0.0169, 0.0384,  ..., 0.0952, 0.0000, 0.0177],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38852.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.4525, 0.0000, 0.2071,  ..., 0.2513, 0.0000, 0.0000],
        [0.3977, 0.0000, 0.1818,  ..., 0.2202, 0.0000, 0.0000],
        [0.3265, 0.0000, 0.1483,  ..., 0.1798, 0.0000, 0.0000],
        ...,
        [0.0147, 0.0057, 0.0104,  ..., 0.0027, 0.0000, 0.0000],
        [0.0147, 0.0057, 0.0104,  ..., 0.0027, 0.0000, 0.0000],
        [0.0147, 0.0057, 0.0104,  ..., 0.0027, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(346555.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8754.0391, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(101.8108, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3123.5396, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-392.4834, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4312.8770, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(373.5605, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0191],
        [ 0.0307],
        [ 0.0449],
        ...,
        [-0.4837],
        [-0.4819],
        [-0.4813]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-140518.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0002],
        [1.0014],
        ...,
        [0.9990],
        [0.9984],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366410.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.1169, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0003],
        [1.0014],
        ...,
        [0.9990],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366422.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.1169, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0424,  0.0085,  0.0189,  ...,  0.0456, -0.0134,  0.0088],
        [-0.0129,  0.0024,  0.0055,  ...,  0.0141, -0.0041,  0.0025],
        [ 0.0000, -0.0003, -0.0004,  ...,  0.0003,  0.0000, -0.0003],
        ...,
        [ 0.0000, -0.0003, -0.0004,  ...,  0.0003,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0004,  ...,  0.0003,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0004,  ...,  0.0003,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(439.8633, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.7718, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.9954, device='cuda:0')



h[100].sum tensor(-16.4145, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.6694, device='cuda:0')



h[200].sum tensor(-6.5269, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.7958, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0135, 0.0307,  ..., 0.0773, 0.0000, 0.0141],
        [0.0000, 0.0165, 0.0372,  ..., 0.0914, 0.0000, 0.0172],
        [0.0000, 0.0064, 0.0145,  ..., 0.0374, 0.0000, 0.0067],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39718.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3350, 0.0000, 0.1529,  ..., 0.1853, 0.0000, 0.0000],
        [0.3346, 0.0000, 0.1531,  ..., 0.1851, 0.0000, 0.0000],
        [0.2502, 0.0000, 0.1144,  ..., 0.1369, 0.0000, 0.0000],
        ...,
        [0.0146, 0.0057, 0.0105,  ..., 0.0027, 0.0000, 0.0000],
        [0.0146, 0.0057, 0.0105,  ..., 0.0027, 0.0000, 0.0000],
        [0.0146, 0.0057, 0.0105,  ..., 0.0027, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(353448.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9016.5215, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(108.4113, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3140.9065, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-398.7679, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4430.4941, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(384.7023, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0254],
        [ 0.0305],
        [ 0.0383],
        ...,
        [-0.4930],
        [-0.4912],
        [-0.4906]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-136809.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0003],
        [1.0014],
        ...,
        [0.9990],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366422.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(350.4592, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0003],
        [1.0015],
        ...,
        [0.9990],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366433.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(350.4592, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0003, -0.0004,  ...,  0.0003,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0004,  ...,  0.0003,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0004,  ...,  0.0003,  0.0000, -0.0003],
        ...,
        [ 0.0000, -0.0003, -0.0004,  ...,  0.0003,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0004,  ...,  0.0003,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0004,  ...,  0.0003,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(531.5616, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-33.3733, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.8912, device='cuda:0')



h[100].sum tensor(-25.1534, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-7.2371, device='cuda:0')



h[200].sum tensor(-2.8883, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-38.4311, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50880.6641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0143, 0.0058, 0.0103,  ..., 0.0026, 0.0000, 0.0000],
        [0.0143, 0.0058, 0.0103,  ..., 0.0026, 0.0000, 0.0000],
        [0.0144, 0.0058, 0.0103,  ..., 0.0026, 0.0000, 0.0000],
        ...,
        [0.0144, 0.0058, 0.0104,  ..., 0.0026, 0.0000, 0.0000],
        [0.0144, 0.0058, 0.0104,  ..., 0.0026, 0.0000, 0.0000],
        [0.0144, 0.0058, 0.0104,  ..., 0.0026, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(405500.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10855.1865, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(153.3785, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3008.2725, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-465.1311, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5322.0386, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(497.7243, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5974],
        [-0.6243],
        [-0.6364],
        ...,
        [-0.4974],
        [-0.4955],
        [-0.4948]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-123538., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0003],
        [1.0015],
        ...,
        [0.9990],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366433.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4517],
        [0.2886],
        [0.2786],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.3673, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0003],
        [1.0015],
        ...,
        [0.9990],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366445.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4517],
        [0.2886],
        [0.2786],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.3673, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0153,  0.0029,  0.0066,  ...,  0.0167, -0.0048,  0.0030],
        [-0.0319,  0.0063,  0.0141,  ...,  0.0345, -0.0100,  0.0066],
        [-0.0255,  0.0050,  0.0112,  ...,  0.0276, -0.0080,  0.0052],
        ...,
        [ 0.0000, -0.0003, -0.0004,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0004,  ...,  0.0004,  0.0000, -0.0003],
        [ 0.0000, -0.0003, -0.0004,  ...,  0.0004,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(523.5641, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.0998, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.6209, device='cuda:0')



h[100].sum tensor(-22.6792, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.3885, device='cuda:0')



h[200].sum tensor(-4.1157, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-33.9250, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0150, 0.0340,  ..., 0.0852, 0.0000, 0.0157],
        [0.0000, 0.0160, 0.0362,  ..., 0.0904, 0.0000, 0.0168],
        [0.0000, 0.0201, 0.0452,  ..., 0.1116, 0.0000, 0.0210],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48487.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2771, 0.0000, 0.1269,  ..., 0.1529, 0.0000, 0.0000],
        [0.3355, 0.0000, 0.1531,  ..., 0.1863, 0.0000, 0.0000],
        [0.3776, 0.0000, 0.1724,  ..., 0.2103, 0.0000, 0.0000],
        ...,
        [0.0143, 0.0059, 0.0102,  ..., 0.0026, 0.0000, 0.0000],
        [0.0143, 0.0059, 0.0102,  ..., 0.0026, 0.0000, 0.0000],
        [0.0143, 0.0059, 0.0102,  ..., 0.0026, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(391110.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10403.8867, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(143.1534, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2957.6831, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-451.3680, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5056.5713, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(473.1149, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0448],
        [ 0.0334],
        [ 0.0247],
        ...,
        [-0.5054],
        [-0.5036],
        [-0.5030]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-117603.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0003],
        [1.0015],
        ...,
        [0.9990],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366445.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2830],
        [0.0000],
        [0.2778],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(397.7091, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0003],
        [1.0016],
        ...,
        [0.9990],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366457.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2830],
        [0.0000],
        [0.2778],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(397.7091, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0009,  0.0023,  ...,  0.0068, -0.0019,  0.0010],
        [-0.0198,  0.0038,  0.0086,  ...,  0.0216, -0.0062,  0.0040],
        [ 0.0000, -0.0003, -0.0004,  ...,  0.0004,  0.0000, -0.0002],
        ...,
        [ 0.0000, -0.0003, -0.0004,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0003, -0.0004,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0003, -0.0004,  ...,  0.0004,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(596.7229, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-38.4869, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.6515, device='cuda:0')



h[100].sum tensor(-28.9897, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-8.2128, device='cuda:0')



h[200].sum tensor(-1.8043, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-43.6125, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0118, 0.0272,  ..., 0.0694, 0.0000, 0.0126],
        [0.0000, 0.0044, 0.0106,  ..., 0.0293, 0.0000, 0.0048],
        [0.0000, 0.0075, 0.0178,  ..., 0.0472, 0.0000, 0.0081],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55408.0586, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2692, 0.0000, 0.1207,  ..., 0.1482, 0.0000, 0.0000],
        [0.2112, 0.0000, 0.0944,  ..., 0.1152, 0.0000, 0.0000],
        [0.1870, 0.0000, 0.0837,  ..., 0.1013, 0.0000, 0.0000],
        ...,
        [0.0140, 0.0060, 0.0097,  ..., 0.0022, 0.0000, 0.0000],
        [0.0140, 0.0059, 0.0097,  ..., 0.0022, 0.0000, 0.0000],
        [0.0140, 0.0059, 0.0097,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(420382.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11300.2188, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(167.5888, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2910.4065, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-489.4897, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5452.6182, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(540.0734, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0511],
        [ 0.0501],
        [ 0.0499],
        ...,
        [-0.5109],
        [-0.5091],
        [-0.5085]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-117703.9844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0003],
        [1.0016],
        ...,
        [0.9990],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366457.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(372.3856, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0003],
        [1.0016],
        ...,
        [0.9990],
        [0.9983],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366469.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(372.3856, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0004, -0.0004,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0004,  ...,  0.0004,  0.0000, -0.0002],
        [-0.0064,  0.0010,  0.0025,  ...,  0.0072, -0.0020,  0.0011],
        ...,
        [ 0.0000, -0.0004, -0.0004,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0004,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0004,  ...,  0.0004,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(593.7534, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-36.2342, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.6362, device='cuda:0')



h[100].sum tensor(-27.2844, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-7.6899, device='cuda:0')



h[200].sum tensor(-2.9373, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-40.8355, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0010, 0.0025,  ..., 0.0083, 0.0000, 0.0011],
        [0.0000, 0.0037, 0.0089,  ..., 0.0245, 0.0000, 0.0041],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54993.5547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0336, 0.0029, 0.0174,  ..., 0.0135, 0.0000, 0.0000],
        [0.0696, 0.0005, 0.0319,  ..., 0.0342, 0.0000, 0.0000],
        [0.1185, 0.0000, 0.0525,  ..., 0.0623, 0.0000, 0.0000],
        ...,
        [0.0138, 0.0061, 0.0094,  ..., 0.0020, 0.0000, 0.0000],
        [0.0138, 0.0061, 0.0094,  ..., 0.0020, 0.0000, 0.0000],
        [0.0138, 0.0061, 0.0094,  ..., 0.0020, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(426263.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11343.1426, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(165.1699, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2974.7949, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-486.3827, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5508.4810, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(535.4442, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1149],
        [ 0.0176],
        [ 0.0913],
        ...,
        [-0.5170],
        [-0.5152],
        [-0.5146]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-123685.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0003],
        [1.0016],
        ...,
        [0.9990],
        [0.9983],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366469.5000, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 90.0 event: 450 loss: tensor(562.6111, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.1106, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0004],
        [1.0017],
        ...,
        [0.9990],
        [0.9983],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366480.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.1106, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0004, -0.0004,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0004,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0004,  ...,  0.0004,  0.0000, -0.0002],
        ...,
        [ 0.0000, -0.0004, -0.0004,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0004,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0004,  ...,  0.0004,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(519.3779, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.2643, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.8254, device='cuda:0')



h[100].sum tensor(-18.2655, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.1442, device='cuda:0')



h[200].sum tensor(-7.3472, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.3173, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45364.1016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0221, 0.0023, 0.0126,  ..., 0.0070, 0.0000, 0.0000],
        [0.0186, 0.0041, 0.0110,  ..., 0.0049, 0.0000, 0.0000],
        [0.0320, 0.0023, 0.0165,  ..., 0.0126, 0.0000, 0.0000],
        ...,
        [0.0139, 0.0063, 0.0097,  ..., 0.0021, 0.0000, 0.0000],
        [0.0139, 0.0063, 0.0097,  ..., 0.0021, 0.0000, 0.0000],
        [0.0139, 0.0063, 0.0097,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(394945.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10284.0771, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(129.2878, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3045.4531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-430.8306, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4964.9028, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(442.2773, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0109],
        [-0.0037],
        [ 0.0238],
        ...,
        [-0.5220],
        [-0.5201],
        [-0.5196]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-120862.6016, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0004],
        [1.0017],
        ...,
        [0.9990],
        [0.9983],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366480.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.4202, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0004],
        [1.0017],
        ...,
        [0.9990],
        [0.9983],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366480.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.4202, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0004, -0.0004,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0004,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0004,  ...,  0.0004,  0.0000, -0.0002],
        ...,
        [ 0.0000, -0.0004, -0.0004,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0004,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0004,  ...,  0.0004,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(496.1702, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.1392, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.6217, device='cuda:0')



h[100].sum tensor(-15.9130, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.5724, device='cuda:0')



h[200].sum tensor(-8.3645, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.2808, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39180.9258, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0376, 0.0031, 0.0201,  ..., 0.0157, 0.0000, 0.0000],
        [0.0280, 0.0017, 0.0155,  ..., 0.0103, 0.0000, 0.0000],
        [0.0478, 0.0014, 0.0233,  ..., 0.0217, 0.0000, 0.0000],
        ...,
        [0.0139, 0.0063, 0.0097,  ..., 0.0021, 0.0000, 0.0000],
        [0.0139, 0.0063, 0.0097,  ..., 0.0021, 0.0000, 0.0000],
        [0.0139, 0.0063, 0.0097,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(356414.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8686.0508, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(102.8038, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3282.6536, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-395.0510, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4259.3975, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(379.7966, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0032],
        [ 0.0177],
        [ 0.0450],
        ...,
        [-0.5200],
        [-0.5172],
        [-0.5100]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-145558.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0004],
        [1.0017],
        ...,
        [0.9990],
        [0.9983],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366480.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2817],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(185.8114, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0004],
        [1.0017],
        ...,
        [0.9990],
        [0.9983],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366491.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2817],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(185.8114, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0069,  0.0011,  0.0027,  ...,  0.0078, -0.0022,  0.0013],
        [-0.0055,  0.0008,  0.0021,  ...,  0.0063, -0.0017,  0.0010],
        [-0.0124,  0.0022,  0.0052,  ...,  0.0137, -0.0039,  0.0024],
        ...,
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(486.1883, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.8215, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.7877, device='cuda:0')



h[100].sum tensor(-13.4114, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.8371, device='cuda:0')



h[200].sum tensor(-10.0268, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-20.3759, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0036, 0.0085,  ..., 0.0236, 0.0000, 0.0040],
        [0.0000, 0.0079, 0.0188,  ..., 0.0499, 0.0000, 0.0088],
        [0.0000, 0.0032, 0.0080,  ..., 0.0237, 0.0000, 0.0037],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36498.0664, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1150, 0.0000, 0.0515,  ..., 0.0606, 0.0000, 0.0000],
        [0.1571, 0.0000, 0.0695,  ..., 0.0848, 0.0000, 0.0000],
        [0.1243, 0.0000, 0.0550,  ..., 0.0659, 0.0000, 0.0000],
        ...,
        [0.0142, 0.0065, 0.0101,  ..., 0.0022, 0.0000, 0.0000],
        [0.0142, 0.0065, 0.0101,  ..., 0.0022, 0.0000, 0.0000],
        [0.0142, 0.0065, 0.0101,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(347644.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8481.9150, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(94.7626, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3262.3784, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-380.8695, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4127.7817, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(357.6039, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0990],
        [ 0.1077],
        [ 0.0971],
        ...,
        [-0.5185],
        [-0.5077],
        [-0.4750]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-137094.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0004],
        [1.0017],
        ...,
        [0.9990],
        [0.9983],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366491.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(297.2622, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0004],
        [1.0018],
        ...,
        [0.9990],
        [0.9983],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366503.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(297.2622, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [-0.0076,  0.0012,  0.0030,  ...,  0.0085, -0.0024,  0.0014],
        ...,
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(577.7245, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.1927, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.6575, device='cuda:0')



h[100].sum tensor(-21.2095, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.1386, device='cuda:0')



h[200].sum tensor(-7.1132, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-32.5976, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0043, 0.0097,  ..., 0.0254, 0.0000, 0.0046],
        [0.0000, 0.0012, 0.0030,  ..., 0.0097, 0.0000, 0.0014],
        [0.0000, 0.0020, 0.0051,  ..., 0.0156, 0.0000, 0.0024],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46456.8008, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1250, 0.0000, 0.0597,  ..., 0.0658, 0.0000, 0.0000],
        [0.0847, 0.0000, 0.0404,  ..., 0.0428, 0.0000, 0.0000],
        [0.1005, 0.0000, 0.0462,  ..., 0.0521, 0.0000, 0.0000],
        ...,
        [0.0142, 0.0067, 0.0103,  ..., 0.0022, 0.0000, 0.0000],
        [0.0142, 0.0067, 0.0103,  ..., 0.0022, 0.0000, 0.0000],
        [0.0142, 0.0067, 0.0103,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(391842.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9971.6992, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(133.0375, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3257.9009, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-439.1547, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4929.5176, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(461.3734, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0727],
        [ 0.0844],
        [ 0.0939],
        ...,
        [-0.5304],
        [-0.5286],
        [-0.5280]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-141475.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0004],
        [1.0018],
        ...,
        [0.9990],
        [0.9983],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366503.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.3600, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0004],
        [1.0018],
        ...,
        [0.9990],
        [0.9983],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366514.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.3600, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [-0.0079,  0.0013,  0.0031,  ...,  0.0089, -0.0025,  0.0015],
        ...,
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(605.3163, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.3759, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.1774, device='cuda:0')



h[100].sum tensor(-22.8449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.5329, device='cuda:0')



h[200].sum tensor(-6.8034, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-34.6918, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0031,  ..., 0.0100, 0.0000, 0.0015],
        [0.0000, 0.0010, 0.0025,  ..., 0.0084, 0.0000, 0.0012],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50353.8086, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0249, 0.0031, 0.0142,  ..., 0.0085, 0.0000, 0.0000],
        [0.0449, 0.0004, 0.0221,  ..., 0.0201, 0.0000, 0.0000],
        [0.0610, 0.0000, 0.0284,  ..., 0.0295, 0.0000, 0.0000],
        ...,
        [0.0142, 0.0069, 0.0104,  ..., 0.0022, 0.0000, 0.0000],
        [0.0142, 0.0069, 0.0104,  ..., 0.0022, 0.0000, 0.0000],
        [0.0142, 0.0069, 0.0104,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(414068.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10828.9258, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(149.8247, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3185.5288, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-461.3084, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5352.0527, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(503.3359, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2166],
        [-0.0586],
        [ 0.0529],
        ...,
        [-0.5366],
        [-0.5347],
        [-0.5341]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-131516.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0004],
        [1.0018],
        ...,
        [0.9990],
        [0.9983],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366514.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(376.0889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0004],
        [1.0019],
        ...,
        [0.9990],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366525.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(376.0889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        ...,
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(665.6220, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-36.5405, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.9309, device='cuda:0')



h[100].sum tensor(-27.4726, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-7.7663, device='cuda:0')



h[200].sum tensor(-5.0286, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-41.2416, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54686.4102, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0140, 0.0071, 0.0107,  ..., 0.0022, 0.0000, 0.0000],
        [0.0140, 0.0071, 0.0107,  ..., 0.0022, 0.0000, 0.0000],
        [0.0141, 0.0071, 0.0107,  ..., 0.0022, 0.0000, 0.0000],
        ...,
        [0.0142, 0.0072, 0.0108,  ..., 0.0022, 0.0000, 0.0000],
        [0.0142, 0.0072, 0.0108,  ..., 0.0022, 0.0000, 0.0000],
        [0.0142, 0.0072, 0.0108,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(425838.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11253.4551, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(170.0641, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3167.8018, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-488.4212, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5611.1035, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(550.5865, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6878],
        [-0.6748],
        [-0.6429],
        ...,
        [-0.5380],
        [-0.5375],
        [-0.5374]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-122010.5391, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0004],
        [1.0019],
        ...,
        [0.9990],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366525.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.9006, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0004],
        [1.0019],
        ...,
        [0.9990],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366536., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.9006, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0065,  0.0010,  0.0025,  ...,  0.0074, -0.0020,  0.0012],
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0003,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0003,  0.0000, -0.0002],
        ...,
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0003,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0003,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0003,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(569.8151, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.4829, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.5699, device='cuda:0')



h[100].sum tensor(-17.6499, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.0779, device='cuda:0')



h[200].sum tensor(-9.4661, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.9653, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0038, 0.0089,  ..., 0.0248, 0.0000, 0.0043],
        [0.0000, 0.0010, 0.0025,  ..., 0.0084, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42196.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1209, 0.0000, 0.0556,  ..., 0.0644, 0.0000, 0.0000],
        [0.0698, 0.0008, 0.0339,  ..., 0.0347, 0.0000, 0.0000],
        [0.0358, 0.0022, 0.0200,  ..., 0.0149, 0.0000, 0.0000],
        ...,
        [0.0140, 0.0075, 0.0109,  ..., 0.0021, 0.0000, 0.0000],
        [0.0140, 0.0075, 0.0109,  ..., 0.0021, 0.0000, 0.0000],
        [0.0140, 0.0075, 0.0109,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(374433.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9236.0781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(122.0332, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3462.5203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-415.9319, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4720.5938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(426.0595, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0779],
        [ 0.0018],
        [-0.1196],
        ...,
        [-0.5516],
        [-0.5496],
        [-0.5490]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-140125.8906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0004],
        [1.0019],
        ...,
        [0.9990],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366536., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.9150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.5858, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0005],
        [1.0020],
        ...,
        [0.9990],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366547.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.9150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.5858, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0378,  0.0075,  0.0168,  ...,  0.0411, -0.0116,  0.0080],
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0003,  0.0000, -0.0002],
        [-0.0224,  0.0043,  0.0098,  ...,  0.0245, -0.0069,  0.0046],
        ...,
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0003,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0003,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0003,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(606.0278, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.7479, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.0120, device='cuda:0')



h[100].sum tensor(-20.0976, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.7116, device='cuda:0')



h[200].sum tensor(-8.5652, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.3302, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0128, 0.0290,  ..., 0.0731, 0.0000, 0.0137],
        [0.0000, 0.0187, 0.0424,  ..., 0.1058, 0.0000, 0.0201],
        [0.0000, 0.0035, 0.0079,  ..., 0.0211, 0.0000, 0.0037],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45869.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3566, 0.0000, 0.1669,  ..., 0.1996, 0.0000, 0.0000],
        [0.2908, 0.0000, 0.1367,  ..., 0.1616, 0.0000, 0.0000],
        [0.1428, 0.0000, 0.0690,  ..., 0.0763, 0.0000, 0.0000],
        ...,
        [0.0138, 0.0077, 0.0107,  ..., 0.0019, 0.0000, 0.0000],
        [0.0138, 0.0077, 0.0107,  ..., 0.0019, 0.0000, 0.0000],
        [0.0138, 0.0077, 0.0107,  ..., 0.0019, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(397923.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9889.1211, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(134.7315, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3524.8901, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-436.2481, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5108.4590, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(463.1292, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0438],
        [ 0.0487],
        [ 0.0157],
        ...,
        [-0.5588],
        [-0.5568],
        [-0.5562]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-152730.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0005],
        [1.0020],
        ...,
        [0.9990],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366547.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(321.4385, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0005],
        [1.0020],
        ...,
        [0.9990],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366558.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(321.4385, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [-0.0106,  0.0019,  0.0043,  ...,  0.0118, -0.0033,  0.0021],
        ...,
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(656.7334, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-31.3333, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.5816, device='cuda:0')



h[100].sum tensor(-23.5358, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.6378, device='cuda:0')



h[200].sum tensor(-7.1988, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-35.2487, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0019, 0.0043,  ..., 0.0129, 0.0000, 0.0021],
        [0.0000, 0.0070, 0.0159,  ..., 0.0412, 0.0000, 0.0076],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52355.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[5.3833e-02, 1.7733e-03, 2.7718e-02,  ..., 2.5202e-02, 0.0000e+00,
         0.0000e+00],
        [1.3269e-01, 2.4204e-04, 6.3291e-02,  ..., 7.0707e-02, 0.0000e+00,
         0.0000e+00],
        [2.6732e-01, 0.0000e+00, 1.2515e-01,  ..., 1.4819e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.3658e-02, 7.8227e-03, 1.0493e-02,  ..., 1.8073e-03, 0.0000e+00,
         0.0000e+00],
        [1.3657e-02, 7.8223e-03, 1.0492e-02,  ..., 1.8071e-03, 0.0000e+00,
         0.0000e+00],
        [1.3656e-02, 7.8221e-03, 1.0492e-02,  ..., 1.8070e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(427388., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10913.2871, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(160.1264, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3421.9312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-474.6750, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5650.6011, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(528.4735, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0258],
        [ 0.0246],
        [ 0.0071],
        ...,
        [-0.5648],
        [-0.5628],
        [-0.5622]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-141350.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0005],
        [1.0020],
        ...,
        [0.9990],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366558.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(181.1398, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0005],
        [1.0020],
        ...,
        [0.9990],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366569.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(181.1398, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        ...,
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(553.1311, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.2172, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.4159, device='cuda:0')



h[100].sum tensor(-12.9285, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.7406, device='cuda:0')



h[200].sum tensor(-11.9673, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.8637, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0044, 0.0100,  ..., 0.0263, 0.0000, 0.0048],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36634.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0192, 0.0061, 0.0128,  ..., 0.0050, 0.0000, 0.0000],
        [0.0384, 0.0039, 0.0215,  ..., 0.0161, 0.0000, 0.0000],
        [0.0995, 0.0000, 0.0493,  ..., 0.0513, 0.0000, 0.0000],
        ...,
        [0.0136, 0.0080, 0.0103,  ..., 0.0017, 0.0000, 0.0000],
        [0.0136, 0.0080, 0.0103,  ..., 0.0017, 0.0000, 0.0000],
        [0.0136, 0.0080, 0.0103,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(359034.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8330.4492, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(95.1159, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3665.6116, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-383.5119, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4449.8916, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(368.4222, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4351],
        [-0.2356],
        [-0.0491],
        ...,
        [-0.4982],
        [-0.5484],
        [-0.5620]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-159569.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0005],
        [1.0020],
        ...,
        [0.9990],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366569.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 100.0 event: 500 loss: tensor(464.7567, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.6600, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0005],
        [1.0020],
        ...,
        [0.9990],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366580.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.6600, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [-0.0064,  0.0010,  0.0024,  ...,  0.0073, -0.0020,  0.0012],
        ...,
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(575.5259, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.4482, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.3328, device='cuda:0')



h[100].sum tensor(-13.8486, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.9785, device='cuda:0')



h[200].sum tensor(-11.7054, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.1270, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0010, 0.0024,  ..., 0.0084, 0.0000, 0.0012],
        [0.0000, 0.0007, 0.0019,  ..., 0.0072, 0.0000, 0.0009],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37852.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0204, 0.0043, 0.0127,  ..., 0.0057, 0.0000, 0.0000],
        [0.0361, 0.0012, 0.0188,  ..., 0.0149, 0.0000, 0.0000],
        [0.0443, 0.0000, 0.0217,  ..., 0.0197, 0.0000, 0.0000],
        ...,
        [0.0135, 0.0080, 0.0101,  ..., 0.0015, 0.0000, 0.0000],
        [0.0135, 0.0080, 0.0101,  ..., 0.0015, 0.0000, 0.0000],
        [0.0142, 0.0077, 0.0103,  ..., 0.0019, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(364730.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8535.0996, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(98.8269, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3608.0344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-390.2091, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4536.7188, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(377.4672, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3670],
        [-0.2376],
        [-0.0846],
        ...,
        [-0.5632],
        [-0.5403],
        [-0.5083]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-154859., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0005],
        [1.0020],
        ...,
        [0.9990],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366580.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2849],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(363.3357, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0005],
        [1.0021],
        ...,
        [0.9990],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366591.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2849],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(363.3357, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0069,  0.0011,  0.0027,  ...,  0.0079, -0.0021,  0.0013],
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [-0.0069,  0.0011,  0.0027,  ...,  0.0079, -0.0021,  0.0013],
        ...,
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(721.9751, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-34.3740, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.9159, device='cuda:0')



h[100].sum tensor(-25.7956, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-7.5030, device='cuda:0')



h[200].sum tensor(-6.5493, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-39.8431, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0038, 0.0088,  ..., 0.0247, 0.0000, 0.0043],
        [0.0000, 0.0060, 0.0142,  ..., 0.0398, 0.0000, 0.0069],
        [0.0000, 0.0019, 0.0044,  ..., 0.0132, 0.0000, 0.0021],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55536.8008, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1295, 0.0000, 0.0581,  ..., 0.0693, 0.0000, 0.0000],
        [0.1293, 0.0000, 0.0574,  ..., 0.0692, 0.0000, 0.0000],
        [0.0833, 0.0000, 0.0382,  ..., 0.0424, 0.0000, 0.0000],
        ...,
        [0.0134, 0.0082, 0.0099,  ..., 0.0013, 0.0000, 0.0000],
        [0.0134, 0.0082, 0.0099,  ..., 0.0013, 0.0000, 0.0000],
        [0.0134, 0.0082, 0.0099,  ..., 0.0013, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(439848.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11524.8340, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(170.4813, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3163.1335, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-492.5633, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5897.9883, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(553.9687, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1341],
        [ 0.1244],
        [ 0.0832],
        ...,
        [-0.5807],
        [-0.5786],
        [-0.5779]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-112914.2031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0005],
        [1.0021],
        ...,
        [0.9990],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366591.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.3185, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0005],
        [1.0021],
        ...,
        [0.9990],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366591.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.3185, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        ...,
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(668.0339, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.1157, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.4232, device='cuda:0')



h[100].sum tensor(-21.0991, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.0778, device='cuda:0')



h[200].sum tensor(-8.6082, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-32.2747, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0009, 0.0022,  ..., 0.0079, 0.0000, 0.0011],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47575.7148, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0257, 0.0023, 0.0138,  ..., 0.0089, 0.0000, 0.0000],
        [0.0299, 0.0020, 0.0160,  ..., 0.0112, 0.0000, 0.0000],
        [0.0500, 0.0009, 0.0247,  ..., 0.0228, 0.0000, 0.0000],
        ...,
        [0.0154, 0.0070, 0.0108,  ..., 0.0025, 0.0000, 0.0000],
        [0.0134, 0.0082, 0.0099,  ..., 0.0013, 0.0000, 0.0000],
        [0.0134, 0.0082, 0.0099,  ..., 0.0013, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(406659.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9975.7969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(137.2052, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3509.5273, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-447.0765, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5239.1328, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(473.5839, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0712],
        [-0.0621],
        [-0.0158],
        ...,
        [-0.5244],
        [-0.5613],
        [-0.5750]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-147609.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0005],
        [1.0021],
        ...,
        [0.9990],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366591.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3997],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(169.7075, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0005],
        [1.0021],
        ...,
        [0.9990],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366591.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3997],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(169.7075, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [-0.0097,  0.0017,  0.0040,  ...,  0.0109, -0.0030,  0.0019],
        [-0.0084,  0.0014,  0.0033,  ...,  0.0094, -0.0026,  0.0016],
        ...,
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0004, -0.0005,  ...,  0.0004,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(565.6781, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.2403, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.5061, device='cuda:0')



h[100].sum tensor(-12.1874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.5045, device='cuda:0')



h[200].sum tensor(-12.5152, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.6100, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0040,  ..., 0.0121, 0.0000, 0.0019],
        [0.0000, 0.0041, 0.0096,  ..., 0.0266, 0.0000, 0.0047],
        [0.0000, 0.0106, 0.0243,  ..., 0.0636, 0.0000, 0.0117],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36124.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1022, 0.0000, 0.0485,  ..., 0.0529, 0.0000, 0.0000],
        [0.1438, 0.0000, 0.0667,  ..., 0.0771, 0.0000, 0.0000],
        [0.2174, 0.0000, 0.0992,  ..., 0.1196, 0.0000, 0.0000],
        ...,
        [0.0134, 0.0082, 0.0099,  ..., 0.0013, 0.0000, 0.0000],
        [0.0134, 0.0082, 0.0099,  ..., 0.0013, 0.0000, 0.0000],
        [0.0134, 0.0082, 0.0099,  ..., 0.0013, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(360506.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8218.0156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(90.7939, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3716.3325, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-380.7857, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4426.7500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(357.8743, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0924],
        [ 0.0950],
        [ 0.0951],
        ...,
        [-0.5814],
        [-0.5789],
        [-0.5743]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-162238.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0005],
        [1.0021],
        ...,
        [0.9990],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366591.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5122],
        [0.4766],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.9486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0005],
        [1.0021],
        ...,
        [0.9990],
        [0.9982],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366602.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5122],
        [0.4766],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.9486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0125,  0.0023,  0.0052,  ...,  0.0139, -0.0038,  0.0025],
        [-0.0212,  0.0041,  0.0092,  ...,  0.0233, -0.0065,  0.0044],
        [-0.0220,  0.0043,  0.0096,  ...,  0.0243, -0.0067,  0.0046],
        ...,
        [ 0.0000, -0.0003, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0003, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0003, -0.0005,  ...,  0.0004,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(616.0029, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.0058, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.6637, device='cuda:0')



h[100].sum tensor(-15.7587, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.5833, device='cuda:0')



h[200].sum tensor(-11.0247, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.3387, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0124, 0.0282,  ..., 0.0729, 0.0000, 0.0136],
        [0.0000, 0.0170, 0.0383,  ..., 0.0966, 0.0000, 0.0183],
        [0.0000, 0.0148, 0.0337,  ..., 0.0856, 0.0000, 0.0161],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41778.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3282, 0.0000, 0.1499,  ..., 0.1838, 0.0000, 0.0000],
        [0.3474, 0.0000, 0.1599,  ..., 0.1947, 0.0000, 0.0000],
        [0.2952, 0.0000, 0.1363,  ..., 0.1644, 0.0000, 0.0000],
        ...,
        [0.0133, 0.0083, 0.0100,  ..., 0.0013, 0.0000, 0.0000],
        [0.0133, 0.0083, 0.0100,  ..., 0.0013, 0.0000, 0.0000],
        [0.0133, 0.0083, 0.0100,  ..., 0.0013, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(390856.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9298.3242, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(112.2916, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3638.2842, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-414.6537, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4994.8516, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(414.6903, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0950],
        [ 0.0917],
        [ 0.0902],
        ...,
        [-0.5883],
        [-0.5862],
        [-0.5855]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-157032.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0005],
        [1.0021],
        ...,
        [0.9990],
        [0.9982],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366602.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3884],
        [0.6616],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.1788, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0005],
        [1.0022],
        ...,
        [0.9989],
        [0.9982],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366613.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3884],
        [0.6616],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.1788, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0161,  0.0030,  0.0069,  ...,  0.0178, -0.0049,  0.0033],
        [-0.0095,  0.0016,  0.0038,  ...,  0.0106, -0.0029,  0.0019],
        [-0.0334,  0.0067,  0.0149,  ...,  0.0367, -0.0102,  0.0071],
        ...,
        [ 0.0000, -0.0003, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0003, -0.0005,  ...,  0.0004,  0.0000, -0.0002],
        [ 0.0000, -0.0003, -0.0005,  ...,  0.0004,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(633.9888, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.1639, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.5575, device='cuda:0')



h[100].sum tensor(-16.6223, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.8152, device='cuda:0')



h[200].sum tensor(-10.6895, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.5702, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0086, 0.0198,  ..., 0.0530, 0.0000, 0.0096],
        [0.0000, 0.0162, 0.0365,  ..., 0.0922, 0.0000, 0.0175],
        [0.0000, 0.0067, 0.0154,  ..., 0.0414, 0.0000, 0.0075],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42054.1602, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1774, 0.0000, 0.0826,  ..., 0.0966, 0.0000, 0.0000],
        [0.2403, 0.0000, 0.1122,  ..., 0.1329, 0.0000, 0.0000],
        [0.1966, 0.0000, 0.0918,  ..., 0.1076, 0.0000, 0.0000],
        ...,
        [0.0134, 0.0084, 0.0101,  ..., 0.0012, 0.0000, 0.0000],
        [0.0134, 0.0084, 0.0101,  ..., 0.0012, 0.0000, 0.0000],
        [0.0134, 0.0084, 0.0101,  ..., 0.0012, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(386742.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9248.0645, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(113.7921, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3596.5566, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-417.4101, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4942.5801, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(417.7642, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0054],
        [ 0.0577],
        [ 0.0803],
        ...,
        [-0.5958],
        [-0.5936],
        [-0.5930]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-145615.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0005],
        [1.0022],
        ...,
        [0.9989],
        [0.9982],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366613.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.7400, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0006],
        [1.0022],
        ...,
        [0.9989],
        [0.9982],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366624.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.7400, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0003, -0.0006,  ...,  0.0003,  0.0000, -0.0002],
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0003,  0.0000, -0.0002],
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0003,  0.0000, -0.0002],
        ...,
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0003,  0.0000, -0.0002],
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0003,  0.0000, -0.0002],
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0003,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(626.6932, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.2766, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.9308, device='cuda:0')



h[100].sum tensor(-15.2021, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.3931, device='cuda:0')



h[200].sum tensor(-11.3355, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.3289, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0013, 0.0029,  ..., 0.0096, 0.0000, 0.0015],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40043.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0557, 0.0009, 0.0274,  ..., 0.0262, 0.0000, 0.0000],
        [0.0290, 0.0038, 0.0165,  ..., 0.0105, 0.0000, 0.0000],
        [0.0169, 0.0068, 0.0116,  ..., 0.0032, 0.0000, 0.0000],
        ...,
        [0.0136, 0.0086, 0.0105,  ..., 0.0012, 0.0000, 0.0000],
        [0.0136, 0.0086, 0.0105,  ..., 0.0012, 0.0000, 0.0000],
        [0.0136, 0.0086, 0.0105,  ..., 0.0012, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(378198., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8978.8486, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(107.1997, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3651.0181, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-408.1565, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4820.2026, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(401.7615, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0252],
        [-0.0671],
        [-0.1544],
        ...,
        [-0.6024],
        [-0.6003],
        [-0.5997]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-146051.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0006],
        [1.0022],
        ...,
        [0.9989],
        [0.9982],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366624.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(419.5275, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [1.0006],
        [1.0023],
        ...,
        [0.9989],
        [0.9981],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366635.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(419.5275, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0052,  0.0008,  0.0019,  ...,  0.0060, -0.0016,  0.0010],
        [-0.0052,  0.0008,  0.0019,  ...,  0.0060, -0.0016,  0.0010],
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0003,  0.0000, -0.0002],
        ...,
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0003,  0.0000, -0.0002],
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0003,  0.0000, -0.0002],
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0003,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(810.2593, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-39.8461, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.3880, device='cuda:0')



h[100].sum tensor(-29.8647, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-8.6634, device='cuda:0')



h[200].sum tensor(-4.8619, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-46.0051, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0057, 0.0132,  ..., 0.0376, 0.0000, 0.0066],
        [0.0000, 0.0032, 0.0075,  ..., 0.0228, 0.0000, 0.0038],
        [0.0000, 0.0070, 0.0160,  ..., 0.0442, 0.0000, 0.0079],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61993.9180, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1660, 0.0000, 0.0744,  ..., 0.0910, 0.0000, 0.0000],
        [0.1509, 0.0000, 0.0679,  ..., 0.0822, 0.0000, 0.0000],
        [0.1747, 0.0000, 0.0792,  ..., 0.0958, 0.0000, 0.0000],
        ...,
        [0.0137, 0.0088, 0.0106,  ..., 0.0011, 0.0000, 0.0000],
        [0.0137, 0.0088, 0.0106,  ..., 0.0011, 0.0000, 0.0000],
        [0.0137, 0.0088, 0.0106,  ..., 0.0011, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(486355.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12524.7773, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(194.6920, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3602.2791, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-539.5997, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6703.5562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(625.9707, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1251],
        [ 0.1240],
        [ 0.1198],
        ...,
        [-0.6110],
        [-0.6089],
        [-0.6082]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-156148.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [1.0006],
        [1.0023],
        ...,
        [0.9989],
        [0.9981],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366635.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(284.3948, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [1.0006],
        [1.0023],
        ...,
        [0.9989],
        [0.9981],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366646.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(284.3948, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0003, -0.0006,  ...,  0.0003,  0.0000, -0.0002],
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0003,  0.0000, -0.0002],
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0003,  0.0000, -0.0002],
        ...,
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0003,  0.0000, -0.0002],
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0003,  0.0000, -0.0002],
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0003,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(697.2573, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.0136, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.6335, device='cuda:0')



h[100].sum tensor(-20.2404, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.8728, device='cuda:0')



h[200].sum tensor(-9.0594, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-31.1865, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0102,  ..., 0.0266, 0.0000, 0.0049],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47107.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0337, 0.0011, 0.0193,  ..., 0.0129, 0.0000, 0.0000],
        [0.0565, 0.0021, 0.0299,  ..., 0.0261, 0.0000, 0.0000],
        [0.1212, 0.0000, 0.0597,  ..., 0.0637, 0.0000, 0.0000],
        ...,
        [0.0136, 0.0091, 0.0107,  ..., 0.0009, 0.0000, 0.0000],
        [0.0136, 0.0091, 0.0107,  ..., 0.0009, 0.0000, 0.0000],
        [0.0136, 0.0091, 0.0107,  ..., 0.0009, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(411911.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10100.0508, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(138.9913, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3691.3838, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-454.2170, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5424.9043, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(476.6341, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0694],
        [ 0.0721],
        [ 0.0784],
        ...,
        [-0.5744],
        [-0.5886],
        [-0.6015]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-150180.4844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [1.0006],
        [1.0023],
        ...,
        [0.9989],
        [0.9981],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366646.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(340.5537, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [1.0006],
        [1.0024],
        ...,
        [0.9988],
        [0.9981],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366657.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(340.5537, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0003, -0.0006,  ...,  0.0003,  0.0000, -0.0002],
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0003,  0.0000, -0.0002],
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0003,  0.0000, -0.0002],
        ...,
        [-0.0141,  0.0027,  0.0060,  ...,  0.0157, -0.0043,  0.0029],
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0003,  0.0000, -0.0002],
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0003,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(754.3504, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-32.7004, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.1029, device='cuda:0')



h[100].sum tensor(-24.4936, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-7.0325, device='cuda:0')



h[200].sum tensor(-7.0691, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-37.3449, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0040, 0.0090,  ..., 0.0264, 0.0000, 0.0045],
        [0.0000, 0.0036, 0.0082,  ..., 0.0231, 0.0000, 0.0040],
        [0.0000, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55040.6172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0202, 0.0065, 0.0134,  ..., 0.0049, 0.0000, 0.0000],
        [0.0132, 0.0091, 0.0105,  ..., 0.0008, 0.0000, 0.0000],
        [0.0133, 0.0091, 0.0106,  ..., 0.0008, 0.0000, 0.0000],
        ...,
        [0.1436, 0.0000, 0.0670,  ..., 0.0775, 0.0000, 0.0000],
        [0.1083, 0.0000, 0.0520,  ..., 0.0566, 0.0000, 0.0000],
        [0.0451, 0.0041, 0.0244,  ..., 0.0195, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(456271.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11563.5322, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(171.8542, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3662.8230, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-502.8467, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6202.4199, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(557.5806, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2390],
        [-0.3765],
        [-0.4642],
        ...,
        [ 0.0545],
        [-0.0387],
        [-0.2114]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-149258.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [1.0006],
        [1.0024],
        ...,
        [0.9988],
        [0.9981],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366657.7500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 110.0 event: 550 loss: tensor(407.2693, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2766],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.9872, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [1.0007],
        [1.0024],
        ...,
        [0.9988],
        [0.9981],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366668.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2766],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.9872, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0052,  0.0008,  0.0019,  ...,  0.0060, -0.0016,  0.0010],
        [-0.0067,  0.0011,  0.0025,  ...,  0.0076, -0.0020,  0.0013],
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0003,  0.0000, -0.0002],
        ...,
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0003,  0.0000, -0.0002],
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0003,  0.0000, -0.0002],
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0003,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(714.0776, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.8895, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.7602, device='cuda:0')



h[100].sum tensor(-20.1347, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.9057, device='cuda:0')



h[200].sum tensor(-8.9405, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-31.3611, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0098, 0.0218,  ..., 0.0577, 0.0000, 0.0107],
        [0.0000, 0.0039, 0.0087,  ..., 0.0244, 0.0000, 0.0043],
        [0.0000, 0.0011, 0.0025,  ..., 0.0085, 0.0000, 0.0013],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46829.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1913, 0.0000, 0.0896,  ..., 0.1058, 0.0000, 0.0000],
        [0.1319, 0.0000, 0.0630,  ..., 0.0709, 0.0000, 0.0000],
        [0.0738, 0.0012, 0.0372,  ..., 0.0366, 0.0000, 0.0000],
        ...,
        [0.0135, 0.0094, 0.0110,  ..., 0.0009, 0.0000, 0.0000],
        [0.0135, 0.0094, 0.0110,  ..., 0.0009, 0.0000, 0.0000],
        [0.0135, 0.0094, 0.0110,  ..., 0.0009, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(414229.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10145.8418, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(139.5882, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3776.3994, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-456.4968, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5485.4619, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(476.8598, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0881],
        [ 0.0663],
        [-0.0139],
        ...,
        [-0.6360],
        [-0.6337],
        [-0.6330]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-153293.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [1.0007],
        [1.0024],
        ...,
        [0.9988],
        [0.9981],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366668.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(359.2189, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [1.0007],
        [1.0025],
        ...,
        [0.9988],
        [0.9980],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366680., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(359.2189, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0003, -0.0006,  ...,  0.0003,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0003,  0.0000, -0.0001],
        [-0.0052,  0.0008,  0.0019,  ...,  0.0060, -0.0016,  0.0010],
        ...,
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0003,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0003,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0003,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(807.0099, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-34.1855, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.5883, device='cuda:0')



h[100].sum tensor(-25.5899, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-7.4180, device='cuda:0')



h[200].sum tensor(-6.4736, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-39.3917, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0006, 0.0014,  ..., 0.0059, 0.0000, 0.0008],
        [0.0000, 0.0038, 0.0085,  ..., 0.0252, 0.0000, 0.0043],
        [0.0000, 0.0072, 0.0160,  ..., 0.0427, 0.0000, 0.0078],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56513.4297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0693, 0.0006, 0.0345,  ..., 0.0347, 0.0000, 0.0000],
        [0.1275, 0.0000, 0.0599,  ..., 0.0692, 0.0000, 0.0000],
        [0.1742, 0.0000, 0.0813,  ..., 0.0966, 0.0000, 0.0000],
        ...,
        [0.0138, 0.0093, 0.0116,  ..., 0.0013, 0.0000, 0.0000],
        [0.0138, 0.0093, 0.0116,  ..., 0.0013, 0.0000, 0.0000],
        [0.0138, 0.0093, 0.0116,  ..., 0.0013, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(459209.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11923.4502, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(179.0994, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3536.2324, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-515.9289, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6329.3237, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(577.8649, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0223],
        [ 0.0606],
        [ 0.0859],
        ...,
        [-0.6386],
        [-0.6364],
        [-0.6357]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-136172.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [1.0007],
        [1.0025],
        ...,
        [0.9988],
        [0.9980],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366680., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.4399, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [1.0007],
        [1.0025],
        ...,
        [0.9988],
        [0.9980],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366691.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.4399, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0003, -0.0006,  ...,  0.0003,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0003,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0003,  0.0000, -0.0001],
        ...,
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0003,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0003,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0003,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(712.9327, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.4119, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.0211, device='cuda:0')



h[100].sum tensor(-16.0230, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.6761, device='cuda:0')



h[200].sum tensor(-10.6505, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.8312, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42752.8789, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0139, 0.0091, 0.0119,  ..., 0.0017, 0.0000, 0.0000],
        [0.0140, 0.0091, 0.0119,  ..., 0.0017, 0.0000, 0.0000],
        [0.0141, 0.0091, 0.0120,  ..., 0.0017, 0.0000, 0.0000],
        ...,
        [0.0142, 0.0092, 0.0121,  ..., 0.0017, 0.0000, 0.0000],
        [0.0142, 0.0092, 0.0121,  ..., 0.0017, 0.0000, 0.0000],
        [0.0142, 0.0092, 0.0121,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(400470.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9615.3379, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(121.0773, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3875.0698, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-438.6459, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5283.6494, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(440.6854, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7263],
        [-0.6485],
        [-0.5345],
        ...,
        [-0.6396],
        [-0.6374],
        [-0.6367]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-160105.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [1.0007],
        [1.0025],
        ...,
        [0.9988],
        [0.9980],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366691.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(279.1526, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [1.0008],
        [1.0026],
        ...,
        [0.9988],
        [0.9980],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366702.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(279.1526, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0003, -0.0006,  ...,  0.0004,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0004,  0.0000, -0.0001],
        [-0.0063,  0.0011,  0.0024,  ...,  0.0073, -0.0019,  0.0013],
        ...,
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0004,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0004,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0004,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(787.2961, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.1984, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.2163, device='cuda:0')



h[100].sum tensor(-19.5987, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.7646, device='cuda:0')



h[200].sum tensor(-9.0240, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.6117, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0011, 0.0024,  ..., 0.0084, 0.0000, 0.0013],
        [0.0000, 0.0017, 0.0038,  ..., 0.0118, 0.0000, 0.0019],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48216.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0262, 0.0045, 0.0172,  ..., 0.0092, 0.0000, 0.0000],
        [0.0549, 0.0011, 0.0293,  ..., 0.0265, 0.0000, 0.0000],
        [0.0818, 0.0000, 0.0405,  ..., 0.0426, 0.0000, 0.0000],
        ...,
        [0.0145, 0.0091, 0.0124,  ..., 0.0020, 0.0000, 0.0000],
        [0.0145, 0.0091, 0.0124,  ..., 0.0020, 0.0000, 0.0000],
        [0.0145, 0.0091, 0.0124,  ..., 0.0020, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(421054.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10716.2207, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(143.0951, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3482.2554, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-471.7549, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5671.1411, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(496.4865, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3775],
        [-0.1762],
        [-0.0185],
        ...,
        [-0.6396],
        [-0.6374],
        [-0.6367]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-123275.2656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [1.0008],
        [1.0026],
        ...,
        [0.9988],
        [0.9980],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366702.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(257.6786, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [1.0008],
        [1.0027],
        ...,
        [0.9988],
        [0.9980],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366713.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(257.6786, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0003, -0.0006,  ...,  0.0004,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0004,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0004,  0.0000, -0.0001],
        ...,
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0004,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0004,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0004,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(787.3919, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.1954, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.5073, device='cuda:0')



h[100].sum tensor(-18.0946, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.3211, device='cuda:0')



h[200].sum tensor(-9.6538, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.2568, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0019, 0.0044,  ..., 0.0145, 0.0000, 0.0023],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44749.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0193, 0.0062, 0.0141,  ..., 0.0051, 0.0000, 0.0000],
        [0.0350, 0.0039, 0.0207,  ..., 0.0146, 0.0000, 0.0000],
        [0.0747, 0.0000, 0.0373,  ..., 0.0385, 0.0000, 0.0000],
        ...,
        [0.0145, 0.0089, 0.0122,  ..., 0.0021, 0.0000, 0.0000],
        [0.0145, 0.0089, 0.0122,  ..., 0.0021, 0.0000, 0.0000],
        [0.0145, 0.0089, 0.0122,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(405521.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9996.4844, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(125.5813, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3576.4514, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-453.1523, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5332.1436, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(459.6081, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4206],
        [-0.2281],
        [-0.0510],
        ...,
        [-0.6279],
        [-0.6224],
        [-0.6203]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-134637.0156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [1.0008],
        [1.0027],
        ...,
        [0.9988],
        [0.9980],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366713.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.8618],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(411.6315, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [1.0008],
        [1.0027],
        ...,
        [0.9988],
        [0.9980],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366725.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.8618],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(411.6315, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0338,  0.0069,  0.0153,  ...,  0.0375, -0.0101,  0.0073],
        [-0.0085,  0.0015,  0.0034,  ...,  0.0097, -0.0025,  0.0017],
        [-0.0275,  0.0055,  0.0123,  ...,  0.0306, -0.0082,  0.0059],
        ...,
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0004,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0004,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0006,  ...,  0.0004,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(944.8317, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-38.8251, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.7596, device='cuda:0')



h[100].sum tensor(-29.0263, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-8.5003, device='cuda:0')



h[200].sum tensor(-4.6128, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-45.1392, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0116, 0.0261,  ..., 0.0681, 0.0000, 0.0128],
        [0.0000, 0.0277, 0.0615,  ..., 0.1512, 0.0000, 0.0294],
        [0.0000, 0.0171, 0.0382,  ..., 0.0965, 0.0000, 0.0185],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61794.8516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3525, 0.0000, 0.1689,  ..., 0.2022, 0.0000, 0.0000],
        [0.4706, 0.0000, 0.2269,  ..., 0.2712, 0.0000, 0.0000],
        [0.4328, 0.0000, 0.2084,  ..., 0.2489, 0.0000, 0.0000],
        ...,
        [0.0142, 0.0090, 0.0120,  ..., 0.0019, 0.0000, 0.0000],
        [0.0142, 0.0090, 0.0120,  ..., 0.0019, 0.0000, 0.0000],
        [0.0142, 0.0090, 0.0120,  ..., 0.0019, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(482278.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12765.5215, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(192.7402, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3255.2993, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-553.8697, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6657.6401, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(629.9005, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0449],
        [ 0.0294],
        [ 0.0176],
        ...,
        [-0.6408],
        [-0.6324],
        [-0.6255]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-117313.5469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [1.0008],
        [1.0027],
        ...,
        [0.9988],
        [0.9980],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366725.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8110],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.4360, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [1.0008],
        [1.0028],
        ...,
        [0.9988],
        [0.9981],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366736.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8110],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.4360, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0003, -0.0005,  ...,  0.0004,  0.0000, -0.0001],
        [-0.0195,  0.0038,  0.0086,  ...,  0.0219, -0.0058,  0.0042],
        [ 0.0000, -0.0003, -0.0005,  ...,  0.0004,  0.0000, -0.0001],
        ...,
        [ 0.0000, -0.0003, -0.0005,  ...,  0.0004,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0005,  ...,  0.0004,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0005,  ...,  0.0004,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(776.2599, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.9491, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.7371, device='cuda:0')



h[100].sum tensor(-16.4043, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.8618, device='cuda:0')



h[200].sum tensor(-9.9168, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.8177, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0038, 0.0086,  ..., 0.0230, 0.0000, 0.0042],
        [0.0000, 0.0031, 0.0069,  ..., 0.0191, 0.0000, 0.0034],
        [0.0000, 0.0138, 0.0310,  ..., 0.0796, 0.0000, 0.0151],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43517.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0825, 0.0000, 0.0437,  ..., 0.0424, 0.0000, 0.0000],
        [0.1020, 0.0000, 0.0526,  ..., 0.0539, 0.0000, 0.0000],
        [0.1652, 0.0000, 0.0818,  ..., 0.0912, 0.0000, 0.0000],
        ...,
        [0.0139, 0.0093, 0.0119,  ..., 0.0017, 0.0000, 0.0000],
        [0.0139, 0.0093, 0.0119,  ..., 0.0017, 0.0000, 0.0000],
        [0.0139, 0.0093, 0.0119,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(406782.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9711.0371, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(119.3704, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3794.4116, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-448.2159, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5305.4414, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(445.5593, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2656],
        [-0.1508],
        [-0.0525],
        ...,
        [-0.6612],
        [-0.6589],
        [-0.6582]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-151263.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [1.0008],
        [1.0028],
        ...,
        [0.9988],
        [0.9981],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366736.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.4908, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [1.0009],
        [1.0028],
        ...,
        [0.9988],
        [0.9981],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366747.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.4908, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0003, -0.0005,  ...,  0.0004,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0005,  ...,  0.0004,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0005,  ...,  0.0004,  0.0000, -0.0001],
        ...,
        [ 0.0000, -0.0003, -0.0005,  ...,  0.0004,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0005,  ...,  0.0004,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0005,  ...,  0.0004,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(780.1876, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.6508, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.0598, device='cuda:0')



h[100].sum tensor(-16.9234, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.9456, device='cuda:0')



h[200].sum tensor(-9.2704, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.2624, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43371.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0132, 0.0095, 0.0115,  ..., 0.0013, 0.0000, 0.0000],
        [0.0138, 0.0093, 0.0117,  ..., 0.0017, 0.0000, 0.0000],
        [0.0155, 0.0085, 0.0121,  ..., 0.0028, 0.0000, 0.0000],
        ...,
        [0.0134, 0.0097, 0.0117,  ..., 0.0014, 0.0000, 0.0000],
        [0.0134, 0.0097, 0.0117,  ..., 0.0014, 0.0000, 0.0000],
        [0.0134, 0.0097, 0.0117,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(406640.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9592.8574, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(120.3450, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3937.4680, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-448.5447, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5288.8159, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(444.4303, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5419],
        [-0.5800],
        [-0.5790],
        ...,
        [-0.6544],
        [-0.6690],
        [-0.6712]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-154007.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [1.0009],
        [1.0028],
        ...,
        [0.9988],
        [0.9981],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366747.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.1547, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [1.0009],
        [1.0029],
        ...,
        [0.9988],
        [0.9981],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366758.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.1547, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0003, -0.0005,  ...,  0.0004,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0005,  ...,  0.0004,  0.0000, -0.0001],
        [-0.0052,  0.0008,  0.0019,  ...,  0.0061, -0.0015,  0.0010],
        ...,
        [ 0.0000, -0.0003, -0.0005,  ...,  0.0004,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0005,  ...,  0.0004,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0005,  ...,  0.0004,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(762.0373, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.5115, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.6005, device='cuda:0')



h[100].sum tensor(-15.3202, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.5669, device='cuda:0')



h[200].sum tensor(-9.6682, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.2517, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0014, 0.0034,  ..., 0.0119, 0.0000, 0.0018],
        [0.0000, 0.0019, 0.0047,  ..., 0.0163, 0.0000, 0.0026],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40780.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0304, 0.0045, 0.0185,  ..., 0.0118, 0.0000, 0.0000],
        [0.0639, 0.0004, 0.0323,  ..., 0.0322, 0.0000, 0.0000],
        [0.0863, 0.0000, 0.0412,  ..., 0.0459, 0.0000, 0.0000],
        ...,
        [0.0132, 0.0099, 0.0116,  ..., 0.0011, 0.0000, 0.0000],
        [0.0132, 0.0099, 0.0116,  ..., 0.0011, 0.0000, 0.0000],
        [0.0132, 0.0099, 0.0116,  ..., 0.0011, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(399671.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8938.8965, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(107.3251, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4320.6846, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-435.6931, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5165.9023, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(418.5811, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2429],
        [-0.0688],
        [ 0.0461],
        ...,
        [-0.6850],
        [-0.6826],
        [-0.6819]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-188067.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [1.0009],
        [1.0029],
        ...,
        [0.9988],
        [0.9981],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366758.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.6978]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(301.0491, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [1.0009],
        [1.0030],
        ...,
        [0.9989],
        [0.9981],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366770., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.6978]], device='cuda:0') 
g.ndata[nfet].sum tensor(301.0491, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0003, -0.0005,  ...,  0.0004,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0005,  ...,  0.0004,  0.0000, -0.0001],
        [-0.0063,  0.0010,  0.0024,  ...,  0.0073, -0.0018,  0.0013],
        ...,
        [ 0.0000, -0.0003, -0.0005,  ...,  0.0004,  0.0000, -0.0001],
        [-0.0167,  0.0032,  0.0073,  ...,  0.0189, -0.0049,  0.0036],
        [ 0.0000, -0.0003, -0.0005,  ...,  0.0004,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(856.6362, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.0994, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.9589, device='cuda:0')



h[100].sum tensor(-20.9810, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.2168, device='cuda:0')



h[200].sum tensor(-7.0417, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-33.0128, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0010, 0.0024,  ..., 0.0085, 0.0000, 0.0013],
        [0.0000, 0.0038, 0.0087,  ..., 0.0246, 0.0000, 0.0043],
        ...,
        [0.0000, 0.0033, 0.0074,  ..., 0.0203, 0.0000, 0.0036],
        [0.0000, 0.0026, 0.0060,  ..., 0.0169, 0.0000, 0.0029],
        [0.0000, 0.0118, 0.0268,  ..., 0.0695, 0.0000, 0.0131]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50528.1367, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0487, 0.0006, 0.0270,  ..., 0.0223, 0.0000, 0.0000],
        [0.0737, 0.0013, 0.0374,  ..., 0.0375, 0.0000, 0.0000],
        [0.1169, 0.0000, 0.0564,  ..., 0.0633, 0.0000, 0.0000],
        ...,
        [0.0740, 0.0000, 0.0395,  ..., 0.0370, 0.0000, 0.0000],
        [0.0914, 0.0000, 0.0474,  ..., 0.0473, 0.0000, 0.0000],
        [0.1473, 0.0000, 0.0732,  ..., 0.0804, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(441285.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10913.9336, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(151.7349, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3799.6743, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-492.0868, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5906.9194, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(515.9617, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0706],
        [ 0.0872],
        [ 0.1006],
        ...,
        [-0.2771],
        [-0.1402],
        [-0.0734]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-136010.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [1.0009],
        [1.0030],
        ...,
        [0.9989],
        [0.9981],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366770., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 120.0 event: 600 loss: tensor(556.2433, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.8708, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [1.0009],
        [1.0030],
        ...,
        [0.9989],
        [0.9981],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366781.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.8708, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0003, -0.0005,  ...,  0.0004,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0005,  ...,  0.0004,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0005,  ...,  0.0004,  0.0000, -0.0001],
        ...,
        [ 0.0000, -0.0003, -0.0005,  ...,  0.0004,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0005,  ...,  0.0004,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0005,  ...,  0.0004,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(824.2228, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.1649, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.7267, device='cuda:0')



h[100].sum tensor(-17.2910, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.1186, device='cuda:0')



h[200].sum tensor(-8.6055, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.1813, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46578.2539, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0180, 0.0072, 0.0136,  ..., 0.0039, 0.0000, 0.0000],
        [0.0136, 0.0097, 0.0117,  ..., 0.0012, 0.0000, 0.0000],
        [0.0135, 0.0099, 0.0117,  ..., 0.0011, 0.0000, 0.0000],
        ...,
        [0.0136, 0.0100, 0.0119,  ..., 0.0011, 0.0000, 0.0000],
        [0.0136, 0.0100, 0.0119,  ..., 0.0011, 0.0000, 0.0000],
        [0.0136, 0.0100, 0.0119,  ..., 0.0011, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(431404.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10311.2354, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(132.9612, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4134.2407, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-469.6469, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5772.7246, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(479.0584, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5258],
        [-0.6924],
        [-0.8039],
        ...,
        [-0.6940],
        [-0.6916],
        [-0.6909]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-167122.1719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [1.0009],
        [1.0030],
        ...,
        [0.9989],
        [0.9981],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366781.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.7633, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [1.0009],
        [1.0031],
        ...,
        [0.9989],
        [0.9981],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366792.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.7633, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0003, -0.0005,  ...,  0.0004,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0005,  ...,  0.0004,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0005,  ...,  0.0004,  0.0000, -0.0001],
        ...,
        [ 0.0000, -0.0003, -0.0005,  ...,  0.0004,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0005,  ...,  0.0004,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0005,  ...,  0.0004,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(808.8488, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.8208, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.7735, device='cuda:0')



h[100].sum tensor(-14.7902, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.3523, device='cuda:0')



h[200].sum tensor(-9.6256, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.1121, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0010, 0.0024,  ..., 0.0086, 0.0000, 0.0013],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0024, 0.0054,  ..., 0.0155, 0.0000, 0.0027],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42770.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0395, 0.0006, 0.0226,  ..., 0.0169, 0.0000, 0.0000],
        [0.0347, 0.0007, 0.0211,  ..., 0.0139, 0.0000, 0.0000],
        [0.0608, 0.0000, 0.0331,  ..., 0.0293, 0.0000, 0.0000],
        ...,
        [0.0140, 0.0100, 0.0121,  ..., 0.0013, 0.0000, 0.0000],
        [0.0140, 0.0100, 0.0121,  ..., 0.0013, 0.0000, 0.0000],
        [0.0140, 0.0100, 0.0121,  ..., 0.0013, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(414049.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9793.4707, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(118.7016, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4180.9800, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-447.4326, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5500.0801, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(442.8885, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5319],
        [-0.5406],
        [-0.4879],
        ...,
        [-0.6967],
        [-0.6942],
        [-0.6934]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-159254.5469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [1.0009],
        [1.0031],
        ...,
        [0.9989],
        [0.9981],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366792.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(287.5363, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [1.0010],
        [1.0032],
        ...,
        [0.9989],
        [0.9981],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366803.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(287.5363, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.2258e-04, -5.4703e-04,  ...,  4.2521e-04,
          0.0000e+00, -9.0732e-05],
        [ 0.0000e+00, -3.2258e-04, -5.4703e-04,  ...,  4.2521e-04,
          0.0000e+00, -9.0732e-05],
        [ 0.0000e+00, -3.2258e-04, -5.4703e-04,  ...,  4.2521e-04,
          0.0000e+00, -9.0732e-05],
        ...,
        [ 0.0000e+00, -3.2258e-04, -5.4703e-04,  ...,  4.2521e-04,
          0.0000e+00, -9.0732e-05],
        [ 0.0000e+00, -3.2258e-04, -5.4703e-04,  ...,  4.2521e-04,
          0.0000e+00, -9.0732e-05],
        [ 0.0000e+00, -3.2258e-04, -5.4703e-04,  ...,  4.2521e-04,
          0.0000e+00, -9.0732e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(911.3911, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.0343, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.8835, device='cuda:0')



h[100].sum tensor(-20.1664, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.9377, device='cuda:0')



h[200].sum tensor(-7.1554, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-31.5310, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48631.2422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0158, 0.0087, 0.0127,  ..., 0.0024, 0.0000, 0.0000],
        [0.0170, 0.0082, 0.0130,  ..., 0.0031, 0.0000, 0.0000],
        [0.0255, 0.0044, 0.0161,  ..., 0.0083, 0.0000, 0.0000],
        ...,
        [0.0142, 0.0100, 0.0122,  ..., 0.0013, 0.0000, 0.0000],
        [0.0142, 0.0100, 0.0122,  ..., 0.0013, 0.0000, 0.0000],
        [0.0142, 0.0100, 0.0122,  ..., 0.0013, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(429377.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10790.2432, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(144.6056, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3784.7915, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-480.6447, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5815.8516, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(501.7552, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4948],
        [-0.3782],
        [-0.2065],
        ...,
        [-0.6977],
        [-0.6955],
        [-0.6949]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-122811.0781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [1.0010],
        [1.0032],
        ...,
        [0.9989],
        [0.9981],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366803.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.1803, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [1.0010],
        [1.0033],
        ...,
        [0.9989],
        [0.9981],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366814.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.1803, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.1875e-04, -5.5538e-04,  ...,  4.1356e-04,
          0.0000e+00, -6.9208e-05],
        [ 0.0000e+00, -3.1875e-04, -5.5538e-04,  ...,  4.1356e-04,
          0.0000e+00, -6.9208e-05],
        [ 0.0000e+00, -3.1875e-04, -5.5538e-04,  ...,  4.1356e-04,
          0.0000e+00, -6.9208e-05],
        ...,
        [ 0.0000e+00, -3.1875e-04, -5.5538e-04,  ...,  4.1356e-04,
          0.0000e+00, -6.9208e-05],
        [ 0.0000e+00, -3.1875e-04, -5.5538e-04,  ...,  4.1356e-04,
          0.0000e+00, -6.9208e-05],
        [ 0.0000e+00, -3.1875e-04, -5.5538e-04,  ...,  4.1356e-04,
          0.0000e+00, -6.9208e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(866.0092, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.7627, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.5576, device='cuda:0')



h[100].sum tensor(-16.2289, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.8152, device='cuda:0')



h[200].sum tensor(-8.6942, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.5704, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0106, 0.0239,  ..., 0.0618, 0.0000, 0.0118],
        [0.0000, 0.0016, 0.0036,  ..., 0.0114, 0.0000, 0.0019],
        [0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43700.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2366, 0.0000, 0.1157,  ..., 0.1337, 0.0000, 0.0000],
        [0.1507, 0.0000, 0.0752,  ..., 0.0828, 0.0000, 0.0000],
        [0.1125, 0.0000, 0.0578,  ..., 0.0601, 0.0000, 0.0000],
        ...,
        [0.0143, 0.0102, 0.0127,  ..., 0.0016, 0.0000, 0.0000],
        [0.0143, 0.0102, 0.0127,  ..., 0.0016, 0.0000, 0.0000],
        [0.0143, 0.0102, 0.0127,  ..., 0.0016, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(413594.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9947.7139, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(125.0169, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4175.8408, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-451.6853, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5548.2041, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(459.1571, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0607],
        [ 0.0597],
        [ 0.0583],
        ...,
        [-0.7067],
        [-0.7043],
        [-0.7036]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-148676.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [1.0010],
        [1.0033],
        ...,
        [0.9989],
        [0.9981],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366814.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.0637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [1.0010],
        [1.0034],
        ...,
        [0.9989],
        [0.9981],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366825.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.0637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.1582e-04, -5.5867e-04,  ...,  3.9889e-04,
          0.0000e+00, -5.2496e-05],
        [ 0.0000e+00, -3.1582e-04, -5.5867e-04,  ...,  3.9889e-04,
          0.0000e+00, -5.2496e-05],
        [ 0.0000e+00, -3.1582e-04, -5.5867e-04,  ...,  3.9889e-04,
          0.0000e+00, -5.2496e-05],
        ...,
        [ 0.0000e+00, -3.1582e-04, -5.5867e-04,  ...,  3.9889e-04,
          0.0000e+00, -5.2496e-05],
        [ 0.0000e+00, -3.1582e-04, -5.5867e-04,  ...,  3.9889e-04,
          0.0000e+00, -5.2496e-05],
        [ 0.0000e+00, -3.1582e-04, -5.5867e-04,  ...,  3.9889e-04,
          0.0000e+00, -5.2496e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(876.0787, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.9721, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.7871, device='cuda:0')



h[100].sum tensor(-16.3798, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.8748, device='cuda:0')



h[200].sum tensor(-8.3207, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.8866, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45132.5859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0243, 0.0051, 0.0169,  ..., 0.0080, 0.0000, 0.0000],
        [0.0163, 0.0089, 0.0136,  ..., 0.0031, 0.0000, 0.0000],
        [0.0140, 0.0104, 0.0126,  ..., 0.0017, 0.0000, 0.0000],
        ...,
        [0.0142, 0.0105, 0.0127,  ..., 0.0017, 0.0000, 0.0000],
        [0.0142, 0.0105, 0.0127,  ..., 0.0017, 0.0000, 0.0000],
        [0.0142, 0.0105, 0.0127,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(423750.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10339.5850, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(131.6119, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4181.2236, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-459.4062, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5763.5718, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(477.1384, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4597],
        [-0.6463],
        [-0.7824],
        ...,
        [-0.7164],
        [-0.7141],
        [-0.7134]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-144606.9844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [1.0010],
        [1.0034],
        ...,
        [0.9989],
        [0.9981],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366825.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.7358, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [1.0011],
        [1.0034],
        ...,
        [0.9989],
        [0.9981],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366837., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.7358, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.1646e-04, -5.5704e-04,  ...,  3.8170e-04,
          0.0000e+00, -4.0788e-05],
        [ 0.0000e+00, -3.1646e-04, -5.5704e-04,  ...,  3.8170e-04,
          0.0000e+00, -4.0788e-05],
        [ 0.0000e+00, -3.1646e-04, -5.5704e-04,  ...,  3.8170e-04,
          0.0000e+00, -4.0788e-05],
        ...,
        [ 0.0000e+00, -3.1646e-04, -5.5704e-04,  ...,  3.8170e-04,
          0.0000e+00, -4.0788e-05],
        [ 0.0000e+00, -3.1646e-04, -5.5704e-04,  ...,  3.8170e-04,
          0.0000e+00, -4.0788e-05],
        [ 0.0000e+00, -3.1646e-04, -5.5704e-04,  ...,  3.8170e-04,
          0.0000e+00, -4.0788e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(887.2115, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.7717, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.3976, device='cuda:0')



h[100].sum tensor(-16.9704, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.0332, device='cuda:0')



h[200].sum tensor(-7.7316, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.7279, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44644.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0458, 0.0015, 0.0262,  ..., 0.0210, 0.0000, 0.0000],
        [0.0368, 0.0025, 0.0223,  ..., 0.0156, 0.0000, 0.0000],
        [0.0277, 0.0041, 0.0182,  ..., 0.0101, 0.0000, 0.0000],
        ...,
        [0.0138, 0.0108, 0.0125,  ..., 0.0014, 0.0000, 0.0000],
        [0.0138, 0.0108, 0.0125,  ..., 0.0014, 0.0000, 0.0000],
        [0.0138, 0.0108, 0.0125,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(418530.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10047.2900, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(131.0858, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4307.8569, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-455.4624, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5607.9414, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(471.9671, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0387],
        [ 0.0256],
        [ 0.0061],
        ...,
        [-0.7266],
        [-0.7247],
        [-0.7247]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-148081.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [1.0011],
        [1.0034],
        ...,
        [0.9989],
        [0.9981],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366837., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.5437, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [1.0011],
        [1.0035],
        ...,
        [0.9989],
        [0.9981],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366848.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.5437, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.1906e-04, -5.5138e-04,  ...,  3.7991e-04,
          0.0000e+00, -2.9472e-05],
        [ 0.0000e+00, -3.1906e-04, -5.5138e-04,  ...,  3.7991e-04,
          0.0000e+00, -2.9472e-05],
        [ 0.0000e+00, -3.1906e-04, -5.5138e-04,  ...,  3.7991e-04,
          0.0000e+00, -2.9472e-05],
        ...,
        [ 0.0000e+00, -3.1906e-04, -5.5138e-04,  ...,  3.7991e-04,
          0.0000e+00, -2.9472e-05],
        [ 0.0000e+00, -3.1906e-04, -5.5138e-04,  ...,  3.7991e-04,
          0.0000e+00, -2.9472e-05],
        [ 0.0000e+00, -3.1906e-04, -5.5138e-04,  ...,  3.7991e-04,
          0.0000e+00, -2.9472e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(891.0059, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.2099, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.9048, device='cuda:0')



h[100].sum tensor(-16.5465, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.9053, device='cuda:0')



h[200].sum tensor(-7.7092, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.0489, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43802.4492, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0131, 0.0107, 0.0119,  ..., 0.0012, 0.0000, 0.0000],
        [0.0131, 0.0107, 0.0120,  ..., 0.0012, 0.0000, 0.0000],
        [0.0133, 0.0107, 0.0120,  ..., 0.0012, 0.0000, 0.0000],
        ...,
        [0.0134, 0.0109, 0.0121,  ..., 0.0013, 0.0000, 0.0000],
        [0.0134, 0.0109, 0.0121,  ..., 0.0013, 0.0000, 0.0000],
        [0.0134, 0.0109, 0.0121,  ..., 0.0013, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(421690.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9672.2891, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(121.4448, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4636.5029, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-450.0800, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5578.9766, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(460.1249, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6869],
        [-0.7151],
        [-0.7290],
        ...,
        [-0.7381],
        [-0.7356],
        [-0.7348]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-184632.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [1.0011],
        [1.0035],
        ...,
        [0.9989],
        [0.9981],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366848.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.0911, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [1.0011],
        [1.0036],
        ...,
        [0.9989],
        [0.9982],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366861., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.0911, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.2596e-04, -5.4362e-04,  ...,  3.8809e-04,
          0.0000e+00, -1.9889e-05],
        [ 0.0000e+00, -3.2596e-04, -5.4362e-04,  ...,  3.8809e-04,
          0.0000e+00, -1.9889e-05],
        [ 0.0000e+00, -3.2596e-04, -5.4362e-04,  ...,  3.8809e-04,
          0.0000e+00, -1.9889e-05],
        ...,
        [ 0.0000e+00, -3.2596e-04, -5.4362e-04,  ...,  3.8809e-04,
          0.0000e+00, -1.9889e-05],
        [ 0.0000e+00, -3.2596e-04, -5.4362e-04,  ...,  3.8809e-04,
          0.0000e+00, -1.9889e-05],
        [ 0.0000e+00, -3.2596e-04, -5.4362e-04,  ...,  3.8809e-04,
          0.0000e+00, -1.9889e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(903.8870, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.1730, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.8688, device='cuda:0')



h[100].sum tensor(-16.5137, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.8960, device='cuda:0')



h[200].sum tensor(-7.7654, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.9992, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44802.6641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0178, 0.0074, 0.0137,  ..., 0.0041, 0.0000, 0.0000],
        [0.0345, 0.0033, 0.0212,  ..., 0.0141, 0.0000, 0.0000],
        [0.0544, 0.0013, 0.0302,  ..., 0.0260, 0.0000, 0.0000],
        ...,
        [0.0132, 0.0108, 0.0117,  ..., 0.0011, 0.0000, 0.0000],
        [0.0132, 0.0108, 0.0117,  ..., 0.0011, 0.0000, 0.0000],
        [0.0132, 0.0108, 0.0117,  ..., 0.0011, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(426567.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10011.1318, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(122.2482, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4403.2563, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-454.0436, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5626.1992, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(464.8260, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0281],
        [ 0.0302],
        [ 0.0670],
        ...,
        [-0.7450],
        [-0.7424],
        [-0.7416]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-160222.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [1.0011],
        [1.0036],
        ...,
        [0.9989],
        [0.9982],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366861., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.5122, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [1.0011],
        [1.0036],
        ...,
        [0.9990],
        [0.9982],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366873.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.5122, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.2682e-04, -5.3945e-04,  ...,  4.0438e-04,
          0.0000e+00, -8.7826e-06],
        [ 0.0000e+00, -3.2682e-04, -5.3945e-04,  ...,  4.0438e-04,
          0.0000e+00, -8.7826e-06],
        [ 0.0000e+00, -3.2682e-04, -5.3945e-04,  ...,  4.0438e-04,
          0.0000e+00, -8.7826e-06],
        ...,
        [ 0.0000e+00, -3.2682e-04, -5.3945e-04,  ...,  4.0438e-04,
          0.0000e+00, -8.7826e-06],
        [ 0.0000e+00, -3.2682e-04, -5.3945e-04,  ...,  4.0438e-04,
          0.0000e+00, -8.7826e-06],
        [ 0.0000e+00, -3.2682e-04, -5.3945e-04,  ...,  4.0438e-04,
          0.0000e+00, -8.7826e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(925.4053, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.0868, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.0615, device='cuda:0')



h[100].sum tensor(-16.4443, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.9460, device='cuda:0')



h[200].sum tensor(-8.0036, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.2647, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44433.3672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0161, 0.0089, 0.0121,  ..., 0.0034, 0.0000, 0.0000],
        [0.0158, 0.0091, 0.0120,  ..., 0.0032, 0.0000, 0.0000],
        [0.0185, 0.0074, 0.0134,  ..., 0.0047, 0.0000, 0.0000],
        ...,
        [0.0206, 0.0065, 0.0148,  ..., 0.0058, 0.0000, 0.0000],
        [0.0354, 0.0026, 0.0214,  ..., 0.0147, 0.0000, 0.0000],
        [0.0428, 0.0012, 0.0247,  ..., 0.0192, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(424464.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9747.0938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(113.2994, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4489.1030, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-452.1436, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5535.8281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(458.2430, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5213],
        [-0.4551],
        [-0.3455],
        ...,
        [-0.5075],
        [-0.3616],
        [-0.2704]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-172542.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [1.0011],
        [1.0036],
        ...,
        [0.9990],
        [0.9982],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366873.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(416.8326, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [1.0012],
        [1.0037],
        ...,
        [0.9990],
        [0.9982],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366884.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(416.8326, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.2068e-04, -5.3847e-04,  ...,  4.1115e-04,
          0.0000e+00,  2.4383e-06],
        [ 0.0000e+00, -3.2068e-04, -5.3847e-04,  ...,  4.1115e-04,
          0.0000e+00,  2.4383e-06],
        [-1.5571e-02,  3.0175e-03,  6.8624e-03,  ...,  1.7784e-02,
         -4.5082e-03,  3.4776e-03],
        ...,
        [ 0.0000e+00, -3.2068e-04, -5.3847e-04,  ...,  4.1115e-04,
          0.0000e+00,  2.4383e-06],
        [ 0.0000e+00, -3.2068e-04, -5.3847e-04,  ...,  4.1115e-04,
          0.0000e+00,  2.4383e-06],
        [ 0.0000e+00, -3.2068e-04, -5.3847e-04,  ...,  4.1115e-04,
          0.0000e+00,  2.4383e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1139.6919, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-38.6598, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.1735, device='cuda:0')



h[100].sum tensor(-28.7741, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-8.6077, device='cuda:0')



h[200].sum tensor(-2.5932, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-45.7096, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.6445e-03, 0.0000e+00,
         9.7526e-06],
        [0.0000e+00, 5.4268e-03, 1.2377e-02,  ..., 3.3228e-02, 0.0000e+00,
         6.3274e-03],
        [0.0000e+00, 5.4335e-03, 1.2392e-02,  ..., 3.3269e-02, 0.0000e+00,
         6.3352e-03],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.6675e-03, 0.0000e+00,
         9.8890e-06],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.6675e-03, 0.0000e+00,
         9.8888e-06],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.6675e-03, 0.0000e+00,
         9.8890e-06]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60793.4258, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0741, 0.0000, 0.0383,  ..., 0.0386, 0.0000, 0.0000],
        [0.1314, 0.0000, 0.0655,  ..., 0.0726, 0.0000, 0.0000],
        [0.1630, 0.0000, 0.0805,  ..., 0.0912, 0.0000, 0.0000],
        ...,
        [0.0133, 0.0106, 0.0115,  ..., 0.0016, 0.0000, 0.0000],
        [0.0133, 0.0106, 0.0115,  ..., 0.0016, 0.0000, 0.0000],
        [0.0133, 0.0106, 0.0115,  ..., 0.0016, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(495200.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12354.3809, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(174.5271, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4097.0742, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-548.4667, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6734.0664, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(622.1104, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1342],
        [ 0.1336],
        [ 0.1344],
        ...,
        [-0.7498],
        [-0.7472],
        [-0.7463]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-143878.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [1.0012],
        [1.0037],
        ...,
        [0.9990],
        [0.9982],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366884.9375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 130.0 event: 650 loss: tensor(564.9163, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(320.2490, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [1.0012],
        [1.0038],
        ...,
        [0.9990],
        [0.9982],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366896.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(320.2490, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.1898e-03,  8.0891e-04,  1.9293e-03,  ...,  6.2080e-03,
         -1.4995e-03,  1.1693e-03],
        [-2.2779e-02,  4.5852e-03,  1.0298e-02,  ...,  2.5854e-02,
         -6.5815e-03,  5.0989e-03],
        [-3.5349e-02,  7.2839e-03,  1.6279e-02,  ...,  3.9893e-02,
         -1.0213e-02,  7.9071e-03],
        ...,
        [ 0.0000e+00, -3.0529e-04, -5.3995e-04,  ...,  4.1161e-04,
          0.0000e+00,  9.8619e-06],
        [ 0.0000e+00, -3.0529e-04, -5.3995e-04,  ...,  4.1161e-04,
          0.0000e+00,  9.8619e-06],
        [ 0.0000e+00, -3.0529e-04, -5.3995e-04,  ...,  4.1161e-04,
          0.0000e+00,  9.8619e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1050.0730, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.6113, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.4869, device='cuda:0')



h[100].sum tensor(-22.0323, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.6132, device='cuda:0')



h[200].sum tensor(-5.7078, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-35.1183, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.4791e-02, 3.3325e-02,  ..., 8.4947e-02, 0.0000e+00,
         1.6702e-02],
        [0.0000e+00, 1.6718e-02, 3.7597e-02,  ..., 9.4977e-02, 0.0000e+00,
         1.8708e-02],
        [0.0000e+00, 2.2604e-02, 5.0641e-02,  ..., 1.2561e-01, 0.0000e+00,
         2.4834e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.6696e-03, 0.0000e+00,
         4.0003e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.6696e-03, 0.0000e+00,
         4.0002e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.6696e-03, 0.0000e+00,
         4.0003e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54255.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3909, 0.0000, 0.1895,  ..., 0.2278, 0.0000, 0.0000],
        [0.4223, 0.0000, 0.2049,  ..., 0.2465, 0.0000, 0.0000],
        [0.4738, 0.0000, 0.2303,  ..., 0.2768, 0.0000, 0.0000],
        ...,
        [0.0133, 0.0107, 0.0117,  ..., 0.0019, 0.0000, 0.0000],
        [0.0133, 0.0107, 0.0117,  ..., 0.0019, 0.0000, 0.0000],
        [0.0133, 0.0107, 0.0117,  ..., 0.0019, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(473475., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11537.3418, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(145.0393, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4180.0791, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-511.7169, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6352.1797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(555.6927, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1112],
        [ 0.1113],
        [ 0.1112],
        ...,
        [-0.7562],
        [-0.7537],
        [-0.7530]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-148177.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [1.0012],
        [1.0038],
        ...,
        [0.9990],
        [0.9982],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366896.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5503],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(399.5388, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [1.0013],
        [1.0039],
        ...,
        [0.9990],
        [0.9982],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366908.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5503],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(399.5388, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3047e-02,  2.5230e-03,  5.6715e-03,  ...,  1.4990e-02,
         -3.7617e-03,  2.9264e-03],
        [ 0.0000e+00, -2.8282e-04, -5.4183e-04,  ...,  4.0350e-04,
          0.0000e+00,  9.2682e-06],
        [-3.5741e-02,  7.4035e-03,  1.6479e-02,  ...,  4.0361e-02,
         -1.0305e-02,  8.0005e-03],
        ...,
        [ 0.0000e+00, -2.8282e-04, -5.4183e-04,  ...,  4.0350e-04,
          0.0000e+00,  9.2682e-06],
        [ 0.0000e+00, -2.8282e-04, -5.4183e-04,  ...,  4.0350e-04,
          0.0000e+00,  9.2682e-06],
        [ 0.0000e+00, -2.8282e-04, -5.4183e-04,  ...,  4.0350e-04,
          0.0000e+00,  9.2682e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1153.8186, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-37.2317, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.7972, device='cuda:0')



h[100].sum tensor(-27.6934, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-8.2506, device='cuda:0')



h[200].sum tensor(-3.0086, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-43.8131, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 2.0080e-03, 4.5309e-03,  ..., 1.3523e-02, 0.0000e+00,
         2.4187e-03],
        [0.0000e+00, 1.5752e-02, 3.5219e-02,  ..., 8.9386e-02, 0.0000e+00,
         1.7591e-02],
        [0.0000e+00, 1.4902e-02, 3.3253e-02,  ..., 8.3505e-02, 0.0000e+00,
         1.6414e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.6370e-03, 0.0000e+00,
         3.7600e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.6369e-03, 0.0000e+00,
         3.7599e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.6370e-03, 0.0000e+00,
         3.7600e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58078.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1235, 0.0000, 0.0627,  ..., 0.0688, 0.0000, 0.0000],
        [0.2655, 0.0000, 0.1305,  ..., 0.1539, 0.0000, 0.0000],
        [0.3324, 0.0000, 0.1629,  ..., 0.1938, 0.0000, 0.0000],
        ...,
        [0.0132, 0.0109, 0.0118,  ..., 0.0021, 0.0000, 0.0000],
        [0.0212, 0.0061, 0.0152,  ..., 0.0070, 0.0000, 0.0000],
        [0.0329, 0.0033, 0.0202,  ..., 0.0143, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(477168.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11522.4453, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(158.3947, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4240.2354, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-536.7766, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6446.2075, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(594.9799, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0633],
        [ 0.1107],
        [ 0.1151],
        ...,
        [-0.6798],
        [-0.5598],
        [-0.3967]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-159257.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [1.0013],
        [1.0039],
        ...,
        [0.9990],
        [0.9982],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366908.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.1643, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [1.0013],
        [1.0039],
        ...,
        [0.9990],
        [0.9982],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366919.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.1643, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.6030e-04, -5.4315e-04,  ...,  3.9102e-04,
          0.0000e+00,  4.8368e-06],
        [ 0.0000e+00, -2.6030e-04, -5.4315e-04,  ...,  3.9102e-04,
          0.0000e+00,  4.8368e-06],
        [-1.2660e-02,  2.4670e-03,  5.4914e-03,  ...,  1.4558e-02,
         -3.6423e-03,  2.8378e-03],
        ...,
        [ 0.0000e+00, -2.6030e-04, -5.4315e-04,  ...,  3.9102e-04,
          0.0000e+00,  4.8368e-06],
        [ 0.0000e+00, -2.6030e-04, -5.4315e-04,  ...,  3.9102e-04,
          0.0000e+00,  4.8368e-06],
        [ 0.0000e+00, -2.6030e-04, -5.4315e-04,  ...,  3.9102e-04,
          0.0000e+00,  4.8368e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(922.2665, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.3590, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.8954, device='cuda:0')



h[100].sum tensor(-12.9077, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.8650, device='cuda:0')



h[200].sum tensor(-9.6118, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-20.5243, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.5641e-03, 0.0000e+00,
         1.9347e-05],
        [0.0000e+00, 3.2087e-03, 7.1653e-03,  ..., 2.0939e-02, 0.0000e+00,
         3.8936e-03],
        [0.0000e+00, 2.6749e-03, 6.0173e-03,  ..., 1.9527e-02, 0.0000e+00,
         3.6107e-03],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.5866e-03, 0.0000e+00,
         1.9625e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.5865e-03, 0.0000e+00,
         1.9624e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.5866e-03, 0.0000e+00,
         1.9625e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40534.3359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0345, 0.0050, 0.0210,  ..., 0.0157, 0.0000, 0.0000],
        [0.0803, 0.0000, 0.0410,  ..., 0.0440, 0.0000, 0.0000],
        [0.0988, 0.0000, 0.0483,  ..., 0.0558, 0.0000, 0.0000],
        ...,
        [0.0132, 0.0112, 0.0120,  ..., 0.0022, 0.0000, 0.0000],
        [0.0132, 0.0112, 0.0120,  ..., 0.0022, 0.0000, 0.0000],
        [0.0132, 0.0112, 0.0120,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(415670.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9249.2832, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(87.9381, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4558.0908, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-434.9137, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5420.8408, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(417.9655, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2420],
        [-0.0595],
        [ 0.0625],
        ...,
        [-0.7745],
        [-0.7720],
        [-0.7712]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-178948.4844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [1.0013],
        [1.0039],
        ...,
        [0.9990],
        [0.9982],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366919.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5137],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(307.2665, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [1.0013],
        [1.0040],
        ...,
        [0.9990],
        [0.9982],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366931.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5137],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(307.2665, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.2820e-02,  4.6762e-03,  1.0347e-02,  ...,  2.5950e-02,
         -6.5517e-03,  5.1077e-03],
        [-2.2984e-02,  4.7116e-03,  1.0425e-02,  ...,  2.6134e-02,
         -6.5989e-03,  5.1445e-03],
        [ 0.0000e+00, -2.4761e-04, -5.4185e-04,  ...,  3.8652e-04,
          0.0000e+00, -3.4300e-06],
        ...,
        [ 0.0000e+00, -2.4761e-04, -5.4185e-04,  ...,  3.8652e-04,
          0.0000e+00, -3.4300e-06],
        [ 0.0000e+00, -2.4761e-04, -5.4185e-04,  ...,  3.8652e-04,
          0.0000e+00, -3.4300e-06],
        [ 0.0000e+00, -2.4761e-04, -5.4185e-04,  ...,  3.8652e-04,
          0.0000e+00, -3.4300e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1065.0060, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.3500, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.4537, device='cuda:0')



h[100].sum tensor(-21.0735, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.3451, device='cuda:0')



h[200].sum tensor(-5.7770, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-33.6946, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0192, 0.0425,  ..., 0.1064, 0.0000, 0.0210],
        [0.0000, 0.0138, 0.0305,  ..., 0.0771, 0.0000, 0.0151],
        [0.0000, 0.0124, 0.0274,  ..., 0.0711, 0.0000, 0.0139],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52212.6992, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3378, 0.0000, 0.1668,  ..., 0.1988, 0.0000, 0.0000],
        [0.3395, 0.0000, 0.1671,  ..., 0.2000, 0.0000, 0.0000],
        [0.3242, 0.0000, 0.1587,  ..., 0.1912, 0.0000, 0.0000],
        ...,
        [0.0131, 0.0114, 0.0120,  ..., 0.0023, 0.0000, 0.0000],
        [0.0131, 0.0114, 0.0120,  ..., 0.0023, 0.0000, 0.0000],
        [0.0131, 0.0114, 0.0120,  ..., 0.0023, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(466729.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11114.3887, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(134.8585, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4316.3120, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-504.7555, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6328.6519, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(536.5920, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0806],
        [ 0.0806],
        [ 0.0826],
        ...,
        [-0.7836],
        [-0.7810],
        [-0.7802]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-162056.0469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [1.0013],
        [1.0040],
        ...,
        [0.9990],
        [0.9982],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366931.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2844],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.7283, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [1.0013],
        [1.0040],
        ...,
        [0.9990],
        [0.9982],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366931.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2844],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.7283, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.9243e-03,  8.1490e-04,  1.8078e-03,  ...,  5.9029e-03,
         -1.4138e-03,  1.0995e-03],
        [-6.7300e-03,  1.2045e-03,  2.6694e-03,  ...,  7.9257e-03,
         -1.9322e-03,  1.5039e-03],
        [ 0.0000e+00, -2.4761e-04, -5.4185e-04,  ...,  3.8652e-04,
          0.0000e+00, -3.4300e-06],
        ...,
        [ 0.0000e+00, -2.4761e-04, -5.4185e-04,  ...,  3.8652e-04,
          0.0000e+00, -3.4300e-06],
        [ 0.0000e+00, -2.4761e-04, -5.4185e-04,  ...,  3.8652e-04,
          0.0000e+00, -3.4300e-06],
        [ 0.0000e+00, -2.4761e-04, -5.4185e-04,  ...,  3.8652e-04,
          0.0000e+00, -3.4300e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(959.0885, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.7041, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.7707, device='cuda:0')



h[100].sum tensor(-14.6467, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.3516, device='cuda:0')



h[200].sum tensor(-8.7244, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.1083, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0085, 0.0189,  ..., 0.0509, 0.0000, 0.0099],
        [0.0000, 0.0026, 0.0058,  ..., 0.0177, 0.0000, 0.0032],
        [0.0000, 0.0012, 0.0027,  ..., 0.0091, 0.0000, 0.0015],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43015.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1920, 0.0000, 0.0940,  ..., 0.1120, 0.0000, 0.0000],
        [0.1107, 0.0000, 0.0558,  ..., 0.0626, 0.0000, 0.0000],
        [0.0580, 0.0016, 0.0318,  ..., 0.0302, 0.0000, 0.0000],
        ...,
        [0.0131, 0.0114, 0.0120,  ..., 0.0023, 0.0000, 0.0000],
        [0.0131, 0.0114, 0.0120,  ..., 0.0023, 0.0000, 0.0000],
        [0.0131, 0.0114, 0.0120,  ..., 0.0023, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(426746.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9674.7920, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(98.0876, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4485.0371, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-450.4501, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5618.8945, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(443.2901, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0883],
        [ 0.0556],
        [-0.0352],
        ...,
        [-0.7836],
        [-0.7810],
        [-0.7802]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-169978.2031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [1.0013],
        [1.0040],
        ...,
        [0.9990],
        [0.9982],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366931.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.2783, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0013],
        [1.0041],
        ...,
        [0.9990],
        [0.9982],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366943.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.2783, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.3508e-04, -5.4140e-04,  ...,  3.8996e-04,
          0.0000e+00, -4.2306e-06],
        [ 0.0000e+00, -2.3508e-04, -5.4140e-04,  ...,  3.8996e-04,
          0.0000e+00, -4.2306e-06],
        [ 0.0000e+00, -2.3508e-04, -5.4140e-04,  ...,  3.8996e-04,
          0.0000e+00, -4.2306e-06],
        ...,
        [ 0.0000e+00, -2.3508e-04, -5.4140e-04,  ...,  3.8996e-04,
          0.0000e+00, -4.2306e-06],
        [ 0.0000e+00, -2.3508e-04, -5.4140e-04,  ...,  3.8996e-04,
          0.0000e+00, -4.2306e-06],
        [ 0.0000e+00, -2.3508e-04, -5.4140e-04,  ...,  3.8996e-04,
          0.0000e+00, -4.2306e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(995.0836, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.2593, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.4858, device='cuda:0')



h[100].sum tensor(-15.7976, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.7966, device='cuda:0')



h[200].sum tensor(-8.2103, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.4715, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0016, 0.0035,  ..., 0.0110, 0.0000, 0.0019],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45295.3164, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0370, 0.0027, 0.0228,  ..., 0.0175, 0.0000, 0.0000],
        [0.0533, 0.0027, 0.0303,  ..., 0.0276, 0.0000, 0.0000],
        [0.0846, 0.0000, 0.0445,  ..., 0.0469, 0.0000, 0.0000],
        ...,
        [0.0131, 0.0116, 0.0122,  ..., 0.0026, 0.0000, 0.0000],
        [0.0131, 0.0116, 0.0122,  ..., 0.0026, 0.0000, 0.0000],
        [0.0131, 0.0116, 0.0122,  ..., 0.0026, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(441376.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10183.2520, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(104.6574, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4428.1133, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-465.4231, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5927.4736, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(467.0748, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0458],
        [ 0.0596],
        [ 0.0781],
        ...,
        [-0.7889],
        [-0.7863],
        [-0.7855]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-168230.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0013],
        [1.0041],
        ...,
        [0.9990],
        [0.9982],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366943.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(225.5844, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0013],
        [1.0041],
        ...,
        [0.9991],
        [0.9982],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366954.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(225.5844, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.2705e-04, -5.3832e-04,  ...,  3.9340e-04,
          0.0000e+00, -7.6267e-06],
        [ 0.0000e+00, -2.2705e-04, -5.3832e-04,  ...,  3.9340e-04,
          0.0000e+00, -7.6267e-06],
        [ 0.0000e+00, -2.2705e-04, -5.3832e-04,  ...,  3.9340e-04,
          0.0000e+00, -7.6267e-06],
        ...,
        [ 0.0000e+00, -2.2705e-04, -5.3832e-04,  ...,  3.9340e-04,
          0.0000e+00, -7.6267e-06],
        [ 0.0000e+00, -2.2705e-04, -5.3832e-04,  ...,  3.9340e-04,
          0.0000e+00, -7.6267e-06],
        [ 0.0000e+00, -2.2705e-04, -5.3832e-04,  ...,  3.9340e-04,
          0.0000e+00, -7.6267e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1005.6677, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.0279, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.9531, device='cuda:0')



h[100].sum tensor(-15.6206, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.6584, device='cuda:0')



h[200].sum tensor(-8.2319, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.7374, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44498.4961, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0187, 0.0079, 0.0143,  ..., 0.0066, 0.0000, 0.0000],
        [0.0148, 0.0104, 0.0127,  ..., 0.0040, 0.0000, 0.0000],
        [0.0132, 0.0114, 0.0121,  ..., 0.0029, 0.0000, 0.0000],
        ...,
        [0.0131, 0.0118, 0.0122,  ..., 0.0027, 0.0000, 0.0000],
        [0.0131, 0.0118, 0.0122,  ..., 0.0027, 0.0000, 0.0000],
        [0.0131, 0.0118, 0.0122,  ..., 0.0027, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(433975., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9875.4277, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(99.8683, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4428.7998, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-462.3882, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5806.0317, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(458.7290, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1337],
        [-0.3121],
        [-0.5045],
        ...,
        [-0.7874],
        [-0.7885],
        [-0.7894]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-171620.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0013],
        [1.0041],
        ...,
        [0.9991],
        [0.9982],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366954.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.5266, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0014],
        [1.0042],
        ...,
        [0.9991],
        [0.9983],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366966.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.5266, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1235e-02,  2.2080e-03,  4.8449e-03,  ...,  1.3025e-02,
         -3.2051e-03,  2.5131e-03],
        [-1.2697e-02,  2.5249e-03,  5.5449e-03,  ...,  1.4668e-02,
         -3.6223e-03,  2.8416e-03],
        [-1.2986e-02,  2.5875e-03,  5.6832e-03,  ...,  1.4993e-02,
         -3.7047e-03,  2.9064e-03],
        ...,
        [ 0.0000e+00, -2.2613e-04, -5.3253e-04,  ...,  3.9949e-04,
          0.0000e+00, -1.0309e-05],
        [ 0.0000e+00, -2.2613e-04, -5.3253e-04,  ...,  3.9949e-04,
          0.0000e+00, -1.0309e-05],
        [ 0.0000e+00, -2.2613e-04, -5.3253e-04,  ...,  3.9949e-04,
          0.0000e+00, -1.0309e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1024.6847, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.7331, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.7443, device='cuda:0')



h[100].sum tensor(-16.1393, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.8637, device='cuda:0')



h[200].sum tensor(-8.0332, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.8277, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0080, 0.0175,  ..., 0.0478, 0.0000, 0.0092],
        [0.0000, 0.0095, 0.0208,  ..., 0.0554, 0.0000, 0.0107],
        [0.0000, 0.0122, 0.0268,  ..., 0.0696, 0.0000, 0.0135],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45634.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2989, 0.0000, 0.1467,  ..., 0.1782, 0.0000, 0.0000],
        [0.2735, 0.0000, 0.1339,  ..., 0.1630, 0.0000, 0.0000],
        [0.2445, 0.0000, 0.1199,  ..., 0.1453, 0.0000, 0.0000],
        ...,
        [0.0128, 0.0119, 0.0121,  ..., 0.0027, 0.0000, 0.0000],
        [0.0128, 0.0119, 0.0121,  ..., 0.0027, 0.0000, 0.0000],
        [0.0128, 0.0119, 0.0121,  ..., 0.0027, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(441814.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10101.7344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(102.8341, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4388.3691, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-470.0787, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5925.3213, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(468.5374, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0580],
        [ 0.0715],
        [ 0.0801],
        ...,
        [-0.8045],
        [-0.8018],
        [-0.8008]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-165339.0781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0014],
        [1.0042],
        ...,
        [0.9991],
        [0.9983],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366966.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2920],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.0199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0014],
        [1.0042],
        ...,
        [0.9991],
        [0.9983],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366978.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2920],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.0199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1899e-02,  2.3586e-03,  5.1746e-03,  ...,  1.3792e-02,
         -3.3874e-03,  2.6640e-03],
        [-5.0173e-03,  8.6586e-04,  1.8771e-03,  ...,  6.0502e-03,
         -1.4284e-03,  1.1168e-03],
        [-6.8814e-03,  1.2702e-03,  2.7704e-03,  ...,  8.1473e-03,
         -1.9590e-03,  1.5360e-03],
        ...,
        [ 0.0000e+00, -2.2254e-04, -5.2711e-04,  ...,  4.0573e-04,
          0.0000e+00, -1.1217e-05],
        [ 0.0000e+00, -2.2254e-04, -5.2711e-04,  ...,  4.0573e-04,
          0.0000e+00, -1.1217e-05],
        [ 0.0000e+00, -2.2254e-04, -5.2711e-04,  ...,  4.0573e-04,
          0.0000e+00, -1.1217e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1009.6815, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.6497, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.8735, device='cuda:0')



h[100].sum tensor(-14.5874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.3783, device='cuda:0')



h[200].sum tensor(-8.8270, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.2499, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0036, 0.0078,  ..., 0.0238, 0.0000, 0.0044],
        [0.0000, 0.0087, 0.0190,  ..., 0.0512, 0.0000, 0.0099],
        [0.0000, 0.0038, 0.0084,  ..., 0.0238, 0.0000, 0.0044],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42696.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1200, 0.0000, 0.0597,  ..., 0.0701, 0.0000, 0.0000],
        [0.1534, 0.0000, 0.0758,  ..., 0.0904, 0.0000, 0.0000],
        [0.1110, 0.0000, 0.0561,  ..., 0.0641, 0.0000, 0.0000],
        ...,
        [0.0126, 0.0120, 0.0119,  ..., 0.0027, 0.0000, 0.0000],
        [0.0126, 0.0120, 0.0119,  ..., 0.0027, 0.0000, 0.0000],
        [0.0126, 0.0120, 0.0119,  ..., 0.0027, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(433204.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9359.3662, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(86.6446, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4716.9795, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-454.5015, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5701.5205, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(436.8761, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0874],
        [ 0.0923],
        [ 0.0260],
        ...,
        [-0.8140],
        [-0.8113],
        [-0.8105]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-200861.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0014],
        [1.0042],
        ...,
        [0.9991],
        [0.9983],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366978.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2932],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(354.2050, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0014],
        [1.0043],
        ...,
        [0.9991],
        [0.9983],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366990.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2932],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(354.2050, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.4769e-03,  9.6704e-04,  2.1070e-03,  ...,  6.5841e-03,
         -1.5559e-03,  1.2211e-03],
        [-1.2380e-02,  2.4663e-03,  5.4187e-03,  ...,  1.4359e-02,
         -3.5169e-03,  2.7747e-03],
        [ 0.0000e+00, -2.2242e-04, -5.2043e-04,  ...,  4.1584e-04,
          0.0000e+00, -1.1533e-05],
        ...,
        [ 0.0000e+00, -2.2242e-04, -5.2043e-04,  ...,  4.1584e-04,
          0.0000e+00, -1.1533e-05],
        [ 0.0000e+00, -2.2242e-04, -5.2043e-04,  ...,  4.1584e-04,
          0.0000e+00, -1.1533e-05],
        [ 0.0000e+00, -2.2242e-04, -5.2043e-04,  ...,  4.1584e-04,
          0.0000e+00, -1.1533e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1188.7224, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-32.5156, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.1893, device='cuda:0')



h[100].sum tensor(-24.1308, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-7.3144, device='cuda:0')



h[200].sum tensor(-4.6177, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-38.8419, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0084, 0.0185,  ..., 0.0501, 0.0000, 0.0096],
        [0.0000, 0.0036, 0.0078,  ..., 0.0237, 0.0000, 0.0044],
        [0.0000, 0.0032, 0.0071,  ..., 0.0207, 0.0000, 0.0038],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60869.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1533, 0.0000, 0.0756,  ..., 0.0906, 0.0000, 0.0000],
        [0.1183, 0.0000, 0.0589,  ..., 0.0692, 0.0000, 0.0000],
        [0.0874, 0.0000, 0.0453,  ..., 0.0496, 0.0000, 0.0000],
        ...,
        [0.0124, 0.0121, 0.0117,  ..., 0.0027, 0.0000, 0.0000],
        [0.0124, 0.0121, 0.0117,  ..., 0.0027, 0.0000, 0.0000],
        [0.0124, 0.0121, 0.0117,  ..., 0.0027, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(522037., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12741.3320, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(157.3508, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4116.7363, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-562.9771, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7276.1577, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(617.2701, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1035],
        [ 0.0744],
        [-0.0316],
        ...,
        [-0.8208],
        [-0.8178],
        [-0.8167]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-158204.2344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0014],
        [1.0043],
        ...,
        [0.9991],
        [0.9983],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366990.0938, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 140.0 event: 700 loss: tensor(457.2847, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5815],
        [0.5928],
        [0.5962],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(206.7020, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0014],
        [1.0043],
        ...,
        [0.9991],
        [0.9983],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367001.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5815],
        [0.5928],
        [0.5962],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(206.7020, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.5241e-02,  5.2615e-03,  1.1608e-02,  ...,  2.8887e-02,
         -7.1552e-03,  5.6761e-03],
        [-5.0449e-02,  1.0742e-02,  2.3715e-02,  ...,  5.7308e-02,
         -1.4301e-02,  1.1355e-02],
        [-2.5391e-02,  5.2941e-03,  1.1680e-02,  ...,  2.9057e-02,
         -7.1977e-03,  5.7099e-03],
        ...,
        [ 0.0000e+00, -2.2648e-04, -5.1440e-04,  ...,  4.2892e-04,
          0.0000e+00, -1.0319e-05],
        [ 0.0000e+00, -2.2648e-04, -5.1440e-04,  ...,  4.2892e-04,
          0.0000e+00, -1.0319e-05],
        [ 0.0000e+00, -2.2648e-04, -5.1440e-04,  ...,  4.2892e-04,
          0.0000e+00, -1.0319e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1031.5859, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.0186, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.4503, device='cuda:0')



h[100].sum tensor(-14.1097, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.2685, device='cuda:0')



h[200].sum tensor(-9.5457, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.6668, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0259, 0.0571,  ..., 0.1406, 0.0000, 0.0277],
        [0.0000, 0.0240, 0.0530,  ..., 0.1310, 0.0000, 0.0258],
        [0.0000, 0.0262, 0.0579,  ..., 0.1425, 0.0000, 0.0281],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42580.7461, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.4265, 0.0000, 0.2128,  ..., 0.2555, 0.0000, 0.0000],
        [0.4527, 0.0000, 0.2257,  ..., 0.2714, 0.0000, 0.0000],
        [0.4526, 0.0000, 0.2254,  ..., 0.2714, 0.0000, 0.0000],
        ...,
        [0.0123, 0.0122, 0.0115,  ..., 0.0027, 0.0000, 0.0000],
        [0.0123, 0.0122, 0.0115,  ..., 0.0027, 0.0000, 0.0000],
        [0.0123, 0.0122, 0.0115,  ..., 0.0027, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(434594.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9347.1738, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(80.6328, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4567.3125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-455.8822, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5633.2344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(428.1670, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0766],
        [ 0.0723],
        [ 0.0759],
        ...,
        [-0.8263],
        [-0.8236],
        [-0.8228]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-192015.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0014],
        [1.0043],
        ...,
        [0.9991],
        [0.9983],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367001.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2639],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.0094, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0014],
        [1.0044],
        ...,
        [0.9991],
        [0.9983],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367013.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2639],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.0094, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.3532e-04, -5.1050e-04,  ...,  4.4491e-04,
          0.0000e+00, -5.3534e-06],
        [-6.2011e-03,  1.1143e-03,  2.4710e-03,  ...,  7.4441e-03,
         -1.7541e-03,  1.3931e-03],
        [ 0.0000e+00, -2.3532e-04, -5.1050e-04,  ...,  4.4491e-04,
          0.0000e+00, -5.3534e-06],
        ...,
        [ 0.0000e+00, -2.3532e-04, -5.1050e-04,  ...,  4.4491e-04,
          0.0000e+00, -5.3534e-06],
        [ 0.0000e+00, -2.3532e-04, -5.1050e-04,  ...,  4.4491e-04,
          0.0000e+00, -5.3534e-06],
        [ 0.0000e+00, -2.3532e-04, -5.1050e-04,  ...,  4.4491e-04,
          0.0000e+00, -5.3534e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1074.5127, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.6133, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.8277, device='cuda:0')



h[100].sum tensor(-15.2878, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.6259, device='cuda:0')



h[200].sum tensor(-9.2725, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.5647, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0061, 0.0135,  ..., 0.0382, 0.0000, 0.0073],
        [0.0000, 0.0009, 0.0019,  ..., 0.0075, 0.0000, 0.0011],
        [0.0000, 0.0011, 0.0025,  ..., 0.0088, 0.0000, 0.0014],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44344.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1075, 0.0000, 0.0533,  ..., 0.0626, 0.0000, 0.0000],
        [0.0588, 0.0000, 0.0315,  ..., 0.0322, 0.0000, 0.0000],
        [0.0535, 0.0000, 0.0295,  ..., 0.0286, 0.0000, 0.0000],
        ...,
        [0.0123, 0.0122, 0.0113,  ..., 0.0027, 0.0000, 0.0000],
        [0.0123, 0.0122, 0.0113,  ..., 0.0027, 0.0000, 0.0000],
        [0.0123, 0.0122, 0.0113,  ..., 0.0027, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(441608.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9527.0293, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(83.4491, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4531.9404, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-466.3676, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5725.6050, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(442.6407, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2645],
        [-0.3735],
        [-0.4255],
        ...,
        [-0.8292],
        [-0.8265],
        [-0.8257]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-191195.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0014],
        [1.0044],
        ...,
        [0.9991],
        [0.9983],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367013.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3689],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.0520, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0014],
        [1.0044],
        ...,
        [0.9991],
        [0.9983],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367013.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3689],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.0520, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.3532e-04, -5.1050e-04,  ...,  4.4491e-04,
          0.0000e+00, -5.3534e-06],
        [-1.5956e-02,  3.2373e-03,  7.1610e-03,  ...,  1.8454e-02,
         -4.5134e-03,  3.5931e-03],
        [-2.3866e-02,  4.9589e-03,  1.0964e-02,  ...,  2.7383e-02,
         -6.7509e-03,  5.3770e-03],
        ...,
        [ 0.0000e+00, -2.3532e-04, -5.1050e-04,  ...,  4.4491e-04,
          0.0000e+00, -5.3534e-06],
        [ 0.0000e+00, -2.3532e-04, -5.1050e-04,  ...,  4.4491e-04,
          0.0000e+00, -5.3534e-06],
        [ 0.0000e+00, -2.3532e-04, -5.1050e-04,  ...,  4.4491e-04,
          0.0000e+00, -5.3534e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1125.2119, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.4812, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.1737, device='cuda:0')



h[100].sum tensor(-18.1565, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.4941, device='cuda:0')



h[200].sum tensor(-7.9429, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-29.1751, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0059, 0.0130,  ..., 0.0359, 0.0000, 0.0068],
        [0.0000, 0.0087, 0.0192,  ..., 0.0505, 0.0000, 0.0097],
        [0.0000, 0.0134, 0.0297,  ..., 0.0763, 0.0000, 0.0149],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48525.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2559, 0.0000, 0.1266,  ..., 0.1525, 0.0000, 0.0000],
        [0.3012, 0.0000, 0.1490,  ..., 0.1799, 0.0000, 0.0000],
        [0.3572, 0.0000, 0.1763,  ..., 0.2138, 0.0000, 0.0000],
        ...,
        [0.0123, 0.0122, 0.0113,  ..., 0.0027, 0.0000, 0.0000],
        [0.0123, 0.0122, 0.0113,  ..., 0.0027, 0.0000, 0.0000],
        [0.0123, 0.0122, 0.0113,  ..., 0.0027, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(456773.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10271.5586, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(102.3022, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4294.5801, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-490.7807, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5989.6211, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(485.1281, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0353],
        [-0.0290],
        [-0.0238],
        ...,
        [-0.8283],
        [-0.8254],
        [-0.8242]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-169200.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0014],
        [1.0044],
        ...,
        [0.9991],
        [0.9983],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367013.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.3961, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0014],
        [1.0044],
        ...,
        [0.9992],
        [0.9983],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367025.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.3961, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.4871e-04, -5.1009e-04,  ...,  4.6730e-04,
          0.0000e+00,  4.3335e-06],
        [ 0.0000e+00, -2.4871e-04, -5.1009e-04,  ...,  4.6730e-04,
          0.0000e+00,  4.3335e-06],
        [-6.6361e-03,  1.1970e-03,  2.6839e-03,  ...,  7.9660e-03,
         -1.8731e-03,  1.5026e-03],
        ...,
        [ 0.0000e+00, -2.4871e-04, -5.1009e-04,  ...,  4.6730e-04,
          0.0000e+00,  4.3335e-06],
        [ 0.0000e+00, -2.4871e-04, -5.1009e-04,  ...,  4.6730e-04,
          0.0000e+00,  4.3335e-06],
        [ 0.0000e+00, -2.4871e-04, -5.1009e-04,  ...,  4.6730e-04,
          0.0000e+00,  4.3335e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1107.1254, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.8205, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.0972, device='cuda:0')



h[100].sum tensor(-15.4365, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.6958, device='cuda:0')



h[200].sum tensor(-9.4897, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.9361, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.8695e-03, 0.0000e+00,
         1.7337e-05],
        [0.0000e+00, 1.1983e-03, 2.6868e-03,  ..., 9.3775e-03, 0.0000e+00,
         1.5173e-03],
        [0.0000e+00, 7.4146e-03, 1.6460e-02,  ..., 4.2917e-02, 0.0000e+00,
         8.2183e-03],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.8986e-03, 0.0000e+00,
         1.7606e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.8984e-03, 0.0000e+00,
         1.7605e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.8985e-03, 0.0000e+00,
         1.7606e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45478.6484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0487, 0.0030, 0.0278,  ..., 0.0254, 0.0000, 0.0000],
        [0.0974, 0.0003, 0.0504,  ..., 0.0556, 0.0000, 0.0000],
        [0.1890, 0.0000, 0.0944,  ..., 0.1115, 0.0000, 0.0000],
        ...,
        [0.0126, 0.0122, 0.0113,  ..., 0.0028, 0.0000, 0.0000],
        [0.0126, 0.0122, 0.0113,  ..., 0.0028, 0.0000, 0.0000],
        [0.0126, 0.0122, 0.0113,  ..., 0.0028, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(447550., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9916.6602, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(86.3364, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4303.6621, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-473.1012, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5802.7734, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(453.0214, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0086],
        [ 0.0622],
        [ 0.0889],
        ...,
        [-0.8284],
        [-0.8258],
        [-0.8250]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-169706.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0014],
        [1.0044],
        ...,
        [0.9992],
        [0.9983],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367025.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(160.5194, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0014],
        [1.0044],
        ...,
        [0.9992],
        [0.9983],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367036.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(160.5194, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.5050e-04, -5.1248e-04,  ...,  4.7212e-04,
          0.0000e+00,  1.0415e-05],
        [ 0.0000e+00, -2.5050e-04, -5.1248e-04,  ...,  4.7212e-04,
          0.0000e+00,  1.0415e-05],
        [ 0.0000e+00, -2.5050e-04, -5.1248e-04,  ...,  4.7212e-04,
          0.0000e+00,  1.0415e-05],
        ...,
        [ 0.0000e+00, -2.5050e-04, -5.1248e-04,  ...,  4.7212e-04,
          0.0000e+00,  1.0415e-05],
        [ 0.0000e+00, -2.5050e-04, -5.1248e-04,  ...,  4.7212e-04,
          0.0000e+00,  1.0415e-05],
        [ 0.0000e+00, -2.5050e-04, -5.1248e-04,  ...,  4.7212e-04,
          0.0000e+00,  1.0415e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1040.9460, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.8250, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.7749, device='cuda:0')



h[100].sum tensor(-10.9878, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.3148, device='cuda:0')



h[200].sum tensor(-11.6194, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-17.6024, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 6.0676e-03, 1.3484e-02,  ..., 3.5956e-02, 0.0000e+00,
         6.8490e-03],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.8900e-03, 0.0000e+00,
         4.1694e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.8923e-03, 0.0000e+00,
         4.1745e-05],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.9184e-03, 0.0000e+00,
         4.2321e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.9183e-03, 0.0000e+00,
         4.2318e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.9183e-03, 0.0000e+00,
         4.2319e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40124.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1195, 0.0000, 0.0622,  ..., 0.0687, 0.0000, 0.0000],
        [0.0480, 0.0045, 0.0283,  ..., 0.0248, 0.0000, 0.0000],
        [0.0256, 0.0077, 0.0177,  ..., 0.0111, 0.0000, 0.0000],
        ...,
        [0.0127, 0.0124, 0.0117,  ..., 0.0030, 0.0000, 0.0000],
        [0.0127, 0.0124, 0.0117,  ..., 0.0030, 0.0000, 0.0000],
        [0.0127, 0.0124, 0.0117,  ..., 0.0030, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(427679.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9270.3145, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(65.8027, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4379.8359, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-441.8065, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5509.8223, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(400.6997, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0384],
        [-0.2644],
        [-0.5321],
        ...,
        [-0.8350],
        [-0.8324],
        [-0.8316]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-170204.9219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0014],
        [1.0044],
        ...,
        [0.9992],
        [0.9983],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367036.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6064],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.6792, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0015],
        [1.0045],
        ...,
        [0.9992],
        [0.9983],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367047.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6064],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.6792, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2991e-02,  2.5958e-03,  5.7474e-03,  ...,  1.5175e-02,
         -3.6512e-03,  2.9549e-03],
        [-2.6207e-02,  5.4823e-03,  1.2121e-02,  ...,  3.0141e-02,
         -7.3657e-03,  5.9461e-03],
        [-3.7992e-02,  8.0562e-03,  1.7804e-02,  ...,  4.3486e-02,
         -1.0678e-02,  8.6133e-03],
        ...,
        [ 0.0000e+00, -2.4148e-04, -5.1751e-04,  ...,  4.6383e-04,
          0.0000e+00,  1.4724e-05],
        [ 0.0000e+00, -2.4148e-04, -5.1751e-04,  ...,  4.6383e-04,
          0.0000e+00,  1.4724e-05],
        [ 0.0000e+00, -2.4148e-04, -5.1751e-04,  ...,  4.6383e-04,
          0.0000e+00,  1.4724e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1066.0912, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.4189, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.2997, device='cuda:0')



h[100].sum tensor(-12.1652, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.7104, device='cuda:0')



h[200].sum tensor(-10.9656, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.7035, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.7594e-02, 3.8911e-02,  ..., 9.8084e-02, 0.0000e+00,
         1.9292e-02],
        [0.0000e+00, 2.5405e-02, 5.6158e-02,  ..., 1.3859e-01, 0.0000e+00,
         2.7387e-02],
        [0.0000e+00, 3.8019e-02, 8.4012e-02,  ..., 2.0400e-01, 0.0000e+00,
         4.0460e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.8850e-03, 0.0000e+00,
         5.9839e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.8848e-03, 0.0000e+00,
         5.9834e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.8849e-03, 0.0000e+00,
         5.9837e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40614.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.4623, 0.0000, 0.2317,  ..., 0.2792, 0.0000, 0.0000],
        [0.6489, 0.0000, 0.3271,  ..., 0.3924, 0.0000, 0.0000],
        [0.8444, 0.0000, 0.4271,  ..., 0.5111, 0.0000, 0.0000],
        ...,
        [0.0128, 0.0127, 0.0122,  ..., 0.0033, 0.0000, 0.0000],
        [0.0128, 0.0127, 0.0122,  ..., 0.0033, 0.0000, 0.0000],
        [0.0128, 0.0127, 0.0122,  ..., 0.0033, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(429420.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9158.2334, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(66.6918, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4654.5840, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-446.2021, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5622.9370, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(408.0489, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0147],
        [-0.0660],
        [-0.1129],
        ...,
        [-0.8461],
        [-0.8434],
        [-0.8426]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-192926.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0015],
        [1.0045],
        ...,
        [0.9992],
        [0.9983],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367047.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.9824, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0015],
        [1.0045],
        ...,
        [0.9992],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367058.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.9824, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.3614e-04, -5.2087e-04,  ...,  4.5454e-04,
          0.0000e+00,  1.2360e-05],
        [ 0.0000e+00, -2.3614e-04, -5.2087e-04,  ...,  4.5454e-04,
          0.0000e+00,  1.2360e-05],
        [ 0.0000e+00, -2.3614e-04, -5.2087e-04,  ...,  4.5454e-04,
          0.0000e+00,  1.2360e-05],
        ...,
        [ 0.0000e+00, -2.3614e-04, -5.2087e-04,  ...,  4.5454e-04,
          0.0000e+00,  1.2360e-05],
        [ 0.0000e+00, -2.3614e-04, -5.2087e-04,  ...,  4.5454e-04,
          0.0000e+00,  1.2360e-05],
        [ 0.0000e+00, -2.3614e-04, -5.2087e-04,  ...,  4.5454e-04,
          0.0000e+00,  1.2360e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1130.5863, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.1022, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.5418, device='cuda:0')



h[100].sum tensor(-15.6301, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.8112, device='cuda:0')



h[200].sum tensor(-9.2105, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.5487, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.3773e-03, 3.0409e-03,  ..., 1.0183e-02, 0.0000e+00,
         1.7215e-03],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.8197e-03, 0.0000e+00,
         4.9484e-05],
        [0.0000e+00, 1.7596e-03, 3.8850e-03,  ..., 1.2172e-02, 0.0000e+00,
         2.1185e-03],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.8475e-03, 0.0000e+00,
         5.0240e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.8474e-03, 0.0000e+00,
         5.0236e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.8474e-03, 0.0000e+00,
         5.0238e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46583.2070, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0417, 0.0008, 0.0255,  ..., 0.0220, 0.0000, 0.0000],
        [0.0331, 0.0007, 0.0217,  ..., 0.0165, 0.0000, 0.0000],
        [0.0581, 0.0001, 0.0335,  ..., 0.0319, 0.0000, 0.0000],
        ...,
        [0.0127, 0.0129, 0.0126,  ..., 0.0034, 0.0000, 0.0000],
        [0.0127, 0.0129, 0.0126,  ..., 0.0034, 0.0000, 0.0000],
        [0.0127, 0.0129, 0.0126,  ..., 0.0034, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(458776.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10309.1230, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(92.9071, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4557.6885, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-482.1245, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6205.5786, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(470.9047, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4355],
        [-0.3587],
        [-0.2063],
        ...,
        [-0.8591],
        [-0.8563],
        [-0.8555]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-184492.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0015],
        [1.0045],
        ...,
        [0.9992],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367058.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(308.5957, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0015],
        [1.0046],
        ...,
        [0.9992],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367069.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(308.5957, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.8513e-03,  8.2420e-04,  1.8246e-03,  ...,  5.9455e-03,
         -1.3576e-03,  1.0989e-03],
        [ 0.0000e+00, -2.3768e-04, -5.1989e-04,  ...,  4.4032e-04,
          0.0000e+00, -1.7427e-06],
        [ 0.0000e+00, -2.3768e-04, -5.1989e-04,  ...,  4.4032e-04,
          0.0000e+00, -1.7427e-06],
        ...,
        [ 0.0000e+00, -2.3768e-04, -5.1989e-04,  ...,  4.4032e-04,
          0.0000e+00, -1.7427e-06],
        [ 0.0000e+00, -2.3768e-04, -5.1989e-04,  ...,  4.4032e-04,
          0.0000e+00, -1.7427e-06],
        [ 0.0000e+00, -2.3768e-04, -5.1989e-04,  ...,  4.4032e-04,
          0.0000e+00, -1.7427e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1226.4899, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.5959, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.5595, device='cuda:0')



h[100].sum tensor(-21.1736, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.3726, device='cuda:0')



h[200].sum tensor(-6.3703, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-33.8404, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0032, 0.0071,  ..., 0.0210, 0.0000, 0.0038],
        [0.0000, 0.0060, 0.0134,  ..., 0.0368, 0.0000, 0.0070],
        [0.0000, 0.0105, 0.0233,  ..., 0.0602, 0.0000, 0.0117],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54028.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1895, 0.0000, 0.0964,  ..., 0.1139, 0.0000, 0.0000],
        [0.2485, 0.0000, 0.1258,  ..., 0.1499, 0.0000, 0.0000],
        [0.3145, 0.0000, 0.1593,  ..., 0.1899, 0.0000, 0.0000],
        ...,
        [0.0123, 0.0131, 0.0126,  ..., 0.0032, 0.0000, 0.0000],
        [0.0123, 0.0131, 0.0126,  ..., 0.0032, 0.0000, 0.0000],
        [0.0123, 0.0131, 0.0126,  ..., 0.0032, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(490088.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11298.9648, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(125.2078, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4623.4902, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-526.0693, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6756.9424, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(547.0568, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0022],
        [-0.0089],
        [-0.0160],
        ...,
        [-0.8729],
        [-0.8700],
        [-0.8692]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-185858.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0015],
        [1.0046],
        ...,
        [0.9992],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367069.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(188.7726, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0015],
        [1.0047],
        ...,
        [0.9992],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367080.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(188.7726, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.5420e-04, -5.1820e-04,  ...,  4.4318e-04,
          0.0000e+00, -9.3685e-06],
        [ 0.0000e+00, -2.5420e-04, -5.1820e-04,  ...,  4.4318e-04,
          0.0000e+00, -9.3685e-06],
        [ 0.0000e+00, -2.5420e-04, -5.1820e-04,  ...,  4.4318e-04,
          0.0000e+00, -9.3685e-06],
        ...,
        [ 0.0000e+00, -2.5420e-04, -5.1820e-04,  ...,  4.4318e-04,
          0.0000e+00, -9.3685e-06],
        [ 0.0000e+00, -2.5420e-04, -5.1820e-04,  ...,  4.4318e-04,
          0.0000e+00, -9.3685e-06],
        [ 0.0000e+00, -2.5420e-04, -5.1820e-04,  ...,  4.4318e-04,
          0.0000e+00, -9.3685e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1073.5941, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.2867, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.0234, device='cuda:0')



h[100].sum tensor(-12.7956, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.8982, device='cuda:0')



h[200].sum tensor(-10.2460, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-20.7007, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0012, 0.0027,  ..., 0.0105, 0.0000, 0.0017],
        [0.0000, 0.0006, 0.0013,  ..., 0.0062, 0.0000, 0.0009],
        [0.0000, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42904.9453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0430, 0.0000, 0.0243,  ..., 0.0239, 0.0000, 0.0000],
        [0.0338, 0.0024, 0.0208,  ..., 0.0178, 0.0000, 0.0000],
        [0.0232, 0.0062, 0.0168,  ..., 0.0106, 0.0000, 0.0000],
        ...,
        [0.0122, 0.0133, 0.0125,  ..., 0.0032, 0.0000, 0.0000],
        [0.0121, 0.0133, 0.0125,  ..., 0.0032, 0.0000, 0.0000],
        [0.0121, 0.0133, 0.0125,  ..., 0.0032, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(446109.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9765.4395, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(81.9441, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4783.2119, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-458.7710, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5987.4160, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(433.3242, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6446],
        [-0.6991],
        [-0.7769],
        ...,
        [-0.8856],
        [-0.8826],
        [-0.8817]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-186419.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0015],
        [1.0047],
        ...,
        [0.9992],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367080.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2610],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.5701, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0015],
        [1.0047],
        ...,
        [0.9992],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367092.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2610],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.5701, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0892e-03,  1.0492e-03,  2.4325e-03,  ...,  7.3879e-03,
         -1.6967e-03,  1.3762e-03],
        [-5.9299e-03,  1.0143e-03,  2.3553e-03,  ...,  7.2067e-03,
         -1.6523e-03,  1.3399e-03],
        [-1.2019e-02,  2.3487e-03,  5.3044e-03,  ...,  1.4132e-02,
         -3.3490e-03,  2.7250e-03],
        ...,
        [ 0.0000e+00, -2.8525e-04, -5.1665e-04,  ...,  4.6219e-04,
          0.0000e+00, -8.8899e-06],
        [ 0.0000e+00, -2.8525e-04, -5.1665e-04,  ...,  4.6219e-04,
          0.0000e+00, -8.8899e-06],
        [ 0.0000e+00, -2.8525e-04, -5.1665e-04,  ...,  4.6219e-04,
          0.0000e+00, -8.8899e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1163.2959, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.8310, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.8619, device='cuda:0')



h[100].sum tensor(-16.8940, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.1537, device='cuda:0')



h[200].sum tensor(-8.5258, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.3677, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0038, 0.0086,  ..., 0.0244, 0.0000, 0.0045],
        [0.0000, 0.0079, 0.0178,  ..., 0.0486, 0.0000, 0.0093],
        [0.0000, 0.0035, 0.0081,  ..., 0.0245, 0.0000, 0.0045],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0019, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47596.9570, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1237, 0.0000, 0.0637,  ..., 0.0735, 0.0000, 0.0000],
        [0.1729, 0.0000, 0.0867,  ..., 0.1042, 0.0000, 0.0000],
        [0.1562, 0.0000, 0.0785,  ..., 0.0939, 0.0000, 0.0000],
        ...,
        [0.0122, 0.0134, 0.0124,  ..., 0.0032, 0.0000, 0.0000],
        [0.0122, 0.0134, 0.0124,  ..., 0.0032, 0.0000, 0.0000],
        [0.0122, 0.0134, 0.0124,  ..., 0.0032, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(460871.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10325.8828, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(99.0238, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4689.7231, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-484.3912, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6252.2822, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(480.3138, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0901],
        [ 0.0974],
        [ 0.1029],
        ...,
        [-0.8892],
        [-0.8864],
        [-0.8856]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-179615.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0015],
        [1.0047],
        ...,
        [0.9992],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367092.3438, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 150.0 event: 750 loss: tensor(458.0103, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.3905, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0016],
        [1.0048],
        ...,
        [0.9992],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367104.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.3905, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.1673e-04, -5.1282e-04,  ...,  4.8090e-04,
          0.0000e+00, -9.0929e-06],
        [ 0.0000e+00, -3.1673e-04, -5.1282e-04,  ...,  4.8090e-04,
          0.0000e+00, -9.0929e-06],
        [ 0.0000e+00, -3.1673e-04, -5.1282e-04,  ...,  4.8090e-04,
          0.0000e+00, -9.0929e-06],
        ...,
        [ 0.0000e+00, -3.1673e-04, -5.1282e-04,  ...,  4.8090e-04,
          0.0000e+00, -9.0929e-06],
        [ 0.0000e+00, -3.1673e-04, -5.1282e-04,  ...,  4.8090e-04,
          0.0000e+00, -9.0929e-06],
        [ 0.0000e+00, -3.1673e-04, -5.1282e-04,  ...,  4.8090e-04,
          0.0000e+00, -9.0929e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1151.6865, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.2659, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.8131, device='cuda:0')



h[100].sum tensor(-15.7307, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.8815, device='cuda:0')



h[200].sum tensor(-9.2822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.9224, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0020, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46650.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0118, 0.0132, 0.0119,  ..., 0.0031, 0.0000, 0.0000],
        [0.0125, 0.0129, 0.0121,  ..., 0.0036, 0.0000, 0.0000],
        [0.0142, 0.0121, 0.0126,  ..., 0.0047, 0.0000, 0.0000],
        ...,
        [0.0123, 0.0135, 0.0122,  ..., 0.0032, 0.0000, 0.0000],
        [0.0123, 0.0135, 0.0122,  ..., 0.0032, 0.0000, 0.0000],
        [0.0123, 0.0135, 0.0122,  ..., 0.0032, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(466865.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10118.0205, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(90.3933, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5038.2363, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-477.1893, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6281.5620, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(468.8835, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7000],
        [-0.7165],
        [-0.7145],
        ...,
        [-0.8931],
        [-0.8909],
        [-0.8907]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-208724.2031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0016],
        [1.0048],
        ...,
        [0.9992],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367104.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(186.5585, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [1.0016],
        [1.0049],
        ...,
        [0.9992],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367116.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(186.5585, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.5238e-04, -5.0435e-04,  ...,  5.0919e-04,
          0.0000e+00, -7.7074e-06],
        [ 0.0000e+00, -3.5238e-04, -5.0435e-04,  ...,  5.0919e-04,
          0.0000e+00, -7.7074e-06],
        [ 0.0000e+00, -3.5238e-04, -5.0435e-04,  ...,  5.0919e-04,
          0.0000e+00, -7.7074e-06],
        ...,
        [ 0.0000e+00, -3.5238e-04, -5.0435e-04,  ...,  5.0919e-04,
          0.0000e+00, -7.7074e-06],
        [ 0.0000e+00, -3.5238e-04, -5.0435e-04,  ...,  5.0919e-04,
          0.0000e+00, -7.7074e-06],
        [ 0.0000e+00, -3.5238e-04, -5.0435e-04,  ...,  5.0919e-04,
          0.0000e+00, -7.7074e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1104.6287, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.9690, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.8472, device='cuda:0')



h[100].sum tensor(-12.5482, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.8525, device='cuda:0')



h[200].sum tensor(-11.0127, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-20.4579, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0005, 0.0014,  ..., 0.0064, 0.0000, 0.0009],
        [0.0000, 0.0010, 0.0027,  ..., 0.0108, 0.0000, 0.0017],
        [0.0000, 0.0005, 0.0014,  ..., 0.0064, 0.0000, 0.0009],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43553., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0432, 0.0010, 0.0238,  ..., 0.0234, 0.0000, 0.0000],
        [0.0518, 0.0000, 0.0272,  ..., 0.0290, 0.0000, 0.0000],
        [0.0413, 0.0016, 0.0229,  ..., 0.0222, 0.0000, 0.0000],
        ...,
        [0.0124, 0.0135, 0.0118,  ..., 0.0032, 0.0000, 0.0000],
        [0.0124, 0.0135, 0.0118,  ..., 0.0032, 0.0000, 0.0000],
        [0.0124, 0.0135, 0.0118,  ..., 0.0032, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(455625.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9997.6104, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(76.2838, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4817.2808, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-454.9079, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6054.0562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(433.5099, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2127],
        [-0.1131],
        [-0.0736],
        ...,
        [-0.8993],
        [-0.8963],
        [-0.8954]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-182349.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [1.0016],
        [1.0049],
        ...,
        [0.9992],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367116.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(296.6881, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [1.0016],
        [1.0050],
        ...,
        [0.9992],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367128.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(296.6881, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.8368e-04, -4.9744e-04,  ...,  5.3612e-04,
          0.0000e+00, -4.1338e-06],
        [ 0.0000e+00, -3.8368e-04, -4.9744e-04,  ...,  5.3612e-04,
          0.0000e+00, -4.1338e-06],
        [-1.2680e-02,  2.3978e-03,  5.6657e-03,  ...,  1.5011e-02,
         -3.5103e-03,  2.8920e-03],
        ...,
        [ 0.0000e+00, -3.8368e-04, -4.9744e-04,  ...,  5.3612e-04,
          0.0000e+00, -4.1338e-06],
        [ 0.0000e+00, -3.8368e-04, -4.9744e-04,  ...,  5.3612e-04,
          0.0000e+00, -4.1338e-06],
        [ 0.0000e+00, -3.8368e-04, -4.9744e-04,  ...,  5.3612e-04,
          0.0000e+00, -4.1338e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1264.7069, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.1195, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.6118, device='cuda:0')



h[100].sum tensor(-20.0475, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.1267, device='cuda:0')



h[200].sum tensor(-7.7399, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-32.5346, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0011, 0.0028,  ..., 0.0100, 0.0000, 0.0016],
        [0.0000, 0.0071, 0.0167,  ..., 0.0449, 0.0000, 0.0085],
        [0.0000, 0.0113, 0.0260,  ..., 0.0667, 0.0000, 0.0129],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52602.3242, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.1271e-01, 6.7954e-05, 5.7455e-02,  ..., 6.5231e-02, 0.0000e+00,
         0.0000e+00],
        [2.3432e-01, 0.0000e+00, 1.1635e-01,  ..., 1.3941e-01, 0.0000e+00,
         0.0000e+00],
        [3.3878e-01, 0.0000e+00, 1.6786e-01,  ..., 2.0271e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.2512e-02, 1.3640e-02, 1.1372e-02,  ..., 3.0986e-03, 0.0000e+00,
         0.0000e+00],
        [1.2509e-02, 1.3639e-02, 1.1370e-02,  ..., 3.0980e-03, 0.0000e+00,
         0.0000e+00],
        [1.2509e-02, 1.3640e-02, 1.1371e-02,  ..., 3.0980e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(485975.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11069.2051, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(109.2246, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4693.3682, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-505.4439, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6529.7310, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(520.7102, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0977],
        [ 0.1261],
        [ 0.1255],
        ...,
        [-0.9021],
        [-0.8993],
        [-0.8985]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-175859.0156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [1.0016],
        [1.0050],
        ...,
        [0.9992],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367128.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(189.5996, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [1.0017],
        [1.0051],
        ...,
        [0.9992],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367139.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(189.5996, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.4304e-03,  1.4546e-03,  3.6018e-03,  ...,  1.0183e-02,
         -2.3287e-03,  1.9367e-03],
        [ 0.0000e+00, -3.9596e-04, -5.0012e-04,  ...,  5.4821e-04,
          0.0000e+00,  8.0508e-06],
        [ 0.0000e+00, -3.9596e-04, -5.0012e-04,  ...,  5.4821e-04,
          0.0000e+00,  8.0508e-06],
        ...,
        [ 0.0000e+00, -3.9596e-04, -5.0012e-04,  ...,  5.4821e-04,
          0.0000e+00,  8.0508e-06],
        [ 0.0000e+00, -3.9596e-04, -5.0012e-04,  ...,  5.4821e-04,
          0.0000e+00,  8.0508e-06],
        [ 0.0000e+00, -3.9596e-04, -5.0012e-04,  ...,  5.4821e-04,
          0.0000e+00,  8.0508e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1135.7021, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.1554, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.0892, device='cuda:0')



h[100].sum tensor(-12.6776, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.9153, device='cuda:0')



h[200].sum tensor(-11.3414, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-20.7914, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 5.1065e-03, 1.2075e-02,  ..., 3.2905e-02, 0.0000e+00,
         6.1799e-03],
        [0.0000e+00, 1.4559e-03, 3.6052e-03,  ..., 1.1839e-02, 0.0000e+00,
         1.9627e-03],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.1981e-03, 0.0000e+00,
         3.2280e-05],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.2306e-03, 0.0000e+00,
         3.2757e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.2304e-03, 0.0000e+00,
         3.2754e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.2305e-03, 0.0000e+00,
         3.2756e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42862.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1732, 0.0000, 0.0871,  ..., 0.1020, 0.0000, 0.0000],
        [0.0882, 0.0018, 0.0465,  ..., 0.0501, 0.0000, 0.0000],
        [0.0373, 0.0046, 0.0225,  ..., 0.0188, 0.0000, 0.0000],
        ...,
        [0.0127, 0.0139, 0.0117,  ..., 0.0034, 0.0000, 0.0000],
        [0.0127, 0.0139, 0.0117,  ..., 0.0034, 0.0000, 0.0000],
        [0.0127, 0.0139, 0.0117,  ..., 0.0034, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(449084.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9730.4805, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(68.6733, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4911.3242, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-448.2384, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5908.4014, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(424.4479, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1206],
        [ 0.0712],
        [-0.0245],
        ...,
        [-0.9072],
        [-0.9043],
        [-0.9035]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-185110.6719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [1.0017],
        [1.0051],
        ...,
        [0.9992],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367139.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9702],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.3213, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0017],
        [1.0052],
        ...,
        [0.9992],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367151.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9702],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.3213, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.9662e-04, -5.0810e-04,  ...,  5.4974e-04,
          0.0000e+00,  2.3955e-05],
        [-2.2523e-02,  4.5521e-03,  1.0462e-02,  ...,  2.6320e-02,
         -6.2079e-03,  5.1848e-03],
        [-2.6387e-02,  5.4012e-03,  1.2344e-02,  ...,  3.0741e-02,
         -7.2730e-03,  6.0703e-03],
        ...,
        [ 0.0000e+00, -3.9662e-04, -5.0810e-04,  ...,  5.4974e-04,
          0.0000e+00,  2.3955e-05],
        [ 0.0000e+00, -3.9662e-04, -5.0810e-04,  ...,  5.4974e-04,
          0.0000e+00,  2.3955e-05],
        [ 0.0000e+00, -3.9662e-04, -5.0810e-04,  ...,  5.4974e-04,
          0.0000e+00,  2.3955e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1157.3280, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.8883, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.6242, device='cuda:0')



h[100].sum tensor(-13.2149, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.0541, device='cuda:0')



h[200].sum tensor(-11.1036, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.5284, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 4.5538e-03, 1.0466e-02,  ..., 2.7979e-02, 0.0000e+00,
         5.2587e-03],
        [0.0000e+00, 1.0119e-02, 2.3173e-02,  ..., 5.9029e-02, 0.0000e+00,
         1.1477e-02],
        [0.0000e+00, 2.4299e-02, 5.5352e-02,  ..., 1.3702e-01, 0.0000e+00,
         2.7095e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.2371e-03, 0.0000e+00,
         9.7483e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.2369e-03, 0.0000e+00,
         9.7474e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.2370e-03, 0.0000e+00,
         9.7479e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44357.6758, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1420, 0.0000, 0.0745,  ..., 0.0829, 0.0000, 0.0000],
        [0.2519, 0.0000, 0.1279,  ..., 0.1500, 0.0000, 0.0000],
        [0.3850, 0.0000, 0.1930,  ..., 0.2310, 0.0000, 0.0000],
        ...,
        [0.0128, 0.0142, 0.0123,  ..., 0.0037, 0.0000, 0.0000],
        [0.0128, 0.0142, 0.0123,  ..., 0.0037, 0.0000, 0.0000],
        [0.0128, 0.0142, 0.0123,  ..., 0.0037, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(460433.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9973.0938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(75.1911, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5098.9312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-459.4585, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6123.4941, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(444.6190, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1046],
        [ 0.1371],
        [ 0.1434],
        ...,
        [-0.9130],
        [-0.9101],
        [-0.9093]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-202663.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0017],
        [1.0052],
        ...,
        [0.9992],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367151.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(298.4885, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0018],
        [1.0053],
        ...,
        [0.9992],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367162.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(298.4885, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.9593e-04, -5.1568e-04,  ...,  5.4959e-04,
          0.0000e+00,  3.4599e-05],
        [ 0.0000e+00, -3.9593e-04, -5.1568e-04,  ...,  5.4959e-04,
          0.0000e+00,  3.4599e-05],
        [ 0.0000e+00, -3.9593e-04, -5.1568e-04,  ...,  5.4959e-04,
          0.0000e+00,  3.4599e-05],
        ...,
        [ 0.0000e+00, -3.9593e-04, -5.1568e-04,  ...,  5.4959e-04,
          0.0000e+00,  3.4599e-05],
        [ 0.0000e+00, -3.9593e-04, -5.1568e-04,  ...,  5.4959e-04,
          0.0000e+00,  3.4599e-05],
        [ 0.0000e+00, -3.9593e-04, -5.1568e-04,  ...,  5.4959e-04,
          0.0000e+00,  3.4599e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1300.6520, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.9782, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.7551, device='cuda:0')



h[100].sum tensor(-19.9234, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.1639, device='cuda:0')



h[200].sum tensor(-7.8841, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-32.7320, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0022, 0.0000, 0.0001],
        [0.0000, 0.0000, 0.0000,  ..., 0.0022, 0.0000, 0.0001],
        [0.0000, 0.0000, 0.0000,  ..., 0.0022, 0.0000, 0.0001],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0022, 0.0000, 0.0001],
        [0.0000, 0.0000, 0.0000,  ..., 0.0022, 0.0000, 0.0001],
        [0.0000, 0.0000, 0.0000,  ..., 0.0022, 0.0000, 0.0001]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51466.7422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0426, 0.0057, 0.0256,  ..., 0.0230, 0.0000, 0.0000],
        [0.0218, 0.0089, 0.0163,  ..., 0.0101, 0.0000, 0.0000],
        [0.0138, 0.0136, 0.0130,  ..., 0.0049, 0.0000, 0.0000],
        ...,
        [0.0128, 0.0146, 0.0129,  ..., 0.0041, 0.0000, 0.0000],
        [0.0128, 0.0146, 0.0129,  ..., 0.0041, 0.0000, 0.0000],
        [0.0128, 0.0146, 0.0129,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(477720.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10935.8115, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(109.5294, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4767.4316, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-502.7979, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6511.9727, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(522.3871, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0456],
        [-0.2361],
        [-0.4461],
        ...,
        [-0.9145],
        [-0.9119],
        [-0.9116]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-168143.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0018],
        [1.0053],
        ...,
        [0.9992],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367162.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(214.1548, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0018],
        [1.0054],
        ...,
        [0.9992],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367173.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(214.1548, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.9283e-04, -5.2117e-04,  ...,  5.3982e-04,
          0.0000e+00,  3.6925e-05],
        [ 0.0000e+00, -3.9283e-04, -5.2117e-04,  ...,  5.3982e-04,
          0.0000e+00,  3.6925e-05],
        [ 0.0000e+00, -3.9283e-04, -5.2117e-04,  ...,  5.3982e-04,
          0.0000e+00,  3.6925e-05],
        ...,
        [ 0.0000e+00, -3.9283e-04, -5.2117e-04,  ...,  5.3982e-04,
          0.0000e+00,  3.6925e-05],
        [ 0.0000e+00, -3.9283e-04, -5.2117e-04,  ...,  5.3982e-04,
          0.0000e+00,  3.6925e-05],
        [ 0.0000e+00, -3.9283e-04, -5.2117e-04,  ...,  5.3982e-04,
          0.0000e+00,  3.6925e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1188.5845, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.3518, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.0434, device='cuda:0')



h[100].sum tensor(-14.2866, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.4224, device='cuda:0')



h[200].sum tensor(-10.4085, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.4841, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0022, 0.0000, 0.0001],
        [0.0000, 0.0000, 0.0000,  ..., 0.0022, 0.0000, 0.0001],
        [0.0000, 0.0000, 0.0000,  ..., 0.0022, 0.0000, 0.0001],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0022, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0022, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0022, 0.0000, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45233.9805, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0121, 0.0147, 0.0130,  ..., 0.0041, 0.0000, 0.0000],
        [0.0169, 0.0113, 0.0153,  ..., 0.0070, 0.0000, 0.0000],
        [0.0232, 0.0082, 0.0184,  ..., 0.0109, 0.0000, 0.0000],
        ...,
        [0.0126, 0.0150, 0.0133,  ..., 0.0042, 0.0000, 0.0000],
        [0.0126, 0.0150, 0.0133,  ..., 0.0042, 0.0000, 0.0000],
        [0.0126, 0.0150, 0.0133,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(458434.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10156.2676, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(88.7919, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4993.1211, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-467.2874, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6212.2041, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(462.8169, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8842],
        [-0.8053],
        [-0.6378],
        ...,
        [-0.9323],
        [-0.9293],
        [-0.9285]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-178930.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0018],
        [1.0054],
        ...,
        [0.9992],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367173.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.4917],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(436.8009, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0019],
        [1.0055],
        ...,
        [0.9992],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367184.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.4917],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(436.8009, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.9442e-03,  1.1398e-03,  2.8723e-03,  ...,  8.5082e-03,
         -1.9015e-03,  1.6244e-03],
        [-1.6395e-02,  3.2225e-03,  7.4895e-03,  ...,  1.9357e-02,
         -4.4892e-03,  3.7992e-03],
        [-2.0715e-02,  4.1748e-03,  9.6006e-03,  ...,  2.4317e-02,
         -5.6724e-03,  4.7936e-03],
        ...,
        [ 0.0000e+00, -3.9059e-04, -5.2049e-04,  ...,  5.3619e-04,
          0.0000e+00,  2.6292e-05],
        [ 0.0000e+00, -3.9059e-04, -5.2049e-04,  ...,  5.3619e-04,
          0.0000e+00,  2.6292e-05],
        [ 0.0000e+00, -3.9059e-04, -5.2049e-04,  ...,  5.3619e-04,
          0.0000e+00,  2.6292e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1505.2839, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-40.6485, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.7627, device='cuda:0')



h[100].sum tensor(-29.9992, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.0201, device='cuda:0')



h[200].sum tensor(-2.7898, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-47.8993, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0061, 0.0141,  ..., 0.0378, 0.0000, 0.0072],
        [0.0000, 0.0137, 0.0318,  ..., 0.0818, 0.0000, 0.0161],
        [0.0000, 0.0169, 0.0389,  ..., 0.0984, 0.0000, 0.0194],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0022, 0.0000, 0.0001],
        [0.0000, 0.0000, 0.0000,  ..., 0.0022, 0.0000, 0.0001],
        [0.0000, 0.0000, 0.0000,  ..., 0.0022, 0.0000, 0.0001]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68270.3672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2046, 0.0000, 0.1046,  ..., 0.1228, 0.0000, 0.0000],
        [0.3256, 0.0000, 0.1641,  ..., 0.1966, 0.0000, 0.0000],
        [0.4119, 0.0000, 0.2070,  ..., 0.2489, 0.0000, 0.0000],
        ...,
        [0.0123, 0.0154, 0.0134,  ..., 0.0040, 0.0000, 0.0000],
        [0.0123, 0.0154, 0.0134,  ..., 0.0040, 0.0000, 0.0000],
        [0.0123, 0.0154, 0.0134,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(566222.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(13444.5703, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(181.8507, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5082.8032, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-604.9401, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8045.9316, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(697.6559, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0847],
        [ 0.0625],
        [ 0.0384],
        ...,
        [-0.9357],
        [-0.9381],
        [-0.9389]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-198529.8906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0019],
        [1.0055],
        ...,
        [0.9992],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367184.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6772],
        [0.4797],
        [0.4402],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.8081, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0019],
        [1.0056],
        ...,
        [0.9992],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367195.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6772],
        [0.4797],
        [0.4402],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.8081, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.9544e-02,  3.9176e-03,  9.0446e-03,  ...,  2.3012e-02,
         -5.3399e-03,  4.5221e-03],
        [-3.4288e-02,  7.1692e-03,  1.6257e-02,  ...,  3.9958e-02,
         -9.3682e-03,  7.9201e-03],
        [-1.8549e-02,  3.6980e-03,  8.5577e-03,  ...,  2.1867e-02,
         -5.0679e-03,  4.2926e-03],
        ...,
        [ 0.0000e+00, -3.9275e-04, -5.1559e-04,  ...,  5.4818e-04,
          0.0000e+00,  1.7676e-05],
        [ 0.0000e+00, -3.9275e-04, -5.1559e-04,  ...,  5.4818e-04,
          0.0000e+00,  1.7676e-05],
        [ 0.0000e+00, -3.9275e-04, -5.1559e-04,  ...,  5.4818e-04,
          0.0000e+00,  1.7676e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1182.9370, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.7024, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.5383, device='cuda:0')



h[100].sum tensor(-13.7980, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.2913, device='cuda:0')



h[200].sum tensor(-10.4077, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.7881, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.9561e-02, 4.4809e-02,  ..., 1.1233e-01, 0.0000e+00,
         2.2155e-02],
        [0.0000e+00, 2.0269e-02, 4.6381e-02,  ..., 1.1603e-01, 0.0000e+00,
         2.2896e-02],
        [0.0000e+00, 2.1546e-02, 4.9214e-02,  ..., 1.2269e-01, 0.0000e+00,
         2.4233e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.2321e-03, 0.0000e+00,
         7.1975e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.2319e-03, 0.0000e+00,
         7.1968e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.2320e-03, 0.0000e+00,
         7.1972e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44312.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3374, 0.0000, 0.1712,  ..., 0.2034, 0.0000, 0.0000],
        [0.3920, 0.0000, 0.1974,  ..., 0.2370, 0.0000, 0.0000],
        [0.4017, 0.0000, 0.2017,  ..., 0.2430, 0.0000, 0.0000],
        ...,
        [0.0121, 0.0157, 0.0134,  ..., 0.0040, 0.0000, 0.0000],
        [0.0121, 0.0157, 0.0134,  ..., 0.0040, 0.0000, 0.0000],
        [0.0121, 0.0157, 0.0134,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(458868.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9827.3682, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(86.8050, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5301.8643, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-462.8255, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6239.4062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(454.2530, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0860],
        [ 0.1029],
        [ 0.1131],
        ...,
        [-0.9546],
        [-0.9515],
        [-0.9506]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-200697.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0019],
        [1.0056],
        ...,
        [0.9992],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367195.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5967],
        [0.6255],
        [0.6196],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(213.3888, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0020],
        [1.0057],
        ...,
        [0.9992],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367206.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5967],
        [0.6255],
        [0.6196],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(213.3888, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.4824e-02,  5.0799e-03,  1.1651e-02,  ...,  2.9132e-02,
         -6.7677e-03,  5.7362e-03],
        [-4.8287e-02,  1.0257e-02,  2.3142e-02,  ...,  5.6132e-02,
         -1.3164e-02,  1.1152e-02],
        [-4.7249e-02,  1.0028e-02,  2.2633e-02,  ...,  5.4937e-02,
         -1.2881e-02,  1.0912e-02],
        ...,
        [ 0.0000e+00, -3.9801e-04, -5.0769e-04,  ...,  5.6509e-04,
          0.0000e+00,  6.3389e-06],
        [ 0.0000e+00, -3.9801e-04, -5.0769e-04,  ...,  5.6509e-04,
          0.0000e+00,  6.3389e-06],
        [ 0.0000e+00, -3.9801e-04, -5.0769e-04,  ...,  5.6509e-04,
          0.0000e+00,  6.3389e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1200.3806, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.2999, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.9825, device='cuda:0')



h[100].sum tensor(-14.2341, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.4065, device='cuda:0')



h[200].sum tensor(-10.1765, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.4001, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 2.5374e-02, 5.7820e-02,  ..., 1.4289e-01, 0.0000e+00,
         2.8232e-02],
        [0.0000e+00, 3.4094e-02, 7.7176e-02,  ..., 1.8838e-01, 0.0000e+00,
         3.7355e-02],
        [0.0000e+00, 3.5788e-02, 8.0939e-02,  ..., 1.9723e-01, 0.0000e+00,
         3.9130e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.3013e-03, 0.0000e+00,
         2.5814e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.3011e-03, 0.0000e+00,
         2.5812e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.3012e-03, 0.0000e+00,
         2.5813e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45993.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.4836, 0.0000, 0.2454,  ..., 0.2919, 0.0000, 0.0000],
        [0.6232, 0.0000, 0.3148,  ..., 0.3768, 0.0000, 0.0000],
        [0.6459, 0.0000, 0.3256,  ..., 0.3908, 0.0000, 0.0000],
        ...,
        [0.0120, 0.0158, 0.0133,  ..., 0.0040, 0.0000, 0.0000],
        [0.0120, 0.0158, 0.0133,  ..., 0.0040, 0.0000, 0.0000],
        [0.0120, 0.0158, 0.0133,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(470330.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10206.6055, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(92.6282, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5206.4238, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-472.6914, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6413.4922, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(470.7768, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0721],
        [ 0.0794],
        [ 0.0888],
        ...,
        [-0.9617],
        [-0.9584],
        [-0.9569]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-194179.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0020],
        [1.0057],
        ...,
        [0.9992],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367206.8750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 160.0 event: 800 loss: tensor(526.5768, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5879],
        [0.4434],
        [0.4792],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(307.9297, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0020],
        [1.0058],
        ...,
        [0.9992],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367218.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5879],
        [0.4434],
        [0.4792],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(307.9297, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0230e-02,  1.8557e-03,  4.5211e-03,  ...,  1.2375e-02,
         -2.7827e-03,  2.3574e-03],
        [-2.4623e-02,  5.0334e-03,  1.1579e-02,  ...,  2.8958e-02,
         -6.6979e-03,  5.6843e-03],
        [-2.1102e-02,  4.2561e-03,  9.8527e-03,  ...,  2.4902e-02,
         -5.7401e-03,  4.8705e-03],
        ...,
        [ 0.0000e+00, -4.0296e-04, -4.9563e-04,  ...,  5.8835e-04,
          0.0000e+00, -7.1836e-06],
        [ 0.0000e+00, -4.0296e-04, -4.9563e-04,  ...,  5.8835e-04,
          0.0000e+00, -7.1836e-06],
        [ 0.0000e+00, -4.0296e-04, -4.9563e-04,  ...,  5.8835e-04,
          0.0000e+00, -7.1836e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1343.0728, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.7577, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.5065, device='cuda:0')



h[100].sum tensor(-20.4653, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.3588, device='cuda:0')



h[200].sum tensor(-7.2134, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-33.7673, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0136, 0.0317,  ..., 0.0815, 0.0000, 0.0159],
        [0.0000, 0.0135, 0.0315,  ..., 0.0810, 0.0000, 0.0157],
        [0.0000, 0.0141, 0.0329,  ..., 0.0843, 0.0000, 0.0164],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0024, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53873.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2421, 0.0000, 0.1229,  ..., 0.1454, 0.0000, 0.0000],
        [0.2696, 0.0000, 0.1360,  ..., 0.1623, 0.0000, 0.0000],
        [0.2680, 0.0000, 0.1351,  ..., 0.1613, 0.0000, 0.0000],
        ...,
        [0.0118, 0.0158, 0.0132,  ..., 0.0041, 0.0000, 0.0000],
        [0.0118, 0.0158, 0.0132,  ..., 0.0041, 0.0000, 0.0000],
        [0.0118, 0.0158, 0.0132,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(504341.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11368.7939, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(119.9070, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4998.8926, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-519.8053, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6977.9038, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(549.2256, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1428],
        [ 0.1475],
        [ 0.1495],
        ...,
        [-0.9673],
        [-0.9653],
        [-0.9650]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-185514.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0020],
        [1.0058],
        ...,
        [0.9992],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367218.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2461],
        [0.3000],
        [0.2915],
        ...,
        [0.0000],
        [0.0000],
        [0.6704]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(279.4607, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0013],
        [1.0021],
        [1.0059],
        ...,
        [0.9992],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367230.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2461],
        [0.3000],
        [0.2915],
        ...,
        [0.0000],
        [0.0000],
        [0.6704]], device='cuda:0') 
g.ndata[nfet].sum tensor(279.4607, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3754e-02,  2.6347e-03,  6.2699e-03,  ...,  1.6472e-02,
         -3.7330e-03,  3.1629e-03],
        [-1.2392e-02,  2.3338e-03,  5.6013e-03,  ...,  1.4901e-02,
         -3.3634e-03,  2.8477e-03],
        [-1.3500e-02,  2.5788e-03,  6.1456e-03,  ...,  1.6180e-02,
         -3.6643e-03,  3.1043e-03],
        ...,
        [ 0.0000e+00, -4.0435e-04, -4.8317e-04,  ...,  6.0697e-04,
          0.0000e+00, -2.0172e-05],
        [-1.5453e-02,  3.0103e-03,  7.1044e-03,  ...,  1.8432e-02,
         -4.1943e-03,  3.5562e-03],
        [ 0.0000e+00, -4.0435e-04, -4.8317e-04,  ...,  6.0697e-04,
          0.0000e+00, -2.0172e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1313.6484, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.9718, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.2408, device='cuda:0')



h[100].sum tensor(-18.4052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.7709, device='cuda:0')



h[200].sum tensor(-8.0784, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.6455, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0089, 0.0214,  ..., 0.0572, 0.0000, 0.0109],
        [0.0000, 0.0091, 0.0220,  ..., 0.0586, 0.0000, 0.0112],
        [0.0000, 0.0088, 0.0213,  ..., 0.0569, 0.0000, 0.0109],
        ...,
        [0.0000, 0.0031, 0.0072,  ..., 0.0206, 0.0000, 0.0036],
        [0.0000, 0.0024, 0.0058,  ..., 0.0173, 0.0000, 0.0030],
        [0.0000, 0.0110, 0.0261,  ..., 0.0684, 0.0000, 0.0131]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50922.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2069, 0.0000, 0.1031,  ..., 0.1251, 0.0000, 0.0000],
        [0.2061, 0.0000, 0.1031,  ..., 0.1244, 0.0000, 0.0000],
        [0.2035, 0.0000, 0.1017,  ..., 0.1227, 0.0000, 0.0000],
        ...,
        [0.0710, 0.0008, 0.0410,  ..., 0.0406, 0.0000, 0.0000],
        [0.0880, 0.0000, 0.0489,  ..., 0.0511, 0.0000, 0.0000],
        [0.1426, 0.0000, 0.0747,  ..., 0.0845, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(487094.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10902.0996, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(106.0035, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4788.0410, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-503.3943, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6675.8398, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(517.2200, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1825],
        [ 0.1827],
        [ 0.1826],
        ...,
        [-0.3367],
        [-0.2527],
        [-0.2184]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-165929.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0013],
        [1.0021],
        [1.0059],
        ...,
        [0.9992],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367230.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(318.2406, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0013],
        [1.0021],
        [1.0059],
        ...,
        [0.9992],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367241.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(318.2406, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.9833e-04, -4.7246e-04,  ...,  6.2582e-04,
          0.0000e+00, -3.1144e-05],
        [ 0.0000e+00, -3.9833e-04, -4.7246e-04,  ...,  6.2582e-04,
          0.0000e+00, -3.1144e-05],
        [ 0.0000e+00, -3.9833e-04, -4.7246e-04,  ...,  6.2582e-04,
          0.0000e+00, -3.1144e-05],
        ...,
        [ 0.0000e+00, -3.9833e-04, -4.7246e-04,  ...,  6.2582e-04,
          0.0000e+00, -3.1144e-05],
        [ 0.0000e+00, -3.9833e-04, -4.7246e-04,  ...,  6.2582e-04,
          0.0000e+00, -3.1144e-05],
        [ 0.0000e+00, -3.9833e-04, -4.7246e-04,  ...,  6.2582e-04,
          0.0000e+00, -3.1144e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1392.8547, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.8168, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.3271, device='cuda:0')



h[100].sum tensor(-21.2320, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.5718, device='cuda:0')



h[200].sum tensor(-6.5527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-34.8980, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55071.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0308, 0.0052, 0.0211,  ..., 0.0167, 0.0000, 0.0000],
        [0.0195, 0.0100, 0.0161,  ..., 0.0096, 0.0000, 0.0000],
        [0.0156, 0.0129, 0.0144,  ..., 0.0071, 0.0000, 0.0000],
        ...,
        [0.0115, 0.0160, 0.0130,  ..., 0.0043, 0.0000, 0.0000],
        [0.0115, 0.0160, 0.0130,  ..., 0.0043, 0.0000, 0.0000],
        [0.0115, 0.0160, 0.0130,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(504073.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11443.6543, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(118.1809, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4644.3193, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-530.0925, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6947.6006, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(558.6094, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0881],
        [-0.2719],
        [-0.4635],
        ...,
        [-0.9836],
        [-0.9805],
        [-0.9795]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-164525.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0013],
        [1.0021],
        [1.0059],
        ...,
        [0.9992],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367241.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(300.6637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0022],
        [1.0061],
        ...,
        [0.9993],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367252.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(300.6637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.6277e-04, -4.7752e-04,  ...,  6.1565e-04,
          0.0000e+00, -3.1659e-05],
        [ 0.0000e+00, -3.6277e-04, -4.7752e-04,  ...,  6.1565e-04,
          0.0000e+00, -3.1659e-05],
        [-1.2197e-02,  2.3404e-03,  5.5244e-03,  ...,  1.4716e-02,
         -3.2960e-03,  2.7981e-03],
        ...,
        [ 0.0000e+00, -3.6277e-04, -4.7752e-04,  ...,  6.1565e-04,
          0.0000e+00, -3.1659e-05],
        [ 0.0000e+00, -3.6277e-04, -4.7752e-04,  ...,  6.1565e-04,
          0.0000e+00, -3.1659e-05],
        [ 0.0000e+00, -3.6277e-04, -4.7752e-04,  ...,  6.1565e-04,
          0.0000e+00, -3.1659e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1381.7283, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.0754, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.9282, device='cuda:0')



h[100].sum tensor(-19.9423, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.2088, device='cuda:0')



h[200].sum tensor(-6.7449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-32.9706, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0000, 0.0042, 0.0100,  ..., 0.0281, 0.0000, 0.0051],
        [0.0000, 0.0057, 0.0137,  ..., 0.0380, 0.0000, 0.0070],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0000, 0.0029, 0.0071,  ..., 0.0214, 0.0000, 0.0037]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52150.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0501, 0.0052, 0.0317,  ..., 0.0290, 0.0000, 0.0000],
        [0.1214, 0.0000, 0.0653,  ..., 0.0732, 0.0000, 0.0000],
        [0.1816, 0.0000, 0.0933,  ..., 0.1107, 0.0000, 0.0000],
        ...,
        [0.0228, 0.0099, 0.0187,  ..., 0.0120, 0.0000, 0.0000],
        [0.0457, 0.0057, 0.0290,  ..., 0.0265, 0.0000, 0.0000],
        [0.0999, 0.0000, 0.0537,  ..., 0.0605, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(492191.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10873.7168, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(107.0386, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4849.3965, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-515.9135, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6826.1172, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(531.6718, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0789],
        [ 0.0428],
        [ 0.1212],
        ...,
        [-0.6156],
        [-0.3361],
        [-0.0933]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-173966.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0022],
        [1.0061],
        ...,
        [0.9993],
        [0.9983],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367252.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.7568],
        [0.0000],
        [0.3125],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.4070, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0023],
        [1.0062],
        ...,
        [0.9993],
        [0.9984],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367263.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.7568],
        [0.0000],
        [0.3125],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.4070, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1114e-02,  2.1387e-03,  4.9935e-03,  ...,  1.3466e-02,
         -2.9966e-03,  2.5490e-03],
        [-2.9571e-02,  6.2367e-03,  1.4084e-02,  ...,  3.4824e-02,
         -7.9730e-03,  6.8360e-03],
        [-4.9977e-03,  7.8067e-04,  1.9809e-03,  ...,  6.3880e-03,
         -1.3475e-03,  1.1284e-03],
        ...,
        [ 0.0000e+00, -3.2898e-04, -4.8062e-04,  ...,  6.0479e-04,
          0.0000e+00, -3.2378e-05],
        [ 0.0000e+00, -3.2898e-04, -4.8062e-04,  ...,  6.0479e-04,
          0.0000e+00, -3.2378e-05],
        [ 0.0000e+00, -3.2898e-04, -4.8062e-04,  ...,  6.0479e-04,
          0.0000e+00, -3.2378e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1265.3501, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.8495, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.7452, device='cuda:0')



h[100].sum tensor(-13.8789, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.3450, device='cuda:0')



h[200].sum tensor(-9.1944, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.0731, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0220, 0.0498,  ..., 0.1239, 0.0000, 0.0242],
        [0.0000, 0.0104, 0.0242,  ..., 0.0637, 0.0000, 0.0122],
        [0.0000, 0.0118, 0.0272,  ..., 0.0707, 0.0000, 0.0136],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44021.2734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3678, 0.0000, 0.1872,  ..., 0.2260, 0.0000, 0.0000],
        [0.2895, 0.0000, 0.1474,  ..., 0.1783, 0.0000, 0.0000],
        [0.2436, 0.0000, 0.1249,  ..., 0.1500, 0.0000, 0.0000],
        ...,
        [0.0113, 0.0169, 0.0142,  ..., 0.0051, 0.0000, 0.0000],
        [0.0113, 0.0169, 0.0142,  ..., 0.0051, 0.0000, 0.0000],
        [0.0113, 0.0169, 0.0142,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(464526.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9492.8887, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(73.2213, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5343.6538, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-470.7155, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6338.0293, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(450.6194, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1191],
        [ 0.1209],
        [ 0.1181],
        ...,
        [-1.0027],
        [-0.9995],
        [-0.9985]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-218601.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0023],
        [1.0062],
        ...,
        [0.9993],
        [0.9984],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367263.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(322.1829, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0023],
        [1.0063],
        ...,
        [0.9993],
        [0.9984],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367275.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(322.1829, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.7473e-03,  1.8608e-03,  4.3267e-03,  ...,  1.1894e-02,
         -2.6222e-03,  2.2351e-03],
        [-1.9559e-02,  4.0426e-03,  9.1641e-03,  ...,  2.3259e-02,
         -5.2617e-03,  4.5166e-03],
        [-9.8113e-03,  1.8750e-03,  4.3583e-03,  ...,  1.1968e-02,
         -2.6395e-03,  2.2500e-03],
        ...,
        [ 0.0000e+00, -3.0683e-04, -4.7916e-04,  ...,  6.0290e-04,
          0.0000e+00, -3.1556e-05],
        [ 0.0000e+00, -3.0683e-04, -4.7916e-04,  ...,  6.0290e-04,
          0.0000e+00, -3.1556e-05],
        [ 0.0000e+00, -3.0683e-04, -4.7916e-04,  ...,  6.0290e-04,
          0.0000e+00, -3.1556e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1443.2352, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.1173, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.6408, device='cuda:0')



h[100].sum tensor(-21.4320, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.6532, device='cuda:0')



h[200].sum tensor(-5.3117, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-35.3303, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0134, 0.0304,  ..., 0.0771, 0.0000, 0.0149],
        [0.0000, 0.0131, 0.0299,  ..., 0.0771, 0.0000, 0.0149],
        [0.0000, 0.0133, 0.0300,  ..., 0.0764, 0.0000, 0.0148],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57210.5547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.4144, 0.0000, 0.2130,  ..., 0.2551, 0.0000, 0.0000],
        [0.4529, 0.0000, 0.2318,  ..., 0.2791, 0.0000, 0.0000],
        [0.4351, 0.0000, 0.2232,  ..., 0.2679, 0.0000, 0.0000],
        ...,
        [0.0112, 0.0172, 0.0146,  ..., 0.0053, 0.0000, 0.0000],
        [0.0112, 0.0172, 0.0146,  ..., 0.0053, 0.0000, 0.0000],
        [0.0112, 0.0172, 0.0146,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(523178.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11670.2285, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(127.6660, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5000.7393, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-551.0046, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7410.9995, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(587.0118, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0158],
        [ 0.0091],
        [ 0.0103],
        ...,
        [-1.0106],
        [-1.0074],
        [-1.0064]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-193291.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0023],
        [1.0063],
        ...,
        [0.9993],
        [0.9984],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367275.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.8624, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0024],
        [1.0064],
        ...,
        [0.9994],
        [0.9984],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367286.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.8624, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.8952e-04, -4.7576e-04,  ...,  6.1136e-04,
          0.0000e+00, -2.6763e-05],
        [ 0.0000e+00, -2.8952e-04, -4.7576e-04,  ...,  6.1136e-04,
          0.0000e+00, -2.6763e-05],
        [ 0.0000e+00, -2.8952e-04, -4.7576e-04,  ...,  6.1136e-04,
          0.0000e+00, -2.6763e-05],
        ...,
        [ 0.0000e+00, -2.8952e-04, -4.7576e-04,  ...,  6.1136e-04,
          0.0000e+00, -2.6763e-05],
        [ 0.0000e+00, -2.8952e-04, -4.7576e-04,  ...,  6.1136e-04,
          0.0000e+00, -2.6763e-05],
        [ 0.0000e+00, -2.8952e-04, -4.7576e-04,  ...,  6.1136e-04,
          0.0000e+00, -2.6763e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1316.2791, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.9710, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.7364, device='cuda:0')



h[100].sum tensor(-14.6949, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.6022, device='cuda:0')



h[200].sum tensor(-8.3898, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.4389, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47205.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0140, 0.0148, 0.0156,  ..., 0.0078, 0.0000, 0.0000],
        [0.0113, 0.0167, 0.0146,  ..., 0.0059, 0.0000, 0.0000],
        [0.0109, 0.0170, 0.0145,  ..., 0.0056, 0.0000, 0.0000],
        ...,
        [0.0112, 0.0173, 0.0148,  ..., 0.0057, 0.0000, 0.0000],
        [0.0112, 0.0173, 0.0148,  ..., 0.0057, 0.0000, 0.0000],
        [0.0112, 0.0173, 0.0148,  ..., 0.0057, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(481555.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10295.5820, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(86.8981, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4992.0659, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-493.8004, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6730.5840, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(486.0351, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5635],
        [-0.7791],
        [-0.9523],
        ...,
        [-1.0156],
        [-1.0124],
        [-1.0115]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-189086.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0024],
        [1.0064],
        ...,
        [0.9994],
        [0.9984],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367286.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.3741, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0025],
        [1.0066],
        ...,
        [0.9994],
        [0.9985],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367297.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.3741, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.8607e-04, -4.6712e-04,  ...,  6.3311e-04,
          0.0000e+00, -1.8524e-05],
        [ 0.0000e+00, -2.8607e-04, -4.6712e-04,  ...,  6.3311e-04,
          0.0000e+00, -1.8524e-05],
        [ 0.0000e+00, -2.8607e-04, -4.6712e-04,  ...,  6.3311e-04,
          0.0000e+00, -1.8524e-05],
        ...,
        [ 0.0000e+00, -2.8607e-04, -4.6712e-04,  ...,  6.3311e-04,
          0.0000e+00, -1.8524e-05],
        [ 0.0000e+00, -2.8607e-04, -4.6712e-04,  ...,  6.3311e-04,
          0.0000e+00, -1.8524e-05],
        [ 0.0000e+00, -2.8607e-04, -4.6712e-04,  ...,  6.3311e-04,
          0.0000e+00, -1.8524e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1310.5590, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.2108, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.0263, device='cuda:0')



h[100].sum tensor(-13.3952, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.1584, device='cuda:0')



h[200].sum tensor(-8.9721, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.0825, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0026, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0026, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0026, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44927.9023, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0107, 0.0169, 0.0145,  ..., 0.0057, 0.0000, 0.0000],
        [0.0107, 0.0169, 0.0145,  ..., 0.0058, 0.0000, 0.0000],
        [0.0109, 0.0170, 0.0146,  ..., 0.0058, 0.0000, 0.0000],
        ...,
        [0.0112, 0.0173, 0.0148,  ..., 0.0059, 0.0000, 0.0000],
        [0.0112, 0.0173, 0.0148,  ..., 0.0059, 0.0000, 0.0000],
        [0.0112, 0.0173, 0.0148,  ..., 0.0059, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(471763.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9843.5693, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(74.3398, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4985.4375, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-481.6252, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6544.7842, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(462.2214, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1013],
        [-1.1598],
        [-1.2058],
        ...,
        [-1.0185],
        [-1.0154],
        [-1.0145]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-193817.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0025],
        [1.0066],
        ...,
        [0.9994],
        [0.9985],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367297.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5435],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(498.5137, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0026],
        [1.0067],
        ...,
        [0.9995],
        [0.9985],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367309.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5435],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(498.5137, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3787e-02,  2.7812e-03,  6.3681e-03,  ...,  1.6684e-02,
         -3.6844e-03,  3.2052e-03],
        [-2.3171e-02,  4.8748e-03,  1.1011e-02,  ...,  2.7590e-02,
         -6.1918e-03,  5.3955e-03],
        [-1.5798e-02,  3.2297e-03,  7.3627e-03,  ...,  1.9021e-02,
         -4.2215e-03,  3.6744e-03],
        ...,
        [ 0.0000e+00, -2.9518e-04, -4.5330e-04,  ...,  6.5987e-04,
          0.0000e+00, -1.3199e-05],
        [ 0.0000e+00, -2.9518e-04, -4.5330e-04,  ...,  6.5987e-04,
          0.0000e+00, -1.3199e-05],
        [ 0.0000e+00, -2.9518e-04, -4.5330e-04,  ...,  6.5987e-04,
          0.0000e+00, -1.3199e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1762.6183, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-44.7434, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(39.6740, device='cuda:0')



h[100].sum tensor(-32.9008, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.2945, device='cuda:0')



h[200].sum tensor(0.4114, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-54.6666, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0076, 0.0175,  ..., 0.0470, 0.0000, 0.0089],
        [0.0000, 0.0113, 0.0258,  ..., 0.0676, 0.0000, 0.0130],
        [0.0000, 0.0227, 0.0512,  ..., 0.1271, 0.0000, 0.0249],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0027, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0027, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0027, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(79488.2734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2461, 0.0000, 0.1268,  ..., 0.1535, 0.0000, 0.0000],
        [0.3419, 0.0000, 0.1746,  ..., 0.2125, 0.0000, 0.0000],
        [0.4782, 0.0000, 0.2437,  ..., 0.2960, 0.0000, 0.0000],
        ...,
        [0.0111, 0.0172, 0.0147,  ..., 0.0059, 0.0000, 0.0000],
        [0.0111, 0.0172, 0.0147,  ..., 0.0059, 0.0000, 0.0000],
        [0.0111, 0.0172, 0.0147,  ..., 0.0059, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(655847.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(16204.9775, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(213.6063, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4299.4829, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-687.8675, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9620.6426, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(812.4225, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1210],
        [ 0.1024],
        [ 0.0810],
        ...,
        [-1.0222],
        [-1.0191],
        [-1.0182]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-153925.3281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0026],
        [1.0067],
        ...,
        [0.9995],
        [0.9985],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367309.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(298.1101, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0017],
        [1.0027],
        [1.0069],
        ...,
        [0.9995],
        [0.9986],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367321.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(298.1101, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2082e-02,  2.4054e-03,  5.5427e-03,  ...,  1.4737e-02,
         -3.2213e-03,  2.8163e-03],
        [-5.4782e-03,  9.3053e-04,  2.2714e-03,  ...,  7.0533e-03,
         -1.4606e-03,  1.2725e-03],
        [-2.1016e-02,  4.4008e-03,  9.9687e-03,  ...,  2.5132e-02,
         -5.6034e-03,  4.9051e-03],
        ...,
        [ 0.0000e+00, -2.9302e-04, -4.4251e-04,  ...,  6.7915e-04,
          0.0000e+00, -8.3251e-06],
        [ 0.0000e+00, -2.9302e-04, -4.4251e-04,  ...,  6.7915e-04,
          0.0000e+00, -8.3251e-06],
        [ 0.0000e+00, -2.9302e-04, -4.4251e-04,  ...,  6.7915e-04,
          0.0000e+00, -8.3251e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1489.9626, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.9341, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.7250, device='cuda:0')



h[100].sum tensor(-19.7986, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.1561, device='cuda:0')



h[200].sum tensor(-5.8350, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-32.6905, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0049, 0.0116,  ..., 0.0342, 0.0000, 0.0063],
        [0.0000, 0.0096, 0.0222,  ..., 0.0591, 0.0000, 0.0113],
        [0.0000, 0.0052, 0.0121,  ..., 0.0344, 0.0000, 0.0063],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57438.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1983, 0.0000, 0.1015,  ..., 0.1248, 0.0000, 0.0000],
        [0.1970, 0.0000, 0.1012,  ..., 0.1237, 0.0000, 0.0000],
        [0.1656, 0.0000, 0.0863,  ..., 0.1039, 0.0000, 0.0000],
        ...,
        [0.0110, 0.0172, 0.0146,  ..., 0.0059, 0.0000, 0.0000],
        [0.0110, 0.0172, 0.0146,  ..., 0.0059, 0.0000, 0.0000],
        [0.0110, 0.0172, 0.0146,  ..., 0.0059, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(535747.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12235.2002, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(124.0739, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4338.8301, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-558.5815, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7607.4009, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(585.7210, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1583],
        [ 0.1628],
        [ 0.1635],
        ...,
        [-1.0272],
        [-1.0241],
        [-1.0232]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-151859.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0017],
        [1.0027],
        [1.0069],
        ...,
        [0.9995],
        [0.9986],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367321.2188, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 170.0 event: 850 loss: tensor(560.6675, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4001],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(293.6479, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0017],
        [1.0028],
        [1.0070],
        ...,
        [0.9995],
        [0.9986],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367332.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4001],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(293.6479, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.9498e-03,  1.7356e-03,  3.9992e-03,  ...,  1.1105e-02,
         -2.3809e-03,  2.0864e-03],
        [-9.1394e-03,  1.7781e-03,  4.0932e-03,  ...,  1.1326e-02,
         -2.4313e-03,  2.1308e-03],
        [ 0.0000e+00, -2.6723e-04, -4.3937e-04,  ...,  6.8111e-04,
          0.0000e+00, -8.3722e-06],
        ...,
        [ 0.0000e+00, -2.6723e-04, -4.3937e-04,  ...,  6.8111e-04,
          0.0000e+00, -8.3722e-06],
        [ 0.0000e+00, -2.6723e-04, -4.3937e-04,  ...,  6.8111e-04,
          0.0000e+00, -8.3722e-06],
        [ 0.0000e+00, -2.6723e-04, -4.3937e-04,  ...,  6.8111e-04,
          0.0000e+00, -8.3722e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1489.4773, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.2049, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.3699, device='cuda:0')



h[100].sum tensor(-19.2561, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.0639, device='cuda:0')



h[200].sum tensor(-5.7908, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-32.2012, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0100, 0.0228,  ..., 0.0603, 0.0000, 0.0115],
        [0.0000, 0.0031, 0.0073,  ..., 0.0219, 0.0000, 0.0038],
        [0.0000, 0.0018, 0.0041,  ..., 0.0134, 0.0000, 0.0021],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55870.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1722, 0.0000, 0.0918,  ..., 0.1079, 0.0000, 0.0000],
        [0.1007, 0.0000, 0.0572,  ..., 0.0631, 0.0000, 0.0000],
        [0.0579, 0.0032, 0.0369,  ..., 0.0360, 0.0000, 0.0000],
        ...,
        [0.0110, 0.0175, 0.0149,  ..., 0.0062, 0.0000, 0.0000],
        [0.0110, 0.0175, 0.0149,  ..., 0.0062, 0.0000, 0.0000],
        [0.0110, 0.0175, 0.0149,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(531630.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11861.6104, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(115.0439, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4540.1709, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-552.4172, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7551.7246, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(570.7661, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0709],
        [-0.3117],
        [-0.5987],
        ...,
        [-1.0373],
        [-1.0342],
        [-1.0333]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-170836.2031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0017],
        [1.0028],
        [1.0070],
        ...,
        [0.9995],
        [0.9986],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367332.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5786],
        [0.0000],
        [0.6211],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.4822, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0018],
        [1.0029],
        [1.0071],
        ...,
        [0.9995],
        [0.9986],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367344.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5786],
        [0.0000],
        [0.6211],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.4822, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.4637e-04, -4.3167e-04,  ...,  6.7653e-04,
          0.0000e+00, -1.2142e-05],
        [-2.7374e-02,  5.8905e-03,  1.3160e-02,  ...,  3.2589e-02,
         -7.2658e-03,  6.4020e-03],
        [ 0.0000e+00, -2.4637e-04, -4.3167e-04,  ...,  6.7653e-04,
          0.0000e+00, -1.2142e-05],
        ...,
        [ 0.0000e+00, -2.4637e-04, -4.3167e-04,  ...,  6.7653e-04,
          0.0000e+00, -1.2142e-05],
        [ 0.0000e+00, -2.4637e-04, -4.3167e-04,  ...,  6.7653e-04,
          0.0000e+00, -1.2142e-05],
        [ 0.0000e+00, -2.4637e-04, -4.3167e-04,  ...,  6.7653e-04,
          0.0000e+00, -1.2142e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1480.4868, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.5956, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.7200, device='cuda:0')



h[100].sum tensor(-18.8020, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.8953, device='cuda:0')



h[200].sum tensor(-5.5936, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-31.3058, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0130, 0.0292,  ..., 0.0752, 0.0000, 0.0145],
        [0.0000, 0.0045, 0.0103,  ..., 0.0288, 0.0000, 0.0052],
        [0.0000, 0.0136, 0.0305,  ..., 0.0784, 0.0000, 0.0152],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54522.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1743, 0.0000, 0.0940,  ..., 0.1093, 0.0000, 0.0000],
        [0.1414, 0.0000, 0.0779,  ..., 0.0887, 0.0000, 0.0000],
        [0.1795, 0.0000, 0.0967,  ..., 0.1124, 0.0000, 0.0000],
        ...,
        [0.0107, 0.0179, 0.0149,  ..., 0.0061, 0.0000, 0.0000],
        [0.0107, 0.0179, 0.0149,  ..., 0.0061, 0.0000, 0.0000],
        [0.0107, 0.0179, 0.0149,  ..., 0.0061, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(521136.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11258.7109, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(108.1793, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4794.5244, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-546.1664, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7362.0308, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(556.5437, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2008],
        [-0.2190],
        [-0.2586],
        ...,
        [-1.0441],
        [-1.0414],
        [-1.0417]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-197160.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0018],
        [1.0029],
        [1.0071],
        ...,
        [0.9995],
        [0.9986],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367344.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3091],
        [0.7192],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.6913, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0019],
        [1.0030],
        [1.0073],
        ...,
        [0.9995],
        [0.9986],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367355.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3091],
        [0.7192],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.6913, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.0451e-03,  1.3520e-03,  3.0774e-03,  ...,  8.8961e-03,
         -1.8658e-03,  1.6354e-03],
        [-2.1035e-02,  4.4941e-03,  1.0032e-02,  ...,  2.5222e-02,
         -5.5709e-03,  4.9169e-03],
        [-2.5938e-02,  5.5952e-03,  1.2469e-02,  ...,  3.0944e-02,
         -6.8693e-03,  6.0668e-03],
        ...,
        [ 0.0000e+00, -2.3033e-04, -4.2451e-04,  ...,  6.7464e-04,
          0.0000e+00, -1.7077e-05],
        [ 0.0000e+00, -2.3033e-04, -4.2451e-04,  ...,  6.7464e-04,
          0.0000e+00, -1.7077e-05],
        [ 0.0000e+00, -2.3033e-04, -4.2451e-04,  ...,  6.7464e-04,
          0.0000e+00, -1.7077e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1416.4492, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.7096, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.3145, device='cuda:0')



h[100].sum tensor(-15.9421, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.0116, device='cuda:0')



h[200].sum tensor(-6.6629, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.6133, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0064, 0.0144,  ..., 0.0384, 0.0000, 0.0071],
        [0.0000, 0.0153, 0.0341,  ..., 0.0868, 0.0000, 0.0168],
        [0.0000, 0.0259, 0.0577,  ..., 0.1422, 0.0000, 0.0280],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49849.3789, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2091, 0.0000, 0.1114,  ..., 0.1315, 0.0000, 0.0000],
        [0.3457, 0.0000, 0.1798,  ..., 0.2166, 0.0000, 0.0000],
        [0.4959, 0.0000, 0.2558,  ..., 0.3095, 0.0000, 0.0000],
        ...,
        [0.0105, 0.0181, 0.0148,  ..., 0.0059, 0.0000, 0.0000],
        [0.0105, 0.0181, 0.0148,  ..., 0.0059, 0.0000, 0.0000],
        [0.0105, 0.0181, 0.0148,  ..., 0.0059, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(500622.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10639.2598, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(90.7720, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4744.0508, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-520.7210, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7010.3486, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(508.0611, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1022],
        [ 0.0829],
        [ 0.0611],
        ...,
        [-1.0681],
        [-1.0648],
        [-1.0638]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-183500.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0019],
        [1.0030],
        [1.0073],
        ...,
        [0.9995],
        [0.9986],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367355.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.2662, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0019],
        [1.0030],
        [1.0074],
        ...,
        [0.9995],
        [0.9985],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367367.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.2662, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0735e-02,  2.1921e-03,  4.9208e-03,  ...,  1.3215e-02,
         -2.8365e-03,  2.5049e-03],
        [ 0.0000e+00, -2.2282e-04, -4.2095e-04,  ...,  6.7525e-04,
          0.0000e+00, -1.5571e-05],
        [ 0.0000e+00, -2.2282e-04, -4.2095e-04,  ...,  6.7525e-04,
          0.0000e+00, -1.5571e-05],
        ...,
        [ 0.0000e+00, -2.2282e-04, -4.2095e-04,  ...,  6.7525e-04,
          0.0000e+00, -1.5571e-05],
        [ 0.0000e+00, -2.2282e-04, -4.2095e-04,  ...,  6.7525e-04,
          0.0000e+00, -1.5571e-05],
        [ 0.0000e+00, -2.2282e-04, -4.2095e-04,  ...,  6.7525e-04,
          0.0000e+00, -1.5571e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1410.4600, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.8503, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.8032, device='cuda:0')



h[100].sum tensor(-15.3060, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.8790, device='cuda:0')



h[200].sum tensor(-6.7791, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.9088, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0044, 0.0099,  ..., 0.0280, 0.0000, 0.0051],
        [0.0000, 0.0022, 0.0049,  ..., 0.0153, 0.0000, 0.0025],
        [0.0000, 0.0000, 0.0000,  ..., 0.0027, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48145.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1252, 0.0000, 0.0701,  ..., 0.0787, 0.0000, 0.0000],
        [0.0742, 0.0009, 0.0449,  ..., 0.0468, 0.0000, 0.0000],
        [0.0350, 0.0055, 0.0255,  ..., 0.0221, 0.0000, 0.0000],
        ...,
        [0.0104, 0.0183, 0.0148,  ..., 0.0058, 0.0000, 0.0000],
        [0.0104, 0.0183, 0.0148,  ..., 0.0058, 0.0000, 0.0000],
        [0.0104, 0.0183, 0.0148,  ..., 0.0058, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(495569.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10159.9805, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(80.6225, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5055.9775, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-512.5241, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6900.3125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(489.1711, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0646],
        [-0.2066],
        [-0.3610],
        ...,
        [-1.0790],
        [-1.0756],
        [-1.0746]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-215651., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0019],
        [1.0030],
        [1.0074],
        ...,
        [0.9995],
        [0.9985],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367367.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2891],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.8975, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0020],
        [1.0031],
        [1.0075],
        ...,
        [0.9995],
        [0.9985],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367379.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2891],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.8975, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1654e-02,  2.4085e-03,  5.3865e-03,  ...,  1.4303e-02,
         -3.0724e-03,  2.7275e-03],
        [-5.0786e-03,  9.2702e-04,  2.1111e-03,  ...,  6.6142e-03,
         -1.3389e-03,  1.1819e-03],
        [-6.5754e-03,  1.2643e-03,  2.8567e-03,  ...,  8.3645e-03,
         -1.7335e-03,  1.5338e-03],
        ...,
        [ 0.0000e+00, -2.1726e-04, -4.1874e-04,  ...,  6.7581e-04,
          0.0000e+00, -1.1812e-05],
        [ 0.0000e+00, -2.1726e-04, -4.1874e-04,  ...,  6.7581e-04,
          0.0000e+00, -1.1812e-05],
        [ 0.0000e+00, -2.1726e-04, -4.1874e-04,  ...,  6.7581e-04,
          0.0000e+00, -1.1812e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1391.6575, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.1651, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.1821, device='cuda:0')



h[100].sum tensor(-14.0642, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.4583, device='cuda:0')



h[200].sum tensor(-7.4493, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.6752, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0037, 0.0083,  ..., 0.0252, 0.0000, 0.0045],
        [0.0000, 0.0087, 0.0195,  ..., 0.0525, 0.0000, 0.0100],
        [0.0000, 0.0039, 0.0088,  ..., 0.0253, 0.0000, 0.0045],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46475.2773, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1186, 0.0000, 0.0649,  ..., 0.0761, 0.0000, 0.0000],
        [0.1528, 0.0000, 0.0816,  ..., 0.0977, 0.0000, 0.0000],
        [0.1116, 0.0000, 0.0617,  ..., 0.0713, 0.0000, 0.0000],
        ...,
        [0.0104, 0.0184, 0.0148,  ..., 0.0058, 0.0000, 0.0000],
        [0.0104, 0.0184, 0.0148,  ..., 0.0058, 0.0000, 0.0000],
        [0.0104, 0.0184, 0.0148,  ..., 0.0058, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(493252.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9917.8750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(73.0943, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5208.9609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-504.2058, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6798.6934, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(471.1722, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0451],
        [ 0.1033],
        [ 0.0780],
        ...,
        [-1.0888],
        [-1.0854],
        [-1.0844]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-228658.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0020],
        [1.0031],
        [1.0075],
        ...,
        [0.9995],
        [0.9985],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367379.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.2286, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0020],
        [1.0032],
        [1.0076],
        ...,
        [0.9994],
        [0.9985],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367391.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.2286, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.8197e-03,  1.0821e-03,  2.4860e-03,  ...,  7.4991e-03,
         -1.5308e-03,  1.3706e-03],
        [ 0.0000e+00, -2.3037e-04, -4.1621e-04,  ...,  6.8645e-04,
          0.0000e+00,  7.5172e-07],
        [ 0.0000e+00, -2.3037e-04, -4.1621e-04,  ...,  6.8645e-04,
          0.0000e+00,  7.5172e-07],
        ...,
        [ 0.0000e+00, -2.3037e-04, -4.1621e-04,  ...,  6.8645e-04,
          0.0000e+00,  7.5172e-07],
        [ 0.0000e+00, -2.3037e-04, -4.1621e-04,  ...,  6.8645e-04,
          0.0000e+00,  7.5172e-07],
        [ 0.0000e+00, -2.3037e-04, -4.1621e-04,  ...,  6.8645e-04,
          0.0000e+00,  7.5172e-07]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1432.1102, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.6478, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.5614, device='cuda:0')



h[100].sum tensor(-15.1471, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.8162, device='cuda:0')



h[200].sum tensor(-7.4669, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.5757, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 3.9794e-03, 8.9863e-03,  ..., 2.5799e-02, 0.0000e+00,
         4.6379e-03],
        [0.0000e+00, 1.0843e-03, 2.4911e-03,  ..., 9.5785e-03, 0.0000e+00,
         1.3757e-03],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.7571e-03, 0.0000e+00,
         3.0192e-06],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.8019e-03, 0.0000e+00,
         3.0684e-06],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.8016e-03, 0.0000e+00,
         3.0680e-06],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.8017e-03, 0.0000e+00,
         3.0681e-06]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48893.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1146, 0.0000, 0.0631,  ..., 0.0731, 0.0000, 0.0000],
        [0.0661, 0.0033, 0.0401,  ..., 0.0421, 0.0000, 0.0000],
        [0.0319, 0.0081, 0.0240,  ..., 0.0199, 0.0000, 0.0000],
        ...,
        [0.0104, 0.0184, 0.0147,  ..., 0.0058, 0.0000, 0.0000],
        [0.0104, 0.0184, 0.0147,  ..., 0.0058, 0.0000, 0.0000],
        [0.0104, 0.0184, 0.0147,  ..., 0.0058, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(501349.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10350.0547, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(81.7280, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4969.4287, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-518.4453, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6949.8359, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(494.1571, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0695],
        [-0.0621],
        [-0.2440],
        ...,
        [-1.0740],
        [-1.0838],
        [-1.0861]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-207929.6719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0020],
        [1.0032],
        [1.0076],
        ...,
        [0.9994],
        [0.9985],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367391.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.6510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0021],
        [1.0032],
        [1.0077],
        ...,
        [0.9995],
        [0.9985],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367403.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.6510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.5388e-03,  1.0062e-03,  2.3506e-03,  ...,  7.1844e-03,
         -1.4536e-03,  1.3157e-03],
        [ 0.0000e+00, -2.4405e-04, -4.1460e-04,  ...,  6.9333e-04,
          0.0000e+00,  1.0132e-05],
        [ 0.0000e+00, -2.4405e-04, -4.1460e-04,  ...,  6.9333e-04,
          0.0000e+00,  1.0132e-05],
        ...,
        [ 0.0000e+00, -2.4405e-04, -4.1460e-04,  ...,  6.9333e-04,
          0.0000e+00,  1.0132e-05],
        [ 0.0000e+00, -2.4405e-04, -4.1460e-04,  ...,  6.9333e-04,
          0.0000e+00,  1.0132e-05],
        [ 0.0000e+00, -2.4405e-04, -4.1460e-04,  ...,  6.9333e-04,
          0.0000e+00,  1.0132e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1456.7413, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.1626, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.7542, device='cuda:0')



h[100].sum tensor(-15.5196, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.8663, device='cuda:0')



h[200].sum tensor(-7.7332, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.8413, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 2.3871e-03, 5.6556e-03,  ..., 1.8975e-02, 0.0000e+00,
         3.2986e-03],
        [0.0000e+00, 1.7864e-03, 4.2018e-03,  ..., 1.4593e-02, 0.0000e+00,
         2.4167e-03],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.7849e-03, 0.0000e+00,
         4.0699e-05],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.8304e-03, 0.0000e+00,
         4.1364e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.8301e-03, 0.0000e+00,
         4.1359e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.8302e-03, 0.0000e+00,
         4.1360e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51149.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0887, 0.0000, 0.0492,  ..., 0.0575, 0.0000, 0.0000],
        [0.0666, 0.0018, 0.0392,  ..., 0.0429, 0.0000, 0.0000],
        [0.0323, 0.0073, 0.0237,  ..., 0.0204, 0.0000, 0.0000],
        ...,
        [0.0104, 0.0185, 0.0146,  ..., 0.0057, 0.0000, 0.0000],
        [0.0104, 0.0185, 0.0146,  ..., 0.0057, 0.0000, 0.0000],
        [0.0104, 0.0185, 0.0146,  ..., 0.0057, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(518590.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10860.3984, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(88.5210, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4987.2334, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-531.2636, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7177.7163, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(514.5711, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0515],
        [-0.0660],
        [-0.2713],
        ...,
        [-1.1011],
        [-1.0977],
        [-1.0967]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-202537.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0021],
        [1.0032],
        [1.0077],
        ...,
        [0.9995],
        [0.9985],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367403.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6099],
        [0.6123],
        [0.6582],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.9604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0022],
        [1.0033],
        [1.0078],
        ...,
        [0.9994],
        [0.9985],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367415.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6099],
        [0.6123],
        [0.6582],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.9604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.4473e-02,  5.2749e-03,  1.1815e-02,  ...,  2.9400e-02,
         -6.4082e-03,  5.7927e-03],
        [-3.9344e-02,  8.6348e-03,  1.9247e-02,  ...,  4.6847e-02,
         -1.0302e-02,  9.3026e-03],
        [-2.7937e-02,  6.0577e-03,  1.3546e-02,  ...,  3.3465e-02,
         -7.3154e-03,  6.6104e-03],
        ...,
        [ 0.0000e+00, -2.5427e-04, -4.1587e-04,  ...,  6.8903e-04,
          0.0000e+00,  1.6519e-05],
        [ 0.0000e+00, -2.5427e-04, -4.1587e-04,  ...,  6.8903e-04,
          0.0000e+00,  1.6519e-05],
        [ 0.0000e+00, -2.5427e-04, -4.1587e-04,  ...,  6.8903e-04,
          0.0000e+00,  1.6519e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1374.1339, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.9518, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.2425, device='cuda:0')



h[100].sum tensor(-11.6943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.6956, device='cuda:0')



h[200].sum tensor(-9.9324, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.6247, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 2.2878e-02, 5.1194e-02,  ..., 1.2685e-01, 0.0000e+00,
         2.5030e-02],
        [0.0000e+00, 2.9618e-02, 6.6102e-02,  ..., 1.6185e-01, 0.0000e+00,
         3.2072e-02],
        [0.0000e+00, 3.1447e-02, 7.0149e-02,  ..., 1.7136e-01, 0.0000e+00,
         3.3985e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.8133e-03, 0.0000e+00,
         6.7448e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.8130e-03, 0.0000e+00,
         6.7441e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.8130e-03, 0.0000e+00,
         6.7441e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43895.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.4338, 0.0000, 0.2251,  ..., 0.2710, 0.0000, 0.0000],
        [0.5432, 0.0000, 0.2802,  ..., 0.3392, 0.0000, 0.0000],
        [0.5614, 0.0000, 0.2889,  ..., 0.3508, 0.0000, 0.0000],
        ...,
        [0.0103, 0.0187, 0.0147,  ..., 0.0057, 0.0000, 0.0000],
        [0.0103, 0.0187, 0.0147,  ..., 0.0057, 0.0000, 0.0000],
        [0.0103, 0.0187, 0.0147,  ..., 0.0057, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(486509.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9484.5508, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(58.9975, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5349.5215, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-487.0332, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6560.0752, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(439.1778, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1156],
        [ 0.1148],
        [ 0.1189],
        ...,
        [-1.1121],
        [-1.1087],
        [-1.1077]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-227906.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0022],
        [1.0033],
        [1.0078],
        ...,
        [0.9994],
        [0.9985],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367415.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.9590, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0022],
        [1.0034],
        [1.0079],
        ...,
        [0.9994],
        [0.9985],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367426.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.9590, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1650e-02,  2.3776e-03,  5.4070e-03,  ...,  1.4374e-02,
         -3.0436e-03,  2.7917e-03],
        [-5.6269e-03,  1.0154e-03,  2.3938e-03,  ...,  7.3000e-03,
         -1.4700e-03,  1.3680e-03],
        [-6.0231e-03,  1.1050e-03,  2.5921e-03,  ...,  7.7654e-03,
         -1.5736e-03,  1.4617e-03],
        ...,
        [ 0.0000e+00, -2.5708e-04, -4.2118e-04,  ...,  6.9122e-04,
          0.0000e+00,  3.8054e-05],
        [ 0.0000e+00, -2.5708e-04, -4.2118e-04,  ...,  6.9122e-04,
          0.0000e+00,  3.8054e-05],
        [ 0.0000e+00, -2.5708e-04, -4.2118e-04,  ...,  6.9122e-04,
          0.0000e+00,  3.8054e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1522.8723, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.1504, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.6887, device='cuda:0')



h[100].sum tensor(-16.9658, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.3682, device='cuda:0')



h[200].sum tensor(-7.7606, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.5069, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0036, 0.0083,  ..., 0.0253, 0.0000, 0.0047],
        [0.0000, 0.0081, 0.0185,  ..., 0.0501, 0.0000, 0.0097],
        [0.0000, 0.0038, 0.0088,  ..., 0.0254, 0.0000, 0.0047],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52691.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1154, 0.0000, 0.0633,  ..., 0.0742, 0.0000, 0.0000],
        [0.1459, 0.0000, 0.0781,  ..., 0.0935, 0.0000, 0.0000],
        [0.1062, 0.0000, 0.0595,  ..., 0.0680, 0.0000, 0.0000],
        ...,
        [0.0103, 0.0189, 0.0150,  ..., 0.0060, 0.0000, 0.0000],
        [0.0103, 0.0189, 0.0150,  ..., 0.0060, 0.0000, 0.0000],
        [0.0103, 0.0189, 0.0150,  ..., 0.0060, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(520114.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11017.1621, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(95.9588, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4892.7310, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-537.4799, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7176.8555, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(528.1782, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0678],
        [ 0.0579],
        [-0.0876],
        ...,
        [-1.1163],
        [-1.1129],
        [-1.1119]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-184668.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0022],
        [1.0034],
        [1.0079],
        ...,
        [0.9994],
        [0.9985],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367426.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(277.2665, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0023],
        [1.0034],
        [1.0080],
        ...,
        [0.9995],
        [0.9985],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367437.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(277.2665, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.8824e-02,  4.0228e-03,  8.9916e-03,  ...,  2.2815e-02,
         -4.9066e-03,  4.5177e-03],
        [ 0.0000e+00, -2.4035e-04, -4.3382e-04,  ...,  6.8340e-04,
          0.0000e+00,  6.2623e-05],
        [ 0.0000e+00, -2.4035e-04, -4.3382e-04,  ...,  6.8340e-04,
          0.0000e+00,  6.2623e-05],
        ...,
        [ 0.0000e+00, -2.4035e-04, -4.3382e-04,  ...,  6.8340e-04,
          0.0000e+00,  6.2623e-05],
        [ 0.0000e+00, -2.4035e-04, -4.3382e-04,  ...,  6.8340e-04,
          0.0000e+00,  6.2623e-05],
        [ 0.0000e+00, -2.4035e-04, -4.3382e-04,  ...,  6.8340e-04,
          0.0000e+00,  6.2623e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1570.0540, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.6231, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.0662, device='cuda:0')



h[100].sum tensor(-18.0390, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.7256, device='cuda:0')



h[200].sum tensor(-7.4688, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.4048, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0150, 0.0334,  ..., 0.0842, 0.0000, 0.0167],
        [0.0000, 0.0099, 0.0223,  ..., 0.0581, 0.0000, 0.0114],
        [0.0000, 0.0057, 0.0128,  ..., 0.0359, 0.0000, 0.0069],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53969.9453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3669, 0.0000, 0.1913,  ..., 0.2311, 0.0000, 0.0000],
        [0.3101, 0.0000, 0.1624,  ..., 0.1959, 0.0000, 0.0000],
        [0.2549, 0.0000, 0.1340,  ..., 0.1618, 0.0000, 0.0000],
        ...,
        [0.0104, 0.0193, 0.0157,  ..., 0.0066, 0.0000, 0.0000],
        [0.0104, 0.0193, 0.0157,  ..., 0.0066, 0.0000, 0.0000],
        [0.0104, 0.0193, 0.0157,  ..., 0.0066, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(523296.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11240.7246, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(100.7770, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4874.7256, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-544.6859, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7314.2686, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(542.2213, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1103],
        [ 0.1144],
        [ 0.1197],
        ...,
        [-1.1217],
        [-1.1183],
        [-1.1174]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-174949.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0023],
        [1.0034],
        [1.0080],
        ...,
        [0.9995],
        [0.9985],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367437.9375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 180.0 event: 900 loss: tensor(549.1878, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.6633, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0024],
        [1.0035],
        [1.0082],
        ...,
        [0.9995],
        [0.9985],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367449.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.6633, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.1920e-04, -4.4480e-04,  ...,  6.6727e-04,
          0.0000e+00,  7.0910e-05],
        [ 0.0000e+00, -2.1920e-04, -4.4480e-04,  ...,  6.6727e-04,
          0.0000e+00,  7.0910e-05],
        [ 0.0000e+00, -2.1920e-04, -4.4480e-04,  ...,  6.6727e-04,
          0.0000e+00,  7.0910e-05],
        ...,
        [ 0.0000e+00, -2.1920e-04, -4.4480e-04,  ...,  6.6727e-04,
          0.0000e+00,  7.0910e-05],
        [ 0.0000e+00, -2.1920e-04, -4.4480e-04,  ...,  6.6727e-04,
          0.0000e+00,  7.0910e-05],
        [ 0.0000e+00, -2.1920e-04, -4.4480e-04,  ...,  6.6727e-04,
          0.0000e+00,  7.0910e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1547.8394, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.6969, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.6652, device='cuda:0')



h[100].sum tensor(-16.6222, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.3621, device='cuda:0')



h[200].sum tensor(-8.1855, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.4745, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0027, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0027, 0.0000, 0.0003],
        [0.0000, 0.0012, 0.0026,  ..., 0.0099, 0.0000, 0.0017],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0027, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0027, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0027, 0.0000, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52099.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0139, 0.0165, 0.0173,  ..., 0.0094, 0.0000, 0.0000],
        [0.0246, 0.0095, 0.0222,  ..., 0.0163, 0.0000, 0.0000],
        [0.0554, 0.0043, 0.0366,  ..., 0.0364, 0.0000, 0.0000],
        ...,
        [0.0104, 0.0196, 0.0163,  ..., 0.0068, 0.0000, 0.0000],
        [0.0104, 0.0196, 0.0163,  ..., 0.0068, 0.0000, 0.0000],
        [0.0104, 0.0196, 0.0163,  ..., 0.0068, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(522675.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10918.5352, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(93.0179, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5291.5801, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-533.0692, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7310.9053, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(524.2168, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5774],
        [-0.3304],
        [-0.0954],
        ...,
        [-1.1329],
        [-1.1294],
        [-1.1285]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-210430.9844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0024],
        [1.0035],
        [1.0082],
        ...,
        [0.9995],
        [0.9985],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367449.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3157],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(483.5917, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0024],
        [1.0036],
        [1.0083],
        ...,
        [0.9995],
        [0.9985],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367460.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3157],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(483.5917, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.1295e-03,  1.4111e-03,  3.1249e-03,  ...,  9.0493e-03,
         -1.8499e-03,  1.7597e-03],
        [ 0.0000e+00, -2.0825e-04, -4.5158e-04,  ...,  6.4965e-04,
          0.0000e+00,  6.8361e-05],
        [-7.1295e-03,  1.4111e-03,  3.1249e-03,  ...,  9.0493e-03,
         -1.8499e-03,  1.7597e-03],
        ...,
        [ 0.0000e+00, -2.0825e-04, -4.5158e-04,  ...,  6.4965e-04,
          0.0000e+00,  6.8361e-05],
        [ 0.0000e+00, -2.0825e-04, -4.5158e-04,  ...,  6.4965e-04,
          0.0000e+00,  6.8361e-05],
        [ 0.0000e+00, -2.0825e-04, -4.5158e-04,  ...,  6.4965e-04,
          0.0000e+00,  6.8361e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1927.5431, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-43.5243, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(38.4865, device='cuda:0')



h[100].sum tensor(-31.8645, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.9863, device='cuda:0')



h[200].sum tensor(-0.6614, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-53.0303, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0029, 0.0065,  ..., 0.0200, 0.0000, 0.0038],
        [0.0000, 0.0061, 0.0135,  ..., 0.0385, 0.0000, 0.0075],
        [0.0000, 0.0011, 0.0025,  ..., 0.0095, 0.0000, 0.0017],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0027, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0027, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0027, 0.0000, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(79581.2422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1028, 0.0000, 0.0594,  ..., 0.0675, 0.0000, 0.0000],
        [0.1161, 0.0000, 0.0657,  ..., 0.0762, 0.0000, 0.0000],
        [0.0746, 0.0015, 0.0461,  ..., 0.0491, 0.0000, 0.0000],
        ...,
        [0.0103, 0.0200, 0.0166,  ..., 0.0067, 0.0000, 0.0000],
        [0.0103, 0.0200, 0.0166,  ..., 0.0067, 0.0000, 0.0000],
        [0.0103, 0.0200, 0.0166,  ..., 0.0067, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(671510.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(15517.6777, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(204.6159, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5460.0850, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-695.1657, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9758.3535, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(804.5465, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1055],
        [ 0.1029],
        [ 0.0437],
        ...,
        [-1.1474],
        [-1.1439],
        [-1.1429]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-223884.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0024],
        [1.0036],
        [1.0083],
        ...,
        [0.9995],
        [0.9985],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367460.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.5162, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0025],
        [1.0037],
        [1.0085],
        ...,
        [0.9995],
        [0.9985],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367471.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.5162, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.5364e-03,  1.0413e-03,  2.3285e-03,  ...,  7.1772e-03,
         -1.4332e-03,  1.3850e-03],
        [-5.5364e-03,  1.0413e-03,  2.3285e-03,  ...,  7.1772e-03,
         -1.4332e-03,  1.3850e-03],
        [ 0.0000e+00, -2.1681e-04, -4.5246e-04,  ...,  6.4681e-04,
          0.0000e+00,  6.8890e-05],
        ...,
        [ 0.0000e+00, -2.1681e-04, -4.5246e-04,  ...,  6.4681e-04,
          0.0000e+00,  6.8890e-05],
        [ 0.0000e+00, -2.1681e-04, -4.5246e-04,  ...,  6.4681e-04,
          0.0000e+00,  6.8890e-05],
        [ 0.0000e+00, -2.1681e-04, -4.5246e-04,  ...,  6.4681e-04,
          0.0000e+00,  6.8890e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1570.5464, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.4566, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.2106, device='cuda:0')



h[100].sum tensor(-17.1669, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.5036, device='cuda:0')



h[200].sum tensor(-7.9845, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-29.2260, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0034, 0.0075,  ..., 0.0234, 0.0000, 0.0045],
        [0.0000, 0.0025, 0.0056,  ..., 0.0190, 0.0000, 0.0036],
        [0.0000, 0.0019, 0.0042,  ..., 0.0145, 0.0000, 0.0027],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0026, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0026, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0026, 0.0000, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52226.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1094, 0.0000, 0.0614,  ..., 0.0725, 0.0000, 0.0000],
        [0.0938, 0.0000, 0.0539,  ..., 0.0625, 0.0000, 0.0000],
        [0.0723, 0.0000, 0.0441,  ..., 0.0483, 0.0000, 0.0000],
        ...,
        [0.0102, 0.0203, 0.0166,  ..., 0.0067, 0.0000, 0.0000],
        [0.0102, 0.0203, 0.0166,  ..., 0.0067, 0.0000, 0.0000],
        [0.0102, 0.0203, 0.0166,  ..., 0.0067, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(522399.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10946.8828, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(98.8474, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5445.8457, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-529.9761, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7302.7617, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(527.2759, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1313],
        [ 0.0961],
        [ 0.0086],
        ...,
        [-1.1567],
        [-1.1530],
        [-1.1517]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-203355.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0025],
        [1.0037],
        [1.0085],
        ...,
        [0.9995],
        [0.9985],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367471.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2661],
        [0.0000],
        [0.3716],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.8510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0026],
        [1.0038],
        [1.0086],
        ...,
        [0.9995],
        [0.9985],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367482.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2661],
        [0.0000],
        [0.3716],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.8510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.4766e-04, -4.4489e-04,  ...,  6.6080e-04,
          0.0000e+00,  6.4701e-05],
        [-1.4373e-02,  3.0188e-03,  6.7846e-03,  ...,  1.7635e-02,
         -3.7122e-03,  3.4881e-03],
        [ 0.0000e+00, -2.4766e-04, -4.4489e-04,  ...,  6.6080e-04,
          0.0000e+00,  6.4701e-05],
        ...,
        [ 0.0000e+00, -2.4766e-04, -4.4489e-04,  ...,  6.6080e-04,
          0.0000e+00,  6.4701e-05],
        [ 0.0000e+00, -2.4766e-04, -4.4489e-04,  ...,  6.6080e-04,
          0.0000e+00,  6.4701e-05],
        [ 0.0000e+00, -2.4766e-04, -4.4489e-04,  ...,  6.6080e-04,
          0.0000e+00,  6.4701e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1504.9275, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.4315, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.4968, device='cuda:0')



h[100].sum tensor(-14.2163, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.5400, device='cuda:0')



h[200].sum tensor(-9.5759, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.1087, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0059, 0.0134,  ..., 0.0383, 0.0000, 0.0075],
        [0.0000, 0.0022, 0.0050,  ..., 0.0165, 0.0000, 0.0031],
        [0.0000, 0.0073, 0.0166,  ..., 0.0459, 0.0000, 0.0090],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0027, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0027, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0027, 0.0000, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48443.6484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1282, 0.0000, 0.0709,  ..., 0.0835, 0.0000, 0.0000],
        [0.1162, 0.0000, 0.0652,  ..., 0.0758, 0.0000, 0.0000],
        [0.1505, 0.0000, 0.0818,  ..., 0.0974, 0.0000, 0.0000],
        ...,
        [0.0101, 0.0205, 0.0165,  ..., 0.0066, 0.0000, 0.0000],
        [0.0101, 0.0205, 0.0165,  ..., 0.0066, 0.0000, 0.0000],
        [0.0101, 0.0205, 0.0165,  ..., 0.0066, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(509223.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10369.0049, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(82.3792, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5623.0405, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-503.5666, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7026.8662, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(487.5551, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1512],
        [ 0.1516],
        [ 0.1525],
        ...,
        [-1.1646],
        [-1.1603],
        [-1.1582]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-212114.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0026],
        [1.0038],
        [1.0086],
        ...,
        [0.9995],
        [0.9985],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367482.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5142],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.0995, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0027],
        [1.0039],
        [1.0087],
        ...,
        [0.9995],
        [0.9985],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367493.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5142],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.0995, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3116e-02,  2.7028e-03,  6.1725e-03,  ...,  1.6192e-02,
         -3.3798e-03,  3.1818e-03],
        [-1.1577e-02,  2.3529e-03,  5.3973e-03,  ...,  1.4372e-02,
         -2.9831e-03,  2.8145e-03],
        [ 0.0000e+00, -2.7900e-04, -4.3369e-04,  ...,  6.8321e-04,
          0.0000e+00,  5.2343e-05],
        ...,
        [ 0.0000e+00, -2.7900e-04, -4.3369e-04,  ...,  6.8321e-04,
          0.0000e+00,  5.2343e-05],
        [ 0.0000e+00, -2.7900e-04, -4.3369e-04,  ...,  6.8321e-04,
          0.0000e+00,  5.2343e-05],
        [ 0.0000e+00, -2.7900e-04, -4.3369e-04,  ...,  6.8321e-04,
          0.0000e+00,  5.2343e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1583.8385, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.2272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.1774, device='cuda:0')



h[100].sum tensor(-16.9875, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.4950, device='cuda:0')



h[200].sum tensor(-8.4171, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-29.1803, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0109, 0.0249,  ..., 0.0652, 0.0000, 0.0128],
        [0.0000, 0.0046, 0.0105,  ..., 0.0295, 0.0000, 0.0056],
        [0.0000, 0.0024, 0.0054,  ..., 0.0165, 0.0000, 0.0030],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51164.6680, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1703, 0.0000, 0.0932,  ..., 0.1079, 0.0000, 0.0000],
        [0.1168, 0.0000, 0.0671,  ..., 0.0742, 0.0000, 0.0000],
        [0.0720, 0.0026, 0.0454,  ..., 0.0460, 0.0000, 0.0000],
        ...,
        [0.0100, 0.0205, 0.0162,  ..., 0.0066, 0.0000, 0.0000],
        [0.0100, 0.0205, 0.0162,  ..., 0.0066, 0.0000, 0.0000],
        [0.0100, 0.0205, 0.0162,  ..., 0.0066, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(518433.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10413.1543, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(88.4565, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5820.8096, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-516.4019, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7124.1797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(512.8519, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2069],
        [-0.2619],
        [-0.3493],
        ...,
        [-1.1736],
        [-1.1701],
        [-1.1691]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-232568.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0027],
        [1.0039],
        [1.0087],
        ...,
        [0.9995],
        [0.9985],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367493.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3350],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.5199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0028],
        [1.0040],
        [1.0088],
        ...,
        [0.9995],
        [0.9985],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367504.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3350],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.5199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.2324e-03,  8.7968e-04,  2.2179e-03,  ...,  6.9079e-03,
         -1.3452e-03,  1.2887e-03],
        [-1.2767e-02,  2.5935e-03,  6.0178e-03,  ...,  1.5827e-02,
         -3.2822e-03,  3.0892e-03],
        [-6.8807e-03,  1.2546e-03,  3.0492e-03,  ...,  8.8592e-03,
         -1.7690e-03,  1.6826e-03],
        ...,
        [ 0.0000e+00, -3.1055e-04, -4.2106e-04,  ...,  7.1338e-04,
          0.0000e+00,  3.8284e-05],
        [ 0.0000e+00, -3.1055e-04, -4.2106e-04,  ...,  7.1338e-04,
          0.0000e+00,  3.8284e-05],
        [ 0.0000e+00, -3.1055e-04, -4.2106e-04,  ...,  7.1338e-04,
          0.0000e+00,  3.8284e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1502.7186, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.0664, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.2766, device='cuda:0')



h[100].sum tensor(-13.2085, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.2234, device='cuda:0')



h[200].sum tensor(-10.5269, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.4275, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0072, 0.0169,  ..., 0.0466, 0.0000, 0.0090],
        [0.0000, 0.0039, 0.0097,  ..., 0.0296, 0.0000, 0.0056],
        [0.0000, 0.0073, 0.0173,  ..., 0.0474, 0.0000, 0.0091],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0029, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0029, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0029, 0.0000, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47396.5508, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1299, 0.0000, 0.0708,  ..., 0.0838, 0.0000, 0.0000],
        [0.1343, 0.0000, 0.0721,  ..., 0.0870, 0.0000, 0.0000],
        [0.1649, 0.0000, 0.0870,  ..., 0.1060, 0.0000, 0.0000],
        ...,
        [0.0122, 0.0188, 0.0170,  ..., 0.0081, 0.0000, 0.0000],
        [0.0100, 0.0204, 0.0160,  ..., 0.0067, 0.0000, 0.0000],
        [0.0100, 0.0204, 0.0160,  ..., 0.0067, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(507363.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10159.3633, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(72.4470, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5655.6797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-490.7309, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6905.0889, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(473.1133, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3292],
        [-0.1818],
        [-0.0926],
        ...,
        [-0.8894],
        [-1.0247],
        [-1.1178]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-206525., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0028],
        [1.0040],
        [1.0088],
        ...,
        [0.9995],
        [0.9985],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367504.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.5524, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0028],
        [1.0040],
        [1.0089],
        ...,
        [0.9995],
        [0.9985],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367515.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.5524, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.2634e-04, -4.1306e-04,  ...,  7.3205e-04,
          0.0000e+00,  2.3114e-05],
        [ 0.0000e+00, -3.2634e-04, -4.1306e-04,  ...,  7.3205e-04,
          0.0000e+00,  2.3114e-05],
        [ 0.0000e+00, -3.2634e-04, -4.1306e-04,  ...,  7.3205e-04,
          0.0000e+00,  2.3114e-05],
        ...,
        [ 0.0000e+00, -3.2634e-04, -4.1306e-04,  ...,  7.3205e-04,
          0.0000e+00,  2.3114e-05],
        [ 0.0000e+00, -3.2634e-04, -4.1306e-04,  ...,  7.3205e-04,
          0.0000e+00,  2.3114e-05],
        [ 0.0000e+00, -3.2634e-04, -4.1306e-04,  ...,  7.3205e-04,
          0.0000e+00,  2.3114e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1476.7761, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.7118, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.2100, device='cuda:0')



h[100].sum tensor(-11.4832, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.6872, device='cuda:0')



h[200].sum tensor(-11.4800, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.5799, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.9327e-03, 0.0000e+00,
         9.2596e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.9371e-03, 0.0000e+00,
         9.2735e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.9428e-03, 0.0000e+00,
         9.2917e-05],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.9928e-03, 0.0000e+00,
         9.4496e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.9925e-03, 0.0000e+00,
         9.4485e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.9925e-03, 0.0000e+00,
         9.4485e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45491.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0096, 0.0201, 0.0157,  ..., 0.0068, 0.0000, 0.0000],
        [0.0113, 0.0189, 0.0164,  ..., 0.0079, 0.0000, 0.0000],
        [0.0141, 0.0171, 0.0176,  ..., 0.0096, 0.0000, 0.0000],
        ...,
        [0.0102, 0.0205, 0.0161,  ..., 0.0071, 0.0000, 0.0000],
        [0.0102, 0.0205, 0.0161,  ..., 0.0071, 0.0000, 0.0000],
        [0.0102, 0.0205, 0.0161,  ..., 0.0071, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(500948.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9919.3125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(57.2104, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5735.8262, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-478.8086, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6769.9141, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(455.7751, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0888],
        [-0.9134],
        [-0.6803],
        ...,
        [-1.1732],
        [-1.1641],
        [-1.1574]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-210549.6406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0028],
        [1.0040],
        [1.0089],
        ...,
        [0.9995],
        [0.9985],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367515.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.8988, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0029],
        [1.0041],
        [1.0090],
        ...,
        [0.9995],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367526., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.8988, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.2828e-04, -4.0738e-04,  ...,  7.3532e-04,
          0.0000e+00, -1.5994e-06],
        [ 0.0000e+00, -3.2828e-04, -4.0738e-04,  ...,  7.3532e-04,
          0.0000e+00, -1.5994e-06],
        [ 0.0000e+00, -3.2828e-04, -4.0738e-04,  ...,  7.3532e-04,
          0.0000e+00, -1.5994e-06],
        ...,
        [ 0.0000e+00, -3.2828e-04, -4.0738e-04,  ...,  7.3532e-04,
          0.0000e+00, -1.5994e-06],
        [ 0.0000e+00, -3.2828e-04, -4.0738e-04,  ...,  7.3532e-04,
          0.0000e+00, -1.5994e-06],
        [ 0.0000e+00, -3.2828e-04, -4.0738e-04,  ...,  7.3532e-04,
          0.0000e+00, -1.5994e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1520.4702, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.6440, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.9089, device='cuda:0')



h[100].sum tensor(-12.8910, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.1280, device='cuda:0')



h[200].sum tensor(-10.7163, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.9208, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47137.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0096, 0.0204, 0.0158,  ..., 0.0069, 0.0000, 0.0000],
        [0.0097, 0.0204, 0.0158,  ..., 0.0069, 0.0000, 0.0000],
        [0.0099, 0.0205, 0.0159,  ..., 0.0070, 0.0000, 0.0000],
        ...,
        [0.0103, 0.0209, 0.0163,  ..., 0.0071, 0.0000, 0.0000],
        [0.0103, 0.0208, 0.0163,  ..., 0.0071, 0.0000, 0.0000],
        [0.0103, 0.0208, 0.0163,  ..., 0.0071, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(509523.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10091.1133, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(59.2003, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5896.3057, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-489.3888, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6906.7129, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(475.2463, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0843],
        [-1.2057],
        [-1.3026],
        ...,
        [-1.1910],
        [-1.1876],
        [-1.1869]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-217783.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0029],
        [1.0041],
        [1.0090],
        ...,
        [0.9995],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367526., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2952],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(177.9586, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0029],
        [1.0042],
        [1.0091],
        ...,
        [0.9995],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367536.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2952],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(177.9586, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.2601e-04, -4.0348e-04,  ...,  7.2994e-04,
          0.0000e+00, -2.3296e-05],
        [-1.2134e-02,  2.4440e-03,  5.7384e-03,  ...,  1.5143e-02,
         -3.0981e-03,  2.8887e-03],
        [-1.8096e-02,  3.8049e-03,  8.7559e-03,  ...,  2.2224e-02,
         -4.6202e-03,  4.3193e-03],
        ...,
        [ 0.0000e+00, -3.2601e-04, -4.0348e-04,  ...,  7.2994e-04,
          0.0000e+00, -2.3296e-05],
        [ 0.0000e+00, -3.2601e-04, -4.0348e-04,  ...,  7.2994e-04,
          0.0000e+00, -2.3296e-05],
        [ 0.0000e+00, -3.2601e-04, -4.0348e-04,  ...,  7.2994e-04,
          0.0000e+00, -2.3296e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1488.0422, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.6467, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.1628, device='cuda:0')



h[100].sum tensor(-11.4278, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.6749, device='cuda:0')



h[200].sum tensor(-11.3021, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.5148, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0065, 0.0157,  ..., 0.0434, 0.0000, 0.0081],
        [0.0000, 0.0058, 0.0134,  ..., 0.0363, 0.0000, 0.0067],
        [0.0000, 0.0055, 0.0129,  ..., 0.0350, 0.0000, 0.0064],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45063.4336, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1278, 0.0000, 0.0706,  ..., 0.0825, 0.0000, 0.0000],
        [0.1485, 0.0000, 0.0813,  ..., 0.0951, 0.0000, 0.0000],
        [0.1665, 0.0000, 0.0899,  ..., 0.1063, 0.0000, 0.0000],
        ...,
        [0.0101, 0.0213, 0.0165,  ..., 0.0072, 0.0000, 0.0000],
        [0.0101, 0.0213, 0.0164,  ..., 0.0072, 0.0000, 0.0000],
        [0.0101, 0.0213, 0.0164,  ..., 0.0072, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(504204.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9804.1221, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(48.2149, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6063.4287, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-479.4658, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6825.6533, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(456.7489, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1273],
        [ 0.1802],
        [ 0.1996],
        ...,
        [-1.2075],
        [-1.2038],
        [-1.2027]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-234259.1406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0029],
        [1.0042],
        [1.0091],
        ...,
        [0.9995],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367536.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.3387, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0030],
        [1.0042],
        [1.0092],
        ...,
        [0.9995],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367546.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.3387, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.2306e-04, -4.0260e-04,  ...,  7.2762e-04,
          0.0000e+00, -3.6353e-05],
        [ 0.0000e+00, -3.2306e-04, -4.0260e-04,  ...,  7.2762e-04,
          0.0000e+00, -3.6353e-05],
        [ 0.0000e+00, -3.2306e-04, -4.0260e-04,  ...,  7.2762e-04,
          0.0000e+00, -3.6353e-05],
        ...,
        [ 0.0000e+00, -3.2306e-04, -4.0260e-04,  ...,  7.2762e-04,
          0.0000e+00, -3.6353e-05],
        [ 0.0000e+00, -3.2306e-04, -4.0260e-04,  ...,  7.2762e-04,
          0.0000e+00, -3.6353e-05],
        [ 0.0000e+00, -3.2306e-04, -4.0260e-04,  ...,  7.2762e-04,
          0.0000e+00, -3.6353e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1565.5647, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.0491, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.1377, device='cuda:0')



h[100].sum tensor(-13.9080, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.4468, device='cuda:0')



h[200].sum tensor(-10.0321, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.6139, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50780.4766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0138, 0.0181, 0.0182,  ..., 0.0100, 0.0000, 0.0000],
        [0.0230, 0.0114, 0.0222,  ..., 0.0160, 0.0000, 0.0000],
        [0.0279, 0.0080, 0.0244,  ..., 0.0192, 0.0000, 0.0000],
        ...,
        [0.0102, 0.0217, 0.0168,  ..., 0.0075, 0.0000, 0.0000],
        [0.0102, 0.0217, 0.0168,  ..., 0.0075, 0.0000, 0.0000],
        [0.0102, 0.0217, 0.0168,  ..., 0.0075, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(534999.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10899.1025, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(67.9138, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6021.8057, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-515.4324, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7351.1431, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(519.4032, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3801],
        [-0.3621],
        [-0.2918],
        ...,
        [-1.2176],
        [-1.2139],
        [-1.2129]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-227936.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0030],
        [1.0042],
        [1.0092],
        ...,
        [0.9995],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367546.5000, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 190.0 event: 950 loss: tensor(512.2022, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.0538, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0031],
        [1.0043],
        [1.0093],
        ...,
        [0.9995],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367556.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.0538, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.6844e-02,  3.5378e-03,  8.1416e-03,  ...,  2.0780e-02,
         -4.2804e-03,  4.0033e-03],
        [ 0.0000e+00, -3.1829e-04, -4.0235e-04,  ...,  7.2986e-04,
          0.0000e+00, -4.8980e-05],
        [ 0.0000e+00, -3.1829e-04, -4.0235e-04,  ...,  7.2986e-04,
          0.0000e+00, -4.8980e-05],
        ...,
        [ 0.0000e+00, -3.1829e-04, -4.0235e-04,  ...,  7.2986e-04,
          0.0000e+00, -4.8980e-05],
        [ 0.0000e+00, -3.1829e-04, -4.0235e-04,  ...,  7.2986e-04,
          0.0000e+00, -4.8980e-05],
        [ 0.0000e+00, -3.1829e-04, -4.0235e-04,  ...,  7.2986e-04,
          0.0000e+00, -4.8980e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1589.1501, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.4008, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.4333, device='cuda:0')



h[100].sum tensor(-14.1600, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.5235, device='cuda:0')



h[200].sum tensor(-9.8619, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.0213, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0114, 0.0258,  ..., 0.0654, 0.0000, 0.0125],
        [0.0000, 0.0064, 0.0148,  ..., 0.0395, 0.0000, 0.0073],
        [0.0000, 0.0000, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50471.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2321, 0.0000, 0.1260,  ..., 0.1464, 0.0000, 0.0000],
        [0.1652, 0.0000, 0.0925,  ..., 0.1049, 0.0000, 0.0000],
        [0.0890, 0.0000, 0.0551,  ..., 0.0572, 0.0000, 0.0000],
        ...,
        [0.0105, 0.0221, 0.0172,  ..., 0.0079, 0.0000, 0.0000],
        [0.0105, 0.0221, 0.0172,  ..., 0.0079, 0.0000, 0.0000],
        [0.0105, 0.0221, 0.0172,  ..., 0.0079, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(530700.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10941.0488, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(60.7674, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5950.7988, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-517.1869, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7298.9678, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(521.8575, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0689],
        [ 0.0638],
        [ 0.0668],
        ...,
        [-1.2262],
        [-1.2225],
        [-1.2215]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-218351.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0031],
        [1.0043],
        [1.0093],
        ...,
        [0.9995],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367556.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3845],
        [0.4937],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.6575, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0031],
        [1.0044],
        [1.0095],
        ...,
        [0.9995],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367567.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3845],
        [0.4937],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.6575, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.9134e-02,  4.0687e-03,  9.3139e-03,  ...,  2.3534e-02,
         -4.8513e-03,  4.5462e-03],
        [-8.5957e-03,  1.6517e-03,  3.9628e-03,  ...,  1.0976e-02,
         -2.1793e-03,  2.0087e-03],
        [-1.1035e-02,  2.2112e-03,  5.2015e-03,  ...,  1.3883e-02,
         -2.7979e-03,  2.5961e-03],
        ...,
        [ 0.0000e+00, -3.1966e-04, -4.0175e-04,  ...,  7.3396e-04,
          0.0000e+00, -6.1014e-05],
        [ 0.0000e+00, -3.1966e-04, -4.0175e-04,  ...,  7.3396e-04,
          0.0000e+00, -6.1014e-05],
        [ 0.0000e+00, -3.1966e-04, -4.0175e-04,  ...,  7.3396e-04,
          0.0000e+00, -6.1014e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1707.2272, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.8592, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.5748, device='cuda:0')



h[100].sum tensor(-18.1377, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.8576, device='cuda:0')



h[200].sum tensor(-7.8580, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-31.1057, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0114, 0.0264,  ..., 0.0687, 0.0000, 0.0130],
        [0.0000, 0.0114, 0.0265,  ..., 0.0689, 0.0000, 0.0131],
        [0.0000, 0.0034, 0.0082,  ..., 0.0240, 0.0000, 0.0041],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56322.7070, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3498, 0.0000, 0.1824,  ..., 0.2215, 0.0000, 0.0000],
        [0.2843, 0.0000, 0.1496,  ..., 0.1809, 0.0000, 0.0000],
        [0.1882, 0.0000, 0.1023,  ..., 0.1208, 0.0000, 0.0000],
        ...,
        [0.0139, 0.0202, 0.0188,  ..., 0.0104, 0.0000, 0.0000],
        [0.0203, 0.0155, 0.0215,  ..., 0.0148, 0.0000, 0.0000],
        [0.0235, 0.0132, 0.0228,  ..., 0.0169, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(551610.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11826.4453, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(81.3938, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5772.7344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-555.0273, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7685.0288, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(586.9567, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1653],
        [ 0.1651],
        [ 0.1640],
        ...,
        [-1.0924],
        [-0.9880],
        [-0.9210]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-207702.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0031],
        [1.0044],
        [1.0095],
        ...,
        [0.9995],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367567.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8577, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0031],
        [1.0044],
        [1.0095],
        ...,
        [0.9995],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367567.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8577, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.1966e-04, -4.0175e-04,  ...,  7.3396e-04,
          0.0000e+00, -6.1014e-05],
        [ 0.0000e+00, -3.1966e-04, -4.0175e-04,  ...,  7.3396e-04,
          0.0000e+00, -6.1014e-05],
        [ 0.0000e+00, -3.1966e-04, -4.0175e-04,  ...,  7.3396e-04,
          0.0000e+00, -6.1014e-05],
        ...,
        [ 0.0000e+00, -3.1966e-04, -4.0175e-04,  ...,  7.3396e-04,
          0.0000e+00, -6.1014e-05],
        [ 0.0000e+00, -3.1966e-04, -4.0175e-04,  ...,  7.3396e-04,
          0.0000e+00, -6.1014e-05],
        [ 0.0000e+00, -3.1966e-04, -4.0175e-04,  ...,  7.3396e-04,
          0.0000e+00, -6.1014e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1611.9678, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.9199, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.1340, device='cuda:0')



h[100].sum tensor(-14.5338, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.7053, device='cuda:0')



h[200].sum tensor(-9.6444, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.9867, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49685.6172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0580, 0.0074, 0.0396,  ..., 0.0386, 0.0000, 0.0000],
        [0.0243, 0.0115, 0.0234,  ..., 0.0173, 0.0000, 0.0000],
        [0.0135, 0.0197, 0.0185,  ..., 0.0102, 0.0000, 0.0000],
        ...,
        [0.0106, 0.0225, 0.0175,  ..., 0.0082, 0.0000, 0.0000],
        [0.0106, 0.0225, 0.0175,  ..., 0.0082, 0.0000, 0.0000],
        [0.0106, 0.0225, 0.0175,  ..., 0.0082, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(524656.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10553.3408, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(51.7672, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6217.5913, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-516.1495, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7186.9854, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(517.7749, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0711],
        [-0.2997],
        [-0.5029],
        ...,
        [-1.2348],
        [-1.2310],
        [-1.2300]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-239726.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0031],
        [1.0044],
        [1.0095],
        ...,
        [0.9995],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367567.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3296],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.5911, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0032],
        [1.0045],
        [1.0096],
        ...,
        [0.9996],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367578.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3296],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.5911, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0070e-02,  1.9881e-03,  4.7172e-03,  ...,  1.2757e-02,
         -2.5471e-03,  2.3555e-03],
        [-7.3602e-03,  1.3655e-03,  3.3400e-03,  ...,  9.5246e-03,
         -1.8617e-03,  1.7027e-03],
        [-5.1326e-03,  8.5362e-04,  2.2078e-03,  ...,  6.8675e-03,
         -1.2983e-03,  1.1660e-03],
        ...,
        [ 0.0000e+00, -3.2572e-04, -4.0093e-04,  ...,  7.4524e-04,
          0.0000e+00, -7.0626e-05],
        [ 0.0000e+00, -3.2572e-04, -4.0093e-04,  ...,  7.4524e-04,
          0.0000e+00, -7.0626e-05],
        [ 0.0000e+00, -3.2572e-04, -4.0093e-04,  ...,  7.4524e-04,
          0.0000e+00, -7.0626e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1579.3611, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.3541, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.8844, device='cuda:0')



h[100].sum tensor(-12.6575, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.1216, device='cuda:0')



h[200].sum tensor(-10.7161, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.8870, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0067, 0.0162,  ..., 0.0448, 0.0000, 0.0082],
        [0.0000, 0.0045, 0.0113,  ..., 0.0334, 0.0000, 0.0059],
        [0.0000, 0.0034, 0.0088,  ..., 0.0274, 0.0000, 0.0046],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47132.7422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1431, 0.0000, 0.0792,  ..., 0.0939, 0.0000, 0.0000],
        [0.1432, 0.0000, 0.0783,  ..., 0.0946, 0.0000, 0.0000],
        [0.1324, 0.0000, 0.0726,  ..., 0.0880, 0.0000, 0.0000],
        ...,
        [0.0110, 0.0227, 0.0178,  ..., 0.0086, 0.0000, 0.0000],
        [0.0110, 0.0227, 0.0178,  ..., 0.0086, 0.0000, 0.0000],
        [0.0110, 0.0227, 0.0178,  ..., 0.0086, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(515019.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10391.1504, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(37.7340, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6099.4097, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-503.8038, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7049.2734, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(497.1692, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1232],
        [ 0.1530],
        [ 0.1547],
        ...,
        [-1.2396],
        [-1.2359],
        [-1.2349]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-224950.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0032],
        [1.0045],
        [1.0096],
        ...,
        [0.9996],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367578.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.1165, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0032],
        [1.0046],
        [1.0097],
        ...,
        [0.9996],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367588.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.1165, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.3376e-04, -3.9799e-04,  ...,  7.5819e-04,
          0.0000e+00, -7.9152e-05],
        [ 0.0000e+00, -3.3376e-04, -3.9799e-04,  ...,  7.5819e-04,
          0.0000e+00, -7.9152e-05],
        [-1.4487e-02,  3.0007e-03,  6.9731e-03,  ...,  1.8057e-02,
         -3.6560e-03,  3.4135e-03],
        ...,
        [ 0.0000e+00, -3.3376e-04, -3.9799e-04,  ...,  7.5819e-04,
          0.0000e+00, -7.9152e-05],
        [ 0.0000e+00, -3.3376e-04, -3.9799e-04,  ...,  7.5819e-04,
          0.0000e+00, -7.9152e-05],
        [ 0.0000e+00, -3.3376e-04, -3.9799e-04,  ...,  7.5819e-04,
          0.0000e+00, -7.9152e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1784.2146, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.2851, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.6009, device='cuda:0')



h[100].sum tensor(-19.8939, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.3833, device='cuda:0')



h[200].sum tensor(-7.1869, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-33.8975, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0030, 0.0070,  ..., 0.0204, 0.0000, 0.0034],
        [0.0000, 0.0104, 0.0237,  ..., 0.0606, 0.0000, 0.0115],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0031, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59717.8359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0453, 0.0093, 0.0341,  ..., 0.0305, 0.0000, 0.0000],
        [0.1391, 0.0025, 0.0810,  ..., 0.0891, 0.0000, 0.0000],
        [0.2973, 0.0000, 0.1602,  ..., 0.1880, 0.0000, 0.0000],
        ...,
        [0.0110, 0.0229, 0.0179,  ..., 0.0087, 0.0000, 0.0000],
        [0.0110, 0.0229, 0.0179,  ..., 0.0087, 0.0000, 0.0000],
        [0.0110, 0.0229, 0.0179,  ..., 0.0087, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(575230., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12447.1875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(87.8211, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5843.3696, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-580.3762, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8099.8877, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(626.6219, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0590],
        [ 0.0375],
        [ 0.1149],
        ...,
        [-1.2465],
        [-1.2428],
        [-1.2418]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-216514.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0032],
        [1.0046],
        [1.0097],
        ...,
        [0.9996],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367588.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.4089, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0033],
        [1.0046],
        [1.0098],
        ...,
        [0.9996],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367599.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.4089, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.4222e-04, -3.9358e-04,  ...,  7.7394e-04,
          0.0000e+00, -8.6102e-05],
        [ 0.0000e+00, -3.4222e-04, -3.9358e-04,  ...,  7.7394e-04,
          0.0000e+00, -8.6102e-05],
        [ 0.0000e+00, -3.4222e-04, -3.9358e-04,  ...,  7.7394e-04,
          0.0000e+00, -8.6102e-05],
        ...,
        [ 0.0000e+00, -3.4222e-04, -3.9358e-04,  ...,  7.7394e-04,
          0.0000e+00, -8.6102e-05],
        [ 0.0000e+00, -3.4222e-04, -3.9358e-04,  ...,  7.7394e-04,
          0.0000e+00, -8.6102e-05],
        [ 0.0000e+00, -3.4222e-04, -3.9358e-04,  ...,  7.7394e-04,
          0.0000e+00, -8.6102e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1648.5972, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.7800, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.3370, device='cuda:0')



h[100].sum tensor(-14.4169, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.7580, device='cuda:0')



h[200].sum tensor(-9.9448, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.2665, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0032, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50633.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0102, 0.0225, 0.0173,  ..., 0.0083, 0.0000, 0.0000],
        [0.0109, 0.0221, 0.0175,  ..., 0.0087, 0.0000, 0.0000],
        [0.0126, 0.0213, 0.0180,  ..., 0.0099, 0.0000, 0.0000],
        ...,
        [0.0109, 0.0230, 0.0178,  ..., 0.0086, 0.0000, 0.0000],
        [0.0109, 0.0230, 0.0178,  ..., 0.0086, 0.0000, 0.0000],
        [0.0109, 0.0230, 0.0178,  ..., 0.0086, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(531238., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11153.4434, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(52.4544, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5629.4453, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-527.6215, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7466.7168, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(534.0654, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2546],
        [-1.1181],
        [-0.9175],
        ...,
        [-1.2509],
        [-1.2463],
        [-1.2445]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-194218.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0033],
        [1.0046],
        [1.0098],
        ...,
        [0.9996],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367599.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(161.3199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0033],
        [1.0047],
        [1.0099],
        ...,
        [0.9996],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367610.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(161.3199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.7139e-02,  3.6011e-03,  8.3551e-03,  ...,  2.1301e-02,
         -4.3048e-03,  4.0414e-03],
        [ 0.0000e+00, -3.5670e-04, -3.8371e-04,  ...,  7.9142e-04,
          0.0000e+00, -9.4347e-05],
        [ 0.0000e+00, -3.5670e-04, -3.8371e-04,  ...,  7.9142e-04,
          0.0000e+00, -9.4347e-05],
        ...,
        [ 0.0000e+00, -3.5670e-04, -3.8371e-04,  ...,  7.9142e-04,
          0.0000e+00, -9.4347e-05],
        [ 0.0000e+00, -3.5670e-04, -3.8371e-04,  ...,  7.9142e-04,
          0.0000e+00, -9.4347e-05],
        [ 0.0000e+00, -3.5670e-04, -3.8371e-04,  ...,  7.9142e-04,
          0.0000e+00, -9.4347e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1537.6968, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.0894, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.8386, device='cuda:0')



h[100].sum tensor(-10.2657, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.3313, device='cuda:0')



h[200].sum tensor(-11.9806, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-17.6902, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0101, 0.0235,  ..., 0.0609, 0.0000, 0.0114],
        [0.0000, 0.0065, 0.0152,  ..., 0.0405, 0.0000, 0.0073],
        [0.0000, 0.0000, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0032, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44507.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2334, 0.0000, 0.1263,  ..., 0.1494, 0.0000, 0.0000],
        [0.1551, 0.0000, 0.0881,  ..., 0.0997, 0.0000, 0.0000],
        [0.0564, 0.0093, 0.0395,  ..., 0.0373, 0.0000, 0.0000],
        ...,
        [0.0105, 0.0230, 0.0174,  ..., 0.0082, 0.0000, 0.0000],
        [0.0105, 0.0230, 0.0174,  ..., 0.0082, 0.0000, 0.0000],
        [0.0105, 0.0230, 0.0174,  ..., 0.0082, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(510989.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9864.8584, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(27.0881, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6078.1719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-491.5483, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7067.6895, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(465.8474, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1094],
        [-0.0198],
        [-0.2908],
        ...,
        [-1.2643],
        [-1.2606],
        [-1.2596]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-237046.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0033],
        [1.0047],
        [1.0099],
        ...,
        [0.9996],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367610.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2603],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(480.4238, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0033],
        [1.0048],
        [1.0101],
        ...,
        [0.9996],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367621.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2603],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(480.4238, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0004, -0.0004,  ...,  0.0008,  0.0000, -0.0001],
        [-0.0058,  0.0010,  0.0026,  ...,  0.0077, -0.0015,  0.0013],
        [ 0.0000, -0.0004, -0.0004,  ...,  0.0008,  0.0000, -0.0001],
        ...,
        [ 0.0000, -0.0004, -0.0004,  ...,  0.0008,  0.0000, -0.0001],
        [ 0.0000, -0.0004, -0.0004,  ...,  0.0008,  0.0000, -0.0001],
        [ 0.0000, -0.0004, -0.0004,  ...,  0.0008,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2088.2568, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-41.5903, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(38.2344, device='cuda:0')



h[100].sum tensor(-30.2928, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.9209, device='cuda:0')



h[200].sum tensor(-1.9514, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-52.6829, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0072, 0.0178,  ..., 0.0486, 0.0000, 0.0087],
        [0.0000, 0.0007, 0.0020,  ..., 0.0090, 0.0000, 0.0010],
        [0.0000, 0.0073, 0.0176,  ..., 0.0471, 0.0000, 0.0085],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0033, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74752.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2218, 0.0000, 0.1185,  ..., 0.1425, 0.0000, 0.0000],
        [0.1985, 0.0000, 0.1075,  ..., 0.1274, 0.0000, 0.0000],
        [0.2788, 0.0000, 0.1478,  ..., 0.1776, 0.0000, 0.0000],
        ...,
        [0.0101, 0.0230, 0.0167,  ..., 0.0075, 0.0000, 0.0000],
        [0.0101, 0.0230, 0.0167,  ..., 0.0075, 0.0000, 0.0000],
        [0.0101, 0.0230, 0.0167,  ..., 0.0075, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(638054., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(14419.5176, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(152.1467, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5261.8052, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-670.5499, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9267.2285, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(768.6924, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0973],
        [ 0.0927],
        [ 0.0879],
        ...,
        [-1.2709],
        [-1.2676],
        [-1.2671]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-199137.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0033],
        [1.0048],
        [1.0101],
        ...,
        [0.9996],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367621.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3623],
        [0.4749],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.7906, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0033],
        [1.0048],
        [1.0101],
        ...,
        [0.9996],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367621.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3623],
        [0.4749],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.7906, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0221,  0.0047,  0.0109,  ...,  0.0273, -0.0055,  0.0052],
        [-0.0239,  0.0052,  0.0118,  ...,  0.0295, -0.0060,  0.0057],
        [-0.0223,  0.0048,  0.0110,  ...,  0.0275, -0.0056,  0.0053],
        ...,
        [ 0.0000, -0.0004, -0.0004,  ...,  0.0008,  0.0000, -0.0001],
        [ 0.0000, -0.0004, -0.0004,  ...,  0.0008,  0.0000, -0.0001],
        [ 0.0000, -0.0004, -0.0004,  ...,  0.0008,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1741.4816, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.1864, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.1875, device='cuda:0')



h[100].sum tensor(-17.6165, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.7571, device='cuda:0')



h[200].sum tensor(-8.2933, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.5720, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0179, 0.0413,  ..., 0.1037, 0.0000, 0.0198],
        [0.0000, 0.0208, 0.0479,  ..., 0.1191, 0.0000, 0.0229],
        [0.0000, 0.0218, 0.0499,  ..., 0.1240, 0.0000, 0.0239],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0033, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56770.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.4827, 0.0000, 0.2510,  ..., 0.3046, 0.0000, 0.0000],
        [0.4938, 0.0000, 0.2567,  ..., 0.3115, 0.0000, 0.0000],
        [0.4711, 0.0000, 0.2451,  ..., 0.2974, 0.0000, 0.0000],
        ...,
        [0.0101, 0.0230, 0.0167,  ..., 0.0075, 0.0000, 0.0000],
        [0.0101, 0.0230, 0.0167,  ..., 0.0075, 0.0000, 0.0000],
        [0.0101, 0.0230, 0.0167,  ..., 0.0075, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(561665.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11906.0020, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(80.6199, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5359.1348, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-563.3544, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8006.4922, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(585.3005, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0986],
        [ 0.1008],
        [ 0.1054],
        ...,
        [-1.2736],
        [-1.2699],
        [-1.2689]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-191711.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0033],
        [1.0048],
        [1.0101],
        ...,
        [0.9996],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367621.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4102],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(250.8693, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0033],
        [1.0048],
        [1.0102],
        ...,
        [0.9996],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367631.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4102],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(250.8693, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0252,  0.0055,  0.0125,  ...,  0.0311, -0.0063,  0.0060],
        [-0.0176,  0.0037,  0.0087,  ...,  0.0220, -0.0044,  0.0041],
        [-0.0059,  0.0010,  0.0027,  ...,  0.0079, -0.0015,  0.0013],
        ...,
        [ 0.0000, -0.0004, -0.0004,  ...,  0.0008,  0.0000, -0.0001],
        [ 0.0000, -0.0004, -0.0004,  ...,  0.0008,  0.0000, -0.0001],
        [ 0.0000, -0.0004, -0.0004,  ...,  0.0008,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1688.3928, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.6937, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.9653, device='cuda:0')



h[100].sum tensor(-15.7954, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.1805, device='cuda:0')



h[200].sum tensor(-9.0774, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.5101, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0197, 0.0454,  ..., 0.1132, 0.0000, 0.0217],
        [0.0000, 0.0103, 0.0248,  ..., 0.0648, 0.0000, 0.0119],
        [0.0000, 0.0086, 0.0211,  ..., 0.0561, 0.0000, 0.0102],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0034, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52947.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3930, 0.0000, 0.2048,  ..., 0.2490, 0.0000, 0.0000],
        [0.3013, 0.0000, 0.1575,  ..., 0.1922, 0.0000, 0.0000],
        [0.2449, 0.0000, 0.1285,  ..., 0.1572, 0.0000, 0.0000],
        ...,
        [0.0096, 0.0229, 0.0161,  ..., 0.0069, 0.0000, 0.0000],
        [0.0096, 0.0229, 0.0161,  ..., 0.0069, 0.0000, 0.0000],
        [0.0096, 0.0229, 0.0161,  ..., 0.0069, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(546745.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11120.2871, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(68.0364, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5424.0830, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-540.6916, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7722.6113, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(539.8308, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1542],
        [ 0.1636],
        [ 0.1712],
        ...,
        [-1.2872],
        [-1.2835],
        [-1.2824]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-204107.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0033],
        [1.0048],
        [1.0102],
        ...,
        [0.9996],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367631.8750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 200.0 event: 1000 loss: tensor(415.8855, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(223.1285, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0034],
        [1.0049],
        [1.0103],
        ...,
        [0.9996],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367642.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(223.1285, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0004, -0.0003,  ...,  0.0008,  0.0000, -0.0001],
        [ 0.0000, -0.0004, -0.0003,  ...,  0.0008,  0.0000, -0.0001],
        [-0.0068,  0.0012,  0.0031,  ...,  0.0090, -0.0017,  0.0015],
        ...,
        [ 0.0000, -0.0004, -0.0003,  ...,  0.0008,  0.0000, -0.0001],
        [ 0.0000, -0.0004, -0.0003,  ...,  0.0008,  0.0000, -0.0001],
        [ 0.0000, -0.0004, -0.0003,  ...,  0.0008,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1639.7400, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.3008, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.7576, device='cuda:0')



h[100].sum tensor(-14.0482, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.6077, device='cuda:0')



h[200].sum tensor(-9.7427, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.4681, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0012, 0.0031,  ..., 0.0116, 0.0000, 0.0015],
        [0.0000, 0.0024, 0.0064,  ..., 0.0200, 0.0000, 0.0031],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0034, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51945.3984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0314, 0.0056, 0.0254,  ..., 0.0214, 0.0000, 0.0000],
        [0.0532, 0.0033, 0.0357,  ..., 0.0355, 0.0000, 0.0000],
        [0.0860, 0.0000, 0.0507,  ..., 0.0568, 0.0000, 0.0000],
        ...,
        [0.0091, 0.0230, 0.0158,  ..., 0.0064, 0.0000, 0.0000],
        [0.0091, 0.0230, 0.0158,  ..., 0.0064, 0.0000, 0.0000],
        [0.0091, 0.0230, 0.0158,  ..., 0.0064, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(551365.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10935.4902, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(65.0489, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5571.7783, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-536.0308, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7760.3848, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(524.0992, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2927],
        [-0.0785],
        [ 0.0659],
        ...,
        [-1.3021],
        [-1.2983],
        [-1.2972]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-222867.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0034],
        [1.0049],
        [1.0103],
        ...,
        [0.9996],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367642.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4946],
        [0.0000],
        [0.2998],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.1536, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0034],
        [1.0050],
        [1.0105],
        ...,
        [0.9996],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367653.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4946],
        [0.0000],
        [0.2998],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.1536, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0190,  0.0040,  0.0094,  ...,  0.0237, -0.0047,  0.0045],
        [-0.0176,  0.0037,  0.0087,  ...,  0.0220, -0.0044,  0.0041],
        [-0.0045,  0.0007,  0.0019,  ...,  0.0062, -0.0011,  0.0010],
        ...,
        [ 0.0000, -0.0004, -0.0003,  ...,  0.0008,  0.0000, -0.0001],
        [ 0.0000, -0.0004, -0.0003,  ...,  0.0008,  0.0000, -0.0001],
        [ 0.0000, -0.0004, -0.0003,  ...,  0.0008,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1739.4868, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.6512, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.6593, device='cuda:0')



h[100].sum tensor(-17.2087, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.6200, device='cuda:0')



h[200].sum tensor(-8.0213, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-29.8442, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0184, 0.0426,  ..., 0.1066, 0.0000, 0.0203],
        [0.0000, 0.0131, 0.0310,  ..., 0.0793, 0.0000, 0.0148],
        [0.0000, 0.0171, 0.0397,  ..., 0.0999, 0.0000, 0.0189],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54571.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.4174, 0.0000, 0.2186,  ..., 0.2652, 0.0000, 0.0000],
        [0.3898, 0.0000, 0.2043,  ..., 0.2480, 0.0000, 0.0000],
        [0.3927, 0.0000, 0.2057,  ..., 0.2497, 0.0000, 0.0000],
        ...,
        [0.0090, 0.0231, 0.0159,  ..., 0.0065, 0.0000, 0.0000],
        [0.0090, 0.0231, 0.0159,  ..., 0.0064, 0.0000, 0.0000],
        [0.0090, 0.0231, 0.0159,  ..., 0.0064, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(557125.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11092.3105, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(74.7071, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5508.7930, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-554.0002, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7881.7217, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(550.0861, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0923],
        [ 0.0894],
        [ 0.0888],
        ...,
        [-1.3075],
        [-1.3036],
        [-1.3026]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-225541.7031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0034],
        [1.0050],
        [1.0105],
        ...,
        [0.9996],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367653.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3232],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.5454, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0034],
        [1.0050],
        [1.0106],
        ...,
        [0.9997],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367664.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3232],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.5454, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0004, -0.0003,  ...,  0.0008,  0.0000, -0.0001],
        [-0.0072,  0.0013,  0.0033,  ...,  0.0095, -0.0018,  0.0016],
        [ 0.0000, -0.0004, -0.0003,  ...,  0.0008,  0.0000, -0.0001],
        ...,
        [ 0.0000, -0.0004, -0.0003,  ...,  0.0008,  0.0000, -0.0001],
        [ 0.0000, -0.0004, -0.0003,  ...,  0.0008,  0.0000, -0.0001],
        [ 0.0000, -0.0004, -0.0003,  ...,  0.0008,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1709.7065, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.4070, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.7804, device='cuda:0')



h[100].sum tensor(-15.5704, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.1325, device='cuda:0')



h[200].sum tensor(-8.7694, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.2553, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0013, 0.0033,  ..., 0.0120, 0.0000, 0.0016],
        [0.0000, 0.0010, 0.0027,  ..., 0.0105, 0.0000, 0.0013],
        [0.0000, 0.0055, 0.0140,  ..., 0.0396, 0.0000, 0.0068],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54377.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0465, 0.0058, 0.0330,  ..., 0.0315, 0.0000, 0.0000],
        [0.0667, 0.0021, 0.0423,  ..., 0.0448, 0.0000, 0.0000],
        [0.1076, 0.0000, 0.0615,  ..., 0.0715, 0.0000, 0.0000],
        ...,
        [0.0089, 0.0232, 0.0161,  ..., 0.0066, 0.0000, 0.0000],
        [0.0089, 0.0232, 0.0161,  ..., 0.0066, 0.0000, 0.0000],
        [0.0089, 0.0232, 0.0161,  ..., 0.0066, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(565678.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11222.4805, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(71.1741, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5603.5879, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-555.0411, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8017.1670, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(547.1875, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2395],
        [-0.0538],
        [ 0.0479],
        ...,
        [-1.3184],
        [-1.3146],
        [-1.3137]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-230142.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0034],
        [1.0050],
        [1.0106],
        ...,
        [0.9997],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367664.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(269.6230, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0035],
        [1.0051],
        [1.0107],
        ...,
        [0.9997],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367675.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(269.6230, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0004, -0.0003,  ...,  0.0009,  0.0000, -0.0001],
        [ 0.0000, -0.0004, -0.0003,  ...,  0.0009,  0.0000, -0.0001],
        [ 0.0000, -0.0004, -0.0003,  ...,  0.0009,  0.0000, -0.0001],
        ...,
        [ 0.0000, -0.0004, -0.0003,  ...,  0.0009,  0.0000, -0.0001],
        [ 0.0000, -0.0004, -0.0003,  ...,  0.0009,  0.0000, -0.0001],
        [ 0.0000, -0.0004, -0.0003,  ...,  0.0009,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1771.9988, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.3216, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.4579, device='cuda:0')



h[100].sum tensor(-16.9572, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.5678, device='cuda:0')



h[200].sum tensor(-8.1124, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-29.5667, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53299.5977, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0101, 0.0216, 0.0169,  ..., 0.0079, 0.0000, 0.0000],
        [0.0091, 0.0225, 0.0164,  ..., 0.0073, 0.0000, 0.0000],
        [0.0111, 0.0213, 0.0171,  ..., 0.0086, 0.0000, 0.0000],
        ...,
        [0.0091, 0.0234, 0.0167,  ..., 0.0070, 0.0000, 0.0000],
        [0.0091, 0.0234, 0.0167,  ..., 0.0070, 0.0000, 0.0000],
        [0.0091, 0.0234, 0.0167,  ..., 0.0070, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(550224.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10724.6533, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(64.9427, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5589.9326, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-549.5935, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7758.9707, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(537.5034, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9421],
        [-1.0595],
        [-1.0881],
        ...,
        [-1.3187],
        [-1.3157],
        [-1.3155]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-230509.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0035],
        [1.0051],
        [1.0107],
        ...,
        [0.9997],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367675.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.8645, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0035],
        [1.0052],
        [1.0108],
        ...,
        [0.9997],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367685.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.8645, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.4153e-04, -3.5231e-04,  ...,  8.5462e-04,
          0.0000e+00, -8.9018e-05],
        [ 0.0000e+00, -3.4153e-04, -3.5231e-04,  ...,  8.5462e-04,
          0.0000e+00, -8.9018e-05],
        [-5.6118e-03,  9.6956e-04,  2.5294e-03,  ...,  7.6212e-03,
         -1.3864e-03,  1.2717e-03],
        ...,
        [ 0.0000e+00, -3.4153e-04, -3.5231e-04,  ...,  8.5462e-04,
          0.0000e+00, -8.9018e-05],
        [ 0.0000e+00, -3.4153e-04, -3.5231e-04,  ...,  8.5462e-04,
          0.0000e+00, -8.9018e-05],
        [ 0.0000e+00, -3.4153e-04, -3.5231e-04,  ...,  8.5462e-04,
          0.0000e+00, -8.9018e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1695.5034, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.7994, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.2591, device='cuda:0')



h[100].sum tensor(-13.6643, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.4783, device='cuda:0')



h[200].sum tensor(-9.7784, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.7812, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0010, 0.0025,  ..., 0.0102, 0.0000, 0.0013],
        [0.0000, 0.0037, 0.0090,  ..., 0.0262, 0.0000, 0.0044],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51187.7461, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[9.7820e-02, 2.5513e-05, 6.0352e-02,  ..., 6.4767e-02, 0.0000e+00,
         0.0000e+00],
        [1.1487e-01, 0.0000e+00, 6.8061e-02,  ..., 7.6106e-02, 0.0000e+00,
         0.0000e+00],
        [1.4475e-01, 0.0000e+00, 8.2242e-02,  ..., 9.5514e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [9.1692e-03, 2.3655e-02, 1.7299e-02,  ..., 7.4616e-03, 0.0000e+00,
         0.0000e+00],
        [2.0001e-02, 1.5426e-02, 2.2378e-02,  ..., 1.4570e-02, 0.0000e+00,
         0.0000e+00],
        [4.4417e-02, 7.1511e-03, 3.4002e-02,  ..., 3.0476e-02, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(547740.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10768.8242, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(55.9457, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5515.2471, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-537.7396, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7743.5850, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(519.2554, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0303],
        [ 0.0428],
        [ 0.0567],
        ...,
        [-1.0934],
        [-0.8187],
        [-0.4854]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-209029.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0035],
        [1.0052],
        [1.0108],
        ...,
        [0.9997],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367685.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2532],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.7178, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0035],
        [1.0052],
        [1.0110],
        ...,
        [0.9997],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367696.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2532],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.7178, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.9329e-03,  1.0588e-03,  2.6956e-03,  ...,  8.0137e-03,
         -1.4622e-03,  1.3621e-03],
        [-5.5898e-03,  9.7852e-04,  2.5193e-03,  ...,  7.5996e-03,
         -1.3777e-03,  1.2788e-03],
        [ 0.0000e+00, -3.2955e-04, -3.5383e-04,  ...,  8.5229e-04,
          0.0000e+00, -7.7937e-05],
        ...,
        [ 0.0000e+00, -3.2955e-04, -3.5383e-04,  ...,  8.5229e-04,
          0.0000e+00, -7.7937e-05],
        [ 0.0000e+00, -3.2955e-04, -3.5383e-04,  ...,  8.5229e-04,
          0.0000e+00, -7.7937e-05],
        [ 0.0000e+00, -3.2955e-04, -3.5383e-04,  ...,  8.5229e-04,
          0.0000e+00, -7.7937e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1733.0388, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.1729, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.4412, device='cuda:0')



h[100].sum tensor(-14.6576, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.7850, device='cuda:0')



h[200].sum tensor(-9.2435, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.4100, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0060, 0.0146,  ..., 0.0410, 0.0000, 0.0072],
        [0.0000, 0.0029, 0.0072,  ..., 0.0220, 0.0000, 0.0036],
        [0.0000, 0.0010, 0.0025,  ..., 0.0102, 0.0000, 0.0013],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52237.1836, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1181, 0.0000, 0.0686,  ..., 0.0795, 0.0000, 0.0000],
        [0.0873, 0.0000, 0.0540,  ..., 0.0593, 0.0000, 0.0000],
        [0.0500, 0.0062, 0.0362,  ..., 0.0348, 0.0000, 0.0000],
        ...,
        [0.0091, 0.0239, 0.0177,  ..., 0.0077, 0.0000, 0.0000],
        [0.0091, 0.0239, 0.0176,  ..., 0.0077, 0.0000, 0.0000],
        [0.0091, 0.0239, 0.0176,  ..., 0.0077, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(555439.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10810.6270, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(58.1221, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5747.0068, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-544.5979, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7887.2168, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(530.3296, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0530],
        [-0.0315],
        [-0.2184],
        ...,
        [-1.3334],
        [-1.3302],
        [-1.3299]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-235893.4844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0035],
        [1.0052],
        [1.0110],
        ...,
        [0.9997],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367696.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.5081, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0036],
        [1.0053],
        [1.0111],
        ...,
        [0.9997],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367707.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.5081, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.1874e-04, -3.5307e-04,  ...,  8.4670e-04,
          0.0000e+00, -6.9622e-05],
        [ 0.0000e+00, -3.1874e-04, -3.5307e-04,  ...,  8.4670e-04,
          0.0000e+00, -6.9622e-05],
        [ 0.0000e+00, -3.1874e-04, -3.5307e-04,  ...,  8.4670e-04,
          0.0000e+00, -6.9622e-05],
        ...,
        [ 0.0000e+00, -3.1874e-04, -3.5307e-04,  ...,  8.4670e-04,
          0.0000e+00, -6.9622e-05],
        [ 0.0000e+00, -3.1874e-04, -3.5307e-04,  ...,  8.4670e-04,
          0.0000e+00, -6.9622e-05],
        [ 0.0000e+00, -3.1874e-04, -3.5307e-04,  ...,  8.4670e-04,
          0.0000e+00, -6.9622e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1708.8429, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.8281, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.3103, device='cuda:0')



h[100].sum tensor(-13.6757, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.4916, device='cuda:0')



h[200].sum tensor(-9.6088, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.8518, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50758.0859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0225, 0.0135, 0.0228,  ..., 0.0173, 0.0000, 0.0000],
        [0.0206, 0.0149, 0.0221,  ..., 0.0160, 0.0000, 0.0000],
        [0.0225, 0.0136, 0.0232,  ..., 0.0171, 0.0000, 0.0000],
        ...,
        [0.0088, 0.0242, 0.0178,  ..., 0.0076, 0.0000, 0.0000],
        [0.0088, 0.0242, 0.0178,  ..., 0.0076, 0.0000, 0.0000],
        [0.0088, 0.0242, 0.0178,  ..., 0.0076, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(548797.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10563.5020, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(54.2437, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5787.0127, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-535.7722, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7780.0801, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(515.6117, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2830],
        [-0.3222],
        [-0.3043],
        ...,
        [-1.3521],
        [-1.3482],
        [-1.3474]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-232646.8594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0036],
        [1.0053],
        [1.0111],
        ...,
        [0.9997],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367707.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(362.7321, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0037],
        [1.0054],
        [1.0113],
        ...,
        [0.9997],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367717.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(362.7321, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.1680e-04, -3.4707e-04,  ...,  8.5527e-04,
          0.0000e+00, -5.8293e-05],
        [ 0.0000e+00, -3.1680e-04, -3.4707e-04,  ...,  8.5527e-04,
          0.0000e+00, -5.8293e-05],
        [ 0.0000e+00, -3.1680e-04, -3.4707e-04,  ...,  8.5527e-04,
          0.0000e+00, -5.8293e-05],
        ...,
        [ 0.0000e+00, -3.1680e-04, -3.4707e-04,  ...,  8.5527e-04,
          0.0000e+00, -5.8293e-05],
        [ 0.0000e+00, -3.1680e-04, -3.4707e-04,  ...,  8.5527e-04,
          0.0000e+00, -5.8293e-05],
        [ 0.0000e+00, -3.1680e-04, -3.4707e-04,  ...,  8.5527e-04,
          0.0000e+00, -5.8293e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1970.4078, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.6888, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.8679, device='cuda:0')



h[100].sum tensor(-22.2829, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-7.4905, device='cuda:0')



h[200].sum tensor(-5.2349, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-39.7769, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64813.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0079, 0.0238, 0.0171,  ..., 0.0073, 0.0000, 0.0000],
        [0.0080, 0.0238, 0.0172,  ..., 0.0073, 0.0000, 0.0000],
        [0.0082, 0.0239, 0.0173,  ..., 0.0074, 0.0000, 0.0000],
        ...,
        [0.0086, 0.0243, 0.0177,  ..., 0.0076, 0.0000, 0.0000],
        [0.0086, 0.0243, 0.0177,  ..., 0.0076, 0.0000, 0.0000],
        [0.0086, 0.0243, 0.0177,  ..., 0.0076, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(616793., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12691.3271, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(108.8428, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5728.8281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-618.8081, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8864.5664, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(655.8369, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3269],
        [-1.4526],
        [-1.5420],
        ...,
        [-1.3589],
        [-1.3560],
        [-1.3557]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-234195.8594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0037],
        [1.0054],
        [1.0113],
        ...,
        [0.9997],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367717.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.0400, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0037],
        [1.0055],
        [1.0114],
        ...,
        [0.9997],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367728.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.0400, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.6365e-03,  1.2370e-03,  3.0834e-03,  ...,  8.9112e-03,
         -1.6240e-03,  1.5749e-03],
        [ 0.0000e+00, -3.2113e-04, -3.3851e-04,  ...,  8.7303e-04,
          0.0000e+00, -4.1531e-05],
        [ 0.0000e+00, -3.2113e-04, -3.3851e-04,  ...,  8.7303e-04,
          0.0000e+00, -4.1531e-05],
        ...,
        [ 0.0000e+00, -3.2113e-04, -3.3851e-04,  ...,  8.7303e-04,
          0.0000e+00, -4.1531e-05],
        [ 0.0000e+00, -3.2113e-04, -3.3851e-04,  ...,  8.7303e-04,
          0.0000e+00, -4.1531e-05],
        [ 0.0000e+00, -3.2113e-04, -3.3851e-04,  ...,  8.7303e-04,
          0.0000e+00, -4.1531e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1716.1232, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.8353, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.4772, device='cuda:0')



h[100].sum tensor(-12.9456, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.2754, device='cuda:0')



h[200].sum tensor(-10.1282, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.7039, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0026, 0.0065,  ..., 0.0203, 0.0000, 0.0033],
        [0.0000, 0.0012, 0.0031,  ..., 0.0116, 0.0000, 0.0016],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49393.1953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[8.9867e-02, 7.0719e-05, 5.5932e-02,  ..., 6.0916e-02, 0.0000e+00,
         0.0000e+00],
        [5.0858e-02, 6.9458e-03, 3.7452e-02,  ..., 3.5383e-02, 0.0000e+00,
         0.0000e+00],
        [2.0269e-02, 1.4464e-02, 2.2920e-02,  ..., 1.5324e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [8.4022e-03, 2.4349e-02, 1.7511e-02,  ..., 7.5032e-03, 0.0000e+00,
         0.0000e+00],
        [8.3990e-03, 2.4346e-02, 1.7508e-02,  ..., 7.5017e-03, 0.0000e+00,
         0.0000e+00],
        [8.3987e-03, 2.4347e-02, 1.7508e-02,  ..., 7.5017e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(544152.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10152.8662, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(45.7971, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5861.2803, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-524.6300, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7542.4878, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(493.9089, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2083],
        [-0.5467],
        [-0.9124],
        ...,
        [-1.3660],
        [-1.3621],
        [-1.3610]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-234123.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0037],
        [1.0055],
        [1.0114],
        ...,
        [0.9997],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367728.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.2281, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0038],
        [1.0056],
        [1.0115],
        ...,
        [0.9997],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367738.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.2281, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.1429e-04, -3.3863e-04,  ...,  8.7747e-04,
          0.0000e+00, -2.1605e-05],
        [-1.2110e-02,  2.5312e-03,  5.9123e-03,  ...,  1.5563e-02,
         -2.9564e-03,  2.9326e-03],
        [-1.0500e-02,  2.1528e-03,  5.0811e-03,  ...,  1.3610e-02,
         -2.5632e-03,  2.5398e-03],
        ...,
        [ 0.0000e+00, -3.1429e-04, -3.3863e-04,  ...,  8.7747e-04,
          0.0000e+00, -2.1605e-05],
        [ 0.0000e+00, -3.1429e-04, -3.3863e-04,  ...,  8.7747e-04,
          0.0000e+00, -2.1605e-05],
        [ 0.0000e+00, -3.1429e-04, -3.3863e-04,  ...,  8.7747e-04,
          0.0000e+00, -2.1605e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1778.9301, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.1404, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.6410, device='cuda:0')



h[100].sum tensor(-14.6136, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.8369, device='cuda:0')



h[200].sum tensor(-9.3177, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.6853, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0025, 0.0059,  ..., 0.0182, 0.0000, 0.0029],
        [0.0000, 0.0042, 0.0099,  ..., 0.0284, 0.0000, 0.0050],
        [0.0000, 0.0139, 0.0319,  ..., 0.0816, 0.0000, 0.0156],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51018.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0734, 0.0033, 0.0494,  ..., 0.0496, 0.0000, 0.0000],
        [0.1290, 0.0000, 0.0772,  ..., 0.0851, 0.0000, 0.0000],
        [0.2349, 0.0000, 0.1309,  ..., 0.1523, 0.0000, 0.0000],
        ...,
        [0.0405, 0.0050, 0.0333,  ..., 0.0285, 0.0000, 0.0000],
        [0.0326, 0.0084, 0.0294,  ..., 0.0234, 0.0000, 0.0000],
        [0.0164, 0.0182, 0.0216,  ..., 0.0129, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(548079.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10275.8213, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(50.3005, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5845.7173, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-533.6236, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7619.6211, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(509.6101, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0500],
        [ 0.0498],
        [ 0.1124],
        ...,
        [-0.6228],
        [-0.7730],
        [-0.9982]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-232335.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0038],
        [1.0056],
        [1.0115],
        ...,
        [0.9997],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367738.5625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 210.0 event: 1050 loss: tensor(506.9615, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(157.4655, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0039],
        [1.0057],
        [1.0116],
        ...,
        [0.9997],
        [0.9987],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367748.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(157.4655, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.0148e-04, -3.3872e-04,  ...,  8.7918e-04,
          0.0000e+00, -1.6356e-06],
        [ 0.0000e+00, -3.0148e-04, -3.3872e-04,  ...,  8.7918e-04,
          0.0000e+00, -1.6356e-06],
        [ 0.0000e+00, -3.0148e-04, -3.3872e-04,  ...,  8.7918e-04,
          0.0000e+00, -1.6356e-06],
        ...,
        [ 0.0000e+00, -3.0148e-04, -3.3872e-04,  ...,  8.7918e-04,
          0.0000e+00, -1.6356e-06],
        [ 0.0000e+00, -3.0148e-04, -3.3872e-04,  ...,  8.7918e-04,
          0.0000e+00, -1.6356e-06],
        [ 0.0000e+00, -3.0148e-04, -3.3872e-04,  ...,  8.7918e-04,
          0.0000e+00, -1.6356e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1651.7930, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.5092, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.5318, device='cuda:0')



h[100].sum tensor(-9.7986, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.2517, device='cuda:0')



h[200].sum tensor(-11.6703, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-17.2675, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45870.9570, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0125, 0.0204, 0.0195,  ..., 0.0109, 0.0000, 0.0000],
        [0.0133, 0.0198, 0.0201,  ..., 0.0113, 0.0000, 0.0000],
        [0.0203, 0.0144, 0.0237,  ..., 0.0157, 0.0000, 0.0000],
        ...,
        [0.0082, 0.0247, 0.0179,  ..., 0.0079, 0.0000, 0.0000],
        [0.0082, 0.0247, 0.0179,  ..., 0.0079, 0.0000, 0.0000],
        [0.0082, 0.0247, 0.0179,  ..., 0.0079, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(533175.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9707.0156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(29.5242, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5944.5444, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-502.2850, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7313.1880, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(456.8374, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4661],
        [-0.5140],
        [-0.4601],
        ...,
        [-1.3809],
        [-1.3770],
        [-1.3759]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-229613.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0039],
        [1.0057],
        [1.0116],
        ...,
        [0.9997],
        [0.9987],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367748.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2578],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.3483, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0039],
        [1.0058],
        [1.0117],
        ...,
        [0.9997],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367758.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2578],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.3483, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.8794e-04, -3.3832e-04,  ...,  8.6642e-04,
          0.0000e+00,  1.3480e-05],
        [-5.6569e-03,  1.0435e-03,  2.5877e-03,  ...,  7.7419e-03,
         -1.3743e-03,  1.3975e-03],
        [-5.7955e-03,  1.0761e-03,  2.6594e-03,  ...,  7.9102e-03,
         -1.4080e-03,  1.4313e-03],
        ...,
        [ 0.0000e+00, -2.8794e-04, -3.3832e-04,  ...,  8.6642e-04,
          0.0000e+00,  1.3480e-05],
        [ 0.0000e+00, -2.8794e-04, -3.3832e-04,  ...,  8.6642e-04,
          0.0000e+00,  1.3480e-05],
        [ 0.0000e+00, -2.8794e-04, -3.3832e-04,  ...,  8.6642e-04,
          0.0000e+00,  1.3480e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1816.4646, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.1700, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.6851, device='cuda:0')



h[100].sum tensor(-15.3499, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.1078, device='cuda:0')



h[200].sum tensor(-8.6011, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.1240, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.0462e-03, 2.5944e-03,  ..., 1.0367e-02, 0.0000e+00,
         1.4416e-03],
        [0.0000e+00, 3.8668e-03, 9.0896e-03,  ..., 2.6437e-02, 0.0000e+00,
         4.6750e-03],
        [0.0000e+00, 9.4776e-03, 2.2014e-02,  ..., 5.8420e-02, 0.0000e+00,
         1.1111e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.5535e-03, 0.0000e+00,
         5.5285e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.5531e-03, 0.0000e+00,
         5.5278e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.5531e-03, 0.0000e+00,
         5.5278e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54169.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0613, 0.0053, 0.0426,  ..., 0.0430, 0.0000, 0.0000],
        [0.1160, 0.0000, 0.0694,  ..., 0.0783, 0.0000, 0.0000],
        [0.1750, 0.0000, 0.0985,  ..., 0.1162, 0.0000, 0.0000],
        ...,
        [0.0078, 0.0252, 0.0180,  ..., 0.0078, 0.0000, 0.0000],
        [0.0078, 0.0252, 0.0180,  ..., 0.0078, 0.0000, 0.0000],
        [0.0078, 0.0252, 0.0180,  ..., 0.0078, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(566100., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10792.2021, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(63.7944, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5910.8164, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-550.8367, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7942.8110, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(539.1641, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0265],
        [ 0.0884],
        [ 0.1522],
        ...,
        [-1.3983],
        [-1.3942],
        [-1.3931]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-233011.6719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0039],
        [1.0058],
        [1.0117],
        ...,
        [0.9997],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367758.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.3258, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0040],
        [1.0059],
        [1.0118],
        ...,
        [0.9997],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367768.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.3258, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.7954e-04, -3.3961e-04,  ...,  8.5936e-04,
          0.0000e+00,  3.4001e-05],
        [ 0.0000e+00, -2.7954e-04, -3.3961e-04,  ...,  8.5936e-04,
          0.0000e+00,  3.4001e-05],
        [ 0.0000e+00, -2.7954e-04, -3.3961e-04,  ...,  8.5936e-04,
          0.0000e+00,  3.4001e-05],
        ...,
        [ 0.0000e+00, -2.7954e-04, -3.3961e-04,  ...,  8.5936e-04,
          0.0000e+00,  3.4001e-05],
        [ 0.0000e+00, -2.7954e-04, -3.3961e-04,  ...,  8.5936e-04,
          0.0000e+00,  3.4001e-05],
        [ 0.0000e+00, -2.7954e-04, -3.3961e-04,  ...,  8.5936e-04,
          0.0000e+00,  3.4001e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1839.6042, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.9964, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.6384, device='cuda:0')



h[100].sum tensor(-15.9435, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.3552, device='cuda:0')



h[200].sum tensor(-8.1278, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.4375, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0034, 0.0000, 0.0001],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0001],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0001],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0001],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0001],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0001]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55695.7539, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0087, 0.0236, 0.0183,  ..., 0.0086, 0.0000, 0.0000],
        [0.0070, 0.0251, 0.0175,  ..., 0.0075, 0.0000, 0.0000],
        [0.0072, 0.0251, 0.0177,  ..., 0.0076, 0.0000, 0.0000],
        ...,
        [0.0076, 0.0256, 0.0181,  ..., 0.0078, 0.0000, 0.0000],
        [0.0076, 0.0256, 0.0181,  ..., 0.0078, 0.0000, 0.0000],
        [0.0076, 0.0256, 0.0181,  ..., 0.0078, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(582118.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11185.9541, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(70.8304, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6071.2334, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-559.2059, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8202.5977, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(554.5590, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0990],
        [-1.3363],
        [-1.4957],
        ...,
        [-1.4115],
        [-1.4074],
        [-1.4062]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-245145.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0040],
        [1.0059],
        [1.0118],
        ...,
        [0.9997],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367768.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5103],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.5892, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0041],
        [1.0059],
        [1.0119],
        ...,
        [0.9998],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367778.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5103],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.5892, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.6445e-04, -3.4594e-04,  ...,  8.4447e-04,
          0.0000e+00,  5.6298e-05],
        [-1.1173e-02,  2.3697e-03,  5.4446e-03,  ...,  1.4455e-02,
         -2.7013e-03,  2.7978e-03],
        [ 0.0000e+00, -2.6445e-04, -3.4594e-04,  ...,  8.4447e-04,
          0.0000e+00,  5.6298e-05],
        ...,
        [ 0.0000e+00, -2.6445e-04, -3.4594e-04,  ...,  8.4447e-04,
          0.0000e+00,  5.6298e-05],
        [ 0.0000e+00, -2.6445e-04, -3.4594e-04,  ...,  8.4447e-04,
          0.0000e+00,  5.6298e-05],
        [ 0.0000e+00, -2.6445e-04, -3.4594e-04,  ...,  8.4447e-04,
          0.0000e+00,  5.6298e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1794.7354, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.7172, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.3514, device='cuda:0')



h[100].sum tensor(-14.2865, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.7617, device='cuda:0')



h[200].sum tensor(-8.8721, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.2862, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0024, 0.0055,  ..., 0.0170, 0.0000, 0.0030],
        [0.0000, 0.0040, 0.0090,  ..., 0.0253, 0.0000, 0.0046],
        [0.0000, 0.0191, 0.0430,  ..., 0.1077, 0.0000, 0.0212],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51761.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1586, 0.0000, 0.0936,  ..., 0.1047, 0.0000, 0.0000],
        [0.2381, 0.0000, 0.1344,  ..., 0.1549, 0.0000, 0.0000],
        [0.3813, 0.0000, 0.2076,  ..., 0.2456, 0.0000, 0.0000],
        ...,
        [0.0076, 0.0261, 0.0184,  ..., 0.0080, 0.0000, 0.0000],
        [0.0076, 0.0261, 0.0184,  ..., 0.0080, 0.0000, 0.0000],
        [0.0076, 0.0261, 0.0184,  ..., 0.0080, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(556970.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10422.8457, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(57.9362, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6148.4648, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-535.5978, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7831.2588, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(516.9702, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1215],
        [ 0.1164],
        [ 0.1132],
        ...,
        [-1.4243],
        [-1.4201],
        [-1.4189]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-243338.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0041],
        [1.0059],
        [1.0119],
        ...,
        [0.9998],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367778.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2837],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.8247, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0042],
        [1.0060],
        [1.0120],
        ...,
        [0.9998],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367788.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2837],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.8247, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.3298e-02,  1.2323e-02,  2.7293e-02,  ...,  6.5843e-02,
         -1.2855e-02,  1.3189e-02],
        [-2.1143e-02,  4.7342e-03,  1.0613e-02,  ...,  2.6628e-02,
         -5.0996e-03,  5.2871e-03],
        [-1.1093e-02,  2.3623e-03,  5.4001e-03,  ...,  1.4372e-02,
         -2.6756e-03,  2.8175e-03],
        ...,
        [ 0.0000e+00, -2.5587e-04, -3.5419e-04,  ...,  8.4352e-04,
          0.0000e+00,  9.1457e-05],
        [ 0.0000e+00, -2.5587e-04, -3.5419e-04,  ...,  8.4352e-04,
          0.0000e+00,  9.1457e-05],
        [ 0.0000e+00, -2.5587e-04, -3.5419e-04,  ...,  8.4352e-04,
          0.0000e+00,  9.1457e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1850.7466, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.4389, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.8026, device='cuda:0')



h[100].sum tensor(-15.5285, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.1383, device='cuda:0')



h[200].sum tensor(-8.3348, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.2859, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0202, 0.0453,  ..., 0.1131, 0.0000, 0.0225],
        [0.0000, 0.0261, 0.0582,  ..., 0.1435, 0.0000, 0.0286],
        [0.0000, 0.0147, 0.0332,  ..., 0.0849, 0.0000, 0.0168],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55210.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.5005, 0.0000, 0.2701,  ..., 0.3213, 0.0000, 0.0000],
        [0.5100, 0.0000, 0.2749,  ..., 0.3277, 0.0000, 0.0000],
        [0.3880, 0.0000, 0.2104,  ..., 0.2512, 0.0000, 0.0000],
        ...,
        [0.0080, 0.0264, 0.0190,  ..., 0.0085, 0.0000, 0.0000],
        [0.0080, 0.0264, 0.0190,  ..., 0.0085, 0.0000, 0.0000],
        [0.0080, 0.0264, 0.0190,  ..., 0.0085, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(572837.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11206.6875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(69.7801, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5977.2349, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-557.3022, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8164.4443, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(555.7414, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1012],
        [ 0.1043],
        [ 0.1149],
        ...,
        [-1.4280],
        [-1.4238],
        [-1.4226]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-227012.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0042],
        [1.0060],
        [1.0120],
        ...,
        [0.9998],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367788.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5215],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(370.6937, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0042],
        [1.0061],
        [1.0121],
        ...,
        [0.9998],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367798.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5215],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(370.6937, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0300,  0.0068,  0.0152,  ...,  0.0374, -0.0072,  0.0075],
        [-0.0215,  0.0048,  0.0108,  ...,  0.0271, -0.0052,  0.0054],
        [ 0.0000, -0.0003, -0.0004,  ...,  0.0009,  0.0000,  0.0001],
        ...,
        [ 0.0000, -0.0003, -0.0004,  ...,  0.0009,  0.0000,  0.0001],
        [ 0.0000, -0.0003, -0.0004,  ...,  0.0009,  0.0000,  0.0001],
        [-0.0112,  0.0024,  0.0054,  ...,  0.0145, -0.0027,  0.0029]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2094.7656, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-31.5288, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.5015, device='cuda:0')



h[100].sum tensor(-22.8288, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-7.6549, device='cuda:0')



h[200].sum tensor(-4.7721, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-40.6500, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0258, 0.0576,  ..., 0.1422, 0.0000, 0.0285],
        [0.0000, 0.0122, 0.0275,  ..., 0.0707, 0.0000, 0.0141],
        [0.0000, 0.0065, 0.0148,  ..., 0.0400, 0.0000, 0.0079],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0005],
        [0.0000, 0.0024, 0.0056,  ..., 0.0175, 0.0000, 0.0033],
        [0.0000, 0.0043, 0.0100,  ..., 0.0288, 0.0000, 0.0056]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68003.9766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.5082, 0.0000, 0.2753,  ..., 0.3256, 0.0000, 0.0000],
        [0.3153, 0.0000, 0.1754,  ..., 0.2039, 0.0000, 0.0000],
        [0.1701, 0.0000, 0.1009,  ..., 0.1120, 0.0000, 0.0000],
        ...,
        [0.0327, 0.0094, 0.0314,  ..., 0.0247, 0.0000, 0.0000],
        [0.0739, 0.0040, 0.0520,  ..., 0.0510, 0.0000, 0.0000],
        [0.1279, 0.0000, 0.0789,  ..., 0.0855, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(635124.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(13495.9863, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(119.2719, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5669.7031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-633.5132, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9202.2588, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(688.6790, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0946],
        [ 0.0469],
        [-0.1911],
        ...,
        [-0.5993],
        [-0.3594],
        [-0.1116]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-208094.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0042],
        [1.0061],
        [1.0121],
        ...,
        [0.9998],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367798.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(317.1766, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0043],
        [1.0062],
        [1.0122],
        ...,
        [0.9998],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367808.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(317.1766, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0003, -0.0004,  ...,  0.0009,  0.0000,  0.0001],
        [ 0.0000, -0.0003, -0.0004,  ...,  0.0009,  0.0000,  0.0001],
        [ 0.0000, -0.0003, -0.0004,  ...,  0.0009,  0.0000,  0.0001],
        ...,
        [ 0.0000, -0.0003, -0.0004,  ...,  0.0009,  0.0000,  0.0001],
        [ 0.0000, -0.0003, -0.0004,  ...,  0.0009,  0.0000,  0.0001],
        [ 0.0000, -0.0003, -0.0004,  ...,  0.0009,  0.0000,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2013.2949, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.0073, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.2424, device='cuda:0')



h[100].sum tensor(-19.5481, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.5498, device='cuda:0')



h[200].sum tensor(-6.5079, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-34.7814, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0006],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59103.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0319, 0.0088, 0.0301,  ..., 0.0248, 0.0000, 0.0000],
        [0.0321, 0.0088, 0.0303,  ..., 0.0249, 0.0000, 0.0000],
        [0.0321, 0.0083, 0.0304,  ..., 0.0248, 0.0000, 0.0000],
        ...,
        [0.0084, 0.0269, 0.0196,  ..., 0.0092, 0.0000, 0.0000],
        [0.0084, 0.0269, 0.0196,  ..., 0.0092, 0.0000, 0.0000],
        [0.0084, 0.0269, 0.0196,  ..., 0.0092, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(582774.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11466.8467, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(78.1200, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6055.9761, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-579.9150, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8271.5527, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(597.4850, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2182],
        [-0.1453],
        [-0.0774],
        ...,
        [-1.4317],
        [-1.4277],
        [-1.4265]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-227630.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0043],
        [1.0062],
        [1.0122],
        ...,
        [0.9998],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367808.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6777],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(340.8691, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0043],
        [1.0063],
        [1.0124],
        ...,
        [0.9998],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367819.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6777],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(340.8691, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0003, -0.0004,  ...,  0.0009,  0.0000,  0.0002],
        [-0.0148,  0.0032,  0.0073,  ...,  0.0190, -0.0035,  0.0038],
        [ 0.0000, -0.0003, -0.0004,  ...,  0.0009,  0.0000,  0.0002],
        ...,
        [ 0.0000, -0.0003, -0.0004,  ...,  0.0009,  0.0000,  0.0002],
        [ 0.0000, -0.0003, -0.0004,  ...,  0.0009,  0.0000,  0.0002],
        [ 0.0000, -0.0003, -0.0004,  ...,  0.0009,  0.0000,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2089.8030, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.8734, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.1280, device='cuda:0')



h[100].sum tensor(-21.6150, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-7.0390, device='cuda:0')



h[200].sum tensor(-5.5403, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-37.3795, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0115, 0.0266,  ..., 0.0694, 0.0000, 0.0139],
        [0.0000, 0.0026, 0.0060,  ..., 0.0184, 0.0000, 0.0036],
        [0.0000, 0.0032, 0.0074,  ..., 0.0218, 0.0000, 0.0043],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66129.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1367, 0.0000, 0.0830,  ..., 0.0909, 0.0000, 0.0000],
        [0.0832, 0.0000, 0.0563,  ..., 0.0568, 0.0000, 0.0000],
        [0.0671, 0.0032, 0.0484,  ..., 0.0465, 0.0000, 0.0000],
        ...,
        [0.0084, 0.0270, 0.0195,  ..., 0.0091, 0.0000, 0.0000],
        [0.0084, 0.0270, 0.0195,  ..., 0.0091, 0.0000, 0.0000],
        [0.0084, 0.0270, 0.0195,  ..., 0.0091, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(624764.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(13060.7754, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(107.5779, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5675.9912, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-619.9099, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8951.3115, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(669.5190, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4326],
        [-0.5549],
        [-0.7379],
        ...,
        [-1.4357],
        [-1.4315],
        [-1.4301]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-191899.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0043],
        [1.0063],
        [1.0124],
        ...,
        [0.9998],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367819.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5762],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.3102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0044],
        [1.0064],
        [1.0125],
        ...,
        [0.9998],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367829.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5762],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.3102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0064,  0.0012,  0.0030,  ...,  0.0087, -0.0015,  0.0017],
        [-0.0126,  0.0026,  0.0062,  ...,  0.0163, -0.0030,  0.0032],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0009,  0.0000,  0.0001],
        ...,
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0009,  0.0000,  0.0001],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0009,  0.0000,  0.0001],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0009,  0.0000,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1775.7738, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.9566, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.9070, device='cuda:0')



h[100].sum tensor(-11.5414, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.8680, device='cuda:0')



h[200].sum tensor(-10.5772, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-20.5403, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0066, 0.0157,  ..., 0.0430, 0.0000, 0.0085],
        [0.0000, 0.0033, 0.0080,  ..., 0.0240, 0.0000, 0.0047],
        [0.0000, 0.0096, 0.0226,  ..., 0.0599, 0.0000, 0.0119],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0005],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0005],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0005]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48783.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2085, 0.0000, 0.1175,  ..., 0.1367, 0.0000, 0.0000],
        [0.1420, 0.0000, 0.0843,  ..., 0.0945, 0.0000, 0.0000],
        [0.1375, 0.0000, 0.0822,  ..., 0.0914, 0.0000, 0.0000],
        ...,
        [0.0080, 0.0272, 0.0189,  ..., 0.0086, 0.0000, 0.0000],
        [0.0080, 0.0272, 0.0189,  ..., 0.0086, 0.0000, 0.0000],
        [0.0080, 0.0272, 0.0189,  ..., 0.0086, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(551765.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10075.0684, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(33.3613, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6320.9521, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-512.7993, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7696.9824, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(486.2518, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1582],
        [ 0.1476],
        [ 0.0963],
        ...,
        [-1.4538],
        [-1.4496],
        [-1.4484]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-245696.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0044],
        [1.0064],
        [1.0125],
        ...,
        [0.9998],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367829.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.2456, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0044],
        [1.0064],
        [1.0126],
        ...,
        [0.9999],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367839.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.2456, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0003, -0.0003,  ...,  0.0009,  0.0000,  0.0001],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0009,  0.0000,  0.0001],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0009,  0.0000,  0.0001],
        ...,
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0009,  0.0000,  0.0001],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0009,  0.0000,  0.0001],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0009,  0.0000,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1944.6306, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.5352, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.6666, device='cuda:0')



h[100].sum tensor(-17.0170, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.6220, device='cuda:0')



h[200].sum tensor(-7.6453, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-29.8542, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0004],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59049.0547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0069, 0.0269, 0.0177,  ..., 0.0077, 0.0000, 0.0000],
        [0.0069, 0.0269, 0.0177,  ..., 0.0077, 0.0000, 0.0000],
        [0.0072, 0.0270, 0.0179,  ..., 0.0078, 0.0000, 0.0000],
        ...,
        [0.0092, 0.0262, 0.0191,  ..., 0.0091, 0.0000, 0.0000],
        [0.0076, 0.0275, 0.0183,  ..., 0.0080, 0.0000, 0.0000],
        [0.0076, 0.0275, 0.0183,  ..., 0.0080, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(592812.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11997.4893, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(82.1256, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5537.1748, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-571.0051, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8501.2930, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(590.4772, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6901],
        [-1.6810],
        [-1.6476],
        ...,
        [-1.4046],
        [-1.4458],
        [-1.4609]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-188810.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0044],
        [1.0064],
        [1.0126],
        ...,
        [0.9999],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367839.8125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 220.0 event: 1100 loss: tensor(520.3896, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5635],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.9114, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0045],
        [1.0065],
        [1.0127],
        ...,
        [0.9999],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367849.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5635],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.9114, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2004e-02,  2.5064e-03,  5.9428e-03,  ...,  1.5630e-02,
         -2.8534e-03,  3.0406e-03],
        [-1.2249e-02,  2.5643e-03,  6.0702e-03,  ...,  1.5930e-02,
         -2.9114e-03,  3.1010e-03],
        [-1.6258e-02,  3.5159e-03,  8.1629e-03,  ...,  2.0854e-02,
         -3.8644e-03,  4.0920e-03],
        ...,
        [ 0.0000e+00, -3.4260e-04, -3.2278e-04,  ...,  8.8799e-04,
          0.0000e+00,  7.3345e-05],
        [ 0.0000e+00, -3.4260e-04, -3.2278e-04,  ...,  8.8799e-04,
          0.0000e+00,  7.3345e-05],
        [ 0.0000e+00, -3.4260e-04, -3.2278e-04,  ...,  8.8799e-04,
          0.0000e+00,  7.3345e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1884.3022, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.0800, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.8095, device='cuda:0')



h[100].sum tensor(-15.2363, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.1401, device='cuda:0')



h[200].sum tensor(-8.2698, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.2954, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0115, 0.0271,  ..., 0.0704, 0.0000, 0.0138],
        [0.0000, 0.0109, 0.0257,  ..., 0.0672, 0.0000, 0.0131],
        [0.0000, 0.0122, 0.0280,  ..., 0.0719, 0.0000, 0.0140],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53872.9570, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2208, 0.0000, 0.1241,  ..., 0.1430, 0.0000, 0.0000],
        [0.2601, 0.0000, 0.1440,  ..., 0.1678, 0.0000, 0.0000],
        [0.2793, 0.0000, 0.1545,  ..., 0.1795, 0.0000, 0.0000],
        ...,
        [0.0074, 0.0279, 0.0180,  ..., 0.0075, 0.0000, 0.0000],
        [0.0074, 0.0279, 0.0180,  ..., 0.0075, 0.0000, 0.0000],
        [0.0074, 0.0279, 0.0180,  ..., 0.0075, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(576362.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10656.9355, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(53.9192, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6421.7642, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-539.0291, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8274.9609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(534.1617, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1515],
        [ 0.1655],
        [ 0.1661],
        ...,
        [-1.4912],
        [-1.4867],
        [-1.4855]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-265846.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0045],
        [1.0065],
        [1.0127],
        ...,
        [0.9999],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367849.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.3855],
        [0.6333]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(273.8981, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0045],
        [1.0065],
        [1.0128],
        ...,
        [0.9999],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367859.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.3855],
        [0.6333]], device='cuda:0') 
g.ndata[nfet].sum tensor(273.8981, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.9716e-02,  4.3456e-03,  9.9822e-03,  ...,  2.5117e-02,
         -4.6749e-03,  4.9172e-03],
        [ 0.0000e+00, -3.4161e-04, -3.1768e-04,  ...,  8.7944e-04,
          0.0000e+00,  4.0949e-05],
        [ 0.0000e+00, -3.4161e-04, -3.1768e-04,  ...,  8.7944e-04,
          0.0000e+00,  4.0949e-05],
        ...,
        [-8.3710e-03,  1.6485e-03,  4.0555e-03,  ...,  1.1170e-02,
         -1.9849e-03,  2.1113e-03],
        [-1.8509e-02,  4.0587e-03,  9.3518e-03,  ...,  2.3634e-02,
         -4.3888e-03,  4.6187e-03],
        [-1.3128e-02,  2.7794e-03,  6.5407e-03,  ...,  1.7019e-02,
         -3.1129e-03,  3.2879e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1936.0742, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.4024, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.7981, device='cuda:0')



h[100].sum tensor(-16.9090, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.6561, device='cuda:0')



h[200].sum tensor(-7.1529, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.0355, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0164, 0.0372,  ..., 0.0934, 0.0000, 0.0182],
        [0.0000, 0.0102, 0.0238,  ..., 0.0617, 0.0000, 0.0119],
        [0.0000, 0.0044, 0.0106,  ..., 0.0299, 0.0000, 0.0055],
        ...,
        [0.0000, 0.0076, 0.0175,  ..., 0.0464, 0.0000, 0.0088],
        [0.0000, 0.0131, 0.0307,  ..., 0.0789, 0.0000, 0.0153],
        [0.0000, 0.0157, 0.0363,  ..., 0.0921, 0.0000, 0.0180]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56153.1016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.4518, 0.0000, 0.2445,  ..., 0.2870, 0.0000, 0.0000],
        [0.3473, 0.0000, 0.1899,  ..., 0.2218, 0.0000, 0.0000],
        [0.2844, 0.0000, 0.1574,  ..., 0.1824, 0.0000, 0.0000],
        ...,
        [0.1806, 0.0000, 0.1041,  ..., 0.1174, 0.0000, 0.0000],
        [0.2511, 0.0000, 0.1393,  ..., 0.1621, 0.0000, 0.0000],
        [0.2574, 0.0000, 0.1430,  ..., 0.1657, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(589369.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10952.7227, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(64.4245, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6559.1387, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-551.4475, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8525.7910, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(558.0667, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0550],
        [ 0.0635],
        [ 0.0700],
        ...,
        [-0.0840],
        [ 0.1111],
        [ 0.0918]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-282464.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0045],
        [1.0065],
        [1.0128],
        ...,
        [0.9999],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367859.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6411],
        [0.5576],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(228.6858, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0045],
        [1.0065],
        [1.0128],
        ...,
        [0.9999],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367859.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6411],
        [0.5576],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(228.6858, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.6327e-02,  5.9173e-03,  1.3436e-02,  ...,  3.3245e-02,
         -6.2425e-03,  6.5523e-03],
        [-2.8437e-02,  6.4189e-03,  1.4538e-02,  ...,  3.5839e-02,
         -6.7428e-03,  7.0742e-03],
        [-2.6126e-02,  5.8694e-03,  1.3331e-02,  ...,  3.2997e-02,
         -6.1947e-03,  6.5025e-03],
        ...,
        [ 0.0000e+00, -3.4161e-04, -3.1768e-04,  ...,  8.7944e-04,
          0.0000e+00,  4.0949e-05],
        [ 0.0000e+00, -3.4161e-04, -3.1768e-04,  ...,  8.7944e-04,
          0.0000e+00,  4.0949e-05],
        [ 0.0000e+00, -3.4161e-04, -3.1768e-04,  ...,  8.7944e-04,
          0.0000e+00,  4.0949e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1844.5986, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.4117, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.1999, device='cuda:0')



h[100].sum tensor(-14.0256, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.7224, device='cuda:0')



h[200].sum tensor(-8.6487, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.0775, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0208, 0.0475,  ..., 0.1182, 0.0000, 0.0232],
        [0.0000, 0.0214, 0.0488,  ..., 0.1215, 0.0000, 0.0239],
        [0.0000, 0.0197, 0.0450,  ..., 0.1125, 0.0000, 0.0221],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52297.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.4018, 0.0000, 0.2186,  ..., 0.2556, 0.0000, 0.0000],
        [0.3875, 0.0000, 0.2115,  ..., 0.2464, 0.0000, 0.0000],
        [0.3436, 0.0000, 0.1889,  ..., 0.2189, 0.0000, 0.0000],
        ...,
        [0.0072, 0.0283, 0.0176,  ..., 0.0071, 0.0000, 0.0000],
        [0.0072, 0.0283, 0.0176,  ..., 0.0071, 0.0000, 0.0000],
        [0.0072, 0.0283, 0.0176,  ..., 0.0071, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(571188.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10554.5332, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(51.3306, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6368.8906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-528.3444, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8258.4453, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(519.5110, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1427],
        [ 0.1335],
        [ 0.0970],
        ...,
        [-1.5094],
        [-1.5049],
        [-1.5035]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-254740.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0045],
        [1.0065],
        [1.0128],
        ...,
        [0.9999],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367859.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3132],
        [0.2466],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(253.8899, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0045],
        [1.0066],
        [1.0129],
        ...,
        [0.9999],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367870.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3132],
        [0.2466],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(253.8899, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.7947e-03,  1.2782e-03,  3.2424e-03,  ...,  9.2388e-03,
         -1.6072e-03,  1.6987e-03],
        [-1.0616e-02,  2.1880e-03,  5.2405e-03,  ...,  1.3941e-02,
         -2.5109e-03,  2.6444e-03],
        [-2.2515e-02,  5.0213e-03,  1.1463e-02,  ...,  2.8585e-02,
         -5.3254e-03,  5.5894e-03],
        ...,
        [ 0.0000e+00, -3.3966e-04, -3.1062e-04,  ...,  8.7661e-04,
          0.0000e+00,  1.6948e-05],
        [ 0.0000e+00, -3.3966e-04, -3.1062e-04,  ...,  8.7661e-04,
          0.0000e+00,  1.6948e-05],
        [ 0.0000e+00, -3.3966e-04, -3.1062e-04,  ...,  8.7661e-04,
          0.0000e+00,  1.6948e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1898.3811, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.5349, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.2057, device='cuda:0')



h[100].sum tensor(-15.5541, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.2429, device='cuda:0')



h[200].sum tensor(-7.7116, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.8414, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 4.8157e-03, 1.1886e-02,  ..., 3.3688e-02, 0.0000e+00,
         6.1358e-03],
        [0.0000e+00, 1.2024e-02, 2.8156e-02,  ..., 7.2727e-02, 0.0000e+00,
         1.3986e-02],
        [0.0000e+00, 1.5628e-02, 3.6077e-02,  ..., 9.1388e-02, 0.0000e+00,
         1.7736e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.6008e-03, 0.0000e+00,
         6.9617e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.6002e-03, 0.0000e+00,
         6.9606e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.6001e-03, 0.0000e+00,
         6.9603e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55122.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1670, 0.0000, 0.0949,  ..., 0.1100, 0.0000, 0.0000],
        [0.2710, 0.0000, 0.1484,  ..., 0.1752, 0.0000, 0.0000],
        [0.3445, 0.0000, 0.1867,  ..., 0.2209, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0285, 0.0174,  ..., 0.0069, 0.0000, 0.0000],
        [0.0071, 0.0285, 0.0174,  ..., 0.0069, 0.0000, 0.0000],
        [0.0071, 0.0285, 0.0174,  ..., 0.0069, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(586216.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10956.9492, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(62.0000, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6388.7764, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-543.9958, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8544.9609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(548.3400, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1383],
        [ 0.1593],
        [ 0.1545],
        ...,
        [-1.5235],
        [-1.5188],
        [-1.5172]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-264331.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0045],
        [1.0066],
        [1.0129],
        ...,
        [0.9999],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367870.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6680],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(228.5220, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0046],
        [1.0066],
        [1.0130],
        ...,
        [0.9999],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367880.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6680],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(228.5220, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0224e-02,  2.0999e-03,  5.0453e-03,  ...,  1.3492e-02,
         -2.4125e-03,  2.5575e-03],
        [-1.9053e-02,  4.2054e-03,  9.6662e-03,  ...,  2.4370e-02,
         -4.4957e-03,  4.7448e-03],
        [-4.7472e-03,  7.9375e-04,  2.1786e-03,  ...,  6.7437e-03,
         -1.1201e-03,  1.2006e-03],
        ...,
        [ 0.0000e+00, -3.3833e-04, -3.0606e-04,  ...,  8.9470e-04,
          0.0000e+00,  2.4508e-05],
        [ 0.0000e+00, -3.3833e-04, -3.0606e-04,  ...,  8.9470e-04,
          0.0000e+00,  2.4508e-05],
        [ 0.0000e+00, -3.3833e-04, -3.0606e-04,  ...,  8.9470e-04,
          0.0000e+00,  2.4508e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1875.6268, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.4841, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.1868, device='cuda:0')



h[100].sum tensor(-14.0679, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.7190, device='cuda:0')



h[200].sum tensor(-8.8210, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.0595, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0172, 0.0396,  ..., 0.0996, 0.0000, 0.0194],
        [0.0000, 0.0088, 0.0210,  ..., 0.0559, 0.0000, 0.0106],
        [0.0000, 0.0082, 0.0197,  ..., 0.0529, 0.0000, 0.0100],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0001],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0001],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0001]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53059.5820, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2888, 0.0000, 0.1576,  ..., 0.1864, 0.0000, 0.0000],
        [0.2428, 0.0000, 0.1329,  ..., 0.1582, 0.0000, 0.0000],
        [0.2058, 0.0000, 0.1134,  ..., 0.1354, 0.0000, 0.0000],
        ...,
        [0.0073, 0.0285, 0.0174,  ..., 0.0074, 0.0000, 0.0000],
        [0.0073, 0.0285, 0.0174,  ..., 0.0074, 0.0000, 0.0000],
        [0.0073, 0.0285, 0.0174,  ..., 0.0074, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(580997.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10688.2432, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(46.5660, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6441.7056, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-531.6022, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8422.8398, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(527.4472, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1656],
        [ 0.1672],
        [ 0.1690],
        ...,
        [-1.5247],
        [-1.5202],
        [-1.5189]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-264987.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0046],
        [1.0066],
        [1.0130],
        ...,
        [0.9999],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367880.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5347],
        [0.5410],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.6595, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0046],
        [1.0067],
        [1.0131],
        ...,
        [0.9999],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367891.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5347],
        [0.5410],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.6595, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.5135e-02,  5.6689e-03,  1.2864e-02,  ...,  3.1931e-02,
         -5.9160e-03,  6.2837e-03],
        [-2.6044e-02,  5.8860e-03,  1.3340e-02,  ...,  3.3053e-02,
         -6.1300e-03,  6.5091e-03],
        [-1.1711e-02,  2.4624e-03,  5.8325e-03,  ...,  1.5373e-02,
         -2.7565e-03,  2.9547e-03],
        ...,
        [ 0.0000e+00, -3.3505e-04, -3.0222e-04,  ...,  9.2708e-04,
          0.0000e+00,  5.0352e-05],
        [ 0.0000e+00, -3.3505e-04, -3.0222e-04,  ...,  9.2708e-04,
          0.0000e+00,  5.0352e-05],
        [ 0.0000e+00, -3.3505e-04, -3.0222e-04,  ...,  9.2708e-04,
          0.0000e+00,  5.0352e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1951.4977, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.0292, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.8691, device='cuda:0')



h[100].sum tensor(-15.1781, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.1555, device='cuda:0')



h[200].sum tensor(-8.9660, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.3775, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0171, 0.0393,  ..., 0.0990, 0.0000, 0.0194],
        [0.0000, 0.0172, 0.0394,  ..., 0.0995, 0.0000, 0.0195],
        [0.0000, 0.0164, 0.0377,  ..., 0.0954, 0.0000, 0.0186],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55489.8203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2998, 0.0000, 0.1648,  ..., 0.1926, 0.0000, 0.0000],
        [0.3022, 0.0000, 0.1659,  ..., 0.1942, 0.0000, 0.0000],
        [0.2655, 0.0000, 0.1468,  ..., 0.1712, 0.0000, 0.0000],
        ...,
        [0.0080, 0.0283, 0.0176,  ..., 0.0084, 0.0000, 0.0000],
        [0.0079, 0.0283, 0.0176,  ..., 0.0084, 0.0000, 0.0000],
        [0.0079, 0.0283, 0.0176,  ..., 0.0084, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(585137.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11295.1279, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(49.5221, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5842.7061, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-546.2584, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8517.8008, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(552.7122, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1545],
        [ 0.1643],
        [ 0.1709],
        ...,
        [-1.5125],
        [-1.5042],
        [-1.4839]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-209912.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0046],
        [1.0067],
        [1.0131],
        ...,
        [0.9999],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367891.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.5929, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0046],
        [1.0067],
        [1.0132],
        ...,
        [0.9999],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367902.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.5929, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.7492e-03,  1.0546e-03,  2.7092e-03,  ...,  8.0381e-03,
         -1.3499e-03,  1.4920e-03],
        [-5.3753e-03,  9.6514e-04,  2.5132e-03,  ...,  7.5763e-03,
         -1.2621e-03,  1.3991e-03],
        [-1.1124e-02,  2.3411e-03,  5.5270e-03,  ...,  1.4676e-02,
         -2.6119e-03,  2.8263e-03],
        ...,
        [ 0.0000e+00, -3.2131e-04, -3.0464e-04,  ...,  9.3860e-04,
          0.0000e+00,  6.4755e-05],
        [ 0.0000e+00, -3.2131e-04, -3.0464e-04,  ...,  9.3860e-04,
          0.0000e+00,  6.4755e-05],
        [ 0.0000e+00, -3.2131e-04, -3.0464e-04,  ...,  9.3860e-04,
          0.0000e+00,  6.4755e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1842.3927, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.1007, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.2928, device='cuda:0')



h[100].sum tensor(-10.8952, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.7086, device='cuda:0')



h[200].sum tensor(-11.7058, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.6940, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0038, 0.0090,  ..., 0.0265, 0.0000, 0.0048],
        [0.0000, 0.0080, 0.0191,  ..., 0.0515, 0.0000, 0.0099],
        [0.0000, 0.0035, 0.0088,  ..., 0.0266, 0.0000, 0.0049],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49154.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1105, 0.0000, 0.0665,  ..., 0.0752, 0.0000, 0.0000],
        [0.1520, 0.0000, 0.0870,  ..., 0.1019, 0.0000, 0.0000],
        [0.1206, 0.0000, 0.0714,  ..., 0.0818, 0.0000, 0.0000],
        ...,
        [0.0085, 0.0283, 0.0180,  ..., 0.0092, 0.0000, 0.0000],
        [0.0085, 0.0283, 0.0180,  ..., 0.0092, 0.0000, 0.0000],
        [0.0085, 0.0283, 0.0180,  ..., 0.0092, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(564744.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10261.3711, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(12.7558, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6305.6445, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-509.7072, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8121.8076, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(488.8322, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0951],
        [ 0.1326],
        [ 0.0663],
        ...,
        [-1.5084],
        [-1.5041],
        [-1.5028]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-249472.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0046],
        [1.0067],
        [1.0132],
        ...,
        [0.9999],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367902.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.1415, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0047],
        [1.0068],
        [1.0134],
        ...,
        [0.9999],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367913.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.1415, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.0954e-04, -3.0940e-04,  ...,  9.4127e-04,
          0.0000e+00,  7.4129e-05],
        [ 0.0000e+00, -3.0954e-04, -3.0940e-04,  ...,  9.4127e-04,
          0.0000e+00,  7.4129e-05],
        [ 0.0000e+00, -3.0954e-04, -3.0940e-04,  ...,  9.4127e-04,
          0.0000e+00,  7.4129e-05],
        ...,
        [ 0.0000e+00, -3.0954e-04, -3.0940e-04,  ...,  9.4127e-04,
          0.0000e+00,  7.4129e-05],
        [ 0.0000e+00, -3.0954e-04, -3.0940e-04,  ...,  9.4127e-04,
          0.0000e+00,  7.4129e-05],
        [ 0.0000e+00, -3.0954e-04, -3.0940e-04,  ...,  9.4127e-04,
          0.0000e+00,  7.4129e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2067.3477, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.7617, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.3745, device='cuda:0')



h[100].sum tensor(-17.1382, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.8057, device='cuda:0')



h[200].sum tensor(-8.9277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.8298, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0003],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59019.2734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0142, 0.0231, 0.0206,  ..., 0.0135, 0.0000, 0.0000],
        [0.0161, 0.0218, 0.0215,  ..., 0.0148, 0.0000, 0.0000],
        [0.0179, 0.0207, 0.0223,  ..., 0.0159, 0.0000, 0.0000],
        ...,
        [0.0223, 0.0180, 0.0250,  ..., 0.0185, 0.0000, 0.0000],
        [0.0120, 0.0261, 0.0200,  ..., 0.0119, 0.0000, 0.0000],
        [0.0089, 0.0285, 0.0185,  ..., 0.0099, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(600392., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11993.3203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(52.1320, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5707.7676, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-569.8094, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8833.4414, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(593.0023, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9324],
        [-0.8178],
        [-0.7107],
        ...,
        [-1.1927],
        [-1.3646],
        [-1.4583]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-202520.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0047],
        [1.0068],
        [1.0134],
        ...,
        [0.9999],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367913.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.9000, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0047],
        [1.0069],
        [1.0135],
        ...,
        [0.9998],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367923.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.9000, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1548e-02,  2.4691e-03,  5.7544e-03,  ...,  1.5228e-02,
         -2.6980e-03,  2.9388e-03],
        [ 0.0000e+00, -3.0355e-04, -3.0922e-04,  ...,  9.3661e-04,
          0.0000e+00,  6.6072e-05],
        [ 0.0000e+00, -3.0355e-04, -3.0922e-04,  ...,  9.3661e-04,
          0.0000e+00,  6.6072e-05],
        ...,
        [ 0.0000e+00, -3.0355e-04, -3.0922e-04,  ...,  9.3661e-04,
          0.0000e+00,  6.6072e-05],
        [ 0.0000e+00, -3.0355e-04, -3.0922e-04,  ...,  9.3661e-04,
          0.0000e+00,  6.6072e-05],
        [ 0.0000e+00, -3.0355e-04, -3.0922e-04,  ...,  9.3661e-04,
          0.0000e+00,  6.6072e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1916.5271, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.0836, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.3069, device='cuda:0')



h[100].sum tensor(-12.3172, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.2312, device='cuda:0')



h[200].sum tensor(-11.7771, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.4692, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0037, 0.0088,  ..., 0.0260, 0.0000, 0.0047],
        [0.0000, 0.0025, 0.0058,  ..., 0.0181, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0003],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52547.3828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1681, 0.0000, 0.0984,  ..., 0.1112, 0.0000, 0.0000],
        [0.1220, 0.0044, 0.0755,  ..., 0.0818, 0.0000, 0.0000],
        [0.0839, 0.0047, 0.0565,  ..., 0.0575, 0.0000, 0.0000],
        ...,
        [0.0090, 0.0287, 0.0186,  ..., 0.0101, 0.0000, 0.0000],
        [0.0090, 0.0287, 0.0186,  ..., 0.0101, 0.0000, 0.0000],
        [0.0090, 0.0287, 0.0186,  ..., 0.0100, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(581850.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10973.7715, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(21.8948, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6191.9053, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-530.8917, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8491.6211, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(526.7972, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0485],
        [ 0.0510],
        [ 0.0553],
        ...,
        [-1.5193],
        [-1.5152],
        [-1.5140]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-243402.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0047],
        [1.0069],
        [1.0135],
        ...,
        [0.9998],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367923.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(250.8436, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0048],
        [1.0070],
        [1.0136],
        ...,
        [0.9998],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367933.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(250.8436, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.6785e-03,  1.2994e-03,  3.2044e-03,  ...,  9.2030e-03,
         -1.5565e-03,  1.7110e-03],
        [ 0.0000e+00, -3.0595e-04, -3.0561e-04,  ...,  9.2853e-04,
          0.0000e+00,  4.7855e-05],
        [ 0.0000e+00, -3.0595e-04, -3.0561e-04,  ...,  9.2853e-04,
          0.0000e+00,  4.7855e-05],
        ...,
        [ 0.0000e+00, -3.0595e-04, -3.0561e-04,  ...,  9.2853e-04,
          0.0000e+00,  4.7855e-05],
        [ 0.0000e+00, -3.0595e-04, -3.0561e-04,  ...,  9.2853e-04,
          0.0000e+00,  4.7855e-05],
        [ 0.0000e+00, -3.0595e-04, -3.0561e-04,  ...,  9.2853e-04,
          0.0000e+00,  4.7855e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2008.3159, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.0668, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.9633, device='cuda:0')



h[100].sum tensor(-15.1836, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.1800, device='cuda:0')



h[200].sum tensor(-10.5566, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.5073, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0020, 0.0050,  ..., 0.0170, 0.0000, 0.0029],
        [0.0000, 0.0013, 0.0032,  ..., 0.0120, 0.0000, 0.0019],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0002],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58697.4180, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0647, 0.0024, 0.0444,  ..., 0.0463, 0.0000, 0.0000],
        [0.0450, 0.0074, 0.0353,  ..., 0.0334, 0.0000, 0.0000],
        [0.0268, 0.0143, 0.0264,  ..., 0.0216, 0.0000, 0.0000],
        ...,
        [0.0088, 0.0290, 0.0184,  ..., 0.0098, 0.0000, 0.0000],
        [0.0088, 0.0290, 0.0184,  ..., 0.0098, 0.0000, 0.0000],
        [0.0088, 0.0290, 0.0184,  ..., 0.0098, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(616435.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12391.0615, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(52.3801, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5848.9346, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-566.1806, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9122.9004, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(590.7008, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7685],
        [-0.8003],
        [-0.7481],
        ...,
        [-1.5356],
        [-1.5313],
        [-1.5301]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-215010.5469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0048],
        [1.0070],
        [1.0136],
        ...,
        [0.9998],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367933.5000, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 230.0 event: 1150 loss: tensor(446.2690, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6602],
        [0.2854],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.6434, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0048],
        [1.0070],
        [1.0138],
        ...,
        [0.9998],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367943.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6602],
        [0.2854],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.6434, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0958e-02,  2.3194e-03,  5.4690e-03,  ...,  1.4513e-02,
         -2.5474e-03,  2.7471e-03],
        [-1.9028e-02,  4.2609e-03,  9.7149e-03,  ...,  2.4523e-02,
         -4.4235e-03,  4.7588e-03],
        [-6.1457e-03,  1.1616e-03,  2.9373e-03,  ...,  8.5438e-03,
         -1.4287e-03,  1.5475e-03],
        ...,
        [ 0.0000e+00, -3.1697e-04, -2.9616e-04,  ...,  9.2055e-04,
          0.0000e+00,  1.5513e-05],
        [ 0.0000e+00, -3.1697e-04, -2.9616e-04,  ...,  9.2055e-04,
          0.0000e+00,  1.5513e-05],
        [ 0.0000e+00, -3.1697e-04, -2.9616e-04,  ...,  9.2055e-04,
          0.0000e+00,  1.5513e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1997.7563, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.0484, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.7882, device='cuda:0')



h[100].sum tensor(-15.1650, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.1346, device='cuda:0')



h[200].sum tensor(-10.7469, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.2661, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.9606e-02, 4.4469e-02,  ..., 1.1134e-01, 0.0000e+00,
         2.1695e-02],
        [0.0000e+00, 1.3202e-02, 3.0468e-02,  ..., 7.8341e-02, 0.0000e+00,
         1.5062e-02],
        [0.0000e+00, 7.2142e-03, 1.6576e-02,  ..., 4.4199e-02, 0.0000e+00,
         8.1990e-03],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.7849e-03, 0.0000e+00,
         6.3781e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.7840e-03, 0.0000e+00,
         6.3766e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.7839e-03, 0.0000e+00,
         6.3765e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57602.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3560, 0.0000, 0.1933,  ..., 0.2288, 0.0000, 0.0000],
        [0.2985, 0.0000, 0.1634,  ..., 0.1929, 0.0000, 0.0000],
        [0.2018, 0.0000, 0.1143,  ..., 0.1318, 0.0000, 0.0000],
        ...,
        [0.0083, 0.0294, 0.0178,  ..., 0.0092, 0.0000, 0.0000],
        [0.0083, 0.0294, 0.0178,  ..., 0.0092, 0.0000, 0.0000],
        [0.0083, 0.0294, 0.0178,  ..., 0.0092, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(604218.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12047.0547, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(53.1584, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5706.1475, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-557.5175, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8936.1836, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(577.4343, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1142],
        [ 0.1151],
        [ 0.1011],
        ...,
        [-1.5579],
        [-1.5535],
        [-1.5522]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-204116.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0048],
        [1.0070],
        [1.0138],
        ...,
        [0.9998],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367943.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.9091, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0048],
        [1.0070],
        [1.0138],
        ...,
        [0.9998],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367943.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.9091, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.1697e-04, -2.9616e-04,  ...,  9.2055e-04,
          0.0000e+00,  1.5513e-05],
        [-1.0388e-02,  2.1823e-03,  5.1693e-03,  ...,  1.3806e-02,
         -2.4150e-03,  2.6051e-03],
        [-2.0235e-02,  4.5514e-03,  1.0350e-02,  ...,  2.6020e-02,
         -4.7042e-03,  5.0598e-03],
        ...,
        [ 0.0000e+00, -3.1697e-04, -2.9616e-04,  ...,  9.2055e-04,
          0.0000e+00,  1.5513e-05],
        [ 0.0000e+00, -3.1697e-04, -2.9616e-04,  ...,  9.2055e-04,
          0.0000e+00,  1.5513e-05],
        [ 0.0000e+00, -3.1697e-04, -2.9616e-04,  ...,  9.2055e-04,
          0.0000e+00,  1.5513e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2070.1191, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.0659, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.7540, device='cuda:0')



h[100].sum tensor(-17.3390, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.9041, device='cuda:0')



h[200].sum tensor(-9.6055, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-31.3526, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 3.9176e-03, 9.3633e-03,  ..., 2.7169e-02, 0.0000e+00,
         4.7801e-03],
        [0.0000e+00, 7.9388e-03, 1.8558e-02,  ..., 4.9561e-02, 0.0000e+00,
         9.2787e-03],
        [0.0000e+00, 1.2101e-02, 2.8065e-02,  ..., 7.2695e-02, 0.0000e+00,
         1.3926e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.7849e-03, 0.0000e+00,
         6.3781e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.7840e-03, 0.0000e+00,
         6.3766e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.7839e-03, 0.0000e+00,
         6.3765e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60524.1289, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1180, 0.0000, 0.0717,  ..., 0.0791, 0.0000, 0.0000],
        [0.2104, 0.0000, 0.1186,  ..., 0.1374, 0.0000, 0.0000],
        [0.2970, 0.0000, 0.1627,  ..., 0.1919, 0.0000, 0.0000],
        ...,
        [0.0194, 0.0208, 0.0231,  ..., 0.0164, 0.0000, 0.0000],
        [0.0083, 0.0294, 0.0178,  ..., 0.0092, 0.0000, 0.0000],
        [0.0083, 0.0294, 0.0178,  ..., 0.0092, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(615744.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12347.3457, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(63.3462, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5765.9312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-575.1791, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9143.4219, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(606.7709, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1284],
        [ 0.1267],
        [ 0.1195],
        ...,
        [-1.1452],
        [-1.3707],
        [-1.4938]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-209453.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0048],
        [1.0070],
        [1.0138],
        ...,
        [0.9998],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367943.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2751],
        [0.7080],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(305.9062, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0049],
        [1.0071],
        [1.0139],
        ...,
        [0.9998],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367952.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2751],
        [0.7080],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(305.9062, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.9187e-03,  1.0987e-03,  2.8297e-03,  ...,  8.2488e-03,
         -1.3726e-03,  1.4467e-03],
        [-1.5230e-02,  3.3409e-03,  7.7340e-03,  ...,  1.9812e-02,
         -3.5319e-03,  3.7701e-03],
        [-1.8611e-02,  4.1551e-03,  9.5149e-03,  ...,  2.4011e-02,
         -4.3160e-03,  4.6138e-03],
        ...,
        [ 0.0000e+00, -3.2655e-04, -2.8764e-04,  ...,  8.9874e-04,
          0.0000e+00, -3.0163e-05],
        [ 0.0000e+00, -3.2655e-04, -2.8764e-04,  ...,  8.9874e-04,
          0.0000e+00, -3.0163e-05],
        [ 0.0000e+00, -3.2655e-04, -2.8764e-04,  ...,  8.9874e-04,
          0.0000e+00, -3.0163e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2091.7258, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.8054, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.3454, device='cuda:0')



h[100].sum tensor(-18.5857, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.3171, device='cuda:0')



h[200].sum tensor(-8.8818, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-33.5455, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0065, 0.0155,  ..., 0.0422, 0.0000, 0.0077],
        [0.0000, 0.0125, 0.0290,  ..., 0.0748, 0.0000, 0.0142],
        [0.0000, 0.0189, 0.0431,  ..., 0.1079, 0.0000, 0.0208],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61646.4609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2270, 0.0000, 0.1265,  ..., 0.1474, 0.0000, 0.0000],
        [0.2839, 0.0000, 0.1556,  ..., 0.1831, 0.0000, 0.0000],
        [0.3266, 0.0000, 0.1778,  ..., 0.2096, 0.0000, 0.0000],
        ...,
        [0.0076, 0.0299, 0.0172,  ..., 0.0083, 0.0000, 0.0000],
        [0.0076, 0.0299, 0.0172,  ..., 0.0083, 0.0000, 0.0000],
        [0.0076, 0.0299, 0.0172,  ..., 0.0083, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(623163.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12160.7090, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(71.7082, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6149.6631, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-580.9535, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9212.8164, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(614.9023, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0209],
        [ 0.0280],
        [ 0.0358],
        ...,
        [-1.5876],
        [-1.5829],
        [-1.5815]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-254885.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0049],
        [1.0071],
        [1.0139],
        ...,
        [0.9998],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367952.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.0836, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0049],
        [1.0072],
        [1.0140],
        ...,
        [0.9998],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367962.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.0836, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.2701e-04, -2.8054e-04,  ...,  8.8074e-04,
          0.0000e+00, -7.0329e-05],
        [ 0.0000e+00, -3.2701e-04, -2.8054e-04,  ...,  8.8074e-04,
          0.0000e+00, -7.0329e-05],
        [ 0.0000e+00, -3.2701e-04, -2.8054e-04,  ...,  8.8074e-04,
          0.0000e+00, -7.0329e-05],
        ...,
        [ 0.0000e+00, -3.2701e-04, -2.8054e-04,  ...,  8.8074e-04,
          0.0000e+00, -7.0329e-05],
        [ 0.0000e+00, -3.2701e-04, -2.8054e-04,  ...,  8.8074e-04,
          0.0000e+00, -7.0329e-05],
        [ 0.0000e+00, -3.2701e-04, -2.8054e-04,  ...,  8.8074e-04,
          0.0000e+00, -7.0329e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1857.0038, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.6602, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.9236, device='cuda:0')



h[100].sum tensor(-11.9948, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.1318, device='cuda:0')



h[200].sum tensor(-12.3077, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.9410, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50895.4453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0064, 0.0297, 0.0163,  ..., 0.0073, 0.0000, 0.0000],
        [0.0065, 0.0298, 0.0163,  ..., 0.0074, 0.0000, 0.0000],
        [0.0067, 0.0299, 0.0165,  ..., 0.0075, 0.0000, 0.0000],
        ...,
        [0.0072, 0.0304, 0.0169,  ..., 0.0077, 0.0000, 0.0000],
        [0.0072, 0.0304, 0.0169,  ..., 0.0077, 0.0000, 0.0000],
        [0.0072, 0.0304, 0.0169,  ..., 0.0077, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(583117.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10531.1133, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(33.5461, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6517.9146, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-516.8451, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8428.6729, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(503.6014, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8561],
        [-1.8713],
        [-1.8761],
        ...,
        [-1.6048],
        [-1.6019],
        [-1.6008]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-267557.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0049],
        [1.0072],
        [1.0140],
        ...,
        [0.9998],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367962.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.8580, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0050],
        [1.0072],
        [1.0141],
        ...,
        [0.9998],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367973.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.8580, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.2870e-04, -2.7149e-04,  ...,  8.9009e-04,
          0.0000e+00, -9.2154e-05],
        [ 0.0000e+00, -3.2870e-04, -2.7149e-04,  ...,  8.9009e-04,
          0.0000e+00, -9.2154e-05],
        [ 0.0000e+00, -3.2870e-04, -2.7149e-04,  ...,  8.9009e-04,
          0.0000e+00, -9.2154e-05],
        ...,
        [ 0.0000e+00, -3.2870e-04, -2.7149e-04,  ...,  8.9009e-04,
          0.0000e+00, -9.2154e-05],
        [ 0.0000e+00, -3.2870e-04, -2.7149e-04,  ...,  8.9009e-04,
          0.0000e+00, -9.2154e-05],
        [ 0.0000e+00, -3.2870e-04, -2.7149e-04,  ...,  8.9009e-04,
          0.0000e+00, -9.2154e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1855.9421, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.1771, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.2690, device='cuda:0')



h[100].sum tensor(-11.6428, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.9619, device='cuda:0')



h[200].sum tensor(-12.6838, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.0390, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50195.5898, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0063, 0.0299, 0.0163,  ..., 0.0075, 0.0000, 0.0000],
        [0.0064, 0.0300, 0.0163,  ..., 0.0075, 0.0000, 0.0000],
        [0.0066, 0.0300, 0.0165,  ..., 0.0076, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0306, 0.0169,  ..., 0.0079, 0.0000, 0.0000],
        [0.0071, 0.0306, 0.0169,  ..., 0.0079, 0.0000, 0.0000],
        [0.0071, 0.0306, 0.0169,  ..., 0.0079, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(582098.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10262.1211, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(26.7472, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6711.1641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-512.7985, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8366.3730, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(495.0884, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6513],
        [-1.7548],
        [-1.8370],
        ...,
        [-1.6191],
        [-1.6143],
        [-1.6128]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-288101.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0050],
        [1.0072],
        [1.0141],
        ...,
        [0.9998],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367973.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8218, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0050],
        [1.0073],
        [1.0142],
        ...,
        [0.9998],
        [0.9987],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367983.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8218, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0003, -0.0003,  ...,  0.0009,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0009,  0.0000, -0.0001],
        [-0.0063,  0.0012,  0.0030,  ...,  0.0087, -0.0014,  0.0015],
        ...,
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0009,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0009,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0009,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1953.1504, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.1563, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.1311, device='cuda:0')



h[100].sum tensor(-13.7820, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.7046, device='cuda:0')



h[200].sum tensor(-11.8128, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.9828, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0044,  ..., 0.0153, 0.0000, 0.0021],
        [0.0000, 0.0012, 0.0031,  ..., 0.0115, 0.0000, 0.0015],
        [0.0000, 0.0009, 0.0025,  ..., 0.0101, 0.0000, 0.0012],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53814.0352, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0689, 0.0013, 0.0449,  ..., 0.0490, 0.0000, 0.0000],
        [0.0511, 0.0021, 0.0369,  ..., 0.0373, 0.0000, 0.0000],
        [0.0445, 0.0044, 0.0339,  ..., 0.0329, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0305, 0.0170,  ..., 0.0084, 0.0000, 0.0000],
        [0.0071, 0.0305, 0.0170,  ..., 0.0084, 0.0000, 0.0000],
        [0.0071, 0.0305, 0.0170,  ..., 0.0084, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(595453.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10796.7256, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(38.6610, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6491.0376, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-532.5981, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8553.4551, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(532.1276, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0979],
        [-0.3386],
        [-0.6105],
        ...,
        [-1.6183],
        [-1.6135],
        [-1.6121]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-271794.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0050],
        [1.0073],
        [1.0142],
        ...,
        [0.9998],
        [0.9987],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367983.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.4836, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0051],
        [1.0074],
        [1.0143],
        ...,
        [0.9998],
        [0.9987],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367994.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.4836, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0048,  0.0008,  0.0023,  ...,  0.0069, -0.0011,  0.0011],
        [-0.0048,  0.0008,  0.0023,  ...,  0.0069, -0.0011,  0.0011],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0009,  0.0000, -0.0001],
        ...,
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0009,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0009,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0009,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2135.4590, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.0333, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.0731, device='cuda:0')



h[100].sum tensor(-18.0038, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.2464, device='cuda:0')



h[200].sum tensor(-9.8189, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-33.1701, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0032, 0.0084,  ..., 0.0255, 0.0000, 0.0040],
        [0.0000, 0.0022, 0.0062,  ..., 0.0201, 0.0000, 0.0030],
        [0.0000, 0.0015, 0.0041,  ..., 0.0147, 0.0000, 0.0020],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63278.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[9.9505e-02, 0.0000e+00, 5.9694e-02,  ..., 6.9433e-02, 0.0000e+00,
         0.0000e+00],
        [8.8268e-02, 4.8897e-05, 5.3968e-02,  ..., 6.2342e-02, 0.0000e+00,
         0.0000e+00],
        [6.5791e-02, 4.8247e-03, 4.3559e-02,  ..., 4.7545e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.3598e-03, 3.0364e-02, 1.7249e-02,  ..., 9.1360e-03, 0.0000e+00,
         0.0000e+00],
        [7.3542e-03, 3.0356e-02, 1.7243e-02,  ..., 9.1323e-03, 0.0000e+00,
         0.0000e+00],
        [7.3513e-03, 3.0356e-02, 1.7242e-02,  ..., 9.1313e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(646012.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12602.8828, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(70.8569, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6175.1797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-589.2512, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9428.7637, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(630.9622, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0231],
        [-0.0357],
        [-0.2581],
        ...,
        [-1.6096],
        [-1.6050],
        [-1.6036]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-247971.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0051],
        [1.0074],
        [1.0143],
        ...,
        [0.9998],
        [0.9987],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367994.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(186.3765, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0051],
        [1.0074],
        [1.0144],
        ...,
        [0.9998],
        [0.9987],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368004.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(186.3765, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0003, -0.0002,  ...,  0.0010,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0002,  ...,  0.0010,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0002,  ...,  0.0010,  0.0000, -0.0001],
        ...,
        [ 0.0000, -0.0003, -0.0002,  ...,  0.0010,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0002,  ...,  0.0010,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0002,  ...,  0.0010,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1944.9619, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.7077, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.8327, device='cuda:0')



h[100].sum tensor(-11.2928, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.8487, device='cuda:0')



h[200].sum tensor(-13.6153, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-20.4379, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52057.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0086, 0.0285, 0.0175,  ..., 0.0107, 0.0000, 0.0000],
        [0.0076, 0.0293, 0.0173,  ..., 0.0099, 0.0000, 0.0000],
        [0.0073, 0.0298, 0.0173,  ..., 0.0097, 0.0000, 0.0000],
        ...,
        [0.0078, 0.0303, 0.0178,  ..., 0.0100, 0.0000, 0.0000],
        [0.0078, 0.0303, 0.0178,  ..., 0.0100, 0.0000, 0.0000],
        [0.0078, 0.0303, 0.0178,  ..., 0.0100, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(586254.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10778.8574, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(22.5614, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6041.1226, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-523.8654, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8407.4570, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(521.9844, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1372],
        [-1.3688],
        [-1.5486],
        ...,
        [-1.5926],
        [-1.5879],
        [-1.5864]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-225221.4219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0051],
        [1.0074],
        [1.0144],
        ...,
        [0.9998],
        [0.9987],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368004.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.7712, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0052],
        [1.0075],
        [1.0145],
        ...,
        [0.9998],
        [0.9987],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368014.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.7712, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0003, -0.0002,  ...,  0.0010,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0002,  ...,  0.0010,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0002,  ...,  0.0010,  0.0000, -0.0001],
        ...,
        [ 0.0000, -0.0003, -0.0002,  ...,  0.0010,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0002,  ...,  0.0010,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0002,  ...,  0.0010,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2035.9358, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.2087, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.3313, device='cuda:0')



h[100].sum tensor(-13.0861, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.4970, device='cuda:0')



h[200].sum tensor(-12.8606, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.8806, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54505.8203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0086, 0.0290, 0.0183,  ..., 0.0113, 0.0000, 0.0000],
        [0.0117, 0.0269, 0.0195,  ..., 0.0134, 0.0000, 0.0000],
        [0.0207, 0.0202, 0.0236,  ..., 0.0194, 0.0000, 0.0000],
        ...,
        [0.0082, 0.0306, 0.0187,  ..., 0.0109, 0.0000, 0.0000],
        [0.0082, 0.0306, 0.0187,  ..., 0.0109, 0.0000, 0.0000],
        [0.0082, 0.0306, 0.0187,  ..., 0.0109, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(592613.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11272.0430, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(28.9903, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5766.0674, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-542.1722, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8618.1377, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(553.7928, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1695],
        [-0.9054],
        [-0.5844],
        ...,
        [-1.5960],
        [-1.5917],
        [-1.5904]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-201476.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0052],
        [1.0075],
        [1.0145],
        ...,
        [0.9998],
        [0.9987],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368014.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(428.8805, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0052],
        [1.0076],
        [1.0145],
        ...,
        [0.9998],
        [0.9987],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368024.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(428.8805, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0003, -0.0003,  ...,  0.0010,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0010,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0010,  0.0000, -0.0001],
        ...,
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0010,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0010,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0003,  ...,  0.0010,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2507.4807, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-36.4924, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.1323, device='cuda:0')



h[100].sum tensor(-26.2168, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-8.8565, device='cuda:0')



h[200].sum tensor(-5.9923, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-47.0307, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(79066.9453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0176, 0.0227, 0.0237,  ..., 0.0175, 0.0000, 0.0000],
        [0.0096, 0.0290, 0.0197,  ..., 0.0123, 0.0000, 0.0000],
        [0.0147, 0.0254, 0.0222,  ..., 0.0156, 0.0000, 0.0000],
        ...,
        [0.0085, 0.0312, 0.0195,  ..., 0.0116, 0.0000, 0.0000],
        [0.0085, 0.0312, 0.0195,  ..., 0.0116, 0.0000, 0.0000],
        [0.0085, 0.0312, 0.0195,  ..., 0.0116, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(715675.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(15534.1191, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(126.9253, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5464.6982, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-692.8753, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10790.2412, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(809.4980, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5680],
        [-0.9102],
        [-1.0700],
        ...,
        [-1.6043],
        [-1.6000],
        [-1.5987]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-192845.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0052],
        [1.0076],
        [1.0145],
        ...,
        [0.9998],
        [0.9987],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368024.3125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 240.0 event: 1200 loss: tensor(464.0639, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(291.8463, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0053],
        [1.0076],
        [1.0146],
        ...,
        [0.9998],
        [0.9987],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368033.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(291.8463, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0079,  0.0017,  0.0039,  ...,  0.0109, -0.0018,  0.0019],
        [ 0.0000, -0.0002, -0.0003,  ...,  0.0009,  0.0000, -0.0001],
        [ 0.0000, -0.0002, -0.0003,  ...,  0.0009,  0.0000, -0.0001],
        ...,
        [ 0.0000, -0.0002, -0.0003,  ...,  0.0009,  0.0000, -0.0001],
        [ 0.0000, -0.0002, -0.0003,  ...,  0.0009,  0.0000, -0.0001],
        [ 0.0000, -0.0002, -0.0003,  ...,  0.0009,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2203.1084, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.5990, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.2265, device='cuda:0')



h[100].sum tensor(-17.6660, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.0267, device='cuda:0')



h[200].sum tensor(-10.5558, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-32.0037, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0044, 0.0101,  ..., 0.0289, 0.0000, 0.0048],
        [0.0000, 0.0017, 0.0040,  ..., 0.0138, 0.0000, 0.0019],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63111.5078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1448, 0.0000, 0.0877,  ..., 0.0995, 0.0000, 0.0000],
        [0.0687, 0.0092, 0.0495,  ..., 0.0507, 0.0000, 0.0000],
        [0.0248, 0.0188, 0.0278,  ..., 0.0222, 0.0000, 0.0000],
        ...,
        [0.0083, 0.0321, 0.0199,  ..., 0.0116, 0.0000, 0.0000],
        [0.0083, 0.0320, 0.0199,  ..., 0.0116, 0.0000, 0.0000],
        [0.0083, 0.0320, 0.0199,  ..., 0.0116, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(643639.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12674.6924, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(58.7935, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6183.8560, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-598.4611, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9586.7207, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(643.1680, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0599],
        [-0.3183],
        [-0.6378],
        ...,
        [-1.6277],
        [-1.6232],
        [-1.6220]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-249730.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0053],
        [1.0076],
        [1.0146],
        ...,
        [0.9998],
        [0.9987],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368033.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(357.3451, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0053],
        [1.0077],
        [1.0146],
        ...,
        [0.9998],
        [0.9987],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368042.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(357.3451, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0002, -0.0002,  ...,  0.0009,  0.0000, -0.0001],
        [ 0.0000, -0.0002, -0.0002,  ...,  0.0009,  0.0000, -0.0001],
        [-0.0070,  0.0015,  0.0035,  ...,  0.0098, -0.0016,  0.0017],
        ...,
        [ 0.0000, -0.0002, -0.0002,  ...,  0.0009,  0.0000, -0.0001],
        [ 0.0000, -0.0002, -0.0002,  ...,  0.0009,  0.0000, -0.0001],
        [ 0.0000, -0.0002, -0.0002,  ...,  0.0009,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2322.2871, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.6808, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.4392, device='cuda:0')



h[100].sum tensor(-21.3079, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-7.3793, device='cuda:0')



h[200].sum tensor(-8.5724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-39.1862, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0015, 0.0035,  ..., 0.0126, 0.0000, 0.0017],
        [0.0000, 0.0012, 0.0028,  ..., 0.0110, 0.0000, 0.0014],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67590.5547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0162, 0.0250, 0.0235,  ..., 0.0169, 0.0000, 0.0000],
        [0.0372, 0.0134, 0.0335,  ..., 0.0308, 0.0000, 0.0000],
        [0.0529, 0.0067, 0.0405,  ..., 0.0415, 0.0000, 0.0000],
        ...,
        [0.0079, 0.0328, 0.0200,  ..., 0.0114, 0.0000, 0.0000],
        [0.0079, 0.0328, 0.0199,  ..., 0.0114, 0.0000, 0.0000],
        [0.0079, 0.0328, 0.0199,  ..., 0.0114, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(658129.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(13496.8135, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(82.1944, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5877.9355, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-624.9880, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9931.8301, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(688.4415, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2356],
        [-0.8635],
        [-0.4643],
        ...,
        [-1.6448],
        [-1.6402],
        [-1.6389]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-223711.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0053],
        [1.0077],
        [1.0146],
        ...,
        [0.9998],
        [0.9987],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368042.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4673],
        [0.4421],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.8761, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0054],
        [1.0078],
        [1.0147],
        ...,
        [0.9998],
        [0.9987],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368051., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4673],
        [0.4421],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.8761, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0099,  0.0022,  0.0050,  ...,  0.0134, -0.0022,  0.0024],
        [-0.0176,  0.0040,  0.0091,  ...,  0.0230, -0.0040,  0.0043],
        [-0.0318,  0.0075,  0.0167,  ...,  0.0408, -0.0072,  0.0079],
        ...,
        [ 0.0000, -0.0003, -0.0002,  ...,  0.0009,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0002,  ...,  0.0009,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0002,  ...,  0.0009,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2077.5249, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.2552, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.4088, device='cuda:0')



h[100].sum tensor(-14.5360, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.0361, device='cuda:0')



h[200].sum tensor(-12.1941, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.7433, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0074, 0.0168,  ..., 0.0445, 0.0000, 0.0080],
        [0.0000, 0.0176, 0.0397,  ..., 0.0998, 0.0000, 0.0189],
        [0.0000, 0.0219, 0.0492,  ..., 0.1222, 0.0000, 0.0234],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56475.0547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2062, 0.0000, 0.1185,  ..., 0.1385, 0.0000, 0.0000],
        [0.3466, 0.0000, 0.1909,  ..., 0.2270, 0.0000, 0.0000],
        [0.4521, 0.0000, 0.2460,  ..., 0.2931, 0.0000, 0.0000],
        ...,
        [0.0252, 0.0197, 0.0285,  ..., 0.0226, 0.0000, 0.0000],
        [0.0076, 0.0334, 0.0198,  ..., 0.0112, 0.0000, 0.0000],
        [0.0076, 0.0334, 0.0198,  ..., 0.0112, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(610422.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11431.9785, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(37.8298, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6470.2324, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-557.8176, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8987.2363, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(572.2533, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1353],
        [ 0.1333],
        [ 0.1235],
        ...,
        [-0.9008],
        [-1.2850],
        [-1.5214]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-262496.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0054],
        [1.0078],
        [1.0147],
        ...,
        [0.9998],
        [0.9987],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368051., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4133],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.4471, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0055],
        [1.0079],
        [1.0148],
        ...,
        [0.9998],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368059.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4133],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.4471, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0088,  0.0019,  0.0044,  ...,  0.0120, -0.0020,  0.0021],
        [ 0.0000, -0.0003, -0.0002,  ...,  0.0009,  0.0000, -0.0001],
        [-0.0088,  0.0019,  0.0044,  ...,  0.0120, -0.0020,  0.0021],
        ...,
        [ 0.0000, -0.0003, -0.0002,  ...,  0.0009,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0002,  ...,  0.0009,  0.0000, -0.0001],
        [ 0.0000, -0.0003, -0.0002,  ...,  0.0009,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2148.1902, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.2134, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.3989, device='cuda:0')



h[100].sum tensor(-16.6529, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.8120, device='cuda:0')



h[200].sum tensor(-11.2016, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.8633, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0015, 0.0036,  ..., 0.0127, 0.0000, 0.0017],
        [0.0000, 0.0067, 0.0162,  ..., 0.0441, 0.0000, 0.0077],
        [0.0000, 0.0015, 0.0036,  ..., 0.0128, 0.0000, 0.0017],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60985.4609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0534, 0.0054, 0.0412,  ..., 0.0409, 0.0000, 0.0000],
        [0.0960, 0.0000, 0.0614,  ..., 0.0689, 0.0000, 0.0000],
        [0.0824, 0.0000, 0.0544,  ..., 0.0603, 0.0000, 0.0000],
        ...,
        [0.0072, 0.0338, 0.0194,  ..., 0.0108, 0.0000, 0.0000],
        [0.0072, 0.0338, 0.0194,  ..., 0.0108, 0.0000, 0.0000],
        [0.0072, 0.0338, 0.0194,  ..., 0.0108, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(635597.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12353.1660, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(60.7079, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6279.1890, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-582.9218, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9375.5742, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(615.5625, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5026],
        [-0.1349],
        [ 0.0788],
        ...,
        [-1.6839],
        [-1.6789],
        [-1.6775]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-238634.7031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0055],
        [1.0079],
        [1.0148],
        ...,
        [0.9998],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368059.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2544],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.3783, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0055],
        [1.0079],
        [1.0148],
        ...,
        [0.9997],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368068.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2544],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.3783, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.0927e-04, -2.1160e-04,  ...,  9.3026e-04,
          0.0000e+00, -9.5463e-05],
        [-5.4037e-03,  1.0044e-03,  2.6688e-03,  ...,  7.7378e-03,
         -1.2158e-03,  1.2790e-03],
        [-5.1319e-03,  9.3832e-04,  2.5239e-03,  ...,  7.3954e-03,
         -1.1547e-03,  1.2099e-03],
        ...,
        [ 0.0000e+00, -3.0927e-04, -2.1160e-04,  ...,  9.3026e-04,
          0.0000e+00, -9.5463e-05],
        [ 0.0000e+00, -3.0927e-04, -2.1160e-04,  ...,  9.3026e-04,
          0.0000e+00, -9.5463e-05],
        [ 0.0000e+00, -3.0927e-04, -2.1160e-04,  ...,  9.3026e-04,
          0.0000e+00, -9.5463e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2014.9185, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.6808, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.9021, device='cuda:0')



h[100].sum tensor(-12.6793, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.3857, device='cuda:0')



h[200].sum tensor(-13.5828, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.2892, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0010, 0.0027,  ..., 0.0106, 0.0000, 0.0013],
        [0.0000, 0.0037, 0.0090,  ..., 0.0261, 0.0000, 0.0043],
        [0.0000, 0.0078, 0.0189,  ..., 0.0504, 0.0000, 0.0090],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53852.7422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0585, 0.0094, 0.0423,  ..., 0.0446, 0.0000, 0.0000],
        [0.1060, 0.0002, 0.0653,  ..., 0.0752, 0.0000, 0.0000],
        [0.1474, 0.0000, 0.0854,  ..., 0.1017, 0.0000, 0.0000],
        ...,
        [0.0069, 0.0337, 0.0190,  ..., 0.0108, 0.0000, 0.0000],
        [0.0069, 0.0337, 0.0190,  ..., 0.0108, 0.0000, 0.0000],
        [0.0069, 0.0337, 0.0190,  ..., 0.0108, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(601019.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10953.2656, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(30.6906, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6453.5947, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-538.4273, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8685.8291, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(538.2360, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2364],
        [ 0.0161],
        [ 0.1250],
        ...,
        [-1.6907],
        [-1.6857],
        [-1.6842]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-257327.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0055],
        [1.0079],
        [1.0148],
        ...,
        [0.9997],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368068.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.4900, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0056],
        [1.0080],
        [1.0149],
        ...,
        [0.9997],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368078.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.4900, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.2976e-04, -1.9646e-04,  ...,  9.5935e-04,
          0.0000e+00, -7.9654e-05],
        [ 0.0000e+00, -3.2976e-04, -1.9646e-04,  ...,  9.5935e-04,
          0.0000e+00, -7.9654e-05],
        [ 0.0000e+00, -3.2976e-04, -1.9646e-04,  ...,  9.5935e-04,
          0.0000e+00, -7.9654e-05],
        ...,
        [ 0.0000e+00, -3.2976e-04, -1.9646e-04,  ...,  9.5935e-04,
          0.0000e+00, -7.9654e-05],
        [ 0.0000e+00, -3.2976e-04, -1.9646e-04,  ...,  9.5935e-04,
          0.0000e+00, -7.9654e-05],
        [ 0.0000e+00, -3.2976e-04, -1.9646e-04,  ...,  9.5935e-04,
          0.0000e+00, -7.9654e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2192.5178, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.9400, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.0390, device='cuda:0')



h[100].sum tensor(-17.1616, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.9781, device='cuda:0')



h[200].sum tensor(-11.5107, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-31.7453, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59578.0547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0060, 0.0328, 0.0181,  ..., 0.0106, 0.0000, 0.0000],
        [0.0061, 0.0329, 0.0182,  ..., 0.0106, 0.0000, 0.0000],
        [0.0063, 0.0330, 0.0184,  ..., 0.0107, 0.0000, 0.0000],
        ...,
        [0.0069, 0.0336, 0.0189,  ..., 0.0111, 0.0000, 0.0000],
        [0.0069, 0.0336, 0.0189,  ..., 0.0111, 0.0000, 0.0000],
        [0.0069, 0.0336, 0.0189,  ..., 0.0111, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(619636.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11609.5195, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(50.0914, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6190.8013, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-573.5299, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8937.2783, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(594.6697, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5082],
        [-1.7022],
        [-1.8381],
        ...,
        [-1.6937],
        [-1.6888],
        [-1.6873]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-241347.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0056],
        [1.0080],
        [1.0149],
        ...,
        [0.9997],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368078.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(348.9005, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0056],
        [1.0081],
        [1.0150],
        ...,
        [0.9997],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368087.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(348.9005, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.3581e-04, -1.8586e-04,  ...,  9.8241e-04,
          0.0000e+00, -6.8361e-05],
        [ 0.0000e+00, -3.3581e-04, -1.8586e-04,  ...,  9.8241e-04,
          0.0000e+00, -6.8361e-05],
        [ 0.0000e+00, -3.3581e-04, -1.8586e-04,  ...,  9.8241e-04,
          0.0000e+00, -6.8361e-05],
        ...,
        [ 0.0000e+00, -3.3581e-04, -1.8586e-04,  ...,  9.8241e-04,
          0.0000e+00, -6.8361e-05],
        [ 0.0000e+00, -3.3581e-04, -1.8586e-04,  ...,  9.8241e-04,
          0.0000e+00, -6.8361e-05],
        [ 0.0000e+00, -3.3581e-04, -1.8586e-04,  ...,  9.8241e-04,
          0.0000e+00, -6.8361e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2344.6614, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.1269, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.7671, device='cuda:0')



h[100].sum tensor(-20.8724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-7.2049, device='cuda:0')



h[200].sum tensor(-9.7280, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-38.2602, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68750.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0296, 0.0154, 0.0291,  ..., 0.0260, 0.0000, 0.0000],
        [0.0254, 0.0183, 0.0272,  ..., 0.0232, 0.0000, 0.0000],
        [0.0719, 0.0126, 0.0507,  ..., 0.0521, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0336, 0.0190,  ..., 0.0114, 0.0000, 0.0000],
        [0.0071, 0.0336, 0.0190,  ..., 0.0114, 0.0000, 0.0000],
        [0.0071, 0.0336, 0.0190,  ..., 0.0114, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(673449.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(13688.7197, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(84.1850, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5714.4287, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-631.0557, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9845.0303, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(688.9960, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 3.6116e-04],
        [ 4.4940e-02],
        [ 9.8525e-02],
        ...,
        [-1.6965e+00],
        [-1.6917e+00],
        [-1.6902e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-213456.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0056],
        [1.0081],
        [1.0150],
        ...,
        [0.9997],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368087.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6138],
        [0.8994],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(335.8342, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0057],
        [1.0082],
        [1.0150],
        ...,
        [0.9997],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368096.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6138],
        [0.8994],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(335.8342, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2996e-02,  2.8516e-03,  6.7515e-03,  ...,  1.7411e-02,
         -2.9018e-03,  3.2672e-03],
        [-1.9044e-02,  4.3262e-03,  9.9842e-03,  ...,  2.5058e-02,
         -4.2523e-03,  4.8136e-03],
        [-1.2996e-02,  2.8516e-03,  6.7515e-03,  ...,  1.7411e-02,
         -2.9018e-03,  3.2672e-03],
        ...,
        [ 0.0000e+00, -3.1691e-04, -1.9460e-04,  ...,  9.8185e-04,
          0.0000e+00, -5.5718e-05],
        [ 0.0000e+00, -3.1691e-04, -1.9460e-04,  ...,  9.8185e-04,
          0.0000e+00, -5.5718e-05],
        [ 0.0000e+00, -3.1691e-04, -1.9460e-04,  ...,  9.8185e-04,
          0.0000e+00, -5.5718e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2327.5859, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.8112, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.7273, device='cuda:0')



h[100].sum tensor(-19.9223, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.9351, device='cuda:0')



h[200].sum tensor(-10.1548, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-36.8273, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0158, 0.0366,  ..., 0.0923, 0.0000, 0.0177],
        [0.0000, 0.0170, 0.0393,  ..., 0.0987, 0.0000, 0.0189],
        [0.0000, 0.0301, 0.0679,  ..., 0.1665, 0.0000, 0.0326],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65785.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3726, 0.0000, 0.2017,  ..., 0.2414, 0.0000, 0.0000],
        [0.4369, 0.0000, 0.2349,  ..., 0.2811, 0.0000, 0.0000],
        [0.5674, 0.0000, 0.3025,  ..., 0.3618, 0.0000, 0.0000],
        ...,
        [0.0074, 0.0339, 0.0199,  ..., 0.0121, 0.0000, 0.0000],
        [0.0074, 0.0339, 0.0199,  ..., 0.0121, 0.0000, 0.0000],
        [0.0128, 0.0298, 0.0224,  ..., 0.0157, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(650223.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12818.5127, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(67.9229, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5878.3428, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-621.5648, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9499.0645, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(666.9738, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1852],
        [ 0.1799],
        [ 0.1749],
        ...,
        [-1.6580],
        [-1.5654],
        [-1.4055]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-235985.1406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0057],
        [1.0082],
        [1.0150],
        ...,
        [0.9997],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368096.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(277.2692, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0057],
        [1.0082],
        [1.0151],
        ...,
        [0.9997],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368105.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(277.2692, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.8780e-04, -2.0443e-04,  ...,  9.7742e-04,
          0.0000e+00, -4.8314e-05],
        [ 0.0000e+00, -2.8780e-04, -2.0443e-04,  ...,  9.7742e-04,
          0.0000e+00, -4.8314e-05],
        [ 0.0000e+00, -2.8780e-04, -2.0443e-04,  ...,  9.7742e-04,
          0.0000e+00, -4.8314e-05],
        ...,
        [ 0.0000e+00, -2.8780e-04, -2.0443e-04,  ...,  9.7742e-04,
          0.0000e+00, -4.8314e-05],
        [ 0.0000e+00, -2.8780e-04, -2.0443e-04,  ...,  9.7742e-04,
          0.0000e+00, -4.8314e-05],
        [ 0.0000e+00, -2.8780e-04, -2.0443e-04,  ...,  9.7742e-04,
          0.0000e+00, -4.8314e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2225.3582, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.1636, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.0664, device='cuda:0')



h[100].sum tensor(-16.5870, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.7257, device='cuda:0')



h[200].sum tensor(-11.6210, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.4051, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61014.1133, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0158, 0.0266, 0.0242,  ..., 0.0182, 0.0000, 0.0000],
        [0.0084, 0.0323, 0.0207,  ..., 0.0134, 0.0000, 0.0000],
        [0.0072, 0.0336, 0.0202,  ..., 0.0125, 0.0000, 0.0000],
        ...,
        [0.0077, 0.0342, 0.0208,  ..., 0.0129, 0.0000, 0.0000],
        [0.0077, 0.0342, 0.0208,  ..., 0.0129, 0.0000, 0.0000],
        [0.0077, 0.0342, 0.0208,  ..., 0.0129, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(630541.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12326.4434, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(47.1863, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5774.1816, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-601.9094, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9278.7754, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(627.1339, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8459],
        [-1.1973],
        [-1.4606],
        ...,
        [-1.6978],
        [-1.6929],
        [-1.6914]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-224707.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0057],
        [1.0082],
        [1.0151],
        ...,
        [0.9997],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368105.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6738],
        [0.4446],
        [0.3091],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(190.2925, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0057],
        [1.0082],
        [1.0151],
        ...,
        [0.9997],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368105.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6738],
        [0.4446],
        [0.3091],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(190.2925, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.8397e-02,  6.6533e-03,  1.4981e-02,  ...,  3.6909e-02,
         -6.3244e-03,  7.2190e-03],
        [-3.1217e-02,  7.3429e-03,  1.6490e-02,  ...,  4.0478e-02,
         -6.9526e-03,  7.9409e-03],
        [-1.9877e-02,  4.5709e-03,  1.0425e-02,  ...,  2.6129e-02,
         -4.4270e-03,  5.0387e-03],
        ...,
        [ 0.0000e+00, -2.8780e-04, -2.0443e-04,  ...,  9.7742e-04,
          0.0000e+00, -4.8314e-05],
        [ 0.0000e+00, -2.8780e-04, -2.0443e-04,  ...,  9.7742e-04,
          0.0000e+00, -4.8314e-05],
        [ 0.0000e+00, -2.8780e-04, -2.0443e-04,  ...,  9.7742e-04,
          0.0000e+00, -4.8314e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2028.6198, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.6756, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.1444, device='cuda:0')



h[100].sum tensor(-11.2250, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.9296, device='cuda:0')



h[200].sum tensor(-14.4922, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-20.8673, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0311, 0.0698,  ..., 0.1711, 0.0000, 0.0336],
        [0.0000, 0.0259, 0.0583,  ..., 0.1439, 0.0000, 0.0281],
        [0.0000, 0.0194, 0.0443,  ..., 0.1106, 0.0000, 0.0214],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51557.9141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.5714, 0.0000, 0.3077,  ..., 0.3667, 0.0000, 0.0000],
        [0.5216, 0.0000, 0.2816,  ..., 0.3360, 0.0000, 0.0000],
        [0.4371, 0.0000, 0.2374,  ..., 0.2835, 0.0000, 0.0000],
        ...,
        [0.0077, 0.0342, 0.0208,  ..., 0.0129, 0.0000, 0.0000],
        [0.0077, 0.0342, 0.0208,  ..., 0.0129, 0.0000, 0.0000],
        [0.0077, 0.0342, 0.0208,  ..., 0.0129, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(592359.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10701.3711, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(6.3239, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6183.4805, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-545.6536, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8578.1094, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(528.9949, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1398],
        [ 0.1394],
        [ 0.1426],
        ...,
        [-1.7041],
        [-1.6992],
        [-1.6978]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-252535.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0057],
        [1.0082],
        [1.0151],
        ...,
        [0.9997],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368105.6875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 250.0 event: 1250 loss: tensor(459.2497, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.0486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0058],
        [1.0083],
        [1.0152],
        ...,
        [0.9997],
        [0.9986],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368114.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.0486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.2540e-03,  7.6890e-04,  2.0706e-03,  ...,  6.3663e-03,
         -9.4502e-04,  1.0451e-03],
        [-4.2540e-03,  7.6890e-04,  2.0706e-03,  ...,  6.3663e-03,
         -9.4502e-04,  1.0451e-03],
        [ 0.0000e+00, -2.7315e-04, -2.0599e-04,  ...,  9.7835e-04,
          0.0000e+00, -4.4597e-05],
        ...,
        [ 0.0000e+00, -2.7315e-04, -2.0599e-04,  ...,  9.7835e-04,
          0.0000e+00, -4.4597e-05],
        [ 0.0000e+00, -2.7315e-04, -2.0599e-04,  ...,  9.7835e-04,
          0.0000e+00, -4.4597e-05],
        [ 0.0000e+00, -2.7315e-04, -2.0599e-04,  ...,  9.7835e-04,
          0.0000e+00, -4.4597e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2112.0867, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.3638, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.8308, device='cuda:0')



h[100].sum tensor(-13.1452, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.6267, device='cuda:0')



h[200].sum tensor(-13.0782, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.5690, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0014, 0.0037,  ..., 0.0138, 0.0000, 0.0019],
        [0.0000, 0.0014, 0.0037,  ..., 0.0138, 0.0000, 0.0019],
        [0.0000, 0.0014, 0.0038,  ..., 0.0138, 0.0000, 0.0019],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54129.4414, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0502, 0.0048, 0.0387,  ..., 0.0426, 0.0000, 0.0000],
        [0.0543, 0.0032, 0.0404,  ..., 0.0455, 0.0000, 0.0000],
        [0.0511, 0.0059, 0.0391,  ..., 0.0433, 0.0000, 0.0000],
        ...,
        [0.0079, 0.0344, 0.0213,  ..., 0.0134, 0.0000, 0.0000],
        [0.0079, 0.0344, 0.0213,  ..., 0.0134, 0.0000, 0.0000],
        [0.0079, 0.0344, 0.0213,  ..., 0.0134, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(598208., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11254.7559, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(17.7001, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5784.9971, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-567.5462, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8773.4512, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(561.6157, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3617],
        [-0.2586],
        [-0.2796],
        ...,
        [-1.7110],
        [-1.7061],
        [-1.7047]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-216445.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0058],
        [1.0083],
        [1.0152],
        ...,
        [0.9997],
        [0.9986],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368114.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(182.3246, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0058],
        [1.0083],
        [1.0152],
        ...,
        [0.9997],
        [0.9986],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368114.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(182.3246, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.7315e-04, -2.0599e-04,  ...,  9.7835e-04,
          0.0000e+00, -4.4597e-05],
        [-1.0044e-02,  2.1871e-03,  5.1690e-03,  ...,  1.3699e-02,
         -2.2312e-03,  2.5280e-03],
        [-1.9206e-02,  4.4315e-03,  1.0072e-02,  ...,  2.5304e-02,
         -4.2666e-03,  4.8750e-03],
        ...,
        [ 0.0000e+00, -2.7315e-04, -2.0599e-04,  ...,  9.7835e-04,
          0.0000e+00, -4.4597e-05],
        [ 0.0000e+00, -2.7315e-04, -2.0599e-04,  ...,  9.7835e-04,
          0.0000e+00, -4.4597e-05],
        [ 0.0000e+00, -2.7315e-04, -2.0599e-04,  ...,  9.7835e-04,
          0.0000e+00, -4.4597e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2020.8660, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.9097, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.5102, device='cuda:0')



h[100].sum tensor(-10.6727, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.7651, device='cuda:0')



h[200].sum tensor(-14.4045, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.9936, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0022, 0.0052,  ..., 0.0167, 0.0000, 0.0025],
        [0.0000, 0.0081, 0.0184,  ..., 0.0486, 0.0000, 0.0089],
        [0.0000, 0.0203, 0.0458,  ..., 0.1144, 0.0000, 0.0222],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50791.1328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0958, 0.0086, 0.0651,  ..., 0.0691, 0.0000, 0.0000],
        [0.2152, 0.0000, 0.1260,  ..., 0.1444, 0.0000, 0.0000],
        [0.3855, 0.0000, 0.2134,  ..., 0.2515, 0.0000, 0.0000],
        ...,
        [0.0079, 0.0344, 0.0213,  ..., 0.0134, 0.0000, 0.0000],
        [0.0079, 0.0344, 0.0213,  ..., 0.0134, 0.0000, 0.0000],
        [0.0079, 0.0344, 0.0213,  ..., 0.0134, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(590854.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10658.6543, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(0.5390, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6177.7241, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-547.8991, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8602.1689, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(525.8563, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3789],
        [-0.0673],
        [ 0.0912],
        ...,
        [-1.7110],
        [-1.7061],
        [-1.7047]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-251416.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0058],
        [1.0083],
        [1.0152],
        ...,
        [0.9997],
        [0.9986],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368114.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3232],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(256.8682, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0059],
        [1.0084],
        [1.0153],
        ...,
        [0.9997],
        [0.9986],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368123.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3232],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(256.8682, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.8227e-03,  1.3969e-03,  3.4562e-03,  ...,  9.6373e-03,
         -1.5117e-03,  1.7055e-03],
        [ 0.0000e+00, -2.7707e-04, -1.9821e-04,  ...,  9.8703e-04,
          0.0000e+00, -4.3603e-05],
        [-1.1144e-02,  2.4571e-03,  5.7707e-03,  ...,  1.5116e-02,
         -2.4692e-03,  2.8132e-03],
        ...,
        [ 0.0000e+00, -2.7707e-04, -1.9821e-04,  ...,  9.8703e-04,
          0.0000e+00, -4.3603e-05],
        [ 0.0000e+00, -2.7707e-04, -1.9821e-04,  ...,  9.8703e-04,
          0.0000e+00, -4.3603e-05],
        [ 0.0000e+00, -2.7707e-04, -1.9821e-04,  ...,  9.8703e-04,
          0.0000e+00, -4.3603e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2190.5112, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.0741, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.4428, device='cuda:0')



h[100].sum tensor(-15.0798, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.3044, device='cuda:0')



h[200].sum tensor(-11.6209, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.1680, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0078, 0.0183,  ..., 0.0487, 0.0000, 0.0089],
        [0.0000, 0.0069, 0.0168,  ..., 0.0456, 0.0000, 0.0082],
        [0.0000, 0.0051, 0.0119,  ..., 0.0331, 0.0000, 0.0058],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0000, 0.0033, 0.0077,  ..., 0.0228, 0.0000, 0.0037]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57028.4727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2385, 0.0000, 0.1357,  ..., 0.1608, 0.0000, 0.0000],
        [0.1889, 0.0000, 0.1098,  ..., 0.1300, 0.0000, 0.0000],
        [0.1740, 0.0000, 0.1028,  ..., 0.1201, 0.0000, 0.0000],
        ...,
        [0.0125, 0.0309, 0.0236,  ..., 0.0164, 0.0000, 0.0000],
        [0.0323, 0.0186, 0.0335,  ..., 0.0290, 0.0000, 0.0000],
        [0.0912, 0.0066, 0.0632,  ..., 0.0663, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(615152.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11627.8652, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(25.8138, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5917.3750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-590.0630, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9029.0684, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(591.9832, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1310],
        [ 0.1373],
        [ 0.1388],
        ...,
        [-1.4818],
        [-1.1739],
        [-0.7614]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-244067.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0059],
        [1.0084],
        [1.0153],
        ...,
        [0.9997],
        [0.9986],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368123.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.8889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0059],
        [1.0085],
        [1.0154],
        ...,
        [0.9997],
        [0.9985],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368132.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.8889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2016e-02,  2.6540e-03,  6.2648e-03,  ...,  1.6264e-02,
         -2.6557e-03,  3.0392e-03],
        [-1.0445e-02,  2.2678e-03,  5.4219e-03,  ...,  1.4268e-02,
         -2.3083e-03,  2.6359e-03],
        [-1.0243e-02,  2.2183e-03,  5.3137e-03,  ...,  1.4012e-02,
         -2.2637e-03,  2.5841e-03],
        ...,
        [ 0.0000e+00, -2.9760e-04, -1.7864e-04,  ...,  1.0110e-03,
          0.0000e+00, -4.3985e-05],
        [ 0.0000e+00, -2.9760e-04, -1.7864e-04,  ...,  1.0110e-03,
          0.0000e+00, -4.3985e-05],
        [ 0.0000e+00, -2.9760e-04, -1.7864e-04,  ...,  1.0110e-03,
          0.0000e+00, -4.3985e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2187.9312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.7202, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.8077, device='cuda:0')



h[100].sum tensor(-14.8211, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.1396, device='cuda:0')



h[200].sum tensor(-11.3761, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.2930, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0095, 0.0226,  ..., 0.0593, 0.0000, 0.0110],
        [0.0000, 0.0129, 0.0301,  ..., 0.0770, 0.0000, 0.0146],
        [0.0000, 0.0123, 0.0288,  ..., 0.0739, 0.0000, 0.0139],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57766.2383, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2548, 0.0000, 0.1428,  ..., 0.1711, 0.0000, 0.0000],
        [0.3184, 0.0000, 0.1754,  ..., 0.2108, 0.0000, 0.0000],
        [0.3285, 0.0000, 0.1807,  ..., 0.2171, 0.0000, 0.0000],
        ...,
        [0.0077, 0.0343, 0.0209,  ..., 0.0134, 0.0000, 0.0000],
        [0.0077, 0.0343, 0.0209,  ..., 0.0134, 0.0000, 0.0000],
        [0.0077, 0.0343, 0.0209,  ..., 0.0134, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(620912., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11785.5264, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(26.6226, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5758.0449, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-597.6958, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9068.6797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(597.1677, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1325],
        [ 0.1429],
        [ 0.1447],
        ...,
        [-1.6439],
        [-1.5732],
        [-1.5264]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-235413.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0059],
        [1.0085],
        [1.0154],
        ...,
        [0.9997],
        [0.9985],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368132.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2520],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(209.9396, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0059],
        [1.0085],
        [1.0154],
        ...,
        [0.9997],
        [0.9985],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368132.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2520],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(209.9396, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.2075e-03,  9.8151e-04,  2.6137e-03,  ...,  7.6210e-03,
         -1.1509e-03,  1.2922e-03],
        [-5.3124e-03,  1.0073e-03,  2.6699e-03,  ...,  7.7541e-03,
         -1.1741e-03,  1.3191e-03],
        [ 0.0000e+00, -2.9760e-04, -1.7864e-04,  ...,  1.0110e-03,
          0.0000e+00, -4.3985e-05],
        ...,
        [ 0.0000e+00, -2.9760e-04, -1.7864e-04,  ...,  1.0110e-03,
          0.0000e+00, -4.3985e-05],
        [ 0.0000e+00, -2.9760e-04, -1.7864e-04,  ...,  1.0110e-03,
          0.0000e+00, -4.3985e-05],
        [ 0.0000e+00, -2.9760e-04, -1.7864e-04,  ...,  1.0110e-03,
          0.0000e+00, -4.3985e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2095.6543, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.2614, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.7080, device='cuda:0')



h[100].sum tensor(-12.3471, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.3353, device='cuda:0')



h[200].sum tensor(-12.7078, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.0218, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0078, 0.0188,  ..., 0.0503, 0.0000, 0.0092],
        [0.0000, 0.0037, 0.0091,  ..., 0.0265, 0.0000, 0.0044],
        [0.0000, 0.0010, 0.0027,  ..., 0.0109, 0.0000, 0.0013],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52929.4453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1485, 0.0000, 0.0882,  ..., 0.1046, 0.0000, 0.0000],
        [0.1097, 0.0000, 0.0691,  ..., 0.0797, 0.0000, 0.0000],
        [0.0666, 0.0050, 0.0480,  ..., 0.0520, 0.0000, 0.0000],
        ...,
        [0.0077, 0.0343, 0.0209,  ..., 0.0134, 0.0000, 0.0000],
        [0.0077, 0.0343, 0.0209,  ..., 0.0134, 0.0000, 0.0000],
        [0.0077, 0.0343, 0.0209,  ..., 0.0134, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(599696.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10803.9902, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(4.0066, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6069.9097, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-569.0090, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8686.9004, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(546.1199, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1485],
        [ 0.1404],
        [ 0.1177],
        ...,
        [-1.7317],
        [-1.7265],
        [-1.7244]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-263888.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0059],
        [1.0085],
        [1.0154],
        ...,
        [0.9997],
        [0.9985],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368132.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.4297, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0060],
        [1.0086],
        [1.0155],
        ...,
        [0.9997],
        [0.9985],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368140.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.4297, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.2686e-04, -1.5630e-04,  ...,  1.0380e-03,
          0.0000e+00, -4.4032e-05],
        [ 0.0000e+00, -3.2686e-04, -1.5630e-04,  ...,  1.0380e-03,
          0.0000e+00, -4.4032e-05],
        [ 0.0000e+00, -3.2686e-04, -1.5630e-04,  ...,  1.0380e-03,
          0.0000e+00, -4.4032e-05],
        ...,
        [ 0.0000e+00, -3.2686e-04, -1.5630e-04,  ...,  1.0380e-03,
          0.0000e+00, -4.4032e-05],
        [ 0.0000e+00, -3.2686e-04, -1.5630e-04,  ...,  1.0380e-03,
          0.0000e+00, -4.4032e-05],
        [ 0.0000e+00, -3.2686e-04, -1.5630e-04,  ...,  1.0380e-03,
          0.0000e+00, -4.4032e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2264.5659, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.4321, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.5567, device='cuda:0')



h[100].sum tensor(-16.7548, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.8529, device='cuda:0')



h[200].sum tensor(-10.0472, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-31.0807, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0011, 0.0035,  ..., 0.0132, 0.0000, 0.0017],
        [0.0000, 0.0005, 0.0018,  ..., 0.0087, 0.0000, 0.0009],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60860.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0398, 0.0091, 0.0333,  ..., 0.0353, 0.0000, 0.0000],
        [0.0302, 0.0162, 0.0294,  ..., 0.0287, 0.0000, 0.0000],
        [0.0190, 0.0246, 0.0248,  ..., 0.0210, 0.0000, 0.0000],
        ...,
        [0.0075, 0.0341, 0.0203,  ..., 0.0132, 0.0000, 0.0000],
        [0.0075, 0.0341, 0.0203,  ..., 0.0132, 0.0000, 0.0000],
        [0.0075, 0.0341, 0.0203,  ..., 0.0132, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(637405.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12200.3965, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(35.0584, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5694.3418, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-618.0366, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9256.2627, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(623.2198, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0953],
        [-1.2617],
        [-1.4511],
        ...,
        [-1.7418],
        [-1.7368],
        [-1.7353]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-240779.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0060],
        [1.0086],
        [1.0155],
        ...,
        [0.9997],
        [0.9985],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368140.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.3491, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0061],
        [1.0087],
        [1.0156],
        ...,
        [0.9997],
        [0.9985],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368149.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.3491, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.5390e-04, -1.3553e-04,  ...,  1.0592e-03,
          0.0000e+00, -4.6334e-05],
        [ 0.0000e+00, -3.5390e-04, -1.3553e-04,  ...,  1.0592e-03,
          0.0000e+00, -4.6334e-05],
        [ 0.0000e+00, -3.5390e-04, -1.3553e-04,  ...,  1.0592e-03,
          0.0000e+00, -4.6334e-05],
        ...,
        [ 0.0000e+00, -3.5390e-04, -1.3553e-04,  ...,  1.0592e-03,
          0.0000e+00, -4.6334e-05],
        [ 0.0000e+00, -3.5390e-04, -1.3553e-04,  ...,  1.0592e-03,
          0.0000e+00, -4.6334e-05],
        [ 0.0000e+00, -3.5390e-04, -1.3553e-04,  ...,  1.0592e-03,
          0.0000e+00, -4.6334e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2108.7021, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.6142, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.8997, device='cuda:0')



h[100].sum tensor(-12.5902, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.3851, device='cuda:0')



h[200].sum tensor(-11.9878, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.2860, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55594.4297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0063, 0.0332, 0.0188,  ..., 0.0122, 0.0000, 0.0000],
        [0.0064, 0.0333, 0.0189,  ..., 0.0123, 0.0000, 0.0000],
        [0.0067, 0.0334, 0.0191,  ..., 0.0124, 0.0000, 0.0000],
        ...,
        [0.0072, 0.0340, 0.0197,  ..., 0.0129, 0.0000, 0.0000],
        [0.0072, 0.0340, 0.0197,  ..., 0.0128, 0.0000, 0.0000],
        [0.0072, 0.0340, 0.0196,  ..., 0.0128, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(618345.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11447.7158, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(14.0128, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5673.8799, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-588.1837, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8832.0254, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(566.0189, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8617],
        [-1.9303],
        [-1.9847],
        ...,
        [-1.7441],
        [-1.7409],
        [-1.7404]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-243795.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0061],
        [1.0087],
        [1.0156],
        ...,
        [0.9997],
        [0.9985],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368149.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(198.2332, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0061],
        [1.0088],
        [1.0157],
        ...,
        [0.9997],
        [0.9985],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368158.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(198.2332, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.6683e-04, -1.2140e-04,  ...,  1.0712e-03,
          0.0000e+00, -4.5563e-05],
        [ 0.0000e+00, -3.6683e-04, -1.2140e-04,  ...,  1.0712e-03,
          0.0000e+00, -4.5563e-05],
        [ 0.0000e+00, -3.6683e-04, -1.2140e-04,  ...,  1.0712e-03,
          0.0000e+00, -4.5563e-05],
        ...,
        [ 0.0000e+00, -3.6683e-04, -1.2140e-04,  ...,  1.0712e-03,
          0.0000e+00, -4.5563e-05],
        [ 0.0000e+00, -3.6683e-04, -1.2140e-04,  ...,  1.0712e-03,
          0.0000e+00, -4.5563e-05],
        [ 0.0000e+00, -3.6683e-04, -1.2140e-04,  ...,  1.0712e-03,
          0.0000e+00, -4.5563e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2075.4014, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.2768, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.7763, device='cuda:0')



h[100].sum tensor(-11.6300, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.0936, device='cuda:0')



h[200].sum tensor(-12.0894, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.7381, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0010, 0.0028,  ..., 0.0112, 0.0000, 0.0013],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52928.0352, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0653, 0.0113, 0.0467,  ..., 0.0499, 0.0000, 0.0000],
        [0.0281, 0.0170, 0.0286,  ..., 0.0262, 0.0000, 0.0000],
        [0.0220, 0.0219, 0.0257,  ..., 0.0223, 0.0000, 0.0000],
        ...,
        [0.0070, 0.0340, 0.0194,  ..., 0.0127, 0.0000, 0.0000],
        [0.0070, 0.0340, 0.0194,  ..., 0.0127, 0.0000, 0.0000],
        [0.0070, 0.0340, 0.0194,  ..., 0.0127, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(609660.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10739.1016, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1.3849, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5984.8989, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-574.9626, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8608.0049, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(534.8788, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2150],
        [-0.4964],
        [-0.6608],
        ...,
        [-1.7674],
        [-1.7623],
        [-1.7606]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-279051.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0061],
        [1.0088],
        [1.0157],
        ...,
        [0.9997],
        [0.9985],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368158.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.1191, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0062],
        [1.0089],
        [1.0158],
        ...,
        [0.9997],
        [0.9985],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368166.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.1191, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.7436e-04, -1.0890e-04,  ...,  1.0738e-03,
          0.0000e+00, -4.6649e-05],
        [ 0.0000e+00, -3.7436e-04, -1.0890e-04,  ...,  1.0738e-03,
          0.0000e+00, -4.6649e-05],
        [ 0.0000e+00, -3.7436e-04, -1.0890e-04,  ...,  1.0738e-03,
          0.0000e+00, -4.6649e-05],
        ...,
        [ 0.0000e+00, -3.7436e-04, -1.0890e-04,  ...,  1.0738e-03,
          0.0000e+00, -4.6649e-05],
        [ 0.0000e+00, -3.7436e-04, -1.0890e-04,  ...,  1.0738e-03,
          0.0000e+00, -4.6649e-05],
        [ 0.0000e+00, -3.7436e-04, -1.0890e-04,  ...,  1.0738e-03,
          0.0000e+00, -4.6649e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2193.7590, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.6868, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.2240, device='cuda:0')



h[100].sum tensor(-14.7756, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.2476, device='cuda:0')



h[200].sum tensor(-9.8867, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.8665, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56602.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0058, 0.0335, 0.0183,  ..., 0.0117, 0.0000, 0.0000],
        [0.0059, 0.0335, 0.0184,  ..., 0.0118, 0.0000, 0.0000],
        [0.0062, 0.0336, 0.0186,  ..., 0.0119, 0.0000, 0.0000],
        ...,
        [0.0067, 0.0342, 0.0192,  ..., 0.0123, 0.0000, 0.0000],
        [0.0067, 0.0342, 0.0192,  ..., 0.0123, 0.0000, 0.0000],
        [0.0067, 0.0342, 0.0191,  ..., 0.0123, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(618128.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11221.2305, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(17.6190, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5677.1729, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-599.1589, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8776.5586, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(573.2290, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9811],
        [-2.0200],
        [-2.0456],
        ...,
        [-1.7837],
        [-1.7780],
        [-1.7761]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-253360.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0062],
        [1.0089],
        [1.0158],
        ...,
        [0.9997],
        [0.9985],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368166.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(229.7982, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0063],
        [1.0090],
        [1.0159],
        ...,
        [0.9997],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368175.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(229.7982, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.7457e-04, -1.0341e-04,  ...,  1.0736e-03,
          0.0000e+00, -4.1809e-05],
        [ 0.0000e+00, -3.7457e-04, -1.0341e-04,  ...,  1.0736e-03,
          0.0000e+00, -4.1809e-05],
        [-1.6723e-02,  3.7575e-03,  8.9108e-03,  ...,  2.2420e-02,
         -3.6485e-03,  4.2667e-03],
        ...,
        [ 0.0000e+00, -3.7457e-04, -1.0341e-04,  ...,  1.0736e-03,
          0.0000e+00, -4.1809e-05],
        [ 0.0000e+00, -3.7457e-04, -1.0341e-04,  ...,  1.0736e-03,
          0.0000e+00, -4.1809e-05],
        [ 0.0000e+00, -3.7457e-04, -1.0341e-04,  ...,  1.0736e-03,
          0.0000e+00, -4.1809e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2149.6370, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.8695, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.2884, device='cuda:0')



h[100].sum tensor(-13.4726, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.7454, device='cuda:0')



h[200].sum tensor(-10.1571, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.1995, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0163,  ..., 0.0434, 0.0000, 0.0078],
        [0.0000, 0.0141, 0.0328,  ..., 0.0828, 0.0000, 0.0157],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55687.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0780, 0.0091, 0.0544,  ..., 0.0572, 0.0000, 0.0000],
        [0.1875, 0.0000, 0.1096,  ..., 0.1259, 0.0000, 0.0000],
        [0.3018, 0.0000, 0.1678,  ..., 0.1974, 0.0000, 0.0000],
        ...,
        [0.0066, 0.0345, 0.0192,  ..., 0.0123, 0.0000, 0.0000],
        [0.0066, 0.0345, 0.0192,  ..., 0.0123, 0.0000, 0.0000],
        [0.0066, 0.0345, 0.0192,  ..., 0.0123, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(621988.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11117.5107, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(12.2281, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5924.1353, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-596.0344, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8832.7793, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(564.0775, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0205],
        [ 0.1092],
        [ 0.1493],
        ...,
        [-1.8018],
        [-1.7965],
        [-1.7948]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-279623.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0063],
        [1.0090],
        [1.0159],
        ...,
        [0.9997],
        [0.9986],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368175.6562, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 260.0 event: 1300 loss: tensor(386.3202, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2754],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.3192, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0064],
        [1.0091],
        [1.0160],
        ...,
        [0.9998],
        [0.9987],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368184.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2754],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.3192, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.7276e-03,  7.9695e-04,  2.4478e-03,  ...,  7.1164e-03,
         -1.0287e-03,  1.1889e-03],
        [-5.7697e-03,  1.0548e-03,  3.0100e-03,  ...,  8.4479e-03,
         -1.2555e-03,  1.4577e-03],
        [ 0.0000e+00, -3.7304e-04, -1.0260e-04,  ...,  1.0755e-03,
          0.0000e+00, -3.0237e-05],
        ...,
        [ 0.0000e+00, -3.7304e-04, -1.0260e-04,  ...,  1.0755e-03,
          0.0000e+00, -3.0237e-05],
        [ 0.0000e+00, -3.7304e-04, -1.0260e-04,  ...,  1.0755e-03,
          0.0000e+00, -3.0237e-05],
        [ 0.0000e+00, -3.7304e-04, -1.0260e-04,  ...,  1.0755e-03,
          0.0000e+00, -3.0237e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2321.6665, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.6751, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.0600, device='cuda:0')



h[100].sum tensor(-17.6114, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.2430, device='cuda:0')



h[200].sum tensor(-7.7940, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-33.1521, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0079, 0.0201,  ..., 0.0529, 0.0000, 0.0097],
        [0.0000, 0.0036, 0.0093,  ..., 0.0268, 0.0000, 0.0045],
        [0.0000, 0.0021, 0.0060,  ..., 0.0191, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65235.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1557, 0.0000, 0.0909,  ..., 0.1080, 0.0000, 0.0000],
        [0.1175, 0.0000, 0.0721,  ..., 0.0838, 0.0000, 0.0000],
        [0.0906, 0.0000, 0.0590,  ..., 0.0667, 0.0000, 0.0000],
        ...,
        [0.0067, 0.0347, 0.0195,  ..., 0.0125, 0.0000, 0.0000],
        [0.0067, 0.0347, 0.0195,  ..., 0.0125, 0.0000, 0.0000],
        [0.0067, 0.0347, 0.0195,  ..., 0.0125, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(680844.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(13133.9668, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(48.8756, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5836.1377, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-655.0133, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9885.8711, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(663.9730, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1533],
        [ 0.1391],
        [ 0.1128],
        ...,
        [-1.8092],
        [-1.8038],
        [-1.8021]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-279591.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0064],
        [1.0091],
        [1.0160],
        ...,
        [0.9998],
        [0.9987],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368184.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.8813, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0065],
        [1.0092],
        [1.0161],
        ...,
        [0.9999],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368194.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.8813, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.7043e-04, -1.0058e-04,  ...,  1.0803e-03,
          0.0000e+00, -1.5880e-05],
        [ 0.0000e+00, -3.7043e-04, -1.0058e-04,  ...,  1.0803e-03,
          0.0000e+00, -1.5880e-05],
        [ 0.0000e+00, -3.7043e-04, -1.0058e-04,  ...,  1.0803e-03,
          0.0000e+00, -1.5880e-05],
        ...,
        [ 0.0000e+00, -3.7043e-04, -1.0058e-04,  ...,  1.0803e-03,
          0.0000e+00, -1.5880e-05],
        [ 0.0000e+00, -3.7043e-04, -1.0058e-04,  ...,  1.0803e-03,
          0.0000e+00, -1.5880e-05],
        [ 0.0000e+00, -3.7043e-04, -1.0058e-04,  ...,  1.0803e-03,
          0.0000e+00, -1.5880e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2383.8843, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.2599, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.2189, device='cuda:0')



h[100].sum tensor(-18.7356, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.5437, device='cuda:0')



h[200].sum tensor(-7.2776, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-34.7490, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65983.8516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0059, 0.0341, 0.0191,  ..., 0.0123, 0.0000, 0.0000],
        [0.0073, 0.0332, 0.0197,  ..., 0.0132, 0.0000, 0.0000],
        [0.0111, 0.0308, 0.0214,  ..., 0.0157, 0.0000, 0.0000],
        ...,
        [0.0068, 0.0349, 0.0200,  ..., 0.0130, 0.0000, 0.0000],
        [0.0110, 0.0318, 0.0220,  ..., 0.0156, 0.0000, 0.0000],
        [0.0248, 0.0212, 0.0289,  ..., 0.0244, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(673821., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12846.1084, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(49.9144, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5901.0254, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-660.8668, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9755.5996, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(673.2469, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5260],
        [-1.4282],
        [-1.2747],
        ...,
        [-1.7551],
        [-1.6247],
        [-1.3735]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-284137.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0065],
        [1.0092],
        [1.0161],
        ...,
        [0.9999],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368194.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.6022, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0066],
        [1.0093],
        [1.0162],
        ...,
        [0.9999],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368203.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.6022, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.6534e-04, -9.8924e-05,  ...,  1.0830e-03,
          0.0000e+00,  2.6724e-07],
        [ 0.0000e+00, -3.6534e-04, -9.8924e-05,  ...,  1.0830e-03,
          0.0000e+00,  2.6724e-07],
        [-6.0586e-03,  1.1388e-03,  3.1750e-03,  ...,  8.8411e-03,
         -1.3115e-03,  1.5659e-03],
        ...,
        [ 0.0000e+00, -3.6534e-04, -9.8924e-05,  ...,  1.0830e-03,
          0.0000e+00,  2.6724e-07],
        [ 0.0000e+00, -3.6534e-04, -9.8924e-05,  ...,  1.0830e-03,
          0.0000e+00,  2.6724e-07],
        [ 0.0000e+00, -3.6534e-04, -9.8924e-05,  ...,  1.0830e-03,
          0.0000e+00,  2.6724e-07]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2176.2441, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.0227, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.5566, device='cuda:0')



h[100].sum tensor(-12.8539, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.5555, device='cuda:0')



h[200].sum tensor(-10.7023, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.1911, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.3521e-03, 0.0000e+00,
         1.0739e-06],
        [0.0000e+00, 1.1483e-03, 3.2016e-03,  ..., 1.2186e-02, 0.0000e+00,
         1.5799e-03],
        [0.0000e+00, 8.7159e-04, 2.6005e-03,  ..., 1.0777e-02, 0.0000e+00,
         1.2925e-03],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.4689e-03, 0.0000e+00,
         1.1027e-06],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.4673e-03, 0.0000e+00,
         1.1023e-06],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.4670e-03, 0.0000e+00,
         1.1022e-06]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56965.1016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0158, 0.0271, 0.0240,  ..., 0.0191, 0.0000, 0.0000],
        [0.0323, 0.0169, 0.0318,  ..., 0.0299, 0.0000, 0.0000],
        [0.0400, 0.0118, 0.0354,  ..., 0.0350, 0.0000, 0.0000],
        ...,
        [0.0070, 0.0352, 0.0204,  ..., 0.0134, 0.0000, 0.0000],
        [0.0069, 0.0351, 0.0203,  ..., 0.0134, 0.0000, 0.0000],
        [0.0069, 0.0351, 0.0203,  ..., 0.0134, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(631064.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11674.1377, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(13.1159, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5693.5254, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-608.0446, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9120.4004, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(582.8239, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0612],
        [-1.2382],
        [-1.3378],
        ...,
        [-1.8136],
        [-1.8083],
        [-1.8066]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-249810.0156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0066],
        [1.0093],
        [1.0162],
        ...,
        [0.9999],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368203.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(329.6671, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0067],
        [1.0094],
        [1.0163],
        ...,
        [1.0000],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368212.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(329.6671, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.5649e-04, -9.8064e-05,  ...,  1.0818e-03,
          0.0000e+00,  1.1680e-05],
        [ 0.0000e+00, -3.5649e-04, -9.8064e-05,  ...,  1.0818e-03,
          0.0000e+00,  1.1680e-05],
        [ 0.0000e+00, -3.5649e-04, -9.8064e-05,  ...,  1.0818e-03,
          0.0000e+00,  1.1680e-05],
        ...,
        [ 0.0000e+00, -3.5649e-04, -9.8064e-05,  ...,  1.0818e-03,
          0.0000e+00,  1.1680e-05],
        [ 0.0000e+00, -3.5649e-04, -9.8064e-05,  ...,  1.0818e-03,
          0.0000e+00,  1.1680e-05],
        [ 0.0000e+00, -3.5649e-04, -9.8064e-05,  ...,  1.0818e-03,
          0.0000e+00,  1.1680e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2443.6399, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.9105, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.2364, device='cuda:0')



h[100].sum tensor(-19.1856, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.8077, device='cuda:0')



h[200].sum tensor(-7.4157, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-36.1511, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.3476e-03, 0.0000e+00,
         4.6940e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.3589e-03, 0.0000e+00,
         4.7062e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.3735e-03, 0.0000e+00,
         4.7220e-05],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.4645e-03, 0.0000e+00,
         4.8202e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.4628e-03, 0.0000e+00,
         4.8184e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.4626e-03, 0.0000e+00,
         4.8182e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66308.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0061, 0.0346, 0.0199,  ..., 0.0131, 0.0000, 0.0000],
        [0.0062, 0.0347, 0.0200,  ..., 0.0132, 0.0000, 0.0000],
        [0.0065, 0.0348, 0.0202,  ..., 0.0133, 0.0000, 0.0000],
        ...,
        [0.0437, 0.0104, 0.0389,  ..., 0.0372, 0.0000, 0.0000],
        [0.0418, 0.0104, 0.0380,  ..., 0.0360, 0.0000, 0.0000],
        [0.0331, 0.0164, 0.0336,  ..., 0.0305, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(672070.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(13181.0254, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(48.9886, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5514.2524, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-665.1861, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9859.5127, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(679.4506, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8846],
        [-1.9462],
        [-1.9771],
        ...,
        [-0.5482],
        [-0.5601],
        [-0.6126]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-233607.5469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0067],
        [1.0094],
        [1.0163],
        ...,
        [1.0000],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368212.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.8719, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0067],
        [1.0095],
        [1.0164],
        ...,
        [1.0000],
        [0.9989],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368222.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.8719, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.5045e-04, -9.3429e-05,  ...,  1.0765e-03,
          0.0000e+00,  1.0586e-05],
        [ 0.0000e+00, -3.5045e-04, -9.3429e-05,  ...,  1.0765e-03,
          0.0000e+00,  1.0586e-05],
        [ 0.0000e+00, -3.5045e-04, -9.3429e-05,  ...,  1.0765e-03,
          0.0000e+00,  1.0586e-05],
        ...,
        [ 0.0000e+00, -3.5045e-04, -9.3429e-05,  ...,  1.0765e-03,
          0.0000e+00,  1.0586e-05],
        [ 0.0000e+00, -3.5045e-04, -9.3429e-05,  ...,  1.0765e-03,
          0.0000e+00,  1.0586e-05],
        [-7.5890e-03,  1.5390e-03,  4.0148e-03,  ...,  1.0815e-02,
         -1.6342e-03,  1.9764e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2264.8296, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.0571, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.5676, device='cuda:0')



h[100].sum tensor(-14.2943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.0773, device='cuda:0')



h[200].sum tensor(-10.2888, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.9621, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.3263e-03, 0.0000e+00,
         4.2545e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.3377e-03, 0.0000e+00,
         4.2658e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.3523e-03, 0.0000e+00,
         4.2801e-05],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.4429e-03, 0.0000e+00,
         4.3692e-05],
        [0.0000e+00, 1.5862e-03, 4.1379e-03,  ..., 1.4479e-02, 0.0000e+00,
         2.0697e-03],
        [0.0000e+00, 1.2293e-03, 3.3624e-03,  ..., 1.2640e-02, 0.0000e+00,
         1.6987e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58616.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0193, 0.0250, 0.0264,  ..., 0.0219, 0.0000, 0.0000],
        [0.0126, 0.0302, 0.0232,  ..., 0.0176, 0.0000, 0.0000],
        [0.0194, 0.0254, 0.0264,  ..., 0.0221, 0.0000, 0.0000],
        ...,
        [0.0177, 0.0277, 0.0261,  ..., 0.0209, 0.0000, 0.0000],
        [0.0413, 0.0150, 0.0376,  ..., 0.0362, 0.0000, 0.0000],
        [0.0511, 0.0082, 0.0422,  ..., 0.0427, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(635204.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11693.6445, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(15.7225, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5803.4277, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-619.5399, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9175.8477, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(598.9723, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4116],
        [-0.5641],
        [-0.6144],
        ...,
        [-1.5752],
        [-1.3635],
        [-1.1879]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-259641.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0067],
        [1.0095],
        [1.0164],
        ...,
        [1.0000],
        [0.9989],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368222.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.3820, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0068],
        [1.0097],
        [1.0166],
        ...,
        [1.0001],
        [0.9989],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368231.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.3820, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.4708e-04, -8.6515e-05,  ...,  1.0712e-03,
          0.0000e+00,  1.7209e-06],
        [ 0.0000e+00, -3.4708e-04, -8.6515e-05,  ...,  1.0712e-03,
          0.0000e+00,  1.7209e-06],
        [ 0.0000e+00, -3.4708e-04, -8.6515e-05,  ...,  1.0712e-03,
          0.0000e+00,  1.7209e-06],
        ...,
        [ 0.0000e+00, -3.4708e-04, -8.6515e-05,  ...,  1.0712e-03,
          0.0000e+00,  1.7209e-06],
        [ 0.0000e+00, -3.4708e-04, -8.6515e-05,  ...,  1.0712e-03,
          0.0000e+00,  1.7209e-06],
        [ 0.0000e+00, -3.4708e-04, -8.6515e-05,  ...,  1.0712e-03,
          0.0000e+00,  1.7209e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2219.1277, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.0259, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.5390, device='cuda:0')



h[100].sum tensor(-12.8420, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.5510, device='cuda:0')



h[200].sum tensor(-11.2153, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.1669, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.3053e-03, 0.0000e+00,
         6.9168e-06],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.3167e-03, 0.0000e+00,
         6.9351e-06],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.3313e-03, 0.0000e+00,
         6.9585e-06],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.4216e-03, 0.0000e+00,
         7.1036e-06],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.4199e-03, 0.0000e+00,
         7.1009e-06],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.4197e-03, 0.0000e+00,
         7.1005e-06]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56215.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0249, 0.0209, 0.0290,  ..., 0.0258, 0.0000, 0.0000],
        [0.0141, 0.0292, 0.0239,  ..., 0.0188, 0.0000, 0.0000],
        [0.0150, 0.0288, 0.0243,  ..., 0.0193, 0.0000, 0.0000],
        ...,
        [0.0067, 0.0360, 0.0210,  ..., 0.0139, 0.0000, 0.0000],
        [0.0067, 0.0360, 0.0210,  ..., 0.0139, 0.0000, 0.0000],
        [0.0067, 0.0360, 0.0210,  ..., 0.0139, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(623996.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11421.0859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(7.0374, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5673.1855, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-604.8148, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8989.3770, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(573.7414, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2740],
        [-0.4574],
        [-0.5604],
        ...,
        [-1.8406],
        [-1.8352],
        [-1.8334]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-241340.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0068],
        [1.0097],
        [1.0166],
        ...,
        [1.0001],
        [0.9989],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368231.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(182.6980, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0069],
        [1.0098],
        [1.0166],
        ...,
        [1.0001],
        [0.9989],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368239.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(182.6980, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.5112e-04, -8.2384e-05,  ...,  1.0578e-03,
          0.0000e+00, -1.2153e-05],
        [ 0.0000e+00, -3.5112e-04, -8.2384e-05,  ...,  1.0578e-03,
          0.0000e+00, -1.2153e-05],
        [ 0.0000e+00, -3.5112e-04, -8.2384e-05,  ...,  1.0578e-03,
          0.0000e+00, -1.2153e-05],
        ...,
        [ 0.0000e+00, -3.5112e-04, -8.2384e-05,  ...,  1.0578e-03,
          0.0000e+00, -1.2153e-05],
        [ 0.0000e+00, -3.5112e-04, -8.2384e-05,  ...,  1.0578e-03,
          0.0000e+00, -1.2153e-05],
        [ 0.0000e+00, -3.5112e-04, -8.2384e-05,  ...,  1.0578e-03,
          0.0000e+00, -1.2153e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2132.6199, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.9519, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.5400, device='cuda:0')



h[100].sum tensor(-10.6480, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.7728, device='cuda:0')



h[200].sum tensor(-12.4195, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-20.0345, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52699.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0054, 0.0355, 0.0200,  ..., 0.0129, 0.0000, 0.0000],
        [0.0055, 0.0356, 0.0201,  ..., 0.0130, 0.0000, 0.0000],
        [0.0058, 0.0357, 0.0202,  ..., 0.0131, 0.0000, 0.0000],
        ...,
        [0.0063, 0.0364, 0.0209,  ..., 0.0136, 0.0000, 0.0000],
        [0.0063, 0.0363, 0.0209,  ..., 0.0136, 0.0000, 0.0000],
        [0.0063, 0.0363, 0.0209,  ..., 0.0136, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(615192.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10806.5674, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-7.6010, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5993.9038, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-582.1619, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8764.8408, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(533.5569, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8657],
        [-1.7482],
        [-1.5596],
        ...,
        [-1.8602],
        [-1.8546],
        [-1.8528]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-267605.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0069],
        [1.0098],
        [1.0166],
        ...,
        [1.0001],
        [0.9989],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368239.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4155],
        [0.3840],
        [0.4084],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.0765, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0070],
        [1.0099],
        [1.0168],
        ...,
        [1.0002],
        [0.9990],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368248.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4155],
        [0.3840],
        [0.4084],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.0765, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.0577e-02,  7.2787e-03,  1.6522e-02,  ...,  4.0416e-02,
         -6.5329e-03,  7.9290e-03],
        [-2.4066e-02,  5.6534e-03,  1.2987e-02,  ...,  3.2032e-02,
         -5.1418e-03,  6.2360e-03],
        [-2.0476e-02,  4.7572e-03,  1.1037e-02,  ...,  2.7408e-02,
         -4.3747e-03,  5.3024e-03],
        ...,
        [ 0.0000e+00, -3.5379e-04, -8.0171e-05,  ...,  1.0426e-03,
          0.0000e+00, -2.1460e-05],
        [ 0.0000e+00, -3.5379e-04, -8.0171e-05,  ...,  1.0426e-03,
          0.0000e+00, -2.1460e-05],
        [ 0.0000e+00, -3.5379e-04, -8.0171e-05,  ...,  1.0426e-03,
          0.0000e+00, -2.1460e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2357.6987, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.8610, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.5286, device='cuda:0')



h[100].sum tensor(-16.2745, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.8456, device='cuda:0')



h[200].sum tensor(-9.2986, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-31.0420, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0212, 0.0488,  ..., 0.1208, 0.0000, 0.0235],
        [0.0000, 0.0269, 0.0614,  ..., 0.1505, 0.0000, 0.0295],
        [0.0000, 0.0286, 0.0649,  ..., 0.1589, 0.0000, 0.0312],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59561.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.5159, 0.0000, 0.2842,  ..., 0.3348, 0.0000, 0.0000],
        [0.6153, 0.0000, 0.3371,  ..., 0.3969, 0.0000, 0.0000],
        [0.6590, 0.0000, 0.3604,  ..., 0.4241, 0.0000, 0.0000],
        ...,
        [0.0060, 0.0367, 0.0208,  ..., 0.0133, 0.0000, 0.0000],
        [0.0060, 0.0367, 0.0208,  ..., 0.0133, 0.0000, 0.0000],
        [0.0060, 0.0367, 0.0208,  ..., 0.0133, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(641036.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11369.0146, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(18.9080, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6299.2090, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-621.5666, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9179.2422, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(600.0099, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0441],
        [ 0.0127],
        [-0.0145],
        ...,
        [-1.8803],
        [-1.8746],
        [-1.8727]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-307550.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0070],
        [1.0099],
        [1.0168],
        ...,
        [1.0002],
        [0.9990],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368248.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2930],
        [0.4873],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(326.8270, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0071],
        [1.0099],
        [1.0168],
        ...,
        [1.0002],
        [0.9990],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368257.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2930],
        [0.4873],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(326.8270, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.4699e-02,  5.8239e-03,  1.3342e-02,  ...,  3.2867e-02,
         -5.2631e-03,  6.4055e-03],
        [-1.6610e-02,  3.8031e-03,  8.9455e-03,  ...,  2.2439e-02,
         -3.5395e-03,  4.2992e-03],
        [-3.3626e-02,  8.0542e-03,  1.8193e-02,  ...,  4.4375e-02,
         -7.1654e-03,  8.7301e-03],
        ...,
        [ 0.0000e+00, -3.4667e-04, -8.1910e-05,  ...,  1.0265e-03,
          0.0000e+00, -2.6063e-05],
        [ 0.0000e+00, -3.4667e-04, -8.1910e-05,  ...,  1.0265e-03,
          0.0000e+00, -2.6063e-05],
        [ 0.0000e+00, -3.4667e-04, -8.1910e-05,  ...,  1.0265e-03,
          0.0000e+00, -2.6063e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2464.5132, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.3738, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.0104, device='cuda:0')



h[100].sum tensor(-18.7682, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.7491, device='cuda:0')



h[200].sum tensor(-7.8625, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-35.8396, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0164, 0.0384,  ..., 0.0961, 0.0000, 0.0185],
        [0.0000, 0.0251, 0.0572,  ..., 0.1407, 0.0000, 0.0275],
        [0.0000, 0.0131, 0.0312,  ..., 0.0790, 0.0000, 0.0150],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65399.8672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3610, 0.0000, 0.2025,  ..., 0.2385, 0.0000, 0.0000],
        [0.4341, 0.0000, 0.2410,  ..., 0.2844, 0.0000, 0.0000],
        [0.3472, 0.0000, 0.1952,  ..., 0.2299, 0.0000, 0.0000],
        ...,
        [0.0059, 0.0371, 0.0210,  ..., 0.0133, 0.0000, 0.0000],
        [0.0058, 0.0370, 0.0210,  ..., 0.0133, 0.0000, 0.0000],
        [0.0058, 0.0370, 0.0210,  ..., 0.0133, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(666100.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12691.7402, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(48.9738, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5777.7031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-655.5273, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9710.7500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(662.5736, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1631],
        [ 0.1617],
        [ 0.1559],
        ...,
        [-1.8885],
        [-1.8825],
        [-1.8801]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-253426.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0071],
        [1.0099],
        [1.0168],
        ...,
        [1.0002],
        [0.9990],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368257.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4746],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.5704, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0072],
        [1.0100],
        [1.0169],
        ...,
        [1.0002],
        [0.9991],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368267.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4746],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.5704, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.6873e-02,  3.8694e-03,  9.1011e-03,  ...,  2.2806e-02,
         -3.5860e-03,  4.3797e-03],
        [-2.6465e-02,  6.2668e-03,  1.4320e-02,  ...,  3.5187e-02,
         -5.6246e-03,  6.8816e-03],
        [-3.0079e-02,  7.1701e-03,  1.6286e-02,  ...,  3.9852e-02,
         -6.3927e-03,  7.8243e-03],
        ...,
        [ 0.0000e+00, -3.4780e-04, -7.8570e-05,  ...,  1.0281e-03,
          0.0000e+00, -2.1357e-05],
        [ 0.0000e+00, -3.4780e-04, -7.8570e-05,  ...,  1.0281e-03,
          0.0000e+00, -2.1357e-05],
        [ 0.0000e+00, -3.4780e-04, -7.8570e-05,  ...,  1.0281e-03,
          0.0000e+00, -2.1357e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2270.1152, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.0524, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.8274, device='cuda:0')



h[100].sum tensor(-13.5532, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.8852, device='cuda:0')



h[200].sum tensor(-10.7427, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.9421, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0188, 0.0437,  ..., 0.1085, 0.0000, 0.0210],
        [0.0000, 0.0196, 0.0453,  ..., 0.1124, 0.0000, 0.0218],
        [0.0000, 0.0223, 0.0514,  ..., 0.1268, 0.0000, 0.0247],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55970.0664, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.4584, 0.0000, 0.2541,  ..., 0.2989, 0.0000, 0.0000],
        [0.4485, 0.0000, 0.2488,  ..., 0.2928, 0.0000, 0.0000],
        [0.4280, 0.0000, 0.2385,  ..., 0.2798, 0.0000, 0.0000],
        ...,
        [0.0058, 0.0372, 0.0210,  ..., 0.0135, 0.0000, 0.0000],
        [0.0058, 0.0372, 0.0210,  ..., 0.0135, 0.0000, 0.0000],
        [0.0057, 0.0372, 0.0210,  ..., 0.0135, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(628555.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10994.6035, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(6.4442, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6258.0977, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-597.2379, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8963.1445, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(561.7527, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1278],
        [ 0.1347],
        [ 0.1362],
        ...,
        [-1.9013],
        [-1.8958],
        [-1.8944]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-291890.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0072],
        [1.0100],
        [1.0169],
        ...,
        [1.0002],
        [0.9991],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368267.2812, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 270.0 event: 1350 loss: tensor(520.3320, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(218.7574, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0073],
        [1.0102],
        [1.0170],
        ...,
        [1.0002],
        [0.9991],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368277.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(218.7574, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.4215e-04, -7.8851e-05,  ...,  1.0380e-03,
          0.0000e+00, -5.1698e-06],
        [ 0.0000e+00, -3.4215e-04, -7.8851e-05,  ...,  1.0380e-03,
          0.0000e+00, -5.1698e-06],
        [ 0.0000e+00, -3.4215e-04, -7.8851e-05,  ...,  1.0380e-03,
          0.0000e+00, -5.1698e-06],
        ...,
        [ 0.0000e+00, -3.4215e-04, -7.8851e-05,  ...,  1.0380e-03,
          0.0000e+00, -5.1698e-06],
        [ 0.0000e+00, -3.4215e-04, -7.8851e-05,  ...,  1.0380e-03,
          0.0000e+00, -5.1698e-06],
        [ 0.0000e+00, -3.4215e-04, -7.8851e-05,  ...,  1.0380e-03,
          0.0000e+00, -5.1698e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2262.3171, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.7758, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.4097, device='cuda:0')



h[100].sum tensor(-12.6403, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.5174, device='cuda:0')



h[200].sum tensor(-11.3255, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.9888, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55842.0391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0067, 0.0352, 0.0208,  ..., 0.0144, 0.0000, 0.0000],
        [0.0251, 0.0216, 0.0299,  ..., 0.0263, 0.0000, 0.0000],
        [0.0516, 0.0108, 0.0432,  ..., 0.0432, 0.0000, 0.0000],
        ...,
        [0.0060, 0.0373, 0.0211,  ..., 0.0140, 0.0000, 0.0000],
        [0.0060, 0.0373, 0.0211,  ..., 0.0140, 0.0000, 0.0000],
        [0.0060, 0.0373, 0.0211,  ..., 0.0140, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(629491.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11005.5527, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2.4409, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6291.0381, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-595.4733, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8955.6855, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(560.2213, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0629],
        [-0.6475],
        [-0.2386],
        ...,
        [-1.8981],
        [-1.8923],
        [-1.8881]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-292698.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0073],
        [1.0102],
        [1.0170],
        ...,
        [1.0002],
        [0.9991],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368277.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.2643, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0074],
        [1.0103],
        [1.0172],
        ...,
        [1.0002],
        [0.9991],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368286.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.2643, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.2976e-04, -8.3615e-05,  ...,  1.0412e-03,
          0.0000e+00,  9.9815e-06],
        [ 0.0000e+00, -3.2976e-04, -8.3615e-05,  ...,  1.0412e-03,
          0.0000e+00,  9.9815e-06],
        [ 0.0000e+00, -3.2976e-04, -8.3615e-05,  ...,  1.0412e-03,
          0.0000e+00,  9.9815e-06],
        ...,
        [ 0.0000e+00, -3.2976e-04, -8.3615e-05,  ...,  1.0412e-03,
          0.0000e+00,  9.9815e-06],
        [ 0.0000e+00, -3.2976e-04, -8.3615e-05,  ...,  1.0412e-03,
          0.0000e+00,  9.9815e-06],
        [ 0.0000e+00, -3.2976e-04, -8.3615e-05,  ...,  1.0412e-03,
          0.0000e+00,  9.9815e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2214.4734, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.2615, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.9034, device='cuda:0')



h[100].sum tensor(-10.8484, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.8671, device='cuda:0')



h[200].sum tensor(-12.3854, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-20.5353, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.0198e-03, 2.8594e-03,  ..., 1.1174e-02, 0.0000e+00,
         1.4540e-03],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.1981e-03, 0.0000e+00,
         4.0245e-05],
        [0.0000e+00, 1.1103e-03, 3.0615e-03,  ..., 1.1681e-02, 0.0000e+00,
         1.5517e-03],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.3009e-03, 0.0000e+00,
         4.1231e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.2993e-03, 0.0000e+00,
         4.1216e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.2990e-03, 0.0000e+00,
         4.1213e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53393.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0305, 0.0181, 0.0321,  ..., 0.0304, 0.0000, 0.0000],
        [0.0205, 0.0254, 0.0276,  ..., 0.0238, 0.0000, 0.0000],
        [0.0328, 0.0177, 0.0335,  ..., 0.0318, 0.0000, 0.0000],
        ...,
        [0.0063, 0.0374, 0.0214,  ..., 0.0145, 0.0000, 0.0000],
        [0.0063, 0.0374, 0.0214,  ..., 0.0145, 0.0000, 0.0000],
        [0.0063, 0.0374, 0.0214,  ..., 0.0145, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(622769.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10802.2578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-11.0479, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6318.5107, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-580.4292, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8841.3760, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(535.5484, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4348],
        [-1.5781],
        [-1.6039],
        ...,
        [-1.9027],
        [-1.8971],
        [-1.8952]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-285877.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0074],
        [1.0103],
        [1.0172],
        ...,
        [1.0002],
        [0.9991],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368286.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.3146, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0075],
        [1.0104],
        [1.0173],
        ...,
        [1.0003],
        [0.9991],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368296.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.3146, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.2215e-04, -8.7218e-05,  ...,  1.0353e-03,
          0.0000e+00,  1.3201e-05],
        [ 0.0000e+00, -3.2215e-04, -8.7218e-05,  ...,  1.0353e-03,
          0.0000e+00,  1.3201e-05],
        [ 0.0000e+00, -3.2215e-04, -8.7218e-05,  ...,  1.0353e-03,
          0.0000e+00,  1.3201e-05],
        ...,
        [ 0.0000e+00, -3.2215e-04, -8.7218e-05,  ...,  1.0353e-03,
          0.0000e+00,  1.3201e-05],
        [ 0.0000e+00, -3.2215e-04, -8.7218e-05,  ...,  1.0353e-03,
          0.0000e+00,  1.3201e-05],
        [ 0.0000e+00, -3.2215e-04, -8.7218e-05,  ...,  1.0353e-03,
          0.0000e+00,  1.3201e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2438.1436, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.5161, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.3883, device='cuda:0')



h[100].sum tensor(-15.9993, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.8092, device='cuda:0')



h[200].sum tensor(-9.6450, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.8487, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.1629e-03, 0.0000e+00,
         5.3085e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.1745e-03, 0.0000e+00,
         5.3233e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.1888e-03, 0.0000e+00,
         5.3416e-05],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.2769e-03, 0.0000e+00,
         5.4539e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.2753e-03, 0.0000e+00,
         5.4518e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.2750e-03, 0.0000e+00,
         5.4515e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61140.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0066, 0.0359, 0.0209,  ..., 0.0148, 0.0000, 0.0000],
        [0.0080, 0.0352, 0.0214,  ..., 0.0157, 0.0000, 0.0000],
        [0.0144, 0.0308, 0.0242,  ..., 0.0199, 0.0000, 0.0000],
        ...,
        [0.0063, 0.0377, 0.0215,  ..., 0.0146, 0.0000, 0.0000],
        [0.0063, 0.0377, 0.0215,  ..., 0.0146, 0.0000, 0.0000],
        [0.0063, 0.0377, 0.0215,  ..., 0.0146, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(647382.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11967.2090, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(23.4486, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5970.7148, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-624.2131, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9306.1738, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(616.3242, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5082],
        [-1.3106],
        [-1.0615],
        ...,
        [-1.9092],
        [-1.9036],
        [-1.9017]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-251541.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0075],
        [1.0104],
        [1.0173],
        ...,
        [1.0003],
        [0.9991],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368296.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(368.3503, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0076],
        [1.0105],
        [1.0174],
        ...,
        [1.0003],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368305.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(368.3503, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.9677e-03,  1.1835e-03,  3.1650e-03,  ...,  8.7618e-03,
         -1.2549e-03,  1.5810e-03],
        [ 0.0000e+00, -3.1168e-04, -9.3391e-05,  ...,  1.0219e-03,
          0.0000e+00,  1.2698e-05],
        [-5.9677e-03,  1.1835e-03,  3.1650e-03,  ...,  8.7618e-03,
         -1.2549e-03,  1.5810e-03],
        ...,
        [ 0.0000e+00, -3.1168e-04, -9.3391e-05,  ...,  1.0219e-03,
          0.0000e+00,  1.2698e-05],
        [ 0.0000e+00, -3.1168e-04, -9.3391e-05,  ...,  1.0219e-03,
          0.0000e+00,  1.2698e-05],
        [ 0.0000e+00, -3.1168e-04, -9.3391e-05,  ...,  1.0219e-03,
          0.0000e+00,  1.2698e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2651.8713, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.5173, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.3150, device='cuda:0')



h[100].sum tensor(-20.9663, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-7.6065, device='cuda:0')



h[200].sum tensor(-6.9394, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-40.3930, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 9.1303e-04, 2.5781e-03,  ..., 1.0456e-02, 0.0000e+00,
         1.3371e-03],
        [0.0000e+00, 4.2197e-03, 1.1558e-02,  ..., 3.2471e-02, 0.0000e+00,
         5.7956e-03],
        [0.0000e+00, 9.1866e-04, 2.5940e-03,  ..., 1.0521e-02, 0.0000e+00,
         1.3453e-03],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.2221e-03, 0.0000e+00,
         5.2467e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.2205e-03, 0.0000e+00,
         5.2447e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.2202e-03, 0.0000e+00,
         5.2444e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70786.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0539, 0.0034, 0.0437,  ..., 0.0458, 0.0000, 0.0000],
        [0.0715, 0.0000, 0.0516,  ..., 0.0576, 0.0000, 0.0000],
        [0.0552, 0.0049, 0.0435,  ..., 0.0471, 0.0000, 0.0000],
        ...,
        [0.0063, 0.0381, 0.0218,  ..., 0.0147, 0.0000, 0.0000],
        [0.0063, 0.0381, 0.0218,  ..., 0.0147, 0.0000, 0.0000],
        [0.0063, 0.0381, 0.0218,  ..., 0.0147, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(693367.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(13714.6836, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(64.2798, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5805.7290, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-680.2424, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10159.6123, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(717.7915, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1665],
        [-0.1788],
        [-0.1265],
        ...,
        [-1.9242],
        [-1.9186],
        [-1.9166]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-240399.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0076],
        [1.0105],
        [1.0174],
        ...,
        [1.0003],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368305.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3040],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.3164, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0076],
        [1.0106],
        [1.0175],
        ...,
        [1.0003],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368314.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3040],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.3164, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.9257e-04, -1.0572e-04,  ...,  9.9085e-04,
          0.0000e+00,  4.6573e-06],
        [-6.2738e-03,  1.2810e-03,  3.3223e-03,  ...,  9.1365e-03,
         -1.3158e-03,  1.6562e-03],
        [ 0.0000e+00, -2.9257e-04, -1.0572e-04,  ...,  9.9085e-04,
          0.0000e+00,  4.6573e-06],
        ...,
        [ 0.0000e+00, -2.9257e-04, -1.0572e-04,  ...,  9.9085e-04,
          0.0000e+00,  4.6573e-06],
        [ 0.0000e+00, -2.9257e-04, -1.0572e-04,  ...,  9.9085e-04,
          0.0000e+00,  4.6573e-06],
        [ 0.0000e+00, -2.9257e-04, -1.0572e-04,  ...,  9.9085e-04,
          0.0000e+00,  4.6573e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2305.1875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.6989, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.4542, device='cuda:0')



h[100].sum tensor(-12.5669, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.5289, device='cuda:0')



h[200].sum tensor(-11.5743, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.0501, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 3.8234e-03, 9.9326e-03,  ..., 2.8344e-02, 0.0000e+00,
         4.9577e-03],
        [0.0000e+00, 3.5507e-03, 8.8070e-03,  ..., 2.5429e-02, 0.0000e+00,
         4.3645e-03],
        [0.0000e+00, 5.4552e-03, 1.4036e-02,  ..., 3.8378e-02, 0.0000e+00,
         6.9872e-03],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.0944e-03, 0.0000e+00,
         1.9245e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.0929e-03, 0.0000e+00,
         1.9238e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.0926e-03, 0.0000e+00,
         1.9237e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56255.5547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1002, 0.0007, 0.0661,  ..., 0.0766, 0.0000, 0.0000],
        [0.1050, 0.0000, 0.0687,  ..., 0.0795, 0.0000, 0.0000],
        [0.1075, 0.0000, 0.0697,  ..., 0.0812, 0.0000, 0.0000],
        ...,
        [0.0062, 0.0387, 0.0222,  ..., 0.0147, 0.0000, 0.0000],
        [0.0062, 0.0387, 0.0222,  ..., 0.0147, 0.0000, 0.0000],
        [0.0061, 0.0387, 0.0222,  ..., 0.0147, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(633228.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11295.5107, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(3.6343, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6400.5576, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-592.8586, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9116.2988, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(568.8578, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1193],
        [-0.0060],
        [-0.0647],
        ...,
        [-1.9408],
        [-1.9350],
        [-1.9331]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-269044.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0076],
        [1.0106],
        [1.0175],
        ...,
        [1.0003],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368314.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3420],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(229.7881, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0077],
        [1.0107],
        [1.0176],
        ...,
        [1.0003],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368323., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3420],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(229.7881, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.9325e-02,  4.5743e-03,  1.0453e-02,  ...,  2.6078e-02,
         -4.0422e-03,  5.0832e-03],
        [-1.2744e-02,  2.9220e-03,  6.8541e-03,  ...,  1.7524e-02,
         -2.6656e-03,  3.3479e-03],
        [-1.4905e-02,  3.4646e-03,  8.0360e-03,  ...,  2.0333e-02,
         -3.1177e-03,  3.9178e-03],
        ...,
        [ 0.0000e+00, -2.7752e-04, -1.1483e-04,  ...,  9.5948e-04,
          0.0000e+00, -1.2322e-05],
        [ 0.0000e+00, -2.7752e-04, -1.1483e-04,  ...,  9.5948e-04,
          0.0000e+00, -1.2322e-05],
        [ 0.0000e+00, -2.7752e-04, -1.1483e-04,  ...,  9.5948e-04,
          0.0000e+00, -1.2322e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2321.5034, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.4420, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.2876, device='cuda:0')



h[100].sum tensor(-13.0897, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.7452, device='cuda:0')



h[200].sum tensor(-11.2160, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.1984, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0125, 0.0291,  ..., 0.0741, 0.0000, 0.0142],
        [0.0000, 0.0145, 0.0336,  ..., 0.0847, 0.0000, 0.0164],
        [0.0000, 0.0131, 0.0305,  ..., 0.0774, 0.0000, 0.0149],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58791.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2997, 0.0000, 0.1705,  ..., 0.2012, 0.0000, 0.0000],
        [0.2999, 0.0000, 0.1708,  ..., 0.2013, 0.0000, 0.0000],
        [0.2775, 0.0000, 0.1592,  ..., 0.1873, 0.0000, 0.0000],
        ...,
        [0.0060, 0.0393, 0.0225,  ..., 0.0146, 0.0000, 0.0000],
        [0.0060, 0.0393, 0.0225,  ..., 0.0146, 0.0000, 0.0000],
        [0.0060, 0.0393, 0.0225,  ..., 0.0146, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(652897.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11916.3027, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(15.2054, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6555.4214, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-606.0682, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9479.1250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(596.4672, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2001],
        [ 0.1992],
        [ 0.1978],
        ...,
        [-1.9602],
        [-1.9543],
        [-1.9523]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-279252., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0077],
        [1.0107],
        [1.0176],
        ...,
        [1.0003],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368323., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.4644, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0077],
        [1.0108],
        [1.0176],
        ...,
        [1.0003],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368332., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.4644, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.7505e-04, -1.1802e-04,  ...,  9.4926e-04,
          0.0000e+00, -2.1983e-05],
        [ 0.0000e+00, -2.7505e-04, -1.1802e-04,  ...,  9.4926e-04,
          0.0000e+00, -2.1983e-05],
        [ 0.0000e+00, -2.7505e-04, -1.1802e-04,  ...,  9.4926e-04,
          0.0000e+00, -2.1983e-05],
        ...,
        [ 0.0000e+00, -2.7505e-04, -1.1802e-04,  ...,  9.4926e-04,
          0.0000e+00, -2.1983e-05],
        [ 0.0000e+00, -2.7505e-04, -1.1802e-04,  ...,  9.4926e-04,
          0.0000e+00, -2.1983e-05],
        [ 0.0000e+00, -2.7505e-04, -1.1802e-04,  ...,  9.4926e-04,
          0.0000e+00, -2.1983e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2262.9609, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.3532, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.1131, device='cuda:0')



h[100].sum tensor(-11.6027, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.1809, device='cuda:0')



h[200].sum tensor(-12.0224, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.2021, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0008, 0.0022,  ..., 0.0094, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54648.2617, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0614, 0.0065, 0.0487,  ..., 0.0511, 0.0000, 0.0000],
        [0.0346, 0.0172, 0.0359,  ..., 0.0334, 0.0000, 0.0000],
        [0.0157, 0.0313, 0.0270,  ..., 0.0210, 0.0000, 0.0000],
        ...,
        [0.0059, 0.0397, 0.0227,  ..., 0.0147, 0.0000, 0.0000],
        [0.0059, 0.0397, 0.0227,  ..., 0.0147, 0.0000, 0.0000],
        [0.0059, 0.0397, 0.0227,  ..., 0.0147, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(632234.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10993.9033, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.1821, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6862.8154, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-580.0292, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9066.2529, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(553.8829, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1102],
        [-0.4660],
        [-0.8659],
        ...,
        [-1.9724],
        [-1.9665],
        [-1.9644]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-297546.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0077],
        [1.0108],
        [1.0176],
        ...,
        [1.0003],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368332., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(418.1561, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0078],
        [1.0108],
        [1.0177],
        ...,
        [1.0003],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368341.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(418.1561, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.7972e-04, -1.1644e-04,  ...,  9.5749e-04,
          0.0000e+00, -2.4803e-05],
        [ 0.0000e+00, -2.7972e-04, -1.1644e-04,  ...,  9.5749e-04,
          0.0000e+00, -2.4803e-05],
        [ 0.0000e+00, -2.7972e-04, -1.1644e-04,  ...,  9.5749e-04,
          0.0000e+00, -2.4803e-05],
        ...,
        [ 0.0000e+00, -2.7972e-04, -1.1644e-04,  ...,  9.5749e-04,
          0.0000e+00, -2.4803e-05],
        [ 0.0000e+00, -2.7972e-04, -1.1644e-04,  ...,  9.5749e-04,
          0.0000e+00, -2.4803e-05],
        [ 0.0000e+00, -2.7972e-04, -1.1644e-04,  ...,  9.5749e-04,
          0.0000e+00, -2.4803e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2772.6279, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-33.0084, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.2788, device='cuda:0')



h[100].sum tensor(-23.4110, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-8.6350, device='cuda:0')



h[200].sum tensor(-5.5562, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-45.8547, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0063, 0.0147,  ..., 0.0393, 0.0000, 0.0071],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73553.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2040, 0.0022, 0.1239,  ..., 0.1399, 0.0000, 0.0000],
        [0.0663, 0.0183, 0.0531,  ..., 0.0531, 0.0000, 0.0000],
        [0.0218, 0.0269, 0.0304,  ..., 0.0250, 0.0000, 0.0000],
        ...,
        [0.0060, 0.0398, 0.0229,  ..., 0.0150, 0.0000, 0.0000],
        [0.0060, 0.0398, 0.0228,  ..., 0.0150, 0.0000, 0.0000],
        [0.0060, 0.0398, 0.0228,  ..., 0.0150, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(707534.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(13763.7510, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(72.6324, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6448.8486, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-692.8404, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10400.3789, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(750.7233, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1167],
        [-0.0253],
        [-0.1510],
        ...,
        [-1.9780],
        [-1.9721],
        [-1.9700]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-271276.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0078],
        [1.0108],
        [1.0177],
        ...,
        [1.0003],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368341.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3867],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.0256, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0078],
        [1.0109],
        [1.0178],
        ...,
        [1.0002],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368350.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3867],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.0256, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0798e-03,  1.2378e-03,  3.2265e-03,  ...,  8.9136e-03,
         -1.2615e-03,  1.5824e-03],
        [-1.4028e-02,  3.2382e-03,  7.5851e-03,  ...,  1.9282e-02,
         -2.9107e-03,  3.6871e-03],
        [ 0.0000e+00, -2.9236e-04, -1.0765e-04,  ...,  9.8266e-04,
          0.0000e+00, -2.7519e-05],
        ...,
        [ 0.0000e+00, -2.9236e-04, -1.0765e-04,  ...,  9.8266e-04,
          0.0000e+00, -2.7519e-05],
        [ 0.0000e+00, -2.9236e-04, -1.0765e-04,  ...,  9.8266e-04,
          0.0000e+00, -2.7519e-05],
        [ 0.0000e+00, -2.9236e-04, -1.0765e-04,  ...,  9.8266e-04,
          0.0000e+00, -2.7519e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2398.2004, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.9064, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.9778, device='cuda:0')



h[100].sum tensor(-14.1132, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.1838, device='cuda:0')



h[200].sum tensor(-10.8122, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.5273, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0156, 0.0361,  ..., 0.0907, 0.0000, 0.0175],
        [0.0000, 0.0078, 0.0186,  ..., 0.0489, 0.0000, 0.0090],
        [0.0000, 0.0082, 0.0194,  ..., 0.0509, 0.0000, 0.0094],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58065.0195, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3229, 0.0000, 0.1828,  ..., 0.2167, 0.0000, 0.0000],
        [0.2363, 0.0000, 0.1378,  ..., 0.1626, 0.0000, 0.0000],
        [0.1873, 0.0000, 0.1129,  ..., 0.1315, 0.0000, 0.0000],
        ...,
        [0.0061, 0.0397, 0.0229,  ..., 0.0154, 0.0000, 0.0000],
        [0.0061, 0.0397, 0.0229,  ..., 0.0154, 0.0000, 0.0000],
        [0.0061, 0.0397, 0.0228,  ..., 0.0154, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(640528.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11431.4570, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(5.3472, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6533.8779, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-600.6105, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9253.6895, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(591.3158, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2224],
        [ 0.2073],
        [ 0.0917],
        ...,
        [-1.9794],
        [-1.9735],
        [-1.9715]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-265949.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0078],
        [1.0109],
        [1.0178],
        ...,
        [1.0002],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368350.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.7970, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0078],
        [1.0110],
        [1.0179],
        ...,
        [1.0002],
        [0.9992],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368359.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.7970, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.1086e-04, -9.1743e-05,  ...,  1.0129e-03,
          0.0000e+00, -3.1591e-05],
        [ 0.0000e+00, -3.1086e-04, -9.1743e-05,  ...,  1.0129e-03,
          0.0000e+00, -3.1591e-05],
        [ 0.0000e+00, -3.1086e-04, -9.1743e-05,  ...,  1.0129e-03,
          0.0000e+00, -3.1591e-05],
        ...,
        [ 0.0000e+00, -3.1086e-04, -9.1743e-05,  ...,  1.0129e-03,
          0.0000e+00, -3.1591e-05],
        [ 0.0000e+00, -3.1086e-04, -9.1743e-05,  ...,  1.0129e-03,
          0.0000e+00, -3.1591e-05],
        [ 0.0000e+00, -3.1086e-04, -9.1743e-05,  ...,  1.0129e-03,
          0.0000e+00, -3.1591e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2272.1995, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.3577, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.2641, device='cuda:0')



h[100].sum tensor(-10.8842, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.9607, device='cuda:0')



h[200].sum tensor(-12.6705, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.0323, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0000, 0.0034, 0.0086,  ..., 0.0251, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54764.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0121, 0.0333, 0.0249,  ..., 0.0194, 0.0000, 0.0000],
        [0.0527, 0.0186, 0.0455,  ..., 0.0452, 0.0000, 0.0000],
        [0.1378, 0.0053, 0.0881,  ..., 0.0996, 0.0000, 0.0000],
        ...,
        [0.0059, 0.0395, 0.0225,  ..., 0.0155, 0.0000, 0.0000],
        [0.0059, 0.0395, 0.0225,  ..., 0.0155, 0.0000, 0.0000],
        [0.0059, 0.0395, 0.0225,  ..., 0.0155, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(635000.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11077.8613, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-12.3714, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6563.2759, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-580.7964, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9112.5742, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(554.3079, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3117],
        [-0.1094],
        [ 0.0599],
        ...,
        [-1.9846],
        [-1.9787],
        [-1.9767]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-269848.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0078],
        [1.0110],
        [1.0179],
        ...,
        [1.0002],
        [0.9992],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368359.9062, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 280.0 event: 1400 loss: tensor(502.7302, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.1177, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0079],
        [1.0110],
        [1.0180],
        ...,
        [1.0002],
        [0.9991],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368369.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.1177, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.1647e-04, -8.0766e-05,  ...,  1.0389e-03,
          0.0000e+00, -3.3075e-05],
        [ 0.0000e+00, -3.1647e-04, -8.0766e-05,  ...,  1.0389e-03,
          0.0000e+00, -3.3075e-05],
        [ 0.0000e+00, -3.1647e-04, -8.0766e-05,  ...,  1.0389e-03,
          0.0000e+00, -3.3075e-05],
        ...,
        [ 0.0000e+00, -3.1647e-04, -8.0766e-05,  ...,  1.0389e-03,
          0.0000e+00, -3.3075e-05],
        [ 0.0000e+00, -3.1647e-04, -8.0766e-05,  ...,  1.0389e-03,
          0.0000e+00, -3.3075e-05],
        [ 0.0000e+00, -3.1647e-04, -8.0766e-05,  ...,  1.0389e-03,
          0.0000e+00, -3.3075e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2370.7944, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.0307, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.8363, device='cuda:0')



h[100].sum tensor(-12.7738, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.6281, device='cuda:0')



h[200].sum tensor(-11.6586, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.5766, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0034, 0.0086,  ..., 0.0250, 0.0000, 0.0041],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58038.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0227, 0.0255, 0.0301,  ..., 0.0264, 0.0000, 0.0000],
        [0.0679, 0.0134, 0.0528,  ..., 0.0554, 0.0000, 0.0000],
        [0.1404, 0.0027, 0.0888,  ..., 0.1022, 0.0000, 0.0000],
        ...,
        [0.0060, 0.0394, 0.0224,  ..., 0.0158, 0.0000, 0.0000],
        [0.0060, 0.0394, 0.0224,  ..., 0.0158, 0.0000, 0.0000],
        [0.0060, 0.0394, 0.0224,  ..., 0.0158, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(649967.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11427.8105, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.3667, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6565.0039, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-602.8450, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9371.7080, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(586.4011, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2377],
        [ 0.0061],
        [ 0.1458],
        ...,
        [-1.9369],
        [-1.9496],
        [-1.9588]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-278424.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0079],
        [1.0110],
        [1.0180],
        ...,
        [1.0002],
        [0.9991],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368369.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(240.7162, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0079],
        [1.0111],
        [1.0181],
        ...,
        [1.0002],
        [0.9991],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368378.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(240.7162, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.2073e-04, -7.1149e-05,  ...,  1.0587e-03,
          0.0000e+00, -3.6325e-05],
        [ 0.0000e+00, -3.2073e-04, -7.1149e-05,  ...,  1.0587e-03,
          0.0000e+00, -3.6325e-05],
        [ 0.0000e+00, -3.2073e-04, -7.1149e-05,  ...,  1.0587e-03,
          0.0000e+00, -3.6325e-05],
        ...,
        [ 0.0000e+00, -3.2073e-04, -7.1149e-05,  ...,  1.0587e-03,
          0.0000e+00, -3.6325e-05],
        [ 0.0000e+00, -3.2073e-04, -7.1149e-05,  ...,  1.0587e-03,
          0.0000e+00, -3.6325e-05],
        [ 0.0000e+00, -3.2073e-04, -7.1149e-05,  ...,  1.0587e-03,
          0.0000e+00, -3.6325e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2426.4009, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.3492, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.1573, device='cuda:0')



h[100].sum tensor(-13.7027, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.9709, device='cuda:0')



h[200].sum tensor(-11.0768, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.3968, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59339.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0508, 0.0088, 0.0440,  ..., 0.0448, 0.0000, 0.0000],
        [0.0577, 0.0073, 0.0475,  ..., 0.0493, 0.0000, 0.0000],
        [0.0579, 0.0073, 0.0477,  ..., 0.0495, 0.0000, 0.0000],
        ...,
        [0.0060, 0.0393, 0.0223,  ..., 0.0160, 0.0000, 0.0000],
        [0.0060, 0.0393, 0.0223,  ..., 0.0160, 0.0000, 0.0000],
        [0.0060, 0.0393, 0.0222,  ..., 0.0160, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(653817.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11534.4697, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.9813, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6459.3267, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-612.6066, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9439.1934, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(598.9885, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0130],
        [ 0.1019],
        [ 0.1617],
        ...,
        [-1.9893],
        [-1.9836],
        [-1.9817]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-272673.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0079],
        [1.0111],
        [1.0181],
        ...,
        [1.0002],
        [0.9991],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368378.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2898],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.2838, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0080],
        [1.0112],
        [1.0182],
        ...,
        [1.0002],
        [0.9991],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368386.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2898],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.2838, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.9304e-03,  1.1830e-03,  3.2022e-03,  ...,  8.8393e-03,
         -1.2174e-03,  1.5321e-03],
        [ 0.0000e+00, -3.1884e-04, -6.2745e-05,  ...,  1.0683e-03,
          0.0000e+00, -4.3289e-05],
        [-5.9304e-03,  1.1830e-03,  3.2022e-03,  ...,  8.8393e-03,
         -1.2174e-03,  1.5321e-03],
        ...,
        [ 0.0000e+00, -3.1884e-04, -6.2745e-05,  ...,  1.0683e-03,
          0.0000e+00, -4.3289e-05],
        [ 0.0000e+00, -3.1884e-04, -6.2745e-05,  ...,  1.0683e-03,
          0.0000e+00, -4.3289e-05],
        [ 0.0000e+00, -3.1884e-04, -6.2745e-05,  ...,  1.0683e-03,
          0.0000e+00, -4.3289e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2418.1882, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.7629, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.6454, device='cuda:0')



h[100].sum tensor(-13.2825, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.8380, device='cuda:0')



h[200].sum tensor(-11.1071, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.6914, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0020, 0.0050,  ..., 0.0163, 0.0000, 0.0024],
        [0.0000, 0.0064, 0.0165,  ..., 0.0441, 0.0000, 0.0079],
        [0.0000, 0.0020, 0.0050,  ..., 0.0164, 0.0000, 0.0024],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58892.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0743, 0.0041, 0.0545,  ..., 0.0613, 0.0000, 0.0000],
        [0.1125, 0.0000, 0.0731,  ..., 0.0866, 0.0000, 0.0000],
        [0.0781, 0.0037, 0.0563,  ..., 0.0639, 0.0000, 0.0000],
        ...,
        [0.0060, 0.0394, 0.0222,  ..., 0.0162, 0.0000, 0.0000],
        [0.0060, 0.0394, 0.0222,  ..., 0.0162, 0.0000, 0.0000],
        [0.0060, 0.0394, 0.0222,  ..., 0.0162, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(650526.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11606.2070, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-7.1664, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6168.2388, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-611.9561, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9437.9238, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(595.4640, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2858],
        [-0.0683],
        [-0.0952],
        ...,
        [-1.9931],
        [-1.9873],
        [-1.9828]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-242549.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0080],
        [1.0112],
        [1.0182],
        ...,
        [1.0002],
        [0.9991],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368386.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2966],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.6265, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0081],
        [1.0113],
        [1.0183],
        ...,
        [1.0002],
        [0.9991],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368395.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2966],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.6265, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3196e-02,  3.0414e-03,  7.2117e-03,  ...,  1.8360e-02,
         -2.7014e-03,  3.4510e-03],
        [-6.0638e-03,  1.2314e-03,  3.2821e-03,  ...,  9.0054e-03,
         -1.2414e-03,  1.5556e-03],
        [ 0.0000e+00, -3.0755e-04, -5.9061e-05,  ...,  1.0520e-03,
          0.0000e+00, -5.5923e-05],
        ...,
        [ 0.0000e+00, -3.0755e-04, -5.9061e-05,  ...,  1.0520e-03,
          0.0000e+00, -5.5923e-05],
        [ 0.0000e+00, -3.0755e-04, -5.9061e-05,  ...,  1.0520e-03,
          0.0000e+00, -5.5923e-05],
        [ 0.0000e+00, -3.0755e-04, -5.9061e-05,  ...,  1.0520e-03,
          0.0000e+00, -5.5923e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2320.8574, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.7502, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.6485, device='cuda:0')



h[100].sum tensor(-11.1456, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.0604, device='cuda:0')



h[200].sum tensor(-11.8378, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.5619, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0037, 0.0092,  ..., 0.0265, 0.0000, 0.0044],
        [0.0000, 0.0040, 0.0100,  ..., 0.0282, 0.0000, 0.0048],
        [0.0000, 0.0064, 0.0164,  ..., 0.0439, 0.0000, 0.0078],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54721.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1397, 0.0000, 0.0889,  ..., 0.1033, 0.0000, 0.0000],
        [0.1311, 0.0000, 0.0837,  ..., 0.0985, 0.0000, 0.0000],
        [0.1488, 0.0000, 0.0920,  ..., 0.1109, 0.0000, 0.0000],
        ...,
        [0.0058, 0.0400, 0.0224,  ..., 0.0163, 0.0000, 0.0000],
        [0.0058, 0.0399, 0.0223,  ..., 0.0163, 0.0000, 0.0000],
        [0.0058, 0.0399, 0.0223,  ..., 0.0163, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(637114.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10899.3574, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-27.3812, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6490.4365, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-588.5457, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9232.6387, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(553.5461, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2209],
        [ 0.2243],
        [ 0.2263],
        ...,
        [-2.0218],
        [-2.0160],
        [-2.0141]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-274802.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0081],
        [1.0113],
        [1.0183],
        ...,
        [1.0002],
        [0.9991],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368395.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5649],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(359.7727, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0082],
        [1.0114],
        [1.0184],
        ...,
        [1.0002],
        [0.9991],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368403.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5649],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(359.7727, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.5400e-03,  1.8691e-03,  4.6570e-03,  ...,  1.2251e-02,
         -1.7436e-03,  2.2045e-03],
        [-1.1536e-02,  2.6310e-03,  6.3094e-03,  ...,  1.6185e-02,
         -2.3553e-03,  3.0011e-03],
        [-1.1337e-02,  2.5803e-03,  6.1995e-03,  ...,  1.5923e-02,
         -2.3146e-03,  2.9481e-03],
        ...,
        [ 0.0000e+00, -3.0231e-04, -5.2781e-05,  ...,  1.0383e-03,
          0.0000e+00, -6.6268e-05],
        [ 0.0000e+00, -3.0231e-04, -5.2781e-05,  ...,  1.0383e-03,
          0.0000e+00, -6.6268e-05],
        [ 0.0000e+00, -3.0231e-04, -5.2781e-05,  ...,  1.0383e-03,
          0.0000e+00, -6.6268e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2713.3232, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.6792, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.6324, device='cuda:0')



h[100].sum tensor(-20.2871, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-7.4294, device='cuda:0')



h[200].sum tensor(-6.2216, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-39.4524, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0041, 0.0102,  ..., 0.0286, 0.0000, 0.0048],
        [0.0000, 0.0066, 0.0161,  ..., 0.0430, 0.0000, 0.0077],
        [0.0000, 0.0153, 0.0355,  ..., 0.0893, 0.0000, 0.0170],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71051.7578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1570, 0.0000, 0.0977,  ..., 0.1156, 0.0000, 0.0000],
        [0.2192, 0.0000, 0.1307,  ..., 0.1551, 0.0000, 0.0000],
        [0.3170, 0.0000, 0.1832,  ..., 0.2166, 0.0000, 0.0000],
        ...,
        [0.0055, 0.0404, 0.0223,  ..., 0.0161, 0.0000, 0.0000],
        [0.0055, 0.0404, 0.0223,  ..., 0.0161, 0.0000, 0.0000],
        [0.0055, 0.0404, 0.0222,  ..., 0.0161, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(713961., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(13314.8760, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(37.2699, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6551.6152, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-687.1780, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10565.0693, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(720.2954, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1855],
        [ 0.1870],
        [ 0.1764],
        ...,
        [-2.0464],
        [-2.0404],
        [-2.0384]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-299134.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0082],
        [1.0114],
        [1.0184],
        ...,
        [1.0002],
        [0.9991],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368403.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.9802, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0082],
        [1.0115],
        [1.0186],
        ...,
        [1.0002],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368412.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.9802, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.0650e-04, -4.3818e-05,  ...,  1.0388e-03,
          0.0000e+00, -7.1153e-05],
        [ 0.0000e+00, -3.0650e-04, -4.3818e-05,  ...,  1.0388e-03,
          0.0000e+00, -7.1153e-05],
        [-1.0065e-02,  2.2570e-03,  5.5121e-03,  ...,  1.4267e-02,
         -2.0493e-03,  2.6065e-03],
        ...,
        [ 0.0000e+00, -3.0650e-04, -4.3818e-05,  ...,  1.0388e-03,
          0.0000e+00, -7.1153e-05],
        [ 0.0000e+00, -3.0650e-04, -4.3818e-05,  ...,  1.0388e-03,
          0.0000e+00, -7.1153e-05],
        [ 0.0000e+00, -3.0650e-04, -4.3818e-05,  ...,  1.0388e-03,
          0.0000e+00, -7.1153e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2328.1233, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.1330, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.2337, device='cuda:0')



h[100].sum tensor(-11.4078, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.2123, device='cuda:0')



h[200].sum tensor(-10.8900, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.3683, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0023, 0.0056,  ..., 0.0176, 0.0000, 0.0026],
        [0.0000, 0.0018, 0.0045,  ..., 0.0151, 0.0000, 0.0021],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55092.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0219, 0.0271, 0.0299,  ..., 0.0266, 0.0000, 0.0000],
        [0.0588, 0.0139, 0.0490,  ..., 0.0506, 0.0000, 0.0000],
        [0.1044, 0.0059, 0.0726,  ..., 0.0802, 0.0000, 0.0000],
        ...,
        [0.0054, 0.0407, 0.0221,  ..., 0.0159, 0.0000, 0.0000],
        [0.0053, 0.0407, 0.0221,  ..., 0.0159, 0.0000, 0.0000],
        [0.0053, 0.0407, 0.0221,  ..., 0.0159, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(646859.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10822.9346, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-29.7180, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6881.4355, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-592.2645, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9411.4561, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(553.9128, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0853],
        [-0.9356],
        [-0.5960],
        ...,
        [-2.0606],
        [-2.0547],
        [-2.0485]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-317884., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0082],
        [1.0115],
        [1.0186],
        ...,
        [1.0002],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368412.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2739],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.1105, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0083],
        [1.0116],
        [1.0186],
        ...,
        [1.0002],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368420.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2739],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.1105, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.5159e-02,  3.5571e-03,  8.3426e-03,  ...,  2.0992e-02,
         -3.0782e-03,  3.9613e-03],
        [-3.5062e-02,  8.6357e-03,  1.9340e-02,  ...,  4.7180e-02,
         -7.1198e-03,  9.2595e-03],
        [-1.4322e-02,  3.3436e-03,  7.8803e-03,  ...,  1.9891e-02,
         -2.9083e-03,  3.7386e-03],
        ...,
        [ 0.0000e+00, -3.1084e-04, -3.3171e-05,  ...,  1.0475e-03,
          0.0000e+00, -7.3771e-05],
        [ 0.0000e+00, -3.1084e-04, -3.3171e-05,  ...,  1.0475e-03,
          0.0000e+00, -7.3771e-05],
        [ 0.0000e+00, -3.1084e-04, -3.3171e-05,  ...,  1.0475e-03,
          0.0000e+00, -7.3771e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2458.5747, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.0948, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.9845, device='cuda:0')



h[100].sum tensor(-14.2039, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.1855, device='cuda:0')



h[200].sum tensor(-9.1299, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.5366, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0177, 0.0409,  ..., 0.1020, 0.0000, 0.0195],
        [0.0000, 0.0137, 0.0323,  ..., 0.0814, 0.0000, 0.0153],
        [0.0000, 0.0143, 0.0330,  ..., 0.0830, 0.0000, 0.0157],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59858.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3216, 0.0000, 0.1863,  ..., 0.2203, 0.0000, 0.0000],
        [0.3159, 0.0000, 0.1833,  ..., 0.2166, 0.0000, 0.0000],
        [0.2835, 0.0000, 0.1670,  ..., 0.1952, 0.0000, 0.0000],
        ...,
        [0.0052, 0.0408, 0.0219,  ..., 0.0159, 0.0000, 0.0000],
        [0.0052, 0.0408, 0.0219,  ..., 0.0158, 0.0000, 0.0000],
        [0.0052, 0.0408, 0.0219,  ..., 0.0158, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(662134.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11552.5146, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-9.2590, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6523.1953, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-622.4892, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9733.5361, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(603.3202, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1649],
        [ 0.1472],
        [ 0.0138],
        ...,
        [-2.0750],
        [-2.0690],
        [-2.0669]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-293745.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0083],
        [1.0116],
        [1.0186],
        ...,
        [1.0002],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368420.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3130],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.7480, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0084],
        [1.0117],
        [1.0187],
        ...,
        [1.0003],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368428.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3130],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.7480, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1039e-02,  2.5095e-03,  6.0813e-03,  ...,  1.5596e-02,
         -2.2355e-03,  2.8649e-03],
        [-6.3707e-03,  1.3161e-03,  3.4995e-03,  ...,  9.4477e-03,
         -1.2901e-03,  1.6215e-03],
        [ 0.0000e+00, -3.1245e-04, -2.3865e-05,  ...,  1.0568e-03,
          0.0000e+00, -7.5334e-05],
        ...,
        [ 0.0000e+00, -3.1245e-04, -2.3865e-05,  ...,  1.0568e-03,
          0.0000e+00, -7.5334e-05],
        [ 0.0000e+00, -3.1245e-04, -2.3865e-05,  ...,  1.0568e-03,
          0.0000e+00, -7.5334e-05],
        [ 0.0000e+00, -3.1245e-04, -2.3865e-05,  ...,  1.0568e-03,
          0.0000e+00, -7.5334e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2379.5061, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.2596, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.3294, device='cuda:0')



h[100].sum tensor(-12.1953, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.4966, device='cuda:0')



h[200].sum tensor(-10.1000, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.8781, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0082, 0.0204,  ..., 0.0531, 0.0000, 0.0096],
        [0.0000, 0.0045, 0.0111,  ..., 0.0309, 0.0000, 0.0052],
        [0.0000, 0.0013, 0.0035,  ..., 0.0128, 0.0000, 0.0016],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56798.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1586, 0.0000, 0.0982,  ..., 0.1177, 0.0000, 0.0000],
        [0.1139, 0.0027, 0.0758,  ..., 0.0879, 0.0000, 0.0000],
        [0.0606, 0.0128, 0.0489,  ..., 0.0527, 0.0000, 0.0000],
        ...,
        [0.0051, 0.0409, 0.0218,  ..., 0.0158, 0.0000, 0.0000],
        [0.0051, 0.0409, 0.0218,  ..., 0.0158, 0.0000, 0.0000],
        [0.0051, 0.0409, 0.0218,  ..., 0.0158, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(654252.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11058.3066, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-25.9439, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6671.6890, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-605.4951, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9559.4561, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(568.8997, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1333],
        [ 0.0083],
        [-0.2245],
        ...,
        [-2.0866],
        [-2.0806],
        [-2.0786]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-305040.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0084],
        [1.0117],
        [1.0187],
        ...,
        [1.0003],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368428.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.0267, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0084],
        [1.0117],
        [1.0187],
        ...,
        [1.0003],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368437.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.0267, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.1983e-04, -1.4596e-05,  ...,  1.0744e-03,
          0.0000e+00, -6.9341e-05],
        [ 0.0000e+00, -3.1983e-04, -1.4596e-05,  ...,  1.0744e-03,
          0.0000e+00, -6.9341e-05],
        [ 0.0000e+00, -3.1983e-04, -1.4596e-05,  ...,  1.0744e-03,
          0.0000e+00, -6.9341e-05],
        ...,
        [ 0.0000e+00, -3.1983e-04, -1.4596e-05,  ...,  1.0744e-03,
          0.0000e+00, -6.9341e-05],
        [ 0.0000e+00, -3.1983e-04, -1.4596e-05,  ...,  1.0744e-03,
          0.0000e+00, -6.9341e-05],
        [ 0.0000e+00, -3.1983e-04, -1.4596e-05,  ...,  1.0744e-03,
          0.0000e+00, -6.9341e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2352.3906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.9008, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.8395, device='cuda:0')



h[100].sum tensor(-11.2309, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.1100, device='cuda:0')



h[200].sum tensor(-10.6283, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.8251, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54917.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0046, 0.0399, 0.0206,  ..., 0.0150, 0.0000, 0.0000],
        [0.0047, 0.0400, 0.0207,  ..., 0.0151, 0.0000, 0.0000],
        [0.0049, 0.0401, 0.0209,  ..., 0.0153, 0.0000, 0.0000],
        ...,
        [0.0051, 0.0408, 0.0216,  ..., 0.0158, 0.0000, 0.0000],
        [0.0051, 0.0408, 0.0216,  ..., 0.0158, 0.0000, 0.0000],
        [0.0051, 0.0408, 0.0216,  ..., 0.0158, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(645722.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10715.2979, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-35.9371, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6618.2305, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-595.8088, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9387.9365, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(548.4237, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1692],
        [-1.5607],
        [-1.8431],
        ...,
        [-2.0918],
        [-2.0858],
        [-2.0838]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-300631.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0084],
        [1.0117],
        [1.0187],
        ...,
        [1.0003],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368437.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.9727, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0085],
        [1.0118],
        [1.0188],
        ...,
        [1.0003],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368446.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.9727, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.2689e-04, -3.7035e-06,  ...,  1.0981e-03,
          0.0000e+00, -5.9157e-05],
        [ 0.0000e+00, -3.2689e-04, -3.7035e-06,  ...,  1.0981e-03,
          0.0000e+00, -5.9157e-05],
        [ 0.0000e+00, -3.2689e-04, -3.7035e-06,  ...,  1.0981e-03,
          0.0000e+00, -5.9157e-05],
        ...,
        [ 0.0000e+00, -3.2689e-04, -3.7035e-06,  ...,  1.0981e-03,
          0.0000e+00, -5.9157e-05],
        [ 0.0000e+00, -3.2689e-04, -3.7035e-06,  ...,  1.0981e-03,
          0.0000e+00, -5.9157e-05],
        [ 0.0000e+00, -3.2689e-04, -3.7035e-06,  ...,  1.0981e-03,
          0.0000e+00, -5.9157e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2475.8516, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.1465, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.4165, device='cuda:0')



h[100].sum tensor(-13.5183, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.0381, device='cuda:0')



h[200].sum tensor(-9.3568, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.7539, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60179.1445, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0046, 0.0397, 0.0204,  ..., 0.0151, 0.0000, 0.0000],
        [0.0054, 0.0394, 0.0207,  ..., 0.0157, 0.0000, 0.0000],
        [0.0074, 0.0382, 0.0216,  ..., 0.0173, 0.0000, 0.0000],
        ...,
        [0.0089, 0.0380, 0.0232,  ..., 0.0186, 0.0000, 0.0000],
        [0.0166, 0.0324, 0.0268,  ..., 0.0240, 0.0000, 0.0000],
        [0.0205, 0.0297, 0.0285,  ..., 0.0267, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(667984.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11687.9678, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-14.4834, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6189.3916, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-629.2014, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9787.6719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(602.6021, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0088],
        [-1.8339],
        [-1.5788],
        ...,
        [-1.8749],
        [-1.7162],
        [-1.6133]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-261289.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0085],
        [1.0118],
        [1.0188],
        ...,
        [1.0003],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368446.6562, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 290.0 event: 1450 loss: tensor(493.5947, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(164.5758, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0085],
        [1.0118],
        [1.0188],
        ...,
        [1.0003],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368455.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(164.5758, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.3222e-04,  6.0095e-06,  ...,  1.1142e-03,
          0.0000e+00, -4.7098e-05],
        [ 0.0000e+00, -3.3222e-04,  6.0095e-06,  ...,  1.1142e-03,
          0.0000e+00, -4.7098e-05],
        [-7.8708e-03,  1.6892e-03,  4.3713e-03,  ...,  1.1515e-02,
         -1.5809e-03,  2.0547e-03],
        ...,
        [ 0.0000e+00, -3.3222e-04,  6.0095e-06,  ...,  1.1142e-03,
          0.0000e+00, -4.7098e-05],
        [ 0.0000e+00, -3.3222e-04,  6.0095e-06,  ...,  1.1142e-03,
          0.0000e+00, -4.7098e-05],
        [ 0.0000e+00, -3.3222e-04,  6.0095e-06,  ...,  1.1142e-03,
          0.0000e+00, -4.7098e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2302.2961, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.0919, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.0977, device='cuda:0')



h[100].sum tensor(-9.2400, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.3985, device='cuda:0')



h[200].sum tensor(-11.8396, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.0473, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 2.4193e-05,  ..., 4.4855e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.7074e-03, 4.4364e-03,  ..., 1.5011e-02, 0.0000e+00,
         2.0768e-03],
        [0.0000e+00, 6.1602e-03, 1.4783e-02,  ..., 3.9680e-02, 0.0000e+00,
         7.0105e-03],
        ...,
        [0.0000e+00, 0.0000e+00, 2.4878e-05,  ..., 4.6124e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.4868e-05,  ..., 4.6106e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.4867e-05,  ..., 4.6104e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53267.9922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0392, 0.0146, 0.0373,  ..., 0.0387, 0.0000, 0.0000],
        [0.0808, 0.0112, 0.0592,  ..., 0.0657, 0.0000, 0.0000],
        [0.1826, 0.0005, 0.1131,  ..., 0.1313, 0.0000, 0.0000],
        ...,
        [0.0050, 0.0407, 0.0212,  ..., 0.0159, 0.0000, 0.0000],
        [0.0050, 0.0407, 0.0212,  ..., 0.0159, 0.0000, 0.0000],
        [0.0050, 0.0407, 0.0212,  ..., 0.0159, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(646764.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10543.0762, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-48.3898, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6591.9414, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-588.6262, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9332.1367, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(527.3219, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1595],
        [-0.0350],
        [ 0.0796],
        ...,
        [-2.0986],
        [-2.0926],
        [-2.0906]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-301006.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0085],
        [1.0118],
        [1.0188],
        ...,
        [1.0003],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368455.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(261.4150, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0086],
        [1.0119],
        [1.0188],
        ...,
        [1.0003],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368463.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(261.4150, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.3819e-02,  5.7952e-03,  1.3219e-02,  ...,  3.2626e-02,
         -4.7712e-03,  6.3331e-03],
        [-2.1784e-02,  5.2717e-03,  1.2089e-02,  ...,  2.9933e-02,
         -4.3634e-03,  5.7890e-03],
        [-2.0426e-02,  4.9226e-03,  1.1335e-02,  ...,  2.8137e-02,
         -4.0915e-03,  5.4261e-03],
        ...,
        [ 0.0000e+00, -3.3034e-04, -3.7025e-06,  ...,  1.1149e-03,
          0.0000e+00, -3.4652e-05],
        [ 0.0000e+00, -3.3034e-04, -3.7025e-06,  ...,  1.1149e-03,
          0.0000e+00, -3.4652e-05],
        [ 0.0000e+00, -3.3034e-04, -3.7025e-06,  ...,  1.1149e-03,
          0.0000e+00, -3.4652e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2556.2585, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.6880, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.8046, device='cuda:0')



h[100].sum tensor(-14.5955, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.3983, device='cuda:0')



h[200].sum tensor(-8.6327, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.6666, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0119, 0.0277,  ..., 0.0706, 0.0000, 0.0133],
        [0.0000, 0.0169, 0.0394,  ..., 0.0983, 0.0000, 0.0188],
        [0.0000, 0.0185, 0.0428,  ..., 0.1065, 0.0000, 0.0205],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61648.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3484, 0.0000, 0.2007,  ..., 0.2391, 0.0000, 0.0000],
        [0.3910, 0.0000, 0.2231,  ..., 0.2668, 0.0000, 0.0000],
        [0.4063, 0.0000, 0.2310,  ..., 0.2768, 0.0000, 0.0000],
        ...,
        [0.0050, 0.0409, 0.0214,  ..., 0.0160, 0.0000, 0.0000],
        [0.0050, 0.0409, 0.0214,  ..., 0.0160, 0.0000, 0.0000],
        [0.0050, 0.0409, 0.0214,  ..., 0.0160, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(678219.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11863.7402, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-12.8753, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6266.7280, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-640.0547, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9972.6865, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(615.6094, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1092],
        [ 0.1098],
        [ 0.1117],
        ...,
        [-2.1104],
        [-2.1043],
        [-2.1023]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-280924.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0086],
        [1.0119],
        [1.0188],
        ...,
        [1.0003],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368463.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2571],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(208.9922, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0087],
        [1.0119],
        [1.0189],
        ...,
        [1.0003],
        [0.9993],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368472.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2571],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(208.9922, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1939e-02,  2.7420e-03,  6.6221e-03,  ...,  1.6924e-02,
         -2.3848e-03,  3.1723e-03],
        [-5.2046e-03,  1.0083e-03,  2.8804e-03,  ...,  8.0054e-03,
         -1.0397e-03,  1.3699e-03],
        [-5.0489e-03,  9.6825e-04,  2.7939e-03,  ...,  7.7992e-03,
         -1.0086e-03,  1.3282e-03],
        ...,
        [ 0.0000e+00, -3.3158e-04, -1.1490e-05,  ...,  1.1123e-03,
          0.0000e+00, -2.3175e-05],
        [ 0.0000e+00, -3.3158e-04, -1.1490e-05,  ...,  1.1123e-03,
          0.0000e+00, -2.3175e-05],
        [ 0.0000e+00, -3.3158e-04, -1.1490e-05,  ...,  1.1123e-03,
          0.0000e+00, -2.3175e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2431.4795, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.5361, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.6326, device='cuda:0')



h[100].sum tensor(-11.6619, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.3157, device='cuda:0')



h[200].sum tensor(-10.1584, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.9179, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0047, 0.0130,  ..., 0.0357, 0.0000, 0.0062],
        [0.0000, 0.0070, 0.0180,  ..., 0.0475, 0.0000, 0.0086],
        [0.0000, 0.0056, 0.0149,  ..., 0.0400, 0.0000, 0.0071],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56978.1328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1727, 0.0000, 0.1043,  ..., 0.1291, 0.0000, 0.0000],
        [0.1828, 0.0000, 0.1098,  ..., 0.1355, 0.0000, 0.0000],
        [0.1638, 0.0000, 0.1001,  ..., 0.1231, 0.0000, 0.0000],
        ...,
        [0.0050, 0.0412, 0.0216,  ..., 0.0161, 0.0000, 0.0000],
        [0.0050, 0.0412, 0.0216,  ..., 0.0161, 0.0000, 0.0000],
        [0.0050, 0.0412, 0.0216,  ..., 0.0161, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(659898., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11077.6562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-32.9234, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6448.5327, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-612.2779, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9632.7520, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(566.3649, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1921],
        [ 0.1930],
        [ 0.1946],
        ...,
        [-2.1255],
        [-2.1194],
        [-2.1173]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-295905.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0087],
        [1.0119],
        [1.0189],
        ...,
        [1.0003],
        [0.9993],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368472.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3762],
        [0.4150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.9767, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0088],
        [1.0120],
        [1.0190],
        ...,
        [1.0003],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368480.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3762],
        [0.4150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.9767, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.6866e-02,  4.0153e-03,  9.3578e-03,  ...,  2.3469e-02,
         -3.3598e-03,  4.5102e-03],
        [-7.6084e-03,  1.6292e-03,  4.2096e-03,  ...,  1.1194e-02,
         -1.5157e-03,  2.0294e-03],
        [-8.3934e-03,  1.8315e-03,  4.6462e-03,  ...,  1.2235e-02,
         -1.6720e-03,  2.2397e-03],
        ...,
        [ 0.0000e+00, -3.3186e-04, -2.1598e-05,  ...,  1.1066e-03,
          0.0000e+00, -9.5362e-06],
        [ 0.0000e+00, -3.3186e-04, -2.1598e-05,  ...,  1.1066e-03,
          0.0000e+00, -9.5362e-06],
        [ 0.0000e+00, -3.3186e-04, -2.1598e-05,  ...,  1.1066e-03,
          0.0000e+00, -9.5362e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2469.6362, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.4730, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.5864, device='cuda:0')



h[100].sum tensor(-12.3179, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.5632, device='cuda:0')



h[200].sum tensor(-9.6700, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.2321, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0094, 0.0231,  ..., 0.0597, 0.0000, 0.0111],
        [0.0000, 0.0104, 0.0253,  ..., 0.0649, 0.0000, 0.0122],
        [0.0000, 0.0060, 0.0150,  ..., 0.0403, 0.0000, 0.0072],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57896.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1831, 0.0000, 0.1128,  ..., 0.1338, 0.0000, 0.0000],
        [0.1846, 0.0000, 0.1139,  ..., 0.1346, 0.0000, 0.0000],
        [0.1542, 0.0000, 0.0981,  ..., 0.1148, 0.0000, 0.0000],
        ...,
        [0.0050, 0.0415, 0.0219,  ..., 0.0162, 0.0000, 0.0000],
        [0.0050, 0.0415, 0.0219,  ..., 0.0162, 0.0000, 0.0000],
        [0.0050, 0.0415, 0.0219,  ..., 0.0162, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(661889.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11280.5811, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-26.9776, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6345.1343, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-617.9807, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9707.2080, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(577.4979, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1792],
        [ 0.1153],
        [-0.0288],
        ...,
        [-2.1401],
        [-2.1339],
        [-2.1319]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-282730.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0088],
        [1.0120],
        [1.0190],
        ...,
        [1.0003],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368480.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2844],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.6323, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0088],
        [1.0121],
        [1.0190],
        ...,
        [1.0003],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368488.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2844],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.6323, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.8676e-03,  1.9472e-03,  4.9097e-03,  ...,  1.2880e-02,
         -1.7617e-03,  2.3851e-03],
        [-5.7457e-03,  1.1418e-03,  3.1719e-03,  ...,  8.7363e-03,
         -1.1415e-03,  1.5475e-03],
        [-4.4578e-03,  8.0951e-04,  2.4550e-03,  ...,  7.0267e-03,
         -8.8559e-04,  1.2019e-03],
        ...,
        [ 0.0000e+00, -3.4054e-04, -2.6358e-05,  ...,  1.1093e-03,
          0.0000e+00,  5.7676e-06],
        [ 0.0000e+00, -3.4054e-04, -2.6358e-05,  ...,  1.1093e-03,
          0.0000e+00,  5.7676e-06],
        [ 0.0000e+00, -3.4054e-04, -2.6358e-05,  ...,  1.1093e-03,
          0.0000e+00,  5.7676e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2675.8975, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.6159, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.0849, device='cuda:0')



h[100].sum tensor(-16.6422, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.2494, device='cuda:0')



h[200].sum tensor(-7.1640, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-33.1864, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 6.8380e-03, 1.6894e-02,  ..., 4.4945e-02, 0.0000e+00,
         8.2053e-03],
        [0.0000e+00, 5.6871e-03, 1.4417e-02,  ..., 3.9051e-02, 0.0000e+00,
         7.0114e-03],
        [0.0000e+00, 1.0504e-02, 2.5536e-02,  ..., 6.5647e-02, 0.0000e+00,
         1.2384e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.5945e-03, 0.0000e+00,
         2.3887e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.5929e-03, 0.0000e+00,
         2.3879e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.5926e-03, 0.0000e+00,
         2.3878e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65063.5195, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2731, 0.0000, 0.1607,  ..., 0.1920, 0.0000, 0.0000],
        [0.2230, 0.0000, 0.1333,  ..., 0.1603, 0.0000, 0.0000],
        [0.2452, 0.0000, 0.1449,  ..., 0.1750, 0.0000, 0.0000],
        ...,
        [0.0050, 0.0417, 0.0218,  ..., 0.0162, 0.0000, 0.0000],
        [0.0050, 0.0417, 0.0218,  ..., 0.0162, 0.0000, 0.0000],
        [0.0050, 0.0417, 0.0218,  ..., 0.0162, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(693883.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12190.6230, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-0.9569, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6479.6729, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-660.8740, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10286.4414, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(649.1988, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0631],
        [ 0.0856],
        [ 0.0990],
        ...,
        [-2.1527],
        [-2.1466],
        [-2.1445]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-310597.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0088],
        [1.0121],
        [1.0190],
        ...,
        [1.0003],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368488.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.4102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0089],
        [1.0122],
        [1.0191],
        ...,
        [1.0003],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368497.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.4102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.5176e-04, -2.7953e-05,  ...,  1.1204e-03,
          0.0000e+00,  2.5769e-05],
        [ 0.0000e+00, -3.5176e-04, -2.7953e-05,  ...,  1.1204e-03,
          0.0000e+00,  2.5769e-05],
        [ 0.0000e+00, -3.5176e-04, -2.7953e-05,  ...,  1.1204e-03,
          0.0000e+00,  2.5769e-05],
        ...,
        [ 0.0000e+00, -3.5176e-04, -2.7953e-05,  ...,  1.1204e-03,
          0.0000e+00,  2.5769e-05],
        [ 0.0000e+00, -3.5176e-04, -2.7953e-05,  ...,  1.1204e-03,
          0.0000e+00,  2.5769e-05],
        [ 0.0000e+00, -3.5176e-04, -2.7953e-05,  ...,  1.1204e-03,
          0.0000e+00,  2.5769e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2386.8977, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.1962, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.2783, device='cuda:0')



h[100].sum tensor(-10.0003, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.7049, device='cuda:0')



h[200].sum tensor(-10.9788, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.6740, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0074, 0.0175,  ..., 0.0464, 0.0000, 0.0086],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0001],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0001],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0001],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0001],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0001]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54500.0352, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1668, 0.0011, 0.1054,  ..., 0.1220, 0.0000, 0.0000],
        [0.0628, 0.0157, 0.0508,  ..., 0.0541, 0.0000, 0.0000],
        [0.0282, 0.0243, 0.0328,  ..., 0.0315, 0.0000, 0.0000],
        ...,
        [0.0051, 0.0418, 0.0218,  ..., 0.0163, 0.0000, 0.0000],
        [0.0051, 0.0418, 0.0217,  ..., 0.0163, 0.0000, 0.0000],
        [0.0051, 0.0418, 0.0217,  ..., 0.0163, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(652098.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10693.9102, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-44.5146, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6612.0537, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-597.5462, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9516.1074, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(538.9785, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0363],
        [-0.2351],
        [-0.6769],
        ...,
        [-2.1563],
        [-2.1492],
        [-2.1456]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-314861.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0089],
        [1.0122],
        [1.0191],
        ...,
        [1.0003],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368497.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.7050, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0090],
        [1.0123],
        [1.0191],
        ...,
        [1.0003],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368505.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.7050, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.5946e-04, -2.8929e-05,  ...,  1.1389e-03,
          0.0000e+00,  5.0503e-05],
        [ 0.0000e+00, -3.5946e-04, -2.8929e-05,  ...,  1.1389e-03,
          0.0000e+00,  5.0503e-05],
        [ 0.0000e+00, -3.5946e-04, -2.8929e-05,  ...,  1.1389e-03,
          0.0000e+00,  5.0503e-05],
        ...,
        [ 0.0000e+00, -3.5946e-04, -2.8929e-05,  ...,  1.1389e-03,
          0.0000e+00,  5.0503e-05],
        [ 0.0000e+00, -3.5946e-04, -2.8929e-05,  ...,  1.1389e-03,
          0.0000e+00,  5.0503e-05],
        [ 0.0000e+00, -3.5946e-04, -2.8929e-05,  ...,  1.1389e-03,
          0.0000e+00,  5.0503e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2697.0356, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.0418, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.2948, device='cuda:0')



h[100].sum tensor(-16.2252, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.0444, device='cuda:0')



h[200].sum tensor(-7.5459, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-32.0978, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0002],
        [0.0000, 0.0034, 0.0089,  ..., 0.0259, 0.0000, 0.0045],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66172.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0219, 0.0282, 0.0296,  ..., 0.0275, 0.0000, 0.0000],
        [0.0394, 0.0197, 0.0383,  ..., 0.0389, 0.0000, 0.0000],
        [0.1056, 0.0060, 0.0719,  ..., 0.0830, 0.0000, 0.0000],
        ...,
        [0.0052, 0.0417, 0.0217,  ..., 0.0165, 0.0000, 0.0000],
        [0.0052, 0.0417, 0.0217,  ..., 0.0165, 0.0000, 0.0000],
        [0.0052, 0.0417, 0.0217,  ..., 0.0165, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(695663.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12844.3145, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(6.6456, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5816.6797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-668.3649, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10402.6289, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(662.1583, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0743],
        [-0.8582],
        [-0.4412],
        ...,
        [-2.1585],
        [-2.1525],
        [-2.1504]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-237577.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0090],
        [1.0123],
        [1.0191],
        ...,
        [1.0003],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368505.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2510],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(322.9968, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0090],
        [1.0123],
        [1.0192],
        ...,
        [1.0003],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368513.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2510],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(322.9968, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.0536e-03,  9.4583e-04,  2.7858e-03,  ...,  7.8743e-03,
         -9.9567e-04,  1.4313e-03],
        [-5.2903e-03,  1.0070e-03,  2.9179e-03,  ...,  8.1896e-03,
         -1.0423e-03,  1.4950e-03],
        [-1.0344e-02,  2.3142e-03,  5.7390e-03,  ...,  1.4922e-02,
         -2.0380e-03,  2.8563e-03],
        ...,
        [ 0.0000e+00, -3.6135e-04, -3.5254e-05,  ...,  1.1421e-03,
          0.0000e+00,  7.0010e-05],
        [ 0.0000e+00, -3.6135e-04, -3.5254e-05,  ...,  1.1421e-03,
          0.0000e+00,  7.0010e-05],
        [ 0.0000e+00, -3.6135e-04, -3.5254e-05,  ...,  1.1421e-03,
          0.0000e+00,  7.0010e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2794.8740, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.4384, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.7056, device='cuda:0')



h[100].sum tensor(-17.9060, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.6700, device='cuda:0')



h[200].sum tensor(-6.4111, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-35.4196, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0037, 0.0096,  ..., 0.0276, 0.0000, 0.0049],
        [0.0000, 0.0076, 0.0195,  ..., 0.0515, 0.0000, 0.0098],
        [0.0000, 0.0059, 0.0158,  ..., 0.0426, 0.0000, 0.0080],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69618.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1609, 0.0000, 0.1005,  ..., 0.1200, 0.0000, 0.0000],
        [0.1720, 0.0000, 0.1053,  ..., 0.1281, 0.0000, 0.0000],
        [0.1806, 0.0000, 0.1098,  ..., 0.1336, 0.0000, 0.0000],
        ...,
        [0.0054, 0.0419, 0.0221,  ..., 0.0169, 0.0000, 0.0000],
        [0.0054, 0.0418, 0.0221,  ..., 0.0169, 0.0000, 0.0000],
        [0.0054, 0.0418, 0.0221,  ..., 0.0169, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(718927.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(13421.7637, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(17.7089, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6063.1777, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-691.0193, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10758.5586, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(698.0917, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1933],
        [ 0.2067],
        [ 0.2109],
        ...,
        [-2.1605],
        [-2.1536],
        [-2.1507]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-271217.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0090],
        [1.0123],
        [1.0192],
        ...,
        [1.0003],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368513.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(244.6172, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0091],
        [1.0124],
        [1.0193],
        ...,
        [1.0003],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368522.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(244.6172, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.6383e-04, -4.1688e-05,  ...,  1.1509e-03,
          0.0000e+00,  9.7456e-05],
        [-9.9196e-03,  2.2039e-03,  5.5007e-03,  ...,  1.4381e-02,
         -1.9489e-03,  2.7735e-03],
        [-2.1430e-02,  5.1834e-03,  1.1932e-02,  ...,  2.9734e-02,
         -4.2105e-03,  5.8787e-03],
        ...,
        [ 0.0000e+00, -3.6383e-04, -4.1688e-05,  ...,  1.1509e-03,
          0.0000e+00,  9.7456e-05],
        [ 0.0000e+00, -3.6383e-04, -4.1688e-05,  ...,  1.1509e-03,
          0.0000e+00,  9.7456e-05],
        [ 0.0000e+00, -3.6383e-04, -4.1688e-05,  ...,  1.1509e-03,
          0.0000e+00,  9.7456e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2613.7646, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.1491, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.4678, device='cuda:0')



h[100].sum tensor(-13.4738, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.0514, device='cuda:0')



h[200].sum tensor(-8.8457, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.8245, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0058, 0.0140,  ..., 0.0382, 0.0000, 0.0072],
        [0.0000, 0.0103, 0.0244,  ..., 0.0633, 0.0000, 0.0123],
        [0.0000, 0.0079, 0.0194,  ..., 0.0512, 0.0000, 0.0098],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60743.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1903, 0.0000, 0.1175,  ..., 0.1382, 0.0000, 0.0000],
        [0.2427, 0.0000, 0.1452,  ..., 0.1721, 0.0000, 0.0000],
        [0.2281, 0.0000, 0.1372,  ..., 0.1630, 0.0000, 0.0000],
        ...,
        [0.0057, 0.0419, 0.0225,  ..., 0.0174, 0.0000, 0.0000],
        [0.0057, 0.0419, 0.0225,  ..., 0.0174, 0.0000, 0.0000],
        [0.0057, 0.0419, 0.0225,  ..., 0.0174, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(670711.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11892.6309, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-18.3088, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6081.7236, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-639.8463, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9899.8613, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(608.9930, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1590],
        [ 0.1622],
        [ 0.1645],
        ...,
        [-2.1635],
        [-2.1576],
        [-2.1556]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-256658.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0091],
        [1.0124],
        [1.0193],
        ...,
        [1.0003],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368522.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.2728, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0092],
        [1.0124],
        [1.0194],
        ...,
        [1.0003],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368530.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.2728, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.6832e-04, -4.3954e-05,  ...,  1.1556e-03,
          0.0000e+00,  1.0777e-04],
        [-9.7174e-03,  2.1489e-03,  5.3908e-03,  ...,  1.4132e-02,
         -1.9039e-03,  2.7330e-03],
        [ 0.0000e+00, -3.6832e-04, -4.3954e-05,  ...,  1.1556e-03,
          0.0000e+00,  1.0777e-04],
        ...,
        [ 0.0000e+00, -3.6832e-04, -4.3954e-05,  ...,  1.1556e-03,
          0.0000e+00,  1.0777e-04],
        [ 0.0000e+00, -3.6832e-04, -4.3954e-05,  ...,  1.1556e-03,
          0.0000e+00,  1.0777e-04],
        [ 0.0000e+00, -3.6832e-04, -4.3954e-05,  ...,  1.1556e-03,
          0.0000e+00,  1.0777e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2806.8916, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.6267, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.0909, device='cuda:0')



h[100].sum tensor(-17.3214, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.5105, device='cuda:0')



h[200].sum tensor(-6.5027, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-34.5726, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0077, 0.0197,  ..., 0.0521, 0.0000, 0.0100],
        [0.0000, 0.0017, 0.0044,  ..., 0.0154, 0.0000, 0.0026],
        [0.0000, 0.0022, 0.0055,  ..., 0.0178, 0.0000, 0.0031],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66858.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1263, 0.0000, 0.0832,  ..., 0.0975, 0.0000, 0.0000],
        [0.0795, 0.0004, 0.0593,  ..., 0.0666, 0.0000, 0.0000],
        [0.1028, 0.0000, 0.0717,  ..., 0.0816, 0.0000, 0.0000],
        ...,
        [0.0058, 0.0421, 0.0227,  ..., 0.0175, 0.0000, 0.0000],
        [0.0058, 0.0421, 0.0227,  ..., 0.0175, 0.0000, 0.0000],
        [0.0057, 0.0421, 0.0227,  ..., 0.0175, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(696411.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12732.9561, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(7.1521, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6108.5796, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-677.2568, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10328.2715, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(673.2098, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1324],
        [ 0.1408],
        [ 0.1460],
        ...,
        [-2.1721],
        [-2.1663],
        [-2.1642]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-267178.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0092],
        [1.0124],
        [1.0194],
        ...,
        [1.0003],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368530.1875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 300.0 event: 1500 loss: tensor(502.6651, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.1709, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0092],
        [1.0125],
        [1.0194],
        ...,
        [1.0003],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368538.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.1709, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.7047e-04, -4.2218e-05,  ...,  1.1595e-03,
          0.0000e+00,  1.0746e-04],
        [ 0.0000e+00, -3.7047e-04, -4.2218e-05,  ...,  1.1595e-03,
          0.0000e+00,  1.0746e-04],
        [ 0.0000e+00, -3.7047e-04, -4.2218e-05,  ...,  1.1595e-03,
          0.0000e+00,  1.0746e-04],
        ...,
        [ 0.0000e+00, -3.7047e-04, -4.2218e-05,  ...,  1.1595e-03,
          0.0000e+00,  1.0746e-04],
        [ 0.0000e+00, -3.7047e-04, -4.2218e-05,  ...,  1.1595e-03,
          0.0000e+00,  1.0746e-04],
        [ 0.0000e+00, -3.7047e-04, -4.2218e-05,  ...,  1.1595e-03,
          0.0000e+00,  1.0746e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2675.7102, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.3548, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.7056, device='cuda:0')



h[100].sum tensor(-14.3112, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.3726, device='cuda:0')



h[200].sum tensor(-8.0773, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.5301, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0004],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62910.4648, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0049, 0.0413, 0.0217,  ..., 0.0165, 0.0000, 0.0000],
        [0.0050, 0.0414, 0.0218,  ..., 0.0166, 0.0000, 0.0000],
        [0.0052, 0.0415, 0.0220,  ..., 0.0168, 0.0000, 0.0000],
        ...,
        [0.0057, 0.0423, 0.0228,  ..., 0.0175, 0.0000, 0.0000],
        [0.0073, 0.0411, 0.0235,  ..., 0.0186, 0.0000, 0.0000],
        [0.0129, 0.0372, 0.0262,  ..., 0.0225, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(683021.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12215.2207, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-8.4354, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6192.0029, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-653.3425, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10066.9209, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(631.4955, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9740],
        [-2.1390],
        [-2.2706],
        ...,
        [-2.1595],
        [-2.0995],
        [-1.9860]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-265211.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0092],
        [1.0125],
        [1.0194],
        ...,
        [1.0003],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368538.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.9764, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0093],
        [1.0125],
        [1.0195],
        ...,
        [1.0003],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368545.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.9764, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.7460e-02,  4.1611e-03,  9.7451e-03,  ...,  2.4529e-02,
         -3.4019e-03,  4.8282e-03],
        [-1.0447e-02,  2.3418e-03,  5.8151e-03,  ...,  1.5142e-02,
         -2.0355e-03,  2.9281e-03],
        [-5.1315e-03,  9.6278e-04,  2.8364e-03,  ...,  8.0271e-03,
         -9.9983e-04,  1.4879e-03],
        ...,
        [ 0.0000e+00, -3.6851e-04, -3.9353e-05,  ...,  1.1584e-03,
          0.0000e+00,  9.7521e-05],
        [ 0.0000e+00, -3.6851e-04, -3.9353e-05,  ...,  1.1584e-03,
          0.0000e+00,  9.7521e-05],
        [ 0.0000e+00, -3.6851e-04, -3.9353e-05,  ...,  1.1584e-03,
          0.0000e+00,  9.7521e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2505.5864, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.1326, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.3580, device='cuda:0')



h[100].sum tensor(-10.6354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.9850, device='cuda:0')



h[200].sum tensor(-9.9770, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.1617, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0117, 0.0282,  ..., 0.0725, 0.0000, 0.0141],
        [0.0000, 0.0113, 0.0275,  ..., 0.0707, 0.0000, 0.0138],
        [0.0000, 0.0098, 0.0243,  ..., 0.0631, 0.0000, 0.0122],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55970.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3423, 0.0000, 0.1970,  ..., 0.2360, 0.0000, 0.0000],
        [0.3128, 0.0000, 0.1810,  ..., 0.2176, 0.0000, 0.0000],
        [0.2901, 0.0000, 0.1685,  ..., 0.2033, 0.0000, 0.0000],
        ...,
        [0.0054, 0.0426, 0.0227,  ..., 0.0172, 0.0000, 0.0000],
        [0.0054, 0.0425, 0.0227,  ..., 0.0172, 0.0000, 0.0000],
        [0.0054, 0.0425, 0.0227,  ..., 0.0172, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(656391.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11009.4102, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-37.1368, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6507.5889, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-610.4297, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9510.9238, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(556.6855, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1538],
        [ 0.1701],
        [ 0.1802],
        ...,
        [-2.2018],
        [-2.1958],
        [-2.1937]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-286281.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0093],
        [1.0125],
        [1.0195],
        ...,
        [1.0003],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368545.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(299.5421, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0094],
        [1.0125],
        [1.0196],
        ...,
        [1.0003],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368553.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(299.5421, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4068e-02,  3.2832e-03,  7.8563e-03,  ...,  2.0010e-02,
         -2.7335e-03,  3.9028e-03],
        [-1.4787e-02,  3.4697e-03,  8.2594e-03,  ...,  2.0973e-02,
         -2.8731e-03,  4.0978e-03],
        [-2.0121e-02,  4.8546e-03,  1.1252e-02,  ...,  2.8121e-02,
         -3.9095e-03,  5.5453e-03],
        ...,
        [ 0.0000e+00, -3.6904e-04, -3.5828e-05,  ...,  1.1560e-03,
          0.0000e+00,  8.5370e-05],
        [ 0.0000e+00, -3.6904e-04, -3.5828e-05,  ...,  1.1560e-03,
          0.0000e+00,  8.5370e-05],
        [ 0.0000e+00, -3.6904e-04, -3.5828e-05,  ...,  1.1560e-03,
          0.0000e+00,  8.5370e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2787.2009, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.5493, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.8390, device='cuda:0')



h[100].sum tensor(-16.5445, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.1856, device='cuda:0')



h[200].sum tensor(-6.2876, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-32.8476, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0188, 0.0437,  ..., 0.1094, 0.0000, 0.0215],
        [0.0000, 0.0140, 0.0326,  ..., 0.0828, 0.0000, 0.0162],
        [0.0000, 0.0075, 0.0177,  ..., 0.0471, 0.0000, 0.0089],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0004],
        [0.0000, 0.0013, 0.0036,  ..., 0.0135, 0.0000, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66574.4609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.4643, 0.0000, 0.2644,  ..., 0.3111, 0.0000, 0.0000],
        [0.3554, 0.0000, 0.2061,  ..., 0.2419, 0.0000, 0.0000],
        [0.2394, 0.0000, 0.1443,  ..., 0.1680, 0.0000, 0.0000],
        ...,
        [0.0073, 0.0414, 0.0236,  ..., 0.0183, 0.0000, 0.0000],
        [0.0189, 0.0333, 0.0293,  ..., 0.0261, 0.0000, 0.0000],
        [0.0580, 0.0163, 0.0490,  ..., 0.0520, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(697849., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12756.8984, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(12.8519, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6028.9873, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-671.5173, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10303.7188, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(667.0174, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1579],
        [ 0.1774],
        [ 0.1961],
        ...,
        [-1.9592],
        [-1.5749],
        [-1.0107]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-249247.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0094],
        [1.0125],
        [1.0196],
        ...,
        [1.0003],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368553.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4307],
        [0.2561],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.4669, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0094],
        [1.0126],
        [1.0197],
        ...,
        [1.0003],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368561.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4307],
        [0.2561],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.4669, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4838e-02,  3.4919e-03,  8.2954e-03,  ...,  2.1049e-02,
         -2.8749e-03,  4.0962e-03],
        [-8.6155e-03,  1.8754e-03,  4.8015e-03,  ...,  1.2701e-02,
         -1.6693e-03,  2.4051e-03],
        [-5.1234e-03,  9.6815e-04,  2.8406e-03,  ...,  8.0153e-03,
         -9.9269e-04,  1.4561e-03],
        ...,
        [ 0.0000e+00, -3.6288e-04, -3.6313e-05,  ...,  1.1413e-03,
          0.0000e+00,  6.3723e-05],
        [ 0.0000e+00, -3.6288e-04, -3.6313e-05,  ...,  1.1413e-03,
          0.0000e+00,  6.3723e-05],
        [ 0.0000e+00, -3.6288e-04, -3.6313e-05,  ...,  1.1413e-03,
          0.0000e+00,  6.3723e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2567.3843, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.1901, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.4662, device='cuda:0')



h[100].sum tensor(-12.0722, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.5321, device='cuda:0')



h[200].sum tensor(-8.5187, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.0666, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0099, 0.0244,  ..., 0.0633, 0.0000, 0.0121],
        [0.0000, 0.0078, 0.0199,  ..., 0.0525, 0.0000, 0.0100],
        [0.0000, 0.0039, 0.0115,  ..., 0.0325, 0.0000, 0.0059],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58495.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1891, 0.0000, 0.1151,  ..., 0.1371, 0.0000, 0.0000],
        [0.1715, 0.0000, 0.1053,  ..., 0.1265, 0.0000, 0.0000],
        [0.1432, 0.0000, 0.0896,  ..., 0.1094, 0.0000, 0.0000],
        ...,
        [0.0050, 0.0433, 0.0223,  ..., 0.0165, 0.0000, 0.0000],
        [0.0050, 0.0433, 0.0223,  ..., 0.0165, 0.0000, 0.0000],
        [0.0050, 0.0433, 0.0223,  ..., 0.0165, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(670410.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11291.3711, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-22.7031, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6663.9678, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-621.1108, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9768.6582, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(578.8666, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2414],
        [ 0.2522],
        [ 0.2377],
        ...,
        [-2.2447],
        [-2.2385],
        [-2.2364]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-306557.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0094],
        [1.0126],
        [1.0197],
        ...,
        [1.0003],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368561.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.2531, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0094],
        [1.0126],
        [1.0197],
        ...,
        [1.0004],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368569.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.2531, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.5421e-04, -3.6270e-05,  ...,  1.1329e-03,
          0.0000e+00,  5.0265e-05],
        [ 0.0000e+00, -3.5421e-04, -3.6270e-05,  ...,  1.1329e-03,
          0.0000e+00,  5.0265e-05],
        [-1.4018e-02,  3.2911e-03,  7.8425e-03,  ...,  1.9963e-02,
         -2.7086e-03,  3.8658e-03],
        ...,
        [ 0.0000e+00, -3.5421e-04, -3.6270e-05,  ...,  1.1329e-03,
          0.0000e+00,  5.0265e-05],
        [ 0.0000e+00, -3.5421e-04, -3.6270e-05,  ...,  1.1329e-03,
          0.0000e+00,  5.0265e-05],
        [ 0.0000e+00, -3.5421e-04, -3.6270e-05,  ...,  1.1329e-03,
          0.0000e+00,  5.0265e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2621.6167, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.8073, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.2796, device='cuda:0')



h[100].sum tensor(-13.2028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.0026, device='cuda:0')



h[200].sum tensor(-7.6982, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.5653, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0002],
        [0.0000, 0.0044, 0.0110,  ..., 0.0311, 0.0000, 0.0056],
        [0.0000, 0.0090, 0.0216,  ..., 0.0565, 0.0000, 0.0107],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59911.0547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0412, 0.0211, 0.0402,  ..., 0.0398, 0.0000, 0.0000],
        [0.1285, 0.0062, 0.0857,  ..., 0.0966, 0.0000, 0.0000],
        [0.2309, 0.0000, 0.1394,  ..., 0.1623, 0.0000, 0.0000],
        ...,
        [0.0049, 0.0436, 0.0223,  ..., 0.0163, 0.0000, 0.0000],
        [0.0049, 0.0436, 0.0223,  ..., 0.0163, 0.0000, 0.0000],
        [0.0049, 0.0436, 0.0223,  ..., 0.0163, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(676436.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11576.4707, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-13.3227, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6550.4497, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-628.1155, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9874.8096, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(593.7864, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2446],
        [-0.7419],
        [-0.2763],
        ...,
        [-2.2623],
        [-2.2557],
        [-2.2534]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-297513.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0094],
        [1.0126],
        [1.0197],
        ...,
        [1.0004],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368569.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.2448],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.7903, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0095],
        [1.0126],
        [1.0198],
        ...,
        [1.0003],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368578.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.2448],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.7903, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1813e-02,  2.7282e-03,  6.6145e-03,  ...,  1.7034e-02,
         -2.2761e-03,  3.2803e-03],
        [-1.0558e-02,  2.4016e-03,  5.9086e-03,  ...,  1.5347e-02,
         -2.0343e-03,  2.9382e-03],
        [-1.0692e-02,  2.4365e-03,  5.9840e-03,  ...,  1.5527e-02,
         -2.0601e-03,  2.9747e-03],
        ...,
        [ 0.0000e+00, -3.4673e-04, -3.0945e-05,  ...,  1.1474e-03,
          0.0000e+00,  5.9745e-05],
        [ 0.0000e+00, -3.4673e-04, -3.0945e-05,  ...,  1.1474e-03,
          0.0000e+00,  5.9745e-05],
        [ 0.0000e+00, -3.4673e-04, -3.0945e-05,  ...,  1.1474e-03,
          0.0000e+00,  5.9745e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2750.8135, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.0669, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.5854, device='cuda:0')



h[100].sum tensor(-15.4851, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.8604, device='cuda:0')



h[200].sum tensor(-6.7056, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-31.1202, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0108, 0.0262,  ..., 0.0675, 0.0000, 0.0130],
        [0.0000, 0.0068, 0.0176,  ..., 0.0469, 0.0000, 0.0088],
        [0.0000, 0.0071, 0.0182,  ..., 0.0485, 0.0000, 0.0091],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66191.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2513, 0.0000, 0.1492,  ..., 0.1754, 0.0000, 0.0000],
        [0.1873, 0.0000, 0.1145,  ..., 0.1355, 0.0000, 0.0000],
        [0.1618, 0.0000, 0.1010,  ..., 0.1193, 0.0000, 0.0000],
        ...,
        [0.0051, 0.0436, 0.0223,  ..., 0.0166, 0.0000, 0.0000],
        [0.0051, 0.0436, 0.0223,  ..., 0.0166, 0.0000, 0.0000],
        [0.0051, 0.0436, 0.0223,  ..., 0.0165, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(709740.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12720.6699, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(9.4586, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6460.1543, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-666.9721, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10415.0742, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(657.2760, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0063],
        [-0.1034],
        [-0.2268],
        ...,
        [-2.2622],
        [-2.2559],
        [-2.2537]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-289949.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0095],
        [1.0126],
        [1.0198],
        ...,
        [1.0003],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368578.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3093],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.0848, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0095],
        [1.0126],
        [1.0199],
        ...,
        [1.0003],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368587., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3093],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.0848, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.1680e-03,  1.2753e-03,  3.4448e-03,  ...,  9.4670e-03,
         -1.1851e-03,  1.7631e-03],
        [-4.0782e-03,  7.3070e-04,  2.2682e-03,  ...,  6.6533e-03,
         -7.8355e-04,  1.1924e-03],
        [-1.0246e-02,  2.3379e-03,  5.7410e-03,  ...,  1.4958e-02,
         -1.9686e-03,  2.8768e-03],
        ...,
        [ 0.0000e+00, -3.3199e-04, -2.7894e-05,  ...,  1.1624e-03,
          0.0000e+00,  7.8615e-05],
        [ 0.0000e+00, -3.3199e-04, -2.7894e-05,  ...,  1.1624e-03,
          0.0000e+00,  7.8615e-05],
        [ 0.0000e+00, -3.3199e-04, -2.7894e-05,  ...,  1.1624e-03,
          0.0000e+00,  7.8615e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2539.6836, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.0309, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.3666, device='cuda:0')



h[100].sum tensor(-10.5437, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.9873, device='cuda:0')



h[200].sum tensor(-9.9639, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.1735, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0038, 0.0096,  ..., 0.0277, 0.0000, 0.0050],
        [0.0000, 0.0100, 0.0244,  ..., 0.0632, 0.0000, 0.0122],
        [0.0000, 0.0057, 0.0143,  ..., 0.0392, 0.0000, 0.0073],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56541., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1288, 0.0029, 0.0838,  ..., 0.0979, 0.0000, 0.0000],
        [0.1931, 0.0000, 0.1168,  ..., 0.1397, 0.0000, 0.0000],
        [0.1694, 0.0000, 0.1044,  ..., 0.1246, 0.0000, 0.0000],
        ...,
        [0.0053, 0.0437, 0.0225,  ..., 0.0171, 0.0000, 0.0000],
        [0.0053, 0.0437, 0.0225,  ..., 0.0171, 0.0000, 0.0000],
        [0.0053, 0.0437, 0.0224,  ..., 0.0171, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(667604.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11109.9326, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-35.2739, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6747.1934, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-611.8496, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9599.5293, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(556.4563, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0137],
        [ 0.1827],
        [ 0.2079],
        ...,
        [-2.2552],
        [-2.2490],
        [-2.2468]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-316693.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0095],
        [1.0126],
        [1.0199],
        ...,
        [1.0003],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368587., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.2621, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0096],
        [1.0126],
        [1.0199],
        ...,
        [1.0003],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368595.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.2621, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2196e-02,  2.8868e-03,  6.8361e-03,  ...,  1.7600e-02,
         -2.3367e-03,  3.4333e-03],
        [ 0.0000e+00, -2.9675e-04, -3.5433e-05,  ...,  1.1614e-03,
          0.0000e+00,  9.7128e-05],
        [ 0.0000e+00, -2.9675e-04, -3.5433e-05,  ...,  1.1614e-03,
          0.0000e+00,  9.7128e-05],
        ...,
        [ 0.0000e+00, -2.9675e-04, -3.5433e-05,  ...,  1.1614e-03,
          0.0000e+00,  9.7128e-05],
        [ 0.0000e+00, -2.9675e-04, -3.5433e-05,  ...,  1.1614e-03,
          0.0000e+00,  9.7128e-05],
        [ 0.0000e+00, -2.9675e-04, -3.5433e-05,  ...,  1.1614e-03,
          0.0000e+00,  9.7128e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2899.4180, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.7766, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.1696, device='cuda:0')



h[100].sum tensor(-17.3732, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.5309, device='cuda:0')



h[200].sum tensor(-6.1246, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-34.6811, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0023, 0.0056,  ..., 0.0182, 0.0000, 0.0031],
        [0.0000, 0.0029, 0.0069,  ..., 0.0213, 0.0000, 0.0038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0004],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69147.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1212, 0.0047, 0.0818,  ..., 0.0927, 0.0000, 0.0000],
        [0.0824, 0.0070, 0.0617,  ..., 0.0677, 0.0000, 0.0000],
        [0.0550, 0.0117, 0.0476,  ..., 0.0501, 0.0000, 0.0000],
        ...,
        [0.0059, 0.0439, 0.0232,  ..., 0.0179, 0.0000, 0.0000],
        [0.0059, 0.0439, 0.0232,  ..., 0.0179, 0.0000, 0.0000],
        [0.0059, 0.0439, 0.0232,  ..., 0.0179, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(719675.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(13352.9492, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(14.8772, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6222.8394, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-691.3846, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10592.7930, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(692.1013, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1875],
        [ 0.1615],
        [ 0.1474],
        ...,
        [-2.2508],
        [-2.2448],
        [-2.2427]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-273783.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0096],
        [1.0126],
        [1.0199],
        ...,
        [1.0003],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368595.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.2311, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0096],
        [1.0126],
        [1.0199],
        ...,
        [1.0003],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368595.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.2311, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.9675e-04, -3.5433e-05,  ...,  1.1614e-03,
          0.0000e+00,  9.7128e-05],
        [ 0.0000e+00, -2.9675e-04, -3.5433e-05,  ...,  1.1614e-03,
          0.0000e+00,  9.7128e-05],
        [ 0.0000e+00, -2.9675e-04, -3.5433e-05,  ...,  1.1614e-03,
          0.0000e+00,  9.7128e-05],
        ...,
        [ 0.0000e+00, -2.9675e-04, -3.5433e-05,  ...,  1.1614e-03,
          0.0000e+00,  9.7128e-05],
        [ 0.0000e+00, -2.9675e-04, -3.5433e-05,  ...,  1.1614e-03,
          0.0000e+00,  9.7128e-05],
        [ 0.0000e+00, -2.9675e-04, -3.5433e-05,  ...,  1.1614e-03,
          0.0000e+00,  9.7128e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2705.9585, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.0978, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.3575, device='cuda:0')



h[100].sum tensor(-13.3912, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.0228, device='cuda:0')



h[200].sum tensor(-8.4420, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.6725, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0004],
        [0.0000, 0.0010, 0.0028,  ..., 0.0115, 0.0000, 0.0018],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62212.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0066, 0.0418, 0.0229,  ..., 0.0181, 0.0000, 0.0000],
        [0.0167, 0.0350, 0.0279,  ..., 0.0250, 0.0000, 0.0000],
        [0.0426, 0.0205, 0.0406,  ..., 0.0426, 0.0000, 0.0000],
        ...,
        [0.0059, 0.0439, 0.0232,  ..., 0.0179, 0.0000, 0.0000],
        [0.0059, 0.0439, 0.0232,  ..., 0.0179, 0.0000, 0.0000],
        [0.0059, 0.0439, 0.0232,  ..., 0.0179, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(683918.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12200.8750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-12.2809, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6205.0811, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-649.9006, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10003.9639, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(621.2708, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0260],
        [-1.6594],
        [-1.1505],
        ...,
        [-2.2507],
        [-2.2448],
        [-2.2427]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-265430.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0096],
        [1.0126],
        [1.0199],
        ...,
        [1.0003],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368595.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2646],
        [0.0000],
        [0.2510],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.5781, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0097],
        [1.0127],
        [1.0200],
        ...,
        [1.0003],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368604.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2646],
        [0.0000],
        [0.2510],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.5781, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.5191e-04, -4.3349e-05,  ...,  1.1503e-03,
          0.0000e+00,  1.0218e-04],
        [-1.0259e-02,  2.4320e-03,  5.7407e-03,  ...,  1.4992e-02,
         -1.9600e-03,  2.9127e-03],
        [-5.9991e-03,  1.3175e-03,  3.3389e-03,  ...,  9.2440e-03,
         -1.1461e-03,  1.7456e-03],
        ...,
        [ 0.0000e+00, -2.5191e-04, -4.3349e-05,  ...,  1.1503e-03,
          0.0000e+00,  1.0218e-04],
        [ 0.0000e+00, -2.5191e-04, -4.3349e-05,  ...,  1.1503e-03,
          0.0000e+00,  1.0218e-04],
        [ 0.0000e+00, -2.5191e-04, -4.3349e-05,  ...,  1.1503e-03,
          0.0000e+00,  1.0218e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2827.7461, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.0015, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.7276, device='cuda:0')



h[100].sum tensor(-15.4213, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.8973, device='cuda:0')



h[200].sum tensor(-7.2232, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-31.3163, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0053, 0.0135,  ..., 0.0374, 0.0000, 0.0071],
        [0.0000, 0.0030, 0.0081,  ..., 0.0243, 0.0000, 0.0044],
        [0.0000, 0.0090, 0.0214,  ..., 0.0563, 0.0000, 0.0109],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66651.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1114, 0.0000, 0.0748,  ..., 0.0897, 0.0000, 0.0000],
        [0.1233, 0.0007, 0.0811,  ..., 0.0975, 0.0000, 0.0000],
        [0.2066, 0.0000, 0.1249,  ..., 0.1511, 0.0000, 0.0000],
        ...,
        [0.0062, 0.0444, 0.0241,  ..., 0.0188, 0.0000, 0.0000],
        [0.0062, 0.0444, 0.0241,  ..., 0.0188, 0.0000, 0.0000],
        [0.0062, 0.0444, 0.0241,  ..., 0.0188, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(704377.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(13116.2988, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2.9301, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5987.6099, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-680.7536, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10416.9688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(671.9732, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2347],
        [ 0.2313],
        [ 0.2266],
        ...,
        [-2.2524],
        [-2.2464],
        [-2.2443]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-249628.5469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0097],
        [1.0127],
        [1.0200],
        ...,
        [1.0003],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368604.3750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 310.0 event: 1550 loss: tensor(464.6194, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.1257, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0097],
        [1.0127],
        [1.0200],
        ...,
        [1.0003],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368612.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.1257, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.1957e-04, -4.4676e-05,  ...,  1.1421e-03,
          0.0000e+00,  9.2833e-05],
        [ 0.0000e+00, -2.1957e-04, -4.4676e-05,  ...,  1.1421e-03,
          0.0000e+00,  9.2833e-05],
        [ 0.0000e+00, -2.1957e-04, -4.4676e-05,  ...,  1.1421e-03,
          0.0000e+00,  9.2833e-05],
        ...,
        [ 0.0000e+00, -2.1957e-04, -4.4676e-05,  ...,  1.1421e-03,
          0.0000e+00,  9.2833e-05],
        [ 0.0000e+00, -2.1957e-04, -4.4676e-05,  ...,  1.1421e-03,
          0.0000e+00,  9.2833e-05],
        [ 0.0000e+00, -2.1957e-04, -4.4676e-05,  ...,  1.1421e-03,
          0.0000e+00,  9.2833e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2596.7153, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.8614, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.2107, device='cuda:0')



h[100].sum tensor(-10.4127, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.9468, device='cuda:0')



h[200].sum tensor(-10.0622, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-20.9587, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0004],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56661.4258, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0081, 0.0417, 0.0245,  ..., 0.0204, 0.0000, 0.0000],
        [0.0057, 0.0436, 0.0236,  ..., 0.0186, 0.0000, 0.0000],
        [0.0060, 0.0437, 0.0238,  ..., 0.0189, 0.0000, 0.0000],
        ...,
        [0.0064, 0.0447, 0.0246,  ..., 0.0193, 0.0000, 0.0000],
        [0.0064, 0.0447, 0.0245,  ..., 0.0193, 0.0000, 0.0000],
        [0.0064, 0.0447, 0.0245,  ..., 0.0193, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(663909.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11376.4531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-43.5180, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6426.5977, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-624.3855, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9613.1328, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(568.3911, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3118],
        [-1.2839],
        [-1.1545],
        ...,
        [-2.2597],
        [-2.2537],
        [-2.2516]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-282722.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0097],
        [1.0127],
        [1.0200],
        ...,
        [1.0003],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368612.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(432.5950, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0098],
        [1.0127],
        [1.0201],
        ...,
        [1.0003],
        [0.9992],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368620.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(432.5950, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.9955e-04, -4.0277e-05,  ...,  1.1334e-03,
          0.0000e+00,  6.7147e-05],
        [ 0.0000e+00, -1.9955e-04, -4.0277e-05,  ...,  1.1334e-03,
          0.0000e+00,  6.7147e-05],
        [ 0.0000e+00, -1.9955e-04, -4.0277e-05,  ...,  1.1334e-03,
          0.0000e+00,  6.7147e-05],
        ...,
        [ 0.0000e+00, -1.9955e-04, -4.0277e-05,  ...,  1.1334e-03,
          0.0000e+00,  6.7147e-05],
        [ 0.0000e+00, -1.9955e-04, -4.0277e-05,  ...,  1.1334e-03,
          0.0000e+00,  6.7147e-05],
        [ 0.0000e+00, -1.9955e-04, -4.0277e-05,  ...,  1.1334e-03,
          0.0000e+00,  6.7147e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3225.5479, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-33.0028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.4279, device='cuda:0')



h[100].sum tensor(-23.1144, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-8.9332, device='cuda:0')



h[200].sum tensor(-2.4288, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-47.4380, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0003],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78200.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0052, 0.0440, 0.0235,  ..., 0.0184, 0.0000, 0.0000],
        [0.0053, 0.0441, 0.0236,  ..., 0.0185, 0.0000, 0.0000],
        [0.0055, 0.0442, 0.0238,  ..., 0.0187, 0.0000, 0.0000],
        ...,
        [0.0062, 0.0451, 0.0246,  ..., 0.0194, 0.0000, 0.0000],
        [0.0061, 0.0451, 0.0246,  ..., 0.0194, 0.0000, 0.0000],
        [0.0061, 0.0451, 0.0246,  ..., 0.0194, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(759302.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(14544.6562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(40.7456, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6281.1855, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-755.0033, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11312.0264, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(789.0656, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1816],
        [-2.2414],
        [-2.2373],
        ...,
        [-2.2764],
        [-2.2703],
        [-2.2682]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-280516.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0098],
        [1.0127],
        [1.0201],
        ...,
        [1.0003],
        [0.9992],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368620.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(166.4302, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0098],
        [1.0127],
        [1.0201],
        ...,
        [1.0003],
        [0.9992],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368620.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(166.4302, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.9955e-04, -4.0277e-05,  ...,  1.1334e-03,
          0.0000e+00,  6.7147e-05],
        [ 0.0000e+00, -1.9955e-04, -4.0277e-05,  ...,  1.1334e-03,
          0.0000e+00,  6.7147e-05],
        [-5.2105e-03,  1.1686e-03,  2.9022e-03,  ...,  8.1776e-03,
         -9.8987e-04,  1.4983e-03],
        ...,
        [ 0.0000e+00, -1.9955e-04, -4.0277e-05,  ...,  1.1334e-03,
          0.0000e+00,  6.7147e-05],
        [ 0.0000e+00, -1.9955e-04, -4.0277e-05,  ...,  1.1334e-03,
          0.0000e+00,  6.7147e-05],
        [ 0.0000e+00, -1.9955e-04, -4.0277e-05,  ...,  1.1334e-03,
          0.0000e+00,  6.7147e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2528.1948, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.7902, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.2453, device='cuda:0')



h[100].sum tensor(-8.9580, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.4368, device='cuda:0')



h[200].sum tensor(-10.7016, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.2506, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0003],
        [0.0000, 0.0012, 0.0029,  ..., 0.0117, 0.0000, 0.0017],
        [0.0000, 0.0039, 0.0092,  ..., 0.0267, 0.0000, 0.0048],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54381.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0266, 0.0295, 0.0343,  ..., 0.0328, 0.0000, 0.0000],
        [0.0706, 0.0145, 0.0567,  ..., 0.0624, 0.0000, 0.0000],
        [0.1305, 0.0032, 0.0872,  ..., 0.1023, 0.0000, 0.0000],
        ...,
        [0.0062, 0.0451, 0.0246,  ..., 0.0194, 0.0000, 0.0000],
        [0.0061, 0.0451, 0.0246,  ..., 0.0194, 0.0000, 0.0000],
        [0.0061, 0.0451, 0.0246,  ..., 0.0194, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(659196.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10962.4561, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-56.2553, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6630.5371, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-612.2206, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9500.8711, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(542.4019, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2853],
        [-0.0590],
        [ 0.0889],
        ...,
        [-2.2764],
        [-2.2703],
        [-2.2682]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-299106.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0098],
        [1.0127],
        [1.0201],
        ...,
        [1.0003],
        [0.9992],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368620.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2493],
        [0.6694],
        [0.2465],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.8746, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0098],
        [1.0128],
        [1.0202],
        ...,
        [1.0003],
        [0.9992],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368628.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2493],
        [0.6694],
        [0.2465],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.8746, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.2305e-02,  5.6707e-03,  1.2578e-02,  ...,  3.1322e-02,
         -4.2254e-03,  6.1746e-03],
        [-1.8955e-02,  4.7899e-03,  1.0685e-02,  ...,  2.6788e-02,
         -3.5908e-03,  5.2533e-03],
        [-1.8150e-02,  4.5782e-03,  1.0230e-02,  ...,  2.5699e-02,
         -3.4383e-03,  5.0320e-03],
        ...,
        [ 0.0000e+00, -1.9445e-04, -3.0160e-05,  ...,  1.1343e-03,
          0.0000e+00,  4.0314e-05],
        [ 0.0000e+00, -1.9445e-04, -3.0160e-05,  ...,  1.1343e-03,
          0.0000e+00,  4.0314e-05],
        [ 0.0000e+00, -1.9445e-04, -3.0160e-05,  ...,  1.1343e-03,
          0.0000e+00,  4.0314e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2682.5762, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.2022, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.6578, device='cuda:0')



h[100].sum tensor(-12.0433, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.5818, device='cuda:0')



h[200].sum tensor(-8.8156, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.3306, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0148, 0.0334,  ..., 0.0849, 0.0000, 0.0165],
        [0.0000, 0.0209, 0.0464,  ..., 0.1161, 0.0000, 0.0228],
        [0.0000, 0.0144, 0.0324,  ..., 0.0826, 0.0000, 0.0160],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59703.2422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2888, 0.0000, 0.1709,  ..., 0.2043, 0.0000, 0.0000],
        [0.3379, 0.0000, 0.1977,  ..., 0.2355, 0.0000, 0.0000],
        [0.2897, 0.0000, 0.1720,  ..., 0.2048, 0.0000, 0.0000],
        ...,
        [0.0059, 0.0452, 0.0245,  ..., 0.0193, 0.0000, 0.0000],
        [0.0059, 0.0452, 0.0245,  ..., 0.0193, 0.0000, 0.0000],
        [0.0059, 0.0452, 0.0245,  ..., 0.0193, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(679149.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11721.2070, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-34.2021, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6413.7930, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-644.9579, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9892.1094, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(596.4711, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2099],
        [ 0.2003],
        [ 0.1912],
        ...,
        [-2.2916],
        [-2.2855],
        [-2.2834]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-289388.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0098],
        [1.0128],
        [1.0202],
        ...,
        [1.0003],
        [0.9992],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368628.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.6370, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0099],
        [1.0128],
        [1.0203],
        ...,
        [1.0003],
        [0.9992],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368636.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.6370, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.9761e-04, -1.7329e-05,  ...,  1.1355e-03,
          0.0000e+00,  4.6706e-06],
        [ 0.0000e+00, -1.9761e-04, -1.7329e-05,  ...,  1.1355e-03,
          0.0000e+00,  4.6706e-06],
        [ 0.0000e+00, -1.9761e-04, -1.7329e-05,  ...,  1.1355e-03,
          0.0000e+00,  4.6706e-06],
        ...,
        [ 0.0000e+00, -1.9761e-04, -1.7329e-05,  ...,  1.1355e-03,
          0.0000e+00,  4.6706e-06],
        [ 0.0000e+00, -1.9761e-04, -1.7329e-05,  ...,  1.1355e-03,
          0.0000e+00,  4.6706e-06],
        [ 0.0000e+00, -1.9761e-04, -1.7329e-05,  ...,  1.1355e-03,
          0.0000e+00,  4.6706e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2881.5947, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.0517, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.5282, device='cuda:0')



h[100].sum tensor(-16.1324, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.1050, device='cuda:0')



h[200].sum tensor(-6.3078, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-32.4193, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.5763e-03, 0.0000e+00,
         1.8823e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.5908e-03, 0.0000e+00,
         1.8883e-05],
        [0.0000e+00, 2.8967e-03, 7.0539e-03,  ..., 2.1584e-02, 0.0000e+00,
         3.4686e-03],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.7119e-03, 0.0000e+00,
         1.9381e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.7111e-03, 0.0000e+00,
         1.9377e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.7107e-03, 0.0000e+00,
         1.9376e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68179.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0220, 0.0326, 0.0313,  ..., 0.0301, 0.0000, 0.0000],
        [0.0501, 0.0170, 0.0456,  ..., 0.0490, 0.0000, 0.0000],
        [0.1037, 0.0070, 0.0730,  ..., 0.0850, 0.0000, 0.0000],
        ...,
        [0.0055, 0.0454, 0.0242,  ..., 0.0190, 0.0000, 0.0000],
        [0.0055, 0.0453, 0.0242,  ..., 0.0190, 0.0000, 0.0000],
        [0.0055, 0.0453, 0.0242,  ..., 0.0190, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(714928.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(13294.3477, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(4.4175, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5834.1655, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-695.0911, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10607.9375, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(683.4104, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6412],
        [-0.2349],
        [ 0.0266],
        ...,
        [-2.3098],
        [-2.3035],
        [-2.3009]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-237740.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0099],
        [1.0128],
        [1.0203],
        ...,
        [1.0003],
        [0.9992],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368636.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3555],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.8713, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0100],
        [1.0129],
        [1.0203],
        ...,
        [1.0003],
        [0.9992],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368644., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3555],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.8713, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.6391e-02,  4.1054e-03,  9.2840e-03,  ...,  2.3376e-02,
         -3.0874e-03,  4.4868e-03],
        [-9.3567e-03,  2.2513e-03,  5.2996e-03,  ...,  1.3834e-02,
         -1.7624e-03,  2.5478e-03],
        [-1.3353e-02,  3.3048e-03,  7.5635e-03,  ...,  1.9256e-02,
         -2.5153e-03,  3.6495e-03],
        ...,
        [ 0.0000e+00, -2.1481e-04, -4.2571e-07,  ...,  1.1427e-03,
          0.0000e+00, -3.1407e-05],
        [-8.1102e-03,  1.9228e-03,  4.5935e-03,  ...,  1.2144e-02,
         -1.5277e-03,  2.2042e-03],
        [-8.1102e-03,  1.9228e-03,  4.5935e-03,  ...,  1.2144e-02,
         -1.5277e-03,  2.2042e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2690.5620, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.6352, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.0555, device='cuda:0')



h[100].sum tensor(-12.3369, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.6850, device='cuda:0')



h[200].sum tensor(-8.4664, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.8785, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0054, 0.0130,  ..., 0.0357, 0.0000, 0.0062],
        [0.0000, 0.0122, 0.0280,  ..., 0.0716, 0.0000, 0.0135],
        [0.0000, 0.0068, 0.0160,  ..., 0.0430, 0.0000, 0.0077],
        ...,
        [0.0000, 0.0036, 0.0087,  ..., 0.0255, 0.0000, 0.0041],
        [0.0000, 0.0036, 0.0087,  ..., 0.0255, 0.0000, 0.0041],
        [0.0000, 0.0036, 0.0087,  ..., 0.0255, 0.0000, 0.0041]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59559.3359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1633, 0.0000, 0.1037,  ..., 0.1233, 0.0000, 0.0000],
        [0.1982, 0.0000, 0.1224,  ..., 0.1456, 0.0000, 0.0000],
        [0.1638, 0.0000, 0.1042,  ..., 0.1234, 0.0000, 0.0000],
        ...,
        [0.0759, 0.0109, 0.0592,  ..., 0.0661, 0.0000, 0.0000],
        [0.0909, 0.0042, 0.0667,  ..., 0.0761, 0.0000, 0.0000],
        [0.0909, 0.0042, 0.0667,  ..., 0.0761, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(683288.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11431.2578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-39.0788, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6580.1191, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-644.6946, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9857.8633, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(584.7717, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1761],
        [ 0.1922],
        [ 0.1958],
        ...,
        [-1.2289],
        [-0.9739],
        [-0.9726]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-308429.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0100],
        [1.0129],
        [1.0203],
        ...,
        [1.0003],
        [0.9992],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368644., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.1000, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0101],
        [1.0129],
        [1.0204],
        ...,
        [1.0003],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368651.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.1000, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.3060e-04,  1.3670e-05,  ...,  1.1465e-03,
          0.0000e+00, -6.2840e-05],
        [ 0.0000e+00, -2.3060e-04,  1.3670e-05,  ...,  1.1465e-03,
          0.0000e+00, -6.2840e-05],
        [ 0.0000e+00, -2.3060e-04,  1.3670e-05,  ...,  1.1465e-03,
          0.0000e+00, -6.2840e-05],
        ...,
        [ 0.0000e+00, -2.3060e-04,  1.3670e-05,  ...,  1.1465e-03,
          0.0000e+00, -6.2840e-05],
        [ 0.0000e+00, -2.3060e-04,  1.3670e-05,  ...,  1.1465e-03,
          0.0000e+00, -6.2840e-05],
        [ 0.0000e+00, -2.3060e-04,  1.3670e-05,  ...,  1.1465e-03,
          0.0000e+00, -6.2840e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2592.8564, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.8738, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.2882, device='cuda:0')



h[100].sum tensor(-10.4011, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.9669, device='cuda:0')



h[200].sum tensor(-9.5449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.0655, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 5.5098e-05,  ..., 4.6212e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 5.5273e-05,  ..., 4.6359e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 5.5495e-05,  ..., 4.6545e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 5.6735e-05,  ..., 4.7585e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 5.6725e-05,  ..., 4.7577e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 5.6720e-05,  ..., 4.7573e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56610.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0046, 0.0446, 0.0222,  ..., 0.0172, 0.0000, 0.0000],
        [0.0051, 0.0444, 0.0224,  ..., 0.0176, 0.0000, 0.0000],
        [0.0125, 0.0395, 0.0259,  ..., 0.0230, 0.0000, 0.0000],
        ...,
        [0.0051, 0.0457, 0.0233,  ..., 0.0182, 0.0000, 0.0000],
        [0.0051, 0.0457, 0.0233,  ..., 0.0181, 0.0000, 0.0000],
        [0.0051, 0.0457, 0.0233,  ..., 0.0181, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(676383., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10955.2832, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.4283, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6725.2573, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-628.5490, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9671.1582, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(549.3640, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1301],
        [-1.8613],
        [-1.4698],
        ...,
        [-2.3426],
        [-2.3363],
        [-2.3341]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-326584.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0101],
        [1.0129],
        [1.0204],
        ...,
        [1.0003],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368651.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3430],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8774, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0102],
        [1.0130],
        [1.0204],
        ...,
        [1.0003],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368659.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3430],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8774, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.4534e-04,  1.9415e-05,  ...,  1.1469e-03,
          0.0000e+00, -8.7742e-05],
        [ 0.0000e+00, -2.4534e-04,  1.9415e-05,  ...,  1.1469e-03,
          0.0000e+00, -8.7742e-05],
        [ 0.0000e+00, -2.4534e-04,  1.9415e-05,  ...,  1.1469e-03,
          0.0000e+00, -8.7742e-05],
        ...,
        [-4.0974e-03,  8.3724e-04,  2.3449e-03,  ...,  6.7169e-03,
         -7.6741e-04,  1.0443e-03],
        [-6.7730e-03,  1.5442e-03,  3.8634e-03,  ...,  1.0354e-02,
         -1.2685e-03,  1.7835e-03],
        [ 0.0000e+00, -2.4534e-04,  1.9415e-05,  ...,  1.1469e-03,
          0.0000e+00, -8.7742e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2688.7229, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.5662, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.1355, device='cuda:0')



h[100].sum tensor(-12.2791, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.7057, device='cuda:0')



h[200].sum tensor(-8.4266, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.9889, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 7.8262e-05,  ..., 4.6232e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 7.8509e-05,  ..., 4.6379e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 7.8823e-05,  ..., 4.6564e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 9.1029e-03, 2.1822e-02,  ..., 5.6835e-02, 0.0000e+00,
         1.0220e-02],
        [0.0000e+00, 3.0482e-03, 7.7221e-03,  ..., 2.3063e-02, 0.0000e+00,
         3.5379e-03],
        [0.0000e+00, 1.6022e-03, 4.0690e-03,  ..., 1.4313e-02, 0.0000e+00,
         1.8506e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61841.5195, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0319, 0.0264, 0.0359,  ..., 0.0357, 0.0000, 0.0000],
        [0.0095, 0.0414, 0.0247,  ..., 0.0209, 0.0000, 0.0000],
        [0.0048, 0.0451, 0.0223,  ..., 0.0173, 0.0000, 0.0000],
        ...,
        [0.1809, 0.0000, 0.1128,  ..., 0.1351, 0.0000, 0.0000],
        [0.1181, 0.0058, 0.0806,  ..., 0.0936, 0.0000, 0.0000],
        [0.0623, 0.0177, 0.0523,  ..., 0.0564, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(703504., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12008.7910, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-31.7521, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6471.5103, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-660.7518, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10170.8965, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(601.8180, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7276],
        [-1.3187],
        [-1.8149],
        ...,
        [ 0.0085],
        [-0.3739],
        [-0.9739]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-302790.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0102],
        [1.0130],
        [1.0204],
        ...,
        [1.0003],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368659.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(172.6564, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0103],
        [1.0130],
        [1.0205],
        ...,
        [1.0003],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368667.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(172.6564, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.5447e-04,  1.4935e-05,  ...,  1.1431e-03,
          0.0000e+00, -1.0751e-04],
        [ 0.0000e+00, -2.5447e-04,  1.4935e-05,  ...,  1.1431e-03,
          0.0000e+00, -1.0751e-04],
        [ 0.0000e+00, -2.5447e-04,  1.4935e-05,  ...,  1.1431e-03,
          0.0000e+00, -1.0751e-04],
        ...,
        [ 0.0000e+00, -2.5447e-04,  1.4935e-05,  ...,  1.1431e-03,
          0.0000e+00, -1.0751e-04],
        [ 0.0000e+00, -2.5447e-04,  1.4935e-05,  ...,  1.1431e-03,
          0.0000e+00, -1.0751e-04],
        [ 0.0000e+00, -2.5447e-04,  1.4935e-05,  ...,  1.1431e-03,
          0.0000e+00, -1.0751e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2540.3013, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.2922, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.7408, device='cuda:0')



h[100].sum tensor(-9.2879, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.5654, device='cuda:0')



h[200].sum tensor(-10.3272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.9334, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 6.0206e-05,  ..., 4.6080e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 6.0396e-05,  ..., 4.6226e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 6.0636e-05,  ..., 4.6409e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 6.1999e-05,  ..., 4.7453e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 6.1986e-05,  ..., 4.7443e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 6.1981e-05,  ..., 4.7439e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55301.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0045, 0.0451, 0.0221,  ..., 0.0171, 0.0000, 0.0000],
        [0.0045, 0.0452, 0.0222,  ..., 0.0172, 0.0000, 0.0000],
        [0.0048, 0.0453, 0.0224,  ..., 0.0174, 0.0000, 0.0000],
        ...,
        [0.0226, 0.0345, 0.0320,  ..., 0.0302, 0.0000, 0.0000],
        [0.0108, 0.0423, 0.0261,  ..., 0.0221, 0.0000, 0.0000],
        [0.0050, 0.0462, 0.0232,  ..., 0.0181, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(673004.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10808.1416, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-61.4303, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6654.2480, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-623.5776, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9599.3633, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(533.5762, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0438],
        [-1.8302],
        [-1.4780],
        ...,
        [-1.6576],
        [-1.9586],
        [-2.1875]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-316075.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0103],
        [1.0130],
        [1.0205],
        ...,
        [1.0003],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368667.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(401.0800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0103],
        [1.0131],
        [1.0205],
        ...,
        [1.0003],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368675.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(401.0800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.4604e-04, -3.7610e-06,  ...,  1.1287e-03,
          0.0000e+00, -1.2263e-04],
        [ 0.0000e+00, -2.4604e-04, -3.7610e-06,  ...,  1.1287e-03,
          0.0000e+00, -1.2263e-04],
        [ 0.0000e+00, -2.4604e-04, -3.7610e-06,  ...,  1.1287e-03,
          0.0000e+00, -1.2263e-04],
        ...,
        [ 0.0000e+00, -2.4604e-04, -3.7610e-06,  ...,  1.1287e-03,
          0.0000e+00, -1.2263e-04],
        [ 0.0000e+00, -2.4604e-04, -3.7610e-06,  ...,  1.1287e-03,
          0.0000e+00, -1.2263e-04],
        [-1.1968e-02,  2.9260e-03,  6.7990e-03,  ...,  1.7430e-02,
         -2.2286e-03,  3.1917e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3166.0542, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.8290, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.9198, device='cuda:0')



h[100].sum tensor(-21.5332, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-8.2824, device='cuda:0')



h[200].sum tensor(-3.1204, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-43.9821, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0030, 0.0070,  ..., 0.0216, 0.0000, 0.0033],
        [0.0000, 0.0055, 0.0128,  ..., 0.0354, 0.0000, 0.0060]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78236.5078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0045, 0.0455, 0.0225,  ..., 0.0174, 0.0000, 0.0000],
        [0.0160, 0.0379, 0.0285,  ..., 0.0255, 0.0000, 0.0000],
        [0.0560, 0.0203, 0.0494,  ..., 0.0520, 0.0000, 0.0000],
        ...,
        [0.0273, 0.0320, 0.0353,  ..., 0.0332, 0.0000, 0.0000],
        [0.0855, 0.0150, 0.0660,  ..., 0.0712, 0.0000, 0.0000],
        [0.1563, 0.0029, 0.1033,  ..., 0.1175, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(772192.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(14696.9258, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(33.8373, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6030.7358, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-762.0718, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11535.3750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(776.6381, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7403],
        [-1.2562],
        [-0.5990],
        ...,
        [-1.7169],
        [-1.1532],
        [-0.5748]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-277896.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0103],
        [1.0131],
        [1.0205],
        ...,
        [1.0003],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368675.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 320.0 event: 1600 loss: tensor(457.9785, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(165.5991, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0104],
        [1.0131],
        [1.0206],
        ...,
        [1.0003],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368683.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(165.5991, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.5539e-04, -1.5594e-05,  ...,  1.1263e-03,
          0.0000e+00, -1.3207e-04],
        [ 0.0000e+00, -2.5539e-04, -1.5594e-05,  ...,  1.1263e-03,
          0.0000e+00, -1.3207e-04],
        [ 0.0000e+00, -2.5539e-04, -1.5594e-05,  ...,  1.1263e-03,
          0.0000e+00, -1.3207e-04],
        ...,
        [ 0.0000e+00, -2.5539e-04, -1.5594e-05,  ...,  1.1263e-03,
          0.0000e+00, -1.3207e-04],
        [ 0.0000e+00, -2.5539e-04, -1.5594e-05,  ...,  1.1263e-03,
          0.0000e+00, -1.3207e-04],
        [ 0.0000e+00, -2.5539e-04, -1.5594e-05,  ...,  1.1263e-03,
          0.0000e+00, -1.3207e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2527.8662, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.7568, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.1792, device='cuda:0')



h[100].sum tensor(-8.9067, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.4197, device='cuda:0')



h[200].sum tensor(-10.8665, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.1595, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0011, 0.0034,  ..., 0.0128, 0.0000, 0.0014],
        [0.0000, 0.0005, 0.0017,  ..., 0.0087, 0.0000, 0.0007],
        [0.0000, 0.0067, 0.0153,  ..., 0.0414, 0.0000, 0.0072],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54249.2461, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0475, 0.0170, 0.0427,  ..., 0.0489, 0.0000, 0.0000],
        [0.0705, 0.0117, 0.0556,  ..., 0.0634, 0.0000, 0.0000],
        [0.1583, 0.0009, 0.1026,  ..., 0.1201, 0.0000, 0.0000],
        ...,
        [0.0051, 0.0469, 0.0238,  ..., 0.0186, 0.0000, 0.0000],
        [0.0051, 0.0469, 0.0238,  ..., 0.0186, 0.0000, 0.0000],
        [0.0051, 0.0469, 0.0238,  ..., 0.0186, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(674329.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10558.8242, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-73.0810, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7015.2314, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-621.6779, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9617.8486, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(523.8336, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8555],
        [-0.3238],
        [ 0.0285],
        ...,
        [-2.3884],
        [-2.3819],
        [-2.3795]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-354636.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0104],
        [1.0131],
        [1.0206],
        ...,
        [1.0003],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368683.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(370.0095, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0105],
        [1.0132],
        [1.0207],
        ...,
        [1.0003],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368691.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(370.0095, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.6699e-04, -2.9186e-05,  ...,  1.1265e-03,
          0.0000e+00, -1.3379e-04],
        [ 0.0000e+00, -2.6699e-04, -2.9186e-05,  ...,  1.1265e-03,
          0.0000e+00, -1.3379e-04],
        [ 0.0000e+00, -2.6699e-04, -2.9186e-05,  ...,  1.1265e-03,
          0.0000e+00, -1.3379e-04],
        ...,
        [ 0.0000e+00, -2.6699e-04, -2.9186e-05,  ...,  1.1265e-03,
          0.0000e+00, -1.3379e-04],
        [ 0.0000e+00, -2.6699e-04, -2.9186e-05,  ...,  1.1265e-03,
          0.0000e+00, -1.3379e-04],
        [ 0.0000e+00, -2.6699e-04, -2.9186e-05,  ...,  1.1265e-03,
          0.0000e+00, -1.3379e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3093.8110, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.3662, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.4471, device='cuda:0')



h[100].sum tensor(-19.7975, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-7.6408, device='cuda:0')



h[200].sum tensor(-4.8466, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-40.5750, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0038, 0.0093,  ..., 0.0269, 0.0000, 0.0043],
        [0.0000, 0.0040, 0.0096,  ..., 0.0277, 0.0000, 0.0044],
        [0.0000, 0.0013, 0.0033,  ..., 0.0126, 0.0000, 0.0015],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73362.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0976, 0.0026, 0.0699,  ..., 0.0816, 0.0000, 0.0000],
        [0.0984, 0.0024, 0.0704,  ..., 0.0820, 0.0000, 0.0000],
        [0.0643, 0.0130, 0.0532,  ..., 0.0591, 0.0000, 0.0000],
        ...,
        [0.0052, 0.0472, 0.0241,  ..., 0.0190, 0.0000, 0.0000],
        [0.0052, 0.0472, 0.0241,  ..., 0.0189, 0.0000, 0.0000],
        [0.0052, 0.0472, 0.0241,  ..., 0.0189, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(747293.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(14026.7930, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(11.9784, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5884.3779, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-736.2551, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11157.7246, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(728.8348, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5631],
        [-0.6493],
        [-0.9844],
        ...,
        [-2.3927],
        [-2.3861],
        [-2.3836]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-249549.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0105],
        [1.0132],
        [1.0207],
        ...,
        [1.0003],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368691.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(350.7477, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0106],
        [1.0133],
        [1.0208],
        ...,
        [1.0003],
        [0.9991],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368699.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(350.7477, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.2210e-03,  1.1034e-03,  2.9345e-03,  ...,  8.2722e-03,
         -9.6393e-04,  1.3264e-03],
        [ 0.0000e+00, -2.8557e-04, -4.0522e-05,  ...,  1.1369e-03,
          0.0000e+00, -1.2555e-04],
        [ 0.0000e+00, -2.8557e-04, -4.0522e-05,  ...,  1.1369e-03,
          0.0000e+00, -1.2555e-04],
        ...,
        [ 0.0000e+00, -2.8557e-04, -4.0522e-05,  ...,  1.1369e-03,
          0.0000e+00, -1.2555e-04],
        [ 0.0000e+00, -2.8557e-04, -4.0522e-05,  ...,  1.1369e-03,
          0.0000e+00, -1.2555e-04],
        [ 0.0000e+00, -2.8557e-04, -4.0522e-05,  ...,  1.1369e-03,
          0.0000e+00, -1.2555e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3070.9646, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.1348, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.9141, device='cuda:0')



h[100].sum tensor(-18.9306, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-7.2430, device='cuda:0')



h[200].sum tensor(-5.7343, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-38.4627, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0040, 0.0098,  ..., 0.0282, 0.0000, 0.0045],
        [0.0000, 0.0011, 0.0030,  ..., 0.0118, 0.0000, 0.0013],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75739.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1154, 0.0041, 0.0794,  ..., 0.0932, 0.0000, 0.0000],
        [0.0757, 0.0085, 0.0592,  ..., 0.0666, 0.0000, 0.0000],
        [0.0610, 0.0109, 0.0524,  ..., 0.0564, 0.0000, 0.0000],
        ...,
        [0.0054, 0.0474, 0.0245,  ..., 0.0195, 0.0000, 0.0000],
        [0.0054, 0.0474, 0.0245,  ..., 0.0195, 0.0000, 0.0000],
        [0.0054, 0.0474, 0.0245,  ..., 0.0195, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(774845.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(14538.2383, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(12.5939, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6319.3779, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-754.1223, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11546.5234, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(752.0013, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.1275e-01],
        [-7.2427e-04],
        [-1.2869e-01],
        ...,
        [-2.3801e+00],
        [-2.3736e+00],
        [-2.3712e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-289961.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0106],
        [1.0133],
        [1.0208],
        ...,
        [1.0003],
        [0.9991],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368699.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.5481, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0107],
        [1.0134],
        [1.0209],
        ...,
        [1.0003],
        [0.9991],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368707.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.5481, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6021e-03,  2.2404e-03,  5.4315e-03,  ...,  1.4290e-02,
         -1.7677e-03,  2.5552e-03],
        [ 0.0000e+00, -3.1534e-04, -4.5163e-05,  ...,  1.1508e-03,
          0.0000e+00, -1.1928e-04],
        [ 0.0000e+00, -3.1534e-04, -4.5163e-05,  ...,  1.1508e-03,
          0.0000e+00, -1.1928e-04],
        ...,
        [ 0.0000e+00, -3.1534e-04, -4.5163e-05,  ...,  1.1508e-03,
          0.0000e+00, -1.1928e-04],
        [ 0.0000e+00, -3.1534e-04, -4.5163e-05,  ...,  1.1508e-03,
          0.0000e+00, -1.1928e-04],
        [ 0.0000e+00, -3.1534e-04, -4.5163e-05,  ...,  1.1508e-03,
          0.0000e+00, -1.1928e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2752.4292, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.8930, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.4277, device='cuda:0')



h[100].sum tensor(-12.4782, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.7815, device='cuda:0')



h[200].sum tensor(-9.8933, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.3914, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0109, 0.0252,  ..., 0.0654, 0.0000, 0.0120],
        [0.0000, 0.0037, 0.0092,  ..., 0.0270, 0.0000, 0.0043],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62672.0820, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2990, 0.0000, 0.1781,  ..., 0.2104, 0.0000, 0.0000],
        [0.2110, 0.0000, 0.1312,  ..., 0.1537, 0.0000, 0.0000],
        [0.1506, 0.0000, 0.0997,  ..., 0.1143, 0.0000, 0.0000],
        ...,
        [0.0055, 0.0475, 0.0245,  ..., 0.0196, 0.0000, 0.0000],
        [0.0055, 0.0475, 0.0245,  ..., 0.0196, 0.0000, 0.0000],
        [0.0055, 0.0475, 0.0245,  ..., 0.0196, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(700595.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12389.9619, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-36.4767, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6020.2227, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-676.2302, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10294.7695, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(619.6318, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1508],
        [ 0.1502],
        [ 0.1517],
        ...,
        [-2.3857],
        [-2.3793],
        [-2.3769]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-247811., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0107],
        [1.0134],
        [1.0209],
        ...,
        [1.0003],
        [0.9991],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368707.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(356.6199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0107],
        [1.0135],
        [1.0210],
        ...,
        [1.0003],
        [0.9991],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368714.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(356.6199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.4192e-04, -4.7796e-05,  ...,  1.1491e-03,
          0.0000e+00, -1.1581e-04],
        [ 0.0000e+00, -3.4192e-04, -4.7796e-05,  ...,  1.1491e-03,
          0.0000e+00, -1.1581e-04],
        [ 0.0000e+00, -3.4192e-04, -4.7796e-05,  ...,  1.1491e-03,
          0.0000e+00, -1.1581e-04],
        ...,
        [ 0.0000e+00, -3.4192e-04, -4.7796e-05,  ...,  1.1491e-03,
          0.0000e+00, -1.1581e-04],
        [ 0.0000e+00, -3.4192e-04, -4.7796e-05,  ...,  1.1491e-03,
          0.0000e+00, -1.1581e-04],
        [ 0.0000e+00, -3.4192e-04, -4.7796e-05,  ...,  1.1491e-03,
          0.0000e+00, -1.1581e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3094.1206, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.4266, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.3815, device='cuda:0')



h[100].sum tensor(-19.1192, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-7.3643, device='cuda:0')



h[200].sum tensor(-6.0009, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-39.1067, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77146.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0064, 0.0457, 0.0236,  ..., 0.0195, 0.0000, 0.0000],
        [0.0054, 0.0466, 0.0235,  ..., 0.0189, 0.0000, 0.0000],
        [0.0051, 0.0470, 0.0236,  ..., 0.0188, 0.0000, 0.0000],
        ...,
        [0.0054, 0.0479, 0.0245,  ..., 0.0195, 0.0000, 0.0000],
        [0.0054, 0.0479, 0.0245,  ..., 0.0195, 0.0000, 0.0000],
        [0.0054, 0.0479, 0.0244,  ..., 0.0195, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(789687., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(15053.4707, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(19.0648, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6281.7393, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-762.8476, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11817.7031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(766.8159, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3058],
        [-1.6567],
        [-1.9694],
        ...,
        [-2.4010],
        [-2.3945],
        [-2.3922]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-289205.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0107],
        [1.0135],
        [1.0210],
        ...,
        [1.0003],
        [0.9991],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368714.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5977],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.3813, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0108],
        [1.0135],
        [1.0210],
        ...,
        [1.0003],
        [0.9991],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368722.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5977],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.3813, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.2138e-03,  7.5444e-04,  2.3606e-03,  ...,  6.9287e-03,
         -7.7126e-04,  1.0658e-03],
        [-1.1698e-02,  2.7492e-03,  6.6380e-03,  ...,  1.7194e-02,
         -2.1411e-03,  3.1561e-03],
        [ 0.0000e+00, -3.6868e-04, -4.7703e-05,  ...,  1.1491e-03,
          0.0000e+00, -1.1108e-04],
        ...,
        [ 0.0000e+00, -3.6868e-04, -4.7703e-05,  ...,  1.1491e-03,
          0.0000e+00, -1.1108e-04],
        [ 0.0000e+00, -3.6868e-04, -4.7703e-05,  ...,  1.1491e-03,
          0.0000e+00, -1.1108e-04],
        [ 0.0000e+00, -3.6868e-04, -4.7703e-05,  ...,  1.1491e-03,
          0.0000e+00, -1.1108e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2746.3562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.9601, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.8123, device='cuda:0')



h[100].sum tensor(-12.5151, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.8813, device='cuda:0')



h[200].sum tensor(-10.0395, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.9214, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0153, 0.0358,  ..., 0.0910, 0.0000, 0.0171],
        [0.0000, 0.0039, 0.0098,  ..., 0.0285, 0.0000, 0.0046],
        [0.0000, 0.0028, 0.0067,  ..., 0.0209, 0.0000, 0.0032],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61247.6211, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2487, 0.0000, 0.1501,  ..., 0.1779, 0.0000, 0.0000],
        [0.1457, 0.0041, 0.0959,  ..., 0.1112, 0.0000, 0.0000],
        [0.0849, 0.0145, 0.0645,  ..., 0.0713, 0.0000, 0.0000],
        ...,
        [0.0052, 0.0484, 0.0244,  ..., 0.0194, 0.0000, 0.0000],
        [0.0052, 0.0484, 0.0244,  ..., 0.0194, 0.0000, 0.0000],
        [0.0052, 0.0484, 0.0244,  ..., 0.0194, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(696826., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11932.4502, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-40.8523, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6339.7041, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-666.5342, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10189.5615, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(603.3729, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1972],
        [-0.0096],
        [-0.4012],
        ...,
        [-2.4064],
        [-2.4076],
        [-2.4067]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-277635.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0108],
        [1.0135],
        [1.0210],
        ...,
        [1.0003],
        [0.9991],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368722.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3311],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(303.7697, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0109],
        [1.0136],
        [1.0211],
        ...,
        [1.0003],
        [0.9991],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368730.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3311],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(303.7697, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.8829e-04, -4.8593e-05,  ...,  1.1370e-03,
          0.0000e+00, -1.0608e-04],
        [-6.4726e-03,  1.3385e-03,  3.6544e-03,  ...,  1.0025e-02,
         -1.1813e-03,  1.7041e-03],
        [ 0.0000e+00, -3.8829e-04, -4.8593e-05,  ...,  1.1370e-03,
          0.0000e+00, -1.0608e-04],
        ...,
        [ 0.0000e+00, -3.8829e-04, -4.8593e-05,  ...,  1.1370e-03,
          0.0000e+00, -1.0608e-04],
        [ 0.0000e+00, -3.8829e-04, -4.8593e-05,  ...,  1.1370e-03,
          0.0000e+00, -1.0608e-04],
        [ 0.0000e+00, -3.8829e-04, -4.8593e-05,  ...,  1.1370e-03,
          0.0000e+00, -1.0608e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2911.8892, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.8836, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.1754, device='cuda:0')



h[100].sum tensor(-15.9396, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.2729, device='cuda:0')



h[200].sum tensor(-7.9347, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-33.3112, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0048, 0.0134,  ..., 0.0371, 0.0000, 0.0062],
        [0.0000, 0.0010, 0.0030,  ..., 0.0119, 0.0000, 0.0014],
        [0.0000, 0.0051, 0.0126,  ..., 0.0350, 0.0000, 0.0060],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65614.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0753, 0.0038, 0.0587,  ..., 0.0666, 0.0000, 0.0000],
        [0.0832, 0.0070, 0.0641,  ..., 0.0709, 0.0000, 0.0000],
        [0.1560, 0.0000, 0.1028,  ..., 0.1176, 0.0000, 0.0000],
        ...,
        [0.0051, 0.0490, 0.0246,  ..., 0.0193, 0.0000, 0.0000],
        [0.0051, 0.0490, 0.0246,  ..., 0.0193, 0.0000, 0.0000],
        [0.0050, 0.0490, 0.0245,  ..., 0.0193, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(712411.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12319.2334, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-20.6119, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6448.9531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-691.9085, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10504.8994, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(650.1327, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0896],
        [-0.6524],
        [-0.1908],
        ...,
        [-2.4401],
        [-2.4334],
        [-2.4309]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-295640.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0109],
        [1.0136],
        [1.0211],
        ...,
        [1.0003],
        [0.9991],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368730.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.5889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0110],
        [1.0136],
        [1.0212],
        ...,
        [1.0003],
        [0.9991],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368738.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.5889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.7408e-03,  1.1240e-03,  3.2441e-03,  ...,  9.0365e-03,
         -1.0447e-03,  1.5103e-03],
        [ 0.0000e+00, -4.0892e-04, -4.3587e-05,  ...,  1.1441e-03,
          0.0000e+00, -9.7347e-05],
        [ 0.0000e+00, -4.0892e-04, -4.3587e-05,  ...,  1.1441e-03,
          0.0000e+00, -9.7347e-05],
        ...,
        [ 0.0000e+00, -4.0892e-04, -4.3587e-05,  ...,  1.1441e-03,
          0.0000e+00, -9.7347e-05],
        [ 0.0000e+00, -4.0892e-04, -4.3587e-05,  ...,  1.1441e-03,
          0.0000e+00, -9.7347e-05],
        [ 0.0000e+00, -4.0892e-04, -4.3587e-05,  ...,  1.1441e-03,
          0.0000e+00, -9.7347e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2677.9158, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.5181, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.2372, device='cuda:0')



h[100].sum tensor(-11.5012, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.4726, device='cuda:0')



h[200].sum tensor(-10.6030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.7510, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0039, 0.0101,  ..., 0.0291, 0.0000, 0.0048],
        [0.0000, 0.0011, 0.0033,  ..., 0.0126, 0.0000, 0.0015],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60528.6797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1832, 0.0039, 0.1162,  ..., 0.1355, 0.0000, 0.0000],
        [0.1038, 0.0112, 0.0749,  ..., 0.0840, 0.0000, 0.0000],
        [0.0612, 0.0163, 0.0531,  ..., 0.0556, 0.0000, 0.0000],
        ...,
        [0.0049, 0.0493, 0.0245,  ..., 0.0191, 0.0000, 0.0000],
        [0.0049, 0.0493, 0.0245,  ..., 0.0191, 0.0000, 0.0000],
        [0.0049, 0.0493, 0.0244,  ..., 0.0191, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(700171.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11766.7246, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-39.4098, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6573.5811, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-660.8262, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10238.6768, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(597.4951, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1249],
        [ 0.1146],
        [ 0.1049],
        ...,
        [-2.4446],
        [-2.4376],
        [-2.4350]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-301444.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0110],
        [1.0136],
        [1.0212],
        ...,
        [1.0003],
        [0.9991],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368738.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.6846, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0110],
        [1.0137],
        [1.0213],
        ...,
        [1.0002],
        [0.9991],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368746.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.6846, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -4.2363e-04, -3.8490e-05,  ...,  1.1518e-03,
          0.0000e+00, -8.8513e-05],
        [ 0.0000e+00, -4.2363e-04, -3.8490e-05,  ...,  1.1518e-03,
          0.0000e+00, -8.8513e-05],
        [ 0.0000e+00, -4.2363e-04, -3.8490e-05,  ...,  1.1518e-03,
          0.0000e+00, -8.8513e-05],
        ...,
        [ 0.0000e+00, -4.2363e-04, -3.8490e-05,  ...,  1.1518e-03,
          0.0000e+00, -8.8513e-05],
        [ 0.0000e+00, -4.2363e-04, -3.8490e-05,  ...,  1.1518e-03,
          0.0000e+00, -8.8513e-05],
        [ 0.0000e+00, -4.2363e-04, -3.8490e-05,  ...,  1.1518e-03,
          0.0000e+00, -8.8513e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2805.2161, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.9584, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.7465, device='cuda:0')



h[100].sum tensor(-13.8911, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.3832, device='cuda:0')



h[200].sum tensor(-9.0684, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.5865, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0018, 0.0057,  ..., 0.0185, 0.0000, 0.0026],
        [0.0000, 0.0004, 0.0017,  ..., 0.0088, 0.0000, 0.0008],
        [0.0000, 0.0011, 0.0031,  ..., 0.0123, 0.0000, 0.0015],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65494.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0751, 0.0103, 0.0573,  ..., 0.0673, 0.0000, 0.0000],
        [0.0645, 0.0112, 0.0522,  ..., 0.0601, 0.0000, 0.0000],
        [0.0715, 0.0071, 0.0561,  ..., 0.0646, 0.0000, 0.0000],
        ...,
        [0.0048, 0.0496, 0.0244,  ..., 0.0190, 0.0000, 0.0000],
        [0.0048, 0.0496, 0.0244,  ..., 0.0190, 0.0000, 0.0000],
        [0.0048, 0.0496, 0.0244,  ..., 0.0190, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(723991.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12649.3340, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-15.8487, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6436.4585, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-689.3474, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10695.2803, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(650.2844, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1271],
        [ 0.0141],
        [ 0.0859],
        ...,
        [-2.4654],
        [-2.4585],
        [-2.4559]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-291838.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0110],
        [1.0137],
        [1.0213],
        ...,
        [1.0002],
        [0.9991],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368746.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.3627, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0111],
        [1.0138],
        [1.0214],
        ...,
        [1.0002],
        [0.9991],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368753.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.3627, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -4.3722e-04, -3.6560e-05,  ...,  1.1549e-03,
          0.0000e+00, -7.5992e-05],
        [ 0.0000e+00, -4.3722e-04, -3.6560e-05,  ...,  1.1549e-03,
          0.0000e+00, -7.5992e-05],
        [-4.3896e-03,  7.3669e-04,  2.4827e-03,  ...,  7.2046e-03,
         -7.9415e-04,  1.1566e-03],
        ...,
        [ 0.0000e+00, -4.3722e-04, -3.6560e-05,  ...,  1.1549e-03,
          0.0000e+00, -7.5992e-05],
        [ 0.0000e+00, -4.3722e-04, -3.6560e-05,  ...,  1.1549e-03,
          0.0000e+00, -7.5992e-05],
        [ 0.0000e+00, -4.3722e-04, -3.6560e-05,  ...,  1.1549e-03,
          0.0000e+00, -7.5992e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2873.2224, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.7386, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.5513, device='cuda:0')



h[100].sum tensor(-15.1242, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.8515, device='cuda:0')



h[200].sum tensor(-8.1590, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-31.0733, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0023, 0.0068,  ..., 0.0212, 0.0000, 0.0032],
        [0.0000, 0.0029, 0.0091,  ..., 0.0267, 0.0000, 0.0043],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66385.6953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0824, 0.0035, 0.0628,  ..., 0.0712, 0.0000, 0.0000],
        [0.0953, 0.0032, 0.0682,  ..., 0.0806, 0.0000, 0.0000],
        [0.1208, 0.0035, 0.0807,  ..., 0.0979, 0.0000, 0.0000],
        ...,
        [0.0047, 0.0499, 0.0246,  ..., 0.0191, 0.0000, 0.0000],
        [0.0047, 0.0499, 0.0246,  ..., 0.0191, 0.0000, 0.0000],
        [0.0047, 0.0499, 0.0246,  ..., 0.0191, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(725710.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12557.9434, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-11.3411, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6566.4033, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-694.9158, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10717.5117, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(661.5771, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2209],
        [ 0.2227],
        [ 0.2247],
        ...,
        [-2.4806],
        [-2.4735],
        [-2.4709]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-312460.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0111],
        [1.0138],
        [1.0214],
        ...,
        [1.0002],
        [0.9991],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368753.8438, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 330.0 event: 1650 loss: tensor(507.2487, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4136],
        [0.4111],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.7673, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0112],
        [1.0138],
        [1.0215],
        ...,
        [1.0002],
        [0.9991],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368761.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4136],
        [0.4111],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.7673, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.2417e-02,  5.5551e-03,  1.2850e-02,  ...,  3.2108e-02,
         -4.0438e-03,  6.2446e-03],
        [-1.5093e-02,  3.5945e-03,  8.6425e-03,  ...,  2.2002e-02,
         -2.7227e-03,  4.1852e-03],
        [-8.0030e-03,  1.6963e-03,  4.5689e-03,  ...,  1.2218e-02,
         -1.4437e-03,  2.1915e-03],
        ...,
        [ 0.0000e+00, -4.4625e-04, -2.9079e-05,  ...,  1.1752e-03,
          0.0000e+00, -5.9005e-05],
        [ 0.0000e+00, -4.4625e-04, -2.9079e-05,  ...,  1.1752e-03,
          0.0000e+00, -5.9005e-05],
        [ 0.0000e+00, -4.4625e-04, -2.9079e-05,  ...,  1.1752e-03,
          0.0000e+00, -5.9005e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2794.9897, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.2376, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.0368, device='cuda:0')



h[100].sum tensor(-13.3788, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.1991, device='cuda:0')



h[200].sum tensor(-9.3331, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.6086, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0166, 0.0393,  ..., 0.0993, 0.0000, 0.0190],
        [0.0000, 0.0163, 0.0387,  ..., 0.0980, 0.0000, 0.0188],
        [0.0000, 0.0080, 0.0191,  ..., 0.0508, 0.0000, 0.0093],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63874.9453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3586, 0.0000, 0.2103,  ..., 0.2483, 0.0000, 0.0000],
        [0.3373, 0.0000, 0.1989,  ..., 0.2348, 0.0000, 0.0000],
        [0.2430, 0.0000, 0.1489,  ..., 0.1743, 0.0000, 0.0000],
        ...,
        [0.0047, 0.0499, 0.0247,  ..., 0.0195, 0.0000, 0.0000],
        [0.0047, 0.0499, 0.0247,  ..., 0.0195, 0.0000, 0.0000],
        [0.0047, 0.0499, 0.0247,  ..., 0.0195, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(715920.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12177.7930, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-21.2093, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6528.3306, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-681.8385, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10454.4277, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(637.4960, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1958],
        [ 0.1937],
        [ 0.1860],
        ...,
        [-2.4823],
        [-2.4753],
        [-2.4727]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-304175.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0112],
        [1.0138],
        [1.0215],
        ...,
        [1.0002],
        [0.9991],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368761.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(282.4510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0113],
        [1.0139],
        [1.0216],
        ...,
        [1.0002],
        [0.9991],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368770.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(282.4510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -4.4772e-04, -1.5291e-05,  ...,  1.2064e-03,
          0.0000e+00, -4.3110e-05],
        [ 0.0000e+00, -4.4772e-04, -1.5291e-05,  ...,  1.2064e-03,
          0.0000e+00, -4.3110e-05],
        [ 0.0000e+00, -4.4772e-04, -1.5291e-05,  ...,  1.2064e-03,
          0.0000e+00, -4.3110e-05],
        ...,
        [ 0.0000e+00, -4.4772e-04, -1.5291e-05,  ...,  1.2064e-03,
          0.0000e+00, -4.3110e-05],
        [ 0.0000e+00, -4.4772e-04, -1.5291e-05,  ...,  1.2064e-03,
          0.0000e+00, -4.3110e-05],
        [ 0.0000e+00, -4.4772e-04, -1.5291e-05,  ...,  1.2064e-03,
          0.0000e+00, -4.3110e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2891.9880, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.2782, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.4788, device='cuda:0')



h[100].sum tensor(-14.7921, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.8327, device='cuda:0')



h[200].sum tensor(-8.6198, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.9734, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66087.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0041, 0.0486, 0.0235,  ..., 0.0189, 0.0000, 0.0000],
        [0.0042, 0.0487, 0.0236,  ..., 0.0190, 0.0000, 0.0000],
        [0.0044, 0.0489, 0.0239,  ..., 0.0192, 0.0000, 0.0000],
        ...,
        [0.0047, 0.0498, 0.0248,  ..., 0.0200, 0.0000, 0.0000],
        [0.0047, 0.0498, 0.0247,  ..., 0.0200, 0.0000, 0.0000],
        [0.0047, 0.0498, 0.0247,  ..., 0.0200, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(723910.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12446.0938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-16.8891, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6370.0498, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-698.8661, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10586.7266, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(658.7128, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.5794],
        [-2.4741],
        [-2.2918],
        ...,
        [-2.4784],
        [-2.4716],
        [-2.4690]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-290706.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0113],
        [1.0139],
        [1.0216],
        ...,
        [1.0002],
        [0.9991],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368770.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.6720, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0114],
        [1.0139],
        [1.0217],
        ...,
        [1.0002],
        [0.9991],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368778.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.6720, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -4.4322e-04, -2.9439e-06,  ...,  1.2332e-03,
          0.0000e+00, -2.9379e-05],
        [ 0.0000e+00, -4.4322e-04, -2.9439e-06,  ...,  1.2332e-03,
          0.0000e+00, -2.9379e-05],
        [ 0.0000e+00, -4.4322e-04, -2.9439e-06,  ...,  1.2332e-03,
          0.0000e+00, -2.9379e-05],
        ...,
        [ 0.0000e+00, -4.4322e-04, -2.9439e-06,  ...,  1.2332e-03,
          0.0000e+00, -2.9379e-05],
        [ 0.0000e+00, -4.4322e-04, -2.9439e-06,  ...,  1.2332e-03,
          0.0000e+00, -2.9379e-05],
        [ 0.0000e+00, -4.4322e-04, -2.9439e-06,  ...,  1.2332e-03,
          0.0000e+00, -2.9379e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2724.5200, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.2340, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.8458, device='cuda:0')



h[100].sum tensor(-11.2810, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.3711, device='cuda:0')



h[200].sum tensor(-10.8479, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.2118, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61150.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0219, 0.0372, 0.0328,  ..., 0.0317, 0.0000, 0.0000],
        [0.0059, 0.0476, 0.0245,  ..., 0.0207, 0.0000, 0.0000],
        [0.0044, 0.0488, 0.0239,  ..., 0.0197, 0.0000, 0.0000],
        ...,
        [0.0047, 0.0497, 0.0248,  ..., 0.0205, 0.0000, 0.0000],
        [0.0047, 0.0497, 0.0248,  ..., 0.0205, 0.0000, 0.0000],
        [0.0047, 0.0497, 0.0248,  ..., 0.0205, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(705028.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11661.1465, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-41.8983, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6401.6934, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-673.3889, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10191.6592, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(606.1553, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2356],
        [-1.6619],
        [-1.9063],
        ...,
        [-2.4742],
        [-2.4672],
        [-2.4646]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-295562.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0114],
        [1.0139],
        [1.0217],
        ...,
        [1.0002],
        [0.9991],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368778.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4641],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.9598, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0115],
        [1.0140],
        [1.0218],
        ...,
        [1.0002],
        [0.9991],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368786.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4641],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.9598, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.7196e-03,  1.6449e-03,  4.4567e-03,  ...,  1.1938e-02,
         -1.3804e-03,  2.1596e-03],
        [-9.0045e-03,  1.9906e-03,  5.1973e-03,  ...,  1.3718e-02,
         -1.6101e-03,  2.5223e-03],
        [ 0.0000e+00, -4.3204e-04,  7.6613e-06,  ...,  1.2482e-03,
          0.0000e+00, -1.9075e-05],
        ...,
        [ 0.0000e+00, -4.3204e-04,  7.6613e-06,  ...,  1.2482e-03,
          0.0000e+00, -1.9075e-05],
        [ 0.0000e+00, -4.3204e-04,  7.6613e-06,  ...,  1.2482e-03,
          0.0000e+00, -1.9075e-05],
        [ 0.0000e+00, -4.3204e-04,  7.6613e-06,  ...,  1.2482e-03,
          0.0000e+00, -1.9075e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2972.4985, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.4984, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.4743, device='cuda:0')



h[100].sum tensor(-15.6280, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.0910, device='cuda:0')



h[200].sum tensor(-8.2926, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-32.3451, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.9234e-02, 4.4966e-02,  ..., 1.1300e-01, 0.0000e+00,
         2.1928e-02],
        [0.0000e+00, 6.4541e-03, 1.5728e-02,  ..., 4.2768e-02, 0.0000e+00,
         7.6481e-03],
        [0.0000e+00, 2.0185e-03, 5.2935e-03,  ..., 1.7716e-02, 0.0000e+00,
         2.5576e-03],
        ...,
        [0.0000e+00, 0.0000e+00, 3.1857e-05,  ..., 5.1902e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 3.1843e-05,  ..., 5.1880e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 3.1838e-05,  ..., 5.1871e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68228.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[5.0303e-01, 0.0000e+00, 2.9135e-01,  ..., 3.4246e-01, 0.0000e+00,
         0.0000e+00],
        [2.9309e-01, 4.0310e-04, 1.7802e-01,  ..., 2.0775e-01, 0.0000e+00,
         0.0000e+00],
        [1.5095e-01, 8.6430e-03, 1.0164e-01,  ..., 1.1638e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [4.6301e-03, 4.9867e-02, 2.4978e-02,  ..., 2.0936e-02, 0.0000e+00,
         0.0000e+00],
        [4.6250e-03, 4.9848e-02, 2.4963e-02,  ..., 2.0923e-02, 0.0000e+00,
         0.0000e+00],
        [4.6155e-03, 4.9844e-02, 2.4952e-02,  ..., 2.0913e-02, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(731137.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12711.7441, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-13.2902, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6081.0029, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-719.3445, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10669.3535, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(680.5076, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0559],
        [ 0.0803],
        [ 0.0971],
        ...,
        [-2.4661],
        [-2.4597],
        [-2.4580]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-269954.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0115],
        [1.0140],
        [1.0218],
        ...,
        [1.0002],
        [0.9991],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368786.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(241.8656, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0116],
        [1.0141],
        [1.0218],
        ...,
        [1.0002],
        [0.9990],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368794.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(241.8656, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -4.1246e-04,  1.2331e-05,  ...,  1.2533e-03,
          0.0000e+00, -1.2972e-05],
        [ 0.0000e+00, -4.1246e-04,  1.2331e-05,  ...,  1.2533e-03,
          0.0000e+00, -1.2972e-05],
        [ 0.0000e+00, -4.1246e-04,  1.2331e-05,  ...,  1.2533e-03,
          0.0000e+00, -1.2972e-05],
        ...,
        [ 0.0000e+00, -4.1246e-04,  1.2331e-05,  ...,  1.2533e-03,
          0.0000e+00, -1.2972e-05],
        [ 0.0000e+00, -4.1246e-04,  1.2331e-05,  ...,  1.2533e-03,
          0.0000e+00, -1.2972e-05],
        [ 0.0000e+00, -4.1246e-04,  1.2331e-05,  ...,  1.2533e-03,
          0.0000e+00, -1.2972e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2826.7749, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.4231, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.2488, device='cuda:0')



h[100].sum tensor(-12.7921, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.9946, device='cuda:0')



h[200].sum tensor(-9.9944, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.5228, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 4.9756e-05,  ..., 5.0573e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 4.9920e-05,  ..., 5.0740e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.2981e-03, 4.6236e-03,  ..., 1.6084e-02, 0.0000e+00,
         2.2135e-03],
        ...,
        [0.0000e+00, 0.0000e+00, 5.1277e-05,  ..., 5.2118e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 5.1255e-05,  ..., 5.2096e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 5.1247e-05,  ..., 5.2088e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64883.6953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0823, 0.0145, 0.0649,  ..., 0.0729, 0.0000, 0.0000],
        [0.1148, 0.0081, 0.0819,  ..., 0.0943, 0.0000, 0.0000],
        [0.1550, 0.0045, 0.1021,  ..., 0.1214, 0.0000, 0.0000],
        ...,
        [0.0046, 0.0501, 0.0252,  ..., 0.0213, 0.0000, 0.0000],
        [0.0046, 0.0501, 0.0252,  ..., 0.0212, 0.0000, 0.0000],
        [0.0045, 0.0501, 0.0252,  ..., 0.0212, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(719960.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12321.1670, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-29.2513, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6019.6016, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-703.1478, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10498.1016, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(647.0935, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0934],
        [ 0.1084],
        [ 0.1152],
        ...,
        [-2.4912],
        [-2.4843],
        [-2.4815]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-262556., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0116],
        [1.0141],
        [1.0218],
        ...,
        [1.0002],
        [0.9990],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368794.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.5421, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0117],
        [1.0141],
        [1.0219],
        ...,
        [1.0002],
        [0.9990],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368802.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.5421, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.9153e-04,  1.0400e-05,  ...,  1.2516e-03,
          0.0000e+00, -7.4700e-06],
        [ 0.0000e+00, -3.9153e-04,  1.0400e-05,  ...,  1.2516e-03,
          0.0000e+00, -7.4700e-06],
        [-1.5389e-02,  3.7658e-03,  8.8969e-03,  ...,  2.2608e-02,
         -2.7356e-03,  4.3451e-03],
        ...,
        [ 0.0000e+00, -3.9153e-04,  1.0400e-05,  ...,  1.2516e-03,
          0.0000e+00, -7.4700e-06],
        [ 0.0000e+00, -3.9153e-04,  1.0400e-05,  ...,  1.2516e-03,
          0.0000e+00, -7.4700e-06],
        [ 0.0000e+00, -3.9153e-04,  1.0400e-05,  ...,  1.2516e-03,
          0.0000e+00, -7.4700e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2738.4963, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.0240, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.9151, device='cuda:0')



h[100].sum tensor(-11.1218, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.3891, device='cuda:0')



h[200].sum tensor(-10.7687, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.3072, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 4.1969e-05,  ..., 5.0506e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.8168e-03, 9.0489e-03,  ..., 2.6713e-02, 0.0000e+00,
         4.4039e-03],
        [0.0000e+00, 3.0515e-03, 7.4156e-03,  ..., 2.2806e-02, 0.0000e+00,
         3.6038e-03],
        ...,
        [0.0000e+00, 0.0000e+00, 4.3253e-05,  ..., 5.2051e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 4.3234e-05,  ..., 5.2029e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 4.3227e-05,  ..., 5.2020e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59445.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0259, 0.0354, 0.0360,  ..., 0.0351, 0.0000, 0.0000],
        [0.0807, 0.0167, 0.0656,  ..., 0.0714, 0.0000, 0.0000],
        [0.1134, 0.0071, 0.0831,  ..., 0.0928, 0.0000, 0.0000],
        ...,
        [0.0044, 0.0505, 0.0253,  ..., 0.0213, 0.0000, 0.0000],
        [0.0044, 0.0504, 0.0253,  ..., 0.0213, 0.0000, 0.0000],
        [0.0044, 0.0504, 0.0253,  ..., 0.0213, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(695567.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11121.2314, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-54.6082, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6355.8276, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-672.1011, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9992.8789, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(589.5851, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9868],
        [-1.5296],
        [-1.0721],
        ...,
        [-2.5079],
        [-2.5013],
        [-2.4989]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-292637.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0117],
        [1.0141],
        [1.0219],
        ...,
        [1.0002],
        [0.9990],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368802.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6606],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.4608, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0118],
        [1.0142],
        [1.0220],
        ...,
        [1.0002],
        [0.9990],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368810.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6606],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.4608, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2775e-02,  3.0823e-03,  7.3960e-03,  ...,  1.9004e-02,
         -2.2643e-03,  3.6176e-03],
        [-1.0030e-02,  2.3394e-03,  5.8092e-03,  ...,  1.5190e-02,
         -1.7777e-03,  2.8403e-03],
        [-2.7734e-02,  7.1306e-03,  1.6043e-02,  ...,  3.9788e-02,
         -4.9157e-03,  7.8533e-03],
        ...,
        [ 0.0000e+00, -3.7498e-04,  1.1338e-05,  ...,  1.2549e-03,
          0.0000e+00,  2.9221e-07],
        [ 0.0000e+00, -3.7498e-04,  1.1338e-05,  ...,  1.2549e-03,
          0.0000e+00,  2.9221e-07],
        [ 0.0000e+00, -3.7498e-04,  1.1338e-05,  ...,  1.2549e-03,
          0.0000e+00,  2.9221e-07]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2910.5986, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.3367, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.2062, device='cuda:0')



h[100].sum tensor(-14.1095, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.5025, device='cuda:0')



h[200].sum tensor(-8.6785, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-29.2199, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 7.0577e-03, 1.6735e-02,  ..., 4.5179e-02, 0.0000e+00,
         8.1764e-03],
        [0.0000e+00, 1.9646e-02, 4.5251e-02,  ..., 1.1373e-01, 0.0000e+00,
         2.2145e-02],
        [0.0000e+00, 1.8392e-02, 4.2587e-02,  ..., 1.0735e-01, 0.0000e+00,
         2.0840e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 4.7159e-05,  ..., 5.2195e-03, 0.0000e+00,
         1.2154e-06],
        [0.0000e+00, 0.0000e+00, 4.7139e-05,  ..., 5.2173e-03, 0.0000e+00,
         1.2149e-06],
        [0.0000e+00, 0.0000e+00, 4.7131e-05,  ..., 5.2164e-03, 0.0000e+00,
         1.2147e-06]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65417.4492, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2164, 0.0016, 0.1384,  ..., 0.1602, 0.0000, 0.0000],
        [0.3704, 0.0000, 0.2212,  ..., 0.2606, 0.0000, 0.0000],
        [0.4316, 0.0000, 0.2537,  ..., 0.3010, 0.0000, 0.0000],
        ...,
        [0.0043, 0.0508, 0.0253,  ..., 0.0214, 0.0000, 0.0000],
        [0.0043, 0.0508, 0.0253,  ..., 0.0214, 0.0000, 0.0000],
        [0.0043, 0.0508, 0.0253,  ..., 0.0213, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(726274.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12017.6719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-33.3498, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6427.4385, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-709.6940, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10571.8701, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(649.9504, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0434],
        [ 0.1569],
        [ 0.1744],
        ...,
        [-2.5235],
        [-2.5166],
        [-2.5140]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-311411.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0118],
        [1.0142],
        [1.0220],
        ...,
        [1.0002],
        [0.9990],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368810.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(342.4047, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0119],
        [1.0143],
        [1.0221],
        ...,
        [1.0001],
        [0.9990],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368818., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(342.4047, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.5508e-03,  1.6807e-03,  4.3826e-03,  ...,  1.1767e-02,
         -1.3344e-03,  2.1539e-03],
        [ 0.0000e+00, -3.6560e-04,  1.3413e-05,  ...,  1.2635e-03,
          0.0000e+00,  1.3288e-05],
        [-3.9544e-03,  7.0610e-04,  2.3016e-03,  ...,  6.7641e-03,
         -6.9883e-04,  1.1344e-03],
        ...,
        [-1.3129e-02,  3.1925e-03,  7.6104e-03,  ...,  1.9526e-02,
         -2.3202e-03,  3.7354e-03],
        [ 0.0000e+00, -3.6560e-04,  1.3413e-05,  ...,  1.2635e-03,
          0.0000e+00,  1.3288e-05],
        [ 0.0000e+00, -3.6560e-04,  1.3413e-05,  ...,  1.2635e-03,
          0.0000e+00,  1.3288e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3146.5181, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.2057, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.2502, device='cuda:0')



h[100].sum tensor(-18.1742, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-7.0708, device='cuda:0')



h[200].sum tensor(-6.0938, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-37.5478, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 6.4032e-03, 1.6089e-02,  ..., 4.3645e-02, 0.0000e+00,
         7.9096e-03],
        [0.0000e+00, 4.2520e-03, 1.2294e-02,  ..., 3.4539e-02, 0.0000e+00,
         6.0504e-03],
        [0.0000e+00, 1.9306e-03, 6.5578e-03,  ..., 2.0769e-02, 0.0000e+00,
         3.2402e-03],
        ...,
        [0.0000e+00, 2.6416e-03, 6.5080e-03,  ..., 2.0766e-02, 0.0000e+00,
         3.2165e-03],
        [0.0000e+00, 3.3200e-03, 7.9562e-03,  ..., 2.4245e-02, 0.0000e+00,
         3.9260e-03],
        [0.0000e+00, 0.0000e+00, 5.5760e-05,  ..., 5.2524e-03, 0.0000e+00,
         5.5238e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71946.1484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2185, 0.0000, 0.1359,  ..., 0.1647, 0.0000, 0.0000],
        [0.1666, 0.0000, 0.1069,  ..., 0.1320, 0.0000, 0.0000],
        [0.1214, 0.0000, 0.0826,  ..., 0.1028, 0.0000, 0.0000],
        ...,
        [0.0882, 0.0096, 0.0699,  ..., 0.0776, 0.0000, 0.0000],
        [0.0693, 0.0199, 0.0601,  ..., 0.0650, 0.0000, 0.0000],
        [0.0244, 0.0385, 0.0361,  ..., 0.0351, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(752547.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12966.9873, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.8095, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6259.2070, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-750.3555, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11011.2275, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(717.5059, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1947],
        [ 0.2126],
        [ 0.2246],
        ...,
        [-1.1851],
        [-1.5310],
        [-1.9516]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-298278.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0119],
        [1.0143],
        [1.0221],
        ...,
        [1.0001],
        [0.9990],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368818., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(274.0854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0120],
        [1.0144],
        [1.0222],
        ...,
        [1.0001],
        [0.9990],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368825.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(274.0854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.6042e-04,  1.5536e-05,  ...,  1.2673e-03,
          0.0000e+00,  1.7355e-05],
        [ 0.0000e+00, -3.6042e-04,  1.5536e-05,  ...,  1.2673e-03,
          0.0000e+00,  1.7355e-05],
        [ 0.0000e+00, -3.6042e-04,  1.5536e-05,  ...,  1.2673e-03,
          0.0000e+00,  1.7355e-05],
        ...,
        [ 0.0000e+00, -3.6042e-04,  1.5536e-05,  ...,  1.2673e-03,
          0.0000e+00,  1.7355e-05],
        [ 0.0000e+00, -3.6042e-04,  1.5536e-05,  ...,  1.2673e-03,
          0.0000e+00,  1.7355e-05],
        [ 0.0000e+00, -3.6042e-04,  1.5536e-05,  ...,  1.2673e-03,
          0.0000e+00,  1.7355e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2939.8469, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.6552, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.8130, device='cuda:0')



h[100].sum tensor(-14.3191, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.6599, device='cuda:0')



h[200].sum tensor(-8.2477, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.0560, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 6.2707e-05,  ..., 5.1152e-03, 0.0000e+00,
         7.0047e-05],
        [0.0000e+00, 0.0000e+00, 6.2914e-05,  ..., 5.1321e-03, 0.0000e+00,
         7.0279e-05],
        [0.0000e+00, 0.0000e+00, 6.3154e-05,  ..., 5.1517e-03, 0.0000e+00,
         7.0547e-05],
        ...,
        [0.0000e+00, 0.0000e+00, 6.4630e-05,  ..., 5.2720e-03, 0.0000e+00,
         7.2195e-05],
        [0.0000e+00, 0.0000e+00, 6.4601e-05,  ..., 5.2697e-03, 0.0000e+00,
         7.2164e-05],
        [0.0000e+00, 0.0000e+00, 6.4591e-05,  ..., 5.2689e-03, 0.0000e+00,
         7.2152e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67804.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0036, 0.0499, 0.0239,  ..., 0.0202, 0.0000, 0.0000],
        [0.0036, 0.0500, 0.0241,  ..., 0.0203, 0.0000, 0.0000],
        [0.0039, 0.0501, 0.0243,  ..., 0.0206, 0.0000, 0.0000],
        ...,
        [0.0041, 0.0511, 0.0252,  ..., 0.0214, 0.0000, 0.0000],
        [0.0041, 0.0511, 0.0252,  ..., 0.0214, 0.0000, 0.0000],
        [0.0041, 0.0511, 0.0252,  ..., 0.0214, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(743179.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12631.5996, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-21.2469, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6288.6382, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-725.9771, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10837.6270, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(674.5701, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7259],
        [-2.7385],
        [-2.7314],
        ...,
        [-2.5477],
        [-2.5408],
        [-2.5382]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-301217.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0120],
        [1.0144],
        [1.0222],
        ...,
        [1.0001],
        [0.9990],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368825.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(343.6942, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0121],
        [1.0145],
        [1.0223],
        ...,
        [1.0001],
        [0.9989],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368832.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(343.6942, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.5844e-04,  3.7760e-06,  ...,  1.2608e-03,
          0.0000e+00,  2.6458e-05],
        [ 0.0000e+00, -3.5844e-04,  3.7760e-06,  ...,  1.2608e-03,
          0.0000e+00,  2.6458e-05],
        [ 0.0000e+00, -3.5844e-04,  3.7760e-06,  ...,  1.2608e-03,
          0.0000e+00,  2.6458e-05],
        ...,
        [ 0.0000e+00, -3.5844e-04,  3.7760e-06,  ...,  1.2608e-03,
          0.0000e+00,  2.6458e-05],
        [ 0.0000e+00, -3.5844e-04,  3.7760e-06,  ...,  1.2608e-03,
          0.0000e+00,  2.6458e-05],
        [ 0.0000e+00, -3.5844e-04,  3.7760e-06,  ...,  1.2608e-03,
          0.0000e+00,  2.6458e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3154.0439, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.1129, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.3528, device='cuda:0')



h[100].sum tensor(-18.0954, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-7.0974, device='cuda:0')



h[200].sum tensor(-5.7607, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-37.6893, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 1.5242e-05,  ..., 5.0894e-03, 0.0000e+00,
         1.0680e-04],
        [0.0000e+00, 0.0000e+00, 1.5292e-05,  ..., 5.1063e-03, 0.0000e+00,
         1.0715e-04],
        [0.0000e+00, 0.0000e+00, 1.5350e-05,  ..., 5.1257e-03, 0.0000e+00,
         1.0756e-04],
        ...,
        [0.0000e+00, 0.0000e+00, 1.5709e-05,  ..., 5.2456e-03, 0.0000e+00,
         1.1008e-04],
        [0.0000e+00, 0.0000e+00, 1.5702e-05,  ..., 5.2432e-03, 0.0000e+00,
         1.1003e-04],
        [0.0000e+00, 0.0000e+00, 1.5700e-05,  ..., 5.2424e-03, 0.0000e+00,
         1.1001e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73312.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0035, 0.0501, 0.0239,  ..., 0.0201, 0.0000, 0.0000],
        [0.0038, 0.0501, 0.0241,  ..., 0.0204, 0.0000, 0.0000],
        [0.0051, 0.0496, 0.0246,  ..., 0.0213, 0.0000, 0.0000],
        ...,
        [0.0040, 0.0514, 0.0252,  ..., 0.0212, 0.0000, 0.0000],
        [0.0040, 0.0513, 0.0252,  ..., 0.0212, 0.0000, 0.0000],
        [0.0040, 0.0513, 0.0251,  ..., 0.0212, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(762869.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(13293.1553, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(4.1080, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6247.2559, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-757.2048, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11139.4268, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(731.7080, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.5511],
        [-2.3967],
        [-2.1679],
        ...,
        [-2.5671],
        [-2.5600],
        [-2.5572]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-294328.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0121],
        [1.0145],
        [1.0223],
        ...,
        [1.0001],
        [0.9989],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368832.0625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 340.0 event: 1700 loss: tensor(507.0319, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.9208, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0122],
        [1.0145],
        [1.0225],
        ...,
        [1.0001],
        [0.9989],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368838.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.9208, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.5931e-04, -4.0442e-06,  ...,  1.2549e-03,
          0.0000e+00,  2.4780e-05],
        [ 0.0000e+00, -3.5931e-04, -4.0442e-06,  ...,  1.2549e-03,
          0.0000e+00,  2.4780e-05],
        [ 0.0000e+00, -3.5931e-04, -4.0442e-06,  ...,  1.2549e-03,
          0.0000e+00,  2.4780e-05],
        ...,
        [-6.7919e-03,  1.4826e-03,  3.9396e-03,  ...,  1.0741e-02,
         -1.1896e-03,  1.9599e-03],
        [ 0.0000e+00, -3.5931e-04, -4.0442e-06,  ...,  1.2549e-03,
          0.0000e+00,  2.4780e-05],
        [ 0.0000e+00, -3.5931e-04, -4.0442e-06,  ...,  1.2549e-03,
          0.0000e+00,  2.4780e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3049.2422, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.3496, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.6649, device='cuda:0')



h[100].sum tensor(-16.1740, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.4000, device='cuda:0')



h[200].sum tensor(-6.7282, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-33.9857, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0001],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0001],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0001],
        ...,
        [0.0000, 0.0051, 0.0125,  ..., 0.0352, 0.0000, 0.0062],
        [0.0000, 0.0015, 0.0041,  ..., 0.0151, 0.0000, 0.0021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0001]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70788.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0117, 0.0448, 0.0284,  ..., 0.0258, 0.0000, 0.0000],
        [0.0369, 0.0299, 0.0411,  ..., 0.0427, 0.0000, 0.0000],
        [0.0681, 0.0146, 0.0573,  ..., 0.0643, 0.0000, 0.0000],
        ...,
        [0.1823, 0.0033, 0.1183,  ..., 0.1382, 0.0000, 0.0000],
        [0.0943, 0.0175, 0.0723,  ..., 0.0810, 0.0000, 0.0000],
        [0.0313, 0.0348, 0.0394,  ..., 0.0393, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(760279.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(13230.8691, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2.5093, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6308.9492, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-739.3185, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11070.0615, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(705.5280, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8134],
        [-0.3325],
        [-0.0137],
        ...,
        [-0.1072],
        [-0.7052],
        [-1.4757]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-294318.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0122],
        [1.0145],
        [1.0225],
        ...,
        [1.0001],
        [0.9989],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368838.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.1219, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0124],
        [1.0146],
        [1.0226],
        ...,
        [1.0001],
        [0.9989],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368846.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.1219, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.6395e-04, -4.8912e-06,  ...,  1.2647e-03,
          0.0000e+00,  3.2338e-05],
        [ 0.0000e+00, -3.6395e-04, -4.8912e-06,  ...,  1.2647e-03,
          0.0000e+00,  3.2338e-05],
        [ 0.0000e+00, -3.6395e-04, -4.8912e-06,  ...,  1.2647e-03,
          0.0000e+00,  3.2338e-05],
        ...,
        [ 0.0000e+00, -3.6395e-04, -4.8912e-06,  ...,  1.2647e-03,
          0.0000e+00,  3.2338e-05],
        [ 0.0000e+00, -3.6395e-04, -4.8912e-06,  ...,  1.2647e-03,
          0.0000e+00,  3.2338e-05],
        [ 0.0000e+00, -3.6395e-04, -4.8912e-06,  ...,  1.2647e-03,
          0.0000e+00,  3.2338e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2767.1204, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.7840, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.8817, device='cuda:0')



h[100].sum tensor(-10.9290, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.3804, device='cuda:0')



h[200].sum tensor(-9.9195, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.2611, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0011, 0.0031,  ..., 0.0126, 0.0000, 0.0017],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0001],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0001],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0001],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0001],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0001]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59259.0078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0452, 0.0244, 0.0445,  ..., 0.0490, 0.0000, 0.0000],
        [0.0150, 0.0428, 0.0295,  ..., 0.0282, 0.0000, 0.0000],
        [0.0055, 0.0482, 0.0255,  ..., 0.0225, 0.0000, 0.0000],
        ...,
        [0.0037, 0.0516, 0.0245,  ..., 0.0206, 0.0000, 0.0000],
        [0.0037, 0.0516, 0.0244,  ..., 0.0206, 0.0000, 0.0000],
        [0.0037, 0.0516, 0.0244,  ..., 0.0206, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(704797.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10835.0322, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-52.4457, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6819.3232, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-669.0551, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9940.8398, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(581.0563, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8573],
        [-1.3208],
        [-1.5652],
        ...,
        [-2.5982],
        [-2.5909],
        [-2.5881]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-336111., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0124],
        [1.0146],
        [1.0226],
        ...,
        [1.0001],
        [0.9989],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368846.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3706],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(273.9095, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0125],
        [1.0147],
        [1.0227],
        ...,
        [1.0000],
        [0.9989],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368853.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3706],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(273.9095, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.1192e-03,  1.5707e-03,  4.1383e-03,  ...,  1.1249e-02,
         -1.2396e-03,  2.0798e-03],
        [ 0.0000e+00, -3.6136e-04, -5.0399e-06,  ...,  1.2784e-03,
          0.0000e+00,  4.4682e-05],
        [-2.0279e-02,  5.1420e-03,  1.1797e-02,  ...,  2.9679e-02,
         -3.5310e-03,  5.8416e-03],
        ...,
        [ 0.0000e+00, -3.6136e-04, -5.0399e-06,  ...,  1.2784e-03,
          0.0000e+00,  4.4682e-05],
        [ 0.0000e+00, -3.6136e-04, -5.0399e-06,  ...,  1.2784e-03,
          0.0000e+00,  4.4682e-05],
        [ 0.0000e+00, -3.6136e-04, -5.0399e-06,  ...,  1.2784e-03,
          0.0000e+00,  4.4682e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2969.3455, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.5395, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.7990, device='cuda:0')



h[100].sum tensor(-14.2161, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.6563, device='cuda:0')



h[200].sum tensor(-8.0000, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.0367, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0012, 0.0034,  ..., 0.0134, 0.0000, 0.0019],
        [0.0000, 0.0093, 0.0230,  ..., 0.0606, 0.0000, 0.0115],
        [0.0000, 0.0046, 0.0115,  ..., 0.0328, 0.0000, 0.0058],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67136.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1506, 0.0000, 0.0992,  ..., 0.1166, 0.0000, 0.0000],
        [0.2049, 0.0000, 0.1270,  ..., 0.1520, 0.0000, 0.0000],
        [0.2023, 0.0000, 0.1260,  ..., 0.1499, 0.0000, 0.0000],
        ...,
        [0.0036, 0.0517, 0.0242,  ..., 0.0206, 0.0000, 0.0000],
        [0.0036, 0.0516, 0.0241,  ..., 0.0206, 0.0000, 0.0000],
        [0.0036, 0.0516, 0.0241,  ..., 0.0206, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(735137.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12240.7969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-13.9006, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6271.1040, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-714.6977, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10450.3340, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(663.8811, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2519],
        [ 0.2559],
        [ 0.2584],
        ...,
        [-2.6022],
        [-2.5950],
        [-2.5923]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-282507.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0125],
        [1.0147],
        [1.0227],
        ...,
        [1.0000],
        [0.9989],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368853.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3145],
        [0.2455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.3746, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0126],
        [1.0148],
        [1.0228],
        ...,
        [1.0000],
        [0.9989],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368861.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3145],
        [0.2455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.3746, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.5684e-03,  2.2691e-03,  5.5572e-03,  ...,  1.4696e-02,
         -1.6611e-03,  2.8003e-03],
        [-6.0338e-03,  1.3081e-03,  3.4985e-03,  ...,  9.7408e-03,
         -1.0475e-03,  1.7886e-03],
        [-4.7104e-03,  9.4821e-04,  2.7276e-03,  ...,  7.8854e-03,
         -8.1774e-04,  1.4098e-03],
        ...,
        [ 0.0000e+00, -3.3259e-04, -1.5947e-05,  ...,  1.2813e-03,
          0.0000e+00,  6.1504e-05],
        [ 0.0000e+00, -3.3259e-04, -1.5947e-05,  ...,  1.2813e-03,
          0.0000e+00,  6.1504e-05],
        [ 0.0000e+00, -3.3259e-04, -1.5947e-05,  ...,  1.2813e-03,
          0.0000e+00,  6.1504e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2785.6626, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.0542, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.9467, device='cuda:0')



h[100].sum tensor(-10.4154, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.1378, device='cuda:0')



h[200].sum tensor(-10.3196, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.9729, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0068, 0.0174,  ..., 0.0472, 0.0000, 0.0088],
        [0.0000, 0.0061, 0.0158,  ..., 0.0434, 0.0000, 0.0080],
        [0.0000, 0.0021, 0.0058,  ..., 0.0193, 0.0000, 0.0031],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59333.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1434, 0.0000, 0.0932,  ..., 0.1140, 0.0000, 0.0000],
        [0.1191, 0.0018, 0.0811,  ..., 0.0982, 0.0000, 0.0000],
        [0.0696, 0.0157, 0.0563,  ..., 0.0657, 0.0000, 0.0000],
        ...,
        [0.0040, 0.0519, 0.0246,  ..., 0.0211, 0.0000, 0.0000],
        [0.0039, 0.0519, 0.0246,  ..., 0.0211, 0.0000, 0.0000],
        [0.0039, 0.0519, 0.0246,  ..., 0.0211, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(705711., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10992.1357, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-54.8028, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6625.9644, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-672.1130, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9946.6309, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(583.1577, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1426],
        [-0.2034],
        [-0.7821],
        ...,
        [-2.5942],
        [-2.5877],
        [-2.5858]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-317491.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0126],
        [1.0148],
        [1.0228],
        ...,
        [1.0000],
        [0.9989],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368861.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(263.1964, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0127],
        [1.0149],
        [1.0229],
        ...,
        [1.0000],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368868.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(263.1964, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.0052e-04, -2.7201e-05,  ...,  1.2810e-03,
          0.0000e+00,  7.5013e-05],
        [ 0.0000e+00, -3.0052e-04, -2.7201e-05,  ...,  1.2810e-03,
          0.0000e+00,  7.5013e-05],
        [ 0.0000e+00, -3.0052e-04, -2.7201e-05,  ...,  1.2810e-03,
          0.0000e+00,  7.5013e-05],
        ...,
        [ 0.0000e+00, -3.0052e-04, -2.7201e-05,  ...,  1.2810e-03,
          0.0000e+00,  7.5013e-05],
        [ 0.0000e+00, -3.0052e-04, -2.7201e-05,  ...,  1.2810e-03,
          0.0000e+00,  7.5013e-05],
        [ 0.0000e+00, -3.0052e-04, -2.7201e-05,  ...,  1.2810e-03,
          0.0000e+00,  7.5013e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3005.1001, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.8350, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.9464, device='cuda:0')



h[100].sum tensor(-13.7174, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.4351, device='cuda:0')



h[200].sum tensor(-8.2760, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.8619, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0003],
        [0.0000, 0.0051, 0.0121,  ..., 0.0345, 0.0000, 0.0063],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66490.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0149, 0.0440, 0.0300,  ..., 0.0282, 0.0000, 0.0000],
        [0.0485, 0.0259, 0.0473,  ..., 0.0502, 0.0000, 0.0000],
        [0.1518, 0.0090, 0.1014,  ..., 0.1176, 0.0000, 0.0000],
        ...,
        [0.0043, 0.0523, 0.0252,  ..., 0.0217, 0.0000, 0.0000],
        [0.0043, 0.0522, 0.0252,  ..., 0.0217, 0.0000, 0.0000],
        [0.0043, 0.0522, 0.0251,  ..., 0.0216, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(732079.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12263.3398, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-28.1861, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6254.8330, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-718.5990, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10510.7461, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(661.0610, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2000],
        [-0.5185],
        [-0.0529],
        ...,
        [-2.5923],
        [-2.5854],
        [-2.5827]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-285844.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0127],
        [1.0149],
        [1.0229],
        ...,
        [1.0000],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368868.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(205.3486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0128],
        [1.0150],
        [1.0229],
        ...,
        [1.0000],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368876.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(205.3486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.0111e-03,  1.1031e-03,  2.8860e-03,  ...,  8.3153e-03,
         -8.6475e-04,  1.5117e-03],
        [ 0.0000e+00, -2.6569e-04, -3.7007e-05,  ...,  1.2756e-03,
          0.0000e+00,  7.4134e-05],
        [ 0.0000e+00, -2.6569e-04, -3.7007e-05,  ...,  1.2756e-03,
          0.0000e+00,  7.4134e-05],
        ...,
        [ 0.0000e+00, -2.6569e-04, -3.7007e-05,  ...,  1.2756e-03,
          0.0000e+00,  7.4134e-05],
        [ 0.0000e+00, -2.6569e-04, -3.7007e-05,  ...,  1.2756e-03,
          0.0000e+00,  7.4134e-05],
        [ 0.0000e+00, -2.6569e-04, -3.7007e-05,  ...,  1.2756e-03,
          0.0000e+00,  7.4134e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2855.1262, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.3384, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.3426, device='cuda:0')



h[100].sum tensor(-10.6035, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.2405, device='cuda:0')



h[200].sum tensor(-9.9738, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.5184, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0037, 0.0095,  ..., 0.0283, 0.0000, 0.0050],
        [0.0000, 0.0030, 0.0074,  ..., 0.0233, 0.0000, 0.0040],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0003],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61894.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1038, 0.0050, 0.0739,  ..., 0.0899, 0.0000, 0.0000],
        [0.0816, 0.0129, 0.0630,  ..., 0.0747, 0.0000, 0.0000],
        [0.0362, 0.0323, 0.0403,  ..., 0.0437, 0.0000, 0.0000],
        ...,
        [0.0046, 0.0526, 0.0257,  ..., 0.0221, 0.0000, 0.0000],
        [0.0046, 0.0526, 0.0257,  ..., 0.0221, 0.0000, 0.0000],
        [0.0046, 0.0526, 0.0257,  ..., 0.0221, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(722117.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11802.6660, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-56.7153, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6481.3208, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-695.8813, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10437.3008, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(613.2078, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0128],
        [-0.2175],
        [-0.5854],
        ...,
        [-2.5934],
        [-2.5866],
        [-2.5839]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-313261.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0128],
        [1.0150],
        [1.0229],
        ...,
        [1.0000],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368876.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(271.5017, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0129],
        [1.0151],
        [1.0230],
        ...,
        [1.0000],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368883.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(271.5017, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.3511e-04, -4.4836e-05,  ...,  1.2698e-03,
          0.0000e+00,  7.3142e-05],
        [ 0.0000e+00, -2.3511e-04, -4.4836e-05,  ...,  1.2698e-03,
          0.0000e+00,  7.3142e-05],
        [ 0.0000e+00, -2.3511e-04, -4.4836e-05,  ...,  1.2698e-03,
          0.0000e+00,  7.3142e-05],
        ...,
        [ 0.0000e+00, -2.3511e-04, -4.4836e-05,  ...,  1.2698e-03,
          0.0000e+00,  7.3142e-05],
        [ 0.0000e+00, -2.3511e-04, -4.4836e-05,  ...,  1.2698e-03,
          0.0000e+00,  7.3142e-05],
        [ 0.0000e+00, -2.3511e-04, -4.4836e-05,  ...,  1.2698e-03,
          0.0000e+00,  7.3142e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3074.4705, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.2701, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.6074, device='cuda:0')



h[100].sum tensor(-14.0071, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.6066, device='cuda:0')



h[200].sum tensor(-7.7157, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-29.7727, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0012, 0.0035,  ..., 0.0138, 0.0000, 0.0021],
        [0.0000, 0.0022, 0.0057,  ..., 0.0190, 0.0000, 0.0031],
        [0.0000, 0.0032, 0.0078,  ..., 0.0242, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67315.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0706, 0.0118, 0.0574,  ..., 0.0685, 0.0000, 0.0000],
        [0.0773, 0.0114, 0.0614,  ..., 0.0725, 0.0000, 0.0000],
        [0.0859, 0.0092, 0.0664,  ..., 0.0778, 0.0000, 0.0000],
        ...,
        [0.0048, 0.0530, 0.0263,  ..., 0.0225, 0.0000, 0.0000],
        [0.0048, 0.0529, 0.0263,  ..., 0.0225, 0.0000, 0.0000],
        [0.0048, 0.0529, 0.0263,  ..., 0.0225, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(739772.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12593.0576, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-37.4887, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6283.3955, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-732.1612, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10820.0801, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(671.9918, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1029],
        [ 0.0260],
        [-0.0948],
        ...,
        [-2.5946],
        [-2.5878],
        [-2.5851]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-291904.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0129],
        [1.0151],
        [1.0230],
        ...,
        [1.0000],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368883.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.0194, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0129],
        [1.0151],
        [1.0230],
        ...,
        [1.0000],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368883.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.0194, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.8457e-03,  1.3656e-03,  3.3675e-03,  ...,  9.4900e-03,
         -1.0058e-03,  1.7517e-03],
        [ 0.0000e+00, -2.3511e-04, -4.4836e-05,  ...,  1.2698e-03,
          0.0000e+00,  7.3142e-05],
        [-1.8013e-02,  4.6973e-03,  1.0470e-02,  ...,  2.6600e-02,
         -3.0992e-03,  5.2457e-03],
        ...,
        [ 0.0000e+00, -2.3511e-04, -4.4836e-05,  ...,  1.2698e-03,
          0.0000e+00,  7.3142e-05],
        [ 0.0000e+00, -2.3511e-04, -4.4836e-05,  ...,  1.2698e-03,
          0.0000e+00,  7.3142e-05],
        [ 0.0000e+00, -2.3511e-04, -4.4836e-05,  ...,  1.2698e-03,
          0.0000e+00,  7.3142e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2932.5625, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.6794, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.6693, device='cuda:0')



h[100].sum tensor(-11.5259, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.5848, device='cuda:0')



h[200].sum tensor(-9.2439, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.3465, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0011, 0.0028,  ..., 0.0119, 0.0000, 0.0017],
        [0.0000, 0.0061, 0.0140,  ..., 0.0392, 0.0000, 0.0072],
        [0.0000, 0.0039, 0.0087,  ..., 0.0262, 0.0000, 0.0046],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61800.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0857, 0.0060, 0.0660,  ..., 0.0772, 0.0000, 0.0000],
        [0.1276, 0.0000, 0.0893,  ..., 0.1032, 0.0000, 0.0000],
        [0.1264, 0.0022, 0.0896,  ..., 0.1016, 0.0000, 0.0000],
        ...,
        [0.0048, 0.0530, 0.0263,  ..., 0.0225, 0.0000, 0.0000],
        [0.0048, 0.0529, 0.0263,  ..., 0.0225, 0.0000, 0.0000],
        [0.0048, 0.0529, 0.0263,  ..., 0.0225, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(711540.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11602.8105, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-58.9711, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6350.3232, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-699.0228, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10297.5654, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(615.3096, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1269],
        [ 0.1299],
        [-0.0085],
        ...,
        [-2.5946],
        [-2.5878],
        [-2.5851]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-293917.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0129],
        [1.0151],
        [1.0230],
        ...,
        [1.0000],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368883.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.7809, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0130],
        [1.0152],
        [1.0231],
        ...,
        [0.9999],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368890.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.7809, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.5967e-03,  1.3206e-03,  3.2230e-03,  ...,  9.1487e-03,
         -9.6003e-04,  1.6665e-03],
        [ 0.0000e+00, -2.1533e-04, -4.6728e-05,  ...,  1.2706e-03,
          0.0000e+00,  5.8024e-05],
        [ 0.0000e+00, -2.1533e-04, -4.6728e-05,  ...,  1.2706e-03,
          0.0000e+00,  5.8024e-05],
        ...,
        [ 0.0000e+00, -2.1533e-04, -4.6728e-05,  ...,  1.2706e-03,
          0.0000e+00,  5.8024e-05],
        [ 0.0000e+00, -2.1533e-04, -4.6728e-05,  ...,  1.2706e-03,
          0.0000e+00,  5.8024e-05],
        [ 0.0000e+00, -2.1533e-04, -4.6728e-05,  ...,  1.2706e-03,
          0.0000e+00,  5.8024e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2951.5254, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.6736, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.7299, device='cuda:0')



h[100].sum tensor(-11.5172, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.6005, device='cuda:0')



h[200].sum tensor(-9.1311, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.4300, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0029, 0.0069,  ..., 0.0220, 0.0000, 0.0037],
        [0.0000, 0.0013, 0.0033,  ..., 0.0131, 0.0000, 0.0019],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0002],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62894.2734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1228, 0.0118, 0.0865,  ..., 0.1014, 0.0000, 0.0000],
        [0.0574, 0.0241, 0.0524,  ..., 0.0583, 0.0000, 0.0000],
        [0.0206, 0.0424, 0.0333,  ..., 0.0334, 0.0000, 0.0000],
        ...,
        [0.0049, 0.0532, 0.0266,  ..., 0.0227, 0.0000, 0.0000],
        [0.0049, 0.0532, 0.0266,  ..., 0.0227, 0.0000, 0.0000],
        [0.0049, 0.0532, 0.0266,  ..., 0.0227, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(717290.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11914.3740, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-57.1263, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6197.4316, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-708.0436, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10460.7227, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(626.3077, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0738],
        [-0.4715],
        [-1.0017],
        ...,
        [-2.5995],
        [-2.5928],
        [-2.5901]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-279559.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0130],
        [1.0152],
        [1.0231],
        ...,
        [0.9999],
        [0.9987],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368890.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.1055, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0131],
        [1.0153],
        [1.0232],
        ...,
        [0.9999],
        [0.9987],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368897.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.1055, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.1572e-03,  1.2033e-03,  2.9747e-03,  ...,  8.5458e-03,
         -8.8199e-04,  1.5100e-03],
        [ 0.0000e+00, -2.1432e-04, -4.1306e-05,  ...,  1.2783e-03,
          0.0000e+00,  2.6585e-05],
        [ 0.0000e+00, -2.1432e-04, -4.1306e-05,  ...,  1.2783e-03,
          0.0000e+00,  2.6585e-05],
        ...,
        [ 0.0000e+00, -2.1432e-04, -4.1306e-05,  ...,  1.2783e-03,
          0.0000e+00,  2.6585e-05],
        [ 0.0000e+00, -2.1432e-04, -4.1306e-05,  ...,  1.2783e-03,
          0.0000e+00,  2.6585e-05],
        [ 0.0000e+00, -2.1432e-04, -4.1306e-05,  ...,  1.2783e-03,
          0.0000e+00,  2.6585e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2900.0129, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.1393, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.0845, device='cuda:0')



h[100].sum tensor(-10.4531, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.1735, device='cuda:0')



h[200].sum tensor(-9.6290, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.1627, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0030, 0.0073,  ..., 0.0229, 0.0000, 0.0037],
        [0.0000, 0.0012, 0.0030,  ..., 0.0125, 0.0000, 0.0016],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0001],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0001],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0001],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0001]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61348.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0964, 0.0110, 0.0720,  ..., 0.0847, 0.0000, 0.0000],
        [0.0592, 0.0198, 0.0533,  ..., 0.0596, 0.0000, 0.0000],
        [0.0373, 0.0325, 0.0424,  ..., 0.0447, 0.0000, 0.0000],
        ...,
        [0.0047, 0.0533, 0.0264,  ..., 0.0225, 0.0000, 0.0000],
        [0.0047, 0.0533, 0.0263,  ..., 0.0225, 0.0000, 0.0000],
        [0.0047, 0.0533, 0.0263,  ..., 0.0224, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(712142., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11726.8711, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-62.8597, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6123.1768, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-698.7737, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10405.6816, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(607.9341, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0414],
        [-0.0871],
        [-0.1794],
        ...,
        [-2.6107],
        [-2.6045],
        [-2.6021]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-272468.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0131],
        [1.0153],
        [1.0232],
        ...,
        [0.9999],
        [0.9987],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368897.6250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 350.0 event: 1750 loss: tensor(463.1737, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6763],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(299.5100, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0132],
        [1.0154],
        [1.0232],
        ...,
        [0.9999],
        [0.9987],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368904.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6763],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(299.5100, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.0763e-02,  5.4983e-03,  1.2121e-02,  ...,  3.0571e-02,
         -3.5403e-03,  5.9577e-03],
        [ 0.0000e+00, -2.1777e-04, -3.4210e-05,  ...,  1.2787e-03,
          0.0000e+00, -1.8948e-05],
        [-1.2890e-02,  3.3309e-03,  7.5121e-03,  ...,  1.9464e-02,
         -2.1979e-03,  3.6915e-03],
        ...,
        [ 0.0000e+00, -2.1777e-04, -3.4210e-05,  ...,  1.2787e-03,
          0.0000e+00, -1.8948e-05],
        [ 0.0000e+00, -2.1777e-04, -3.4210e-05,  ...,  1.2787e-03,
          0.0000e+00, -1.8948e-05],
        [ 0.0000e+00, -2.1777e-04, -3.4210e-05,  ...,  1.2787e-03,
          0.0000e+00, -1.8948e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3183.9734, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.2587, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.8364, device='cuda:0')



h[100].sum tensor(-15.3627, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.1850, device='cuda:0')



h[200].sum tensor(-6.2651, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-32.8440, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0151, 0.0335,  ..., 0.0860, 0.0000, 0.0164],
        [0.0000, 0.0179, 0.0398,  ..., 0.1013, 0.0000, 0.0195],
        [0.0000, 0.0039, 0.0092,  ..., 0.0276, 0.0000, 0.0045],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69437.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.4468, 0.0000, 0.2628,  ..., 0.3074, 0.0000, 0.0000],
        [0.3582, 0.0000, 0.2141,  ..., 0.2516, 0.0000, 0.0000],
        [0.2060, 0.0000, 0.1311,  ..., 0.1550, 0.0000, 0.0000],
        ...,
        [0.0045, 0.0535, 0.0259,  ..., 0.0219, 0.0000, 0.0000],
        [0.0045, 0.0535, 0.0259,  ..., 0.0219, 0.0000, 0.0000],
        [0.0045, 0.0535, 0.0258,  ..., 0.0219, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(748608.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12672.8193, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-33.2189, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6272.8794, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-745.9863, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11055.9492, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(684.7277, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0940],
        [ 0.1273],
        [ 0.1609],
        ...,
        [-2.6355],
        [-2.6288],
        [-2.6260]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-297956.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0132],
        [1.0154],
        [1.0232],
        ...,
        [0.9999],
        [0.9987],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368904.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6245],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(246.7333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0132],
        [1.0155],
        [1.0233],
        ...,
        [0.9999],
        [0.9987],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368911.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6245],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(246.7333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.3119e-04, -2.5281e-05,  ...,  1.2926e-03,
          0.0000e+00, -5.7801e-05],
        [-1.1891e-02,  3.0472e-03,  6.9430e-03,  ...,  1.8087e-02,
         -2.0213e-03,  3.3676e-03],
        [-1.1872e-02,  3.0421e-03,  6.9321e-03,  ...,  1.8060e-02,
         -2.0182e-03,  3.3622e-03],
        ...,
        [ 0.0000e+00, -2.3119e-04, -2.5281e-05,  ...,  1.2926e-03,
          0.0000e+00, -5.7801e-05],
        [ 0.0000e+00, -2.3119e-04, -2.5281e-05,  ...,  1.2926e-03,
          0.0000e+00, -5.7801e-05],
        [ 0.0000e+00, -2.3119e-04, -2.5281e-05,  ...,  1.2926e-03,
          0.0000e+00, -5.7801e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3030.4604, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.2583, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.6362, device='cuda:0')



h[100].sum tensor(-12.5965, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.0951, device='cuda:0')



h[200].sum tensor(-7.9745, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.0566, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0031, 0.0070,  ..., 0.0222, 0.0000, 0.0034],
        [0.0000, 0.0056, 0.0128,  ..., 0.0361, 0.0000, 0.0062],
        [0.0000, 0.0177, 0.0396,  ..., 0.1009, 0.0000, 0.0193],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63663.9297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0847, 0.0179, 0.0678,  ..., 0.0743, 0.0000, 0.0000],
        [0.1572, 0.0059, 0.1066,  ..., 0.1213, 0.0000, 0.0000],
        [0.2829, 0.0000, 0.1743,  ..., 0.2021, 0.0000, 0.0000],
        ...,
        [0.0043, 0.0536, 0.0254,  ..., 0.0216, 0.0000, 0.0000],
        [0.0043, 0.0536, 0.0254,  ..., 0.0216, 0.0000, 0.0000],
        [0.0043, 0.0536, 0.0254,  ..., 0.0215, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(723339.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11782.0410, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-56.5230, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6206.1470, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-710.9223, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10588.1523, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(619.9788, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7008],
        [-0.2967],
        [ 0.0502],
        ...,
        [-2.6466],
        [-2.6398],
        [-2.6369]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-277775.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0132],
        [1.0155],
        [1.0233],
        ...,
        [0.9999],
        [0.9987],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368911.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.8062],
        [0.4016],
        [0.3042],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.1854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0133],
        [1.0155],
        [1.0233],
        ...,
        [0.9998],
        [0.9986],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368918.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.8062],
        [0.4016],
        [0.3042],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.1854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.6381e-03,  1.8740e-03,  4.4634e-03,  ...,  1.2091e-02,
         -1.2945e-03,  2.1104e-03],
        [-2.5273e-02,  6.7430e-03,  1.4809e-02,  ...,  3.7026e-02,
         -4.2832e-03,  7.1948e-03],
        [-1.1793e-02,  3.0212e-03,  6.9010e-03,  ...,  1.7966e-02,
         -1.9987e-03,  3.3084e-03],
        ...,
        [ 0.0000e+00, -2.3496e-04, -1.7516e-05,  ...,  1.2911e-03,
          0.0000e+00, -9.1906e-05],
        [ 0.0000e+00, -2.3496e-04, -1.7516e-05,  ...,  1.2911e-03,
          0.0000e+00, -9.1906e-05],
        [ 0.0000e+00, -2.3496e-04, -1.7516e-05,  ...,  1.2911e-03,
          0.0000e+00, -9.1906e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2864.4797, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.2865, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.3746, device='cuda:0')



h[100].sum tensor(-9.8524, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.9893, device='cuda:0')



h[200].sum tensor(-9.4394, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.1846, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0200, 0.0445,  ..., 0.1127, 0.0000, 0.0215],
        [0.0000, 0.0147, 0.0332,  ..., 0.0854, 0.0000, 0.0160],
        [0.0000, 0.0151, 0.0339,  ..., 0.0872, 0.0000, 0.0163],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60082.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3082, 0.0000, 0.1870,  ..., 0.2191, 0.0000, 0.0000],
        [0.3217, 0.0000, 0.1937,  ..., 0.2284, 0.0000, 0.0000],
        [0.3203, 0.0000, 0.1930,  ..., 0.2276, 0.0000, 0.0000],
        ...,
        [0.0040, 0.0538, 0.0251,  ..., 0.0211, 0.0000, 0.0000],
        [0.0040, 0.0538, 0.0251,  ..., 0.0211, 0.0000, 0.0000],
        [0.0040, 0.0538, 0.0250,  ..., 0.0211, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(717309.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11146.3262, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-74.7558, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6583.5879, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-689.6104, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10440.9219, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(576.2465, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1737],
        [ 0.1689],
        [ 0.1582],
        ...,
        [-2.6724],
        [-2.6655],
        [-2.6625]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-317345.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0133],
        [1.0155],
        [1.0233],
        ...,
        [0.9998],
        [0.9986],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368918.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(330.7854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0134],
        [1.0156],
        [1.0234],
        ...,
        [0.9998],
        [0.9986],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368925.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(330.7854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.3585e-04, -1.1947e-05,  ...,  1.2889e-03,
          0.0000e+00, -1.1392e-04],
        [ 0.0000e+00, -2.3585e-04, -1.1947e-05,  ...,  1.2889e-03,
          0.0000e+00, -1.1392e-04],
        [ 0.0000e+00, -2.3585e-04, -1.1947e-05,  ...,  1.2889e-03,
          0.0000e+00, -1.1392e-04],
        ...,
        [ 0.0000e+00, -2.3585e-04, -1.1947e-05,  ...,  1.2889e-03,
          0.0000e+00, -1.1392e-04],
        [ 0.0000e+00, -2.3585e-04, -1.1947e-05,  ...,  1.2889e-03,
          0.0000e+00, -1.1392e-04],
        [ 0.0000e+00, -2.3585e-04, -1.1947e-05,  ...,  1.2889e-03,
          0.0000e+00, -1.1392e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3276.6743, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.4663, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.3254, device='cuda:0')



h[100].sum tensor(-16.8658, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.8308, device='cuda:0')



h[200].sum tensor(-4.9880, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-36.2737, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0013, 0.0032,  ..., 0.0129, 0.0000, 0.0015],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0123,  ..., 0.0351, 0.0000, 0.0058],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72899.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0857, 0.0076, 0.0665,  ..., 0.0766, 0.0000, 0.0000],
        [0.1079, 0.0093, 0.0795,  ..., 0.0900, 0.0000, 0.0000],
        [0.2381, 0.0048, 0.1503,  ..., 0.1737, 0.0000, 0.0000],
        ...,
        [0.0039, 0.0541, 0.0251,  ..., 0.0210, 0.0000, 0.0000],
        [0.0039, 0.0541, 0.0251,  ..., 0.0210, 0.0000, 0.0000],
        [0.0039, 0.0541, 0.0250,  ..., 0.0210, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(776376.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(13242.2246, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-22.0744, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6396.4512, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-767.3182, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11565.9893, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(707.7983, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1814],
        [ 0.1493],
        [ 0.1119],
        ...,
        [-2.6906],
        [-2.6835],
        [-2.6805]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-313163.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0134],
        [1.0156],
        [1.0234],
        ...,
        [0.9998],
        [0.9986],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368925.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2432],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(263.5742, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0135],
        [1.0157],
        [1.0234],
        ...,
        [0.9998],
        [0.9986],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368932.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2432],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(263.5742, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.9721e-03,  2.2459e-03,  5.2676e-03,  ...,  1.4003e-02,
         -1.5114e-03,  2.4646e-03],
        [-4.3577e-03,  9.6819e-04,  2.5551e-03,  ...,  7.4639e-03,
         -7.3410e-04,  1.1313e-03],
        [-1.4194e-02,  3.6918e-03,  8.3373e-03,  ...,  2.1404e-02,
         -2.3911e-03,  3.9736e-03],
        ...,
        [ 0.0000e+00, -2.3844e-04, -6.6261e-06,  ...,  1.2881e-03,
          0.0000e+00, -1.2798e-04],
        [ 0.0000e+00, -2.3844e-04, -6.6261e-06,  ...,  1.2881e-03,
          0.0000e+00, -1.2798e-04],
        [ 0.0000e+00, -2.3844e-04, -6.6261e-06,  ...,  1.2881e-03,
          0.0000e+00, -1.2798e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3075.3994, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.4101, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.9765, device='cuda:0')



h[100].sum tensor(-13.3749, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.4429, device='cuda:0')



h[200].sum tensor(-7.1721, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.9034, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0072, 0.0174,  ..., 0.0471, 0.0000, 0.0080],
        [0.0000, 0.0096, 0.0225,  ..., 0.0594, 0.0000, 0.0105],
        [0.0000, 0.0067, 0.0163,  ..., 0.0446, 0.0000, 0.0075],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65231.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2042, 0.0000, 0.1280,  ..., 0.1562, 0.0000, 0.0000],
        [0.2167, 0.0000, 0.1351,  ..., 0.1640, 0.0000, 0.0000],
        [0.2085, 0.0000, 0.1306,  ..., 0.1591, 0.0000, 0.0000],
        ...,
        [0.0039, 0.0543, 0.0252,  ..., 0.0210, 0.0000, 0.0000],
        [0.0039, 0.0543, 0.0251,  ..., 0.0210, 0.0000, 0.0000],
        [0.0039, 0.0543, 0.0251,  ..., 0.0210, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(734296.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11639.3359, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-54.0508, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6579.8428, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-722.3163, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10801.2070, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(627.5709, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2238],
        [ 0.1994],
        [ 0.1664],
        ...,
        [-2.7054],
        [-2.6988],
        [-2.6958]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-328047.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0135],
        [1.0157],
        [1.0234],
        ...,
        [0.9998],
        [0.9986],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368932.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2986],
        [0.4417],
        [0.4258],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.1504, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0136],
        [1.0157],
        [1.0234],
        ...,
        [0.9998],
        [0.9985],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368939.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2986],
        [0.4417],
        [0.4258],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.1504, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2638e-02,  3.2516e-03,  7.4364e-03,  ...,  1.9229e-02,
         -2.1225e-03,  3.5244e-03],
        [-1.7997e-02,  4.7371e-03,  1.0590e-02,  ...,  2.6833e-02,
         -3.0225e-03,  5.0750e-03],
        [-1.5790e-02,  4.1254e-03,  9.2915e-03,  ...,  2.3702e-02,
         -2.6519e-03,  4.4366e-03],
        ...,
        [ 0.0000e+00, -2.5153e-04, -4.9939e-07,  ...,  1.2976e-03,
          0.0000e+00, -1.3220e-04],
        [ 0.0000e+00, -2.5153e-04, -4.9939e-07,  ...,  1.2976e-03,
          0.0000e+00, -1.3220e-04],
        [ 0.0000e+00, -2.5153e-04, -4.9939e-07,  ...,  1.2976e-03,
          0.0000e+00, -1.3220e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3149.0071, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.9781, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.6936, device='cuda:0')



h[100].sum tensor(-14.4495, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.8884, device='cuda:0')



h[200].sum tensor(-6.7659, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-31.2694, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0130, 0.0297,  ..., 0.0769, 0.0000, 0.0141],
        [0.0000, 0.0159, 0.0359,  ..., 0.0918, 0.0000, 0.0171],
        [0.0000, 0.0167, 0.0376,  ..., 0.0960, 0.0000, 0.0180],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66254.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2649, 0.0000, 0.1629,  ..., 0.1936, 0.0000, 0.0000],
        [0.3188, 0.0000, 0.1922,  ..., 0.2283, 0.0000, 0.0000],
        [0.3368, 0.0000, 0.2023,  ..., 0.2397, 0.0000, 0.0000],
        ...,
        [0.0084, 0.0518, 0.0275,  ..., 0.0244, 0.0000, 0.0000],
        [0.0106, 0.0504, 0.0286,  ..., 0.0260, 0.0000, 0.0000],
        [0.0084, 0.0517, 0.0275,  ..., 0.0244, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(737169.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11721.1270, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-51.4872, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6545.3120, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-728.9422, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10855.0469, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(638.5325, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1836],
        [ 0.1817],
        [ 0.1708],
        ...,
        [-2.4013],
        [-2.3399],
        [-2.3414]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-329829.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0136],
        [1.0157],
        [1.0234],
        ...,
        [0.9998],
        [0.9985],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368939.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4211],
        [0.5288],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.3395, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0136],
        [1.0158],
        [1.0235],
        ...,
        [0.9997],
        [0.9985],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368946.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4211],
        [0.5288],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.3395, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.5863e-02,  6.9031e-03,  1.5243e-02,  ...,  3.8058e-02,
         -4.3305e-03,  7.3551e-03],
        [-1.0013e-02,  2.5046e-03,  5.9054e-03,  ...,  1.5539e-02,
         -1.6765e-03,  2.7632e-03],
        [-1.3840e-02,  3.5667e-03,  8.1601e-03,  ...,  2.0977e-02,
         -2.3173e-03,  3.8720e-03],
        ...,
        [ 0.0000e+00, -2.7380e-04,  7.1469e-06,  ...,  1.3151e-03,
          0.0000e+00, -1.3745e-04],
        [ 0.0000e+00, -2.7380e-04,  7.1469e-06,  ...,  1.3151e-03,
          0.0000e+00, -1.3745e-04],
        [ 0.0000e+00, -2.7380e-04,  7.1469e-06,  ...,  1.3151e-03,
          0.0000e+00, -1.3745e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3036.4131, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.9576, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.2865, device='cuda:0')



h[100].sum tensor(-12.3639, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.0044, device='cuda:0')



h[200].sum tensor(-8.5188, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.5748, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.2806e-02, 2.9564e-02,  ..., 7.6542e-02, 0.0000e+00,
         1.3969e-02],
        [0.0000e+00, 1.6021e-02, 3.6397e-02,  ..., 9.3037e-02, 0.0000e+00,
         1.7327e-02],
        [0.0000e+00, 1.4476e-02, 3.3126e-02,  ..., 8.5169e-02, 0.0000e+00,
         1.5717e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 2.9782e-05,  ..., 5.4800e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.9770e-05,  ..., 5.4779e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.9762e-05,  ..., 5.4764e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64052.3828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3177, 0.0000, 0.1935,  ..., 0.2256, 0.0000, 0.0000],
        [0.3140, 0.0000, 0.1911,  ..., 0.2237, 0.0000, 0.0000],
        [0.2879, 0.0000, 0.1758,  ..., 0.2079, 0.0000, 0.0000],
        ...,
        [0.0039, 0.0543, 0.0253,  ..., 0.0213, 0.0000, 0.0000],
        [0.0039, 0.0543, 0.0253,  ..., 0.0213, 0.0000, 0.0000],
        [0.0039, 0.0543, 0.0253,  ..., 0.0213, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(731293.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11527.4951, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-61.5560, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6459.7573, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-716.0478, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10743.4414, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(614.3303, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2137],
        [ 0.2217],
        [ 0.2282],
        ...,
        [-2.7103],
        [-2.7034],
        [-2.7006]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-316460.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0136],
        [1.0158],
        [1.0235],
        ...,
        [0.9997],
        [0.9985],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368946.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3000],
        [0.3091],
        [0.3157],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.9658, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0137],
        [1.0158],
        [1.0235],
        ...,
        [0.9997],
        [0.9985],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368953., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3000],
        [0.3091],
        [0.3157],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.9658, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4120e-02,  3.6313e-03,  8.3312e-03,  ...,  2.1405e-02,
         -2.3571e-03,  3.9530e-03],
        [-1.1645e-02,  2.9438e-03,  6.8717e-03,  ...,  1.7885e-02,
         -1.9439e-03,  3.2352e-03],
        [-1.3940e-02,  3.5813e-03,  8.2250e-03,  ...,  2.1149e-02,
         -2.3270e-03,  3.9008e-03],
        ...,
        [ 0.0000e+00, -2.9108e-04,  4.9215e-06,  ...,  1.3212e-03,
          0.0000e+00, -1.4254e-04],
        [ 0.0000e+00, -2.9108e-04,  4.9215e-06,  ...,  1.3212e-03,
          0.0000e+00, -1.4254e-04],
        [ 0.0000e+00, -2.9108e-04,  4.9215e-06,  ...,  1.3212e-03,
          0.0000e+00, -1.4254e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2939.0469, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.5200, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.7896, device='cuda:0')



h[100].sum tensor(-10.6813, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.3565, device='cuda:0')



h[200].sum tensor(-9.8896, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.1344, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.0591e-02, 2.4999e-02,  ..., 6.5591e-02, 0.0000e+00,
         1.1711e-02],
        [0.0000e+00, 1.4157e-02, 3.2578e-02,  ..., 8.3890e-02, 0.0000e+00,
         1.5437e-02],
        [0.0000e+00, 1.6064e-02, 3.6635e-02,  ..., 9.3697e-02, 0.0000e+00,
         1.7430e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 2.0510e-05,  ..., 5.5062e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.0502e-05,  ..., 5.5041e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.0497e-05,  ..., 5.5026e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60782.7891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3509, 0.0000, 0.2097,  ..., 0.2486, 0.0000, 0.0000],
        [0.4177, 0.0000, 0.2466,  ..., 0.2911, 0.0000, 0.0000],
        [0.4324, 0.0000, 0.2552,  ..., 0.3002, 0.0000, 0.0000],
        ...,
        [0.0039, 0.0544, 0.0253,  ..., 0.0214, 0.0000, 0.0000],
        [0.0039, 0.0543, 0.0253,  ..., 0.0214, 0.0000, 0.0000],
        [0.0039, 0.0543, 0.0253,  ..., 0.0214, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(716305.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11058.9824, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-73.6936, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6415.2529, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-696.0462, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10459.8096, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(581.0356, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0608],
        [ 0.0465],
        [ 0.0452],
        ...,
        [-2.7229],
        [-2.7160],
        [-2.7130]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-303550.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0137],
        [1.0158],
        [1.0235],
        ...,
        [0.9997],
        [0.9985],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368953., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(382.9866, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0137],
        [1.0158],
        [1.0236],
        ...,
        [0.9997],
        [0.9984],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368959.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(382.9866, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.1473e-03,  1.1245e-03,  3.0292e-03,  ...,  8.6418e-03,
         -8.5662e-04,  1.3467e-03],
        [ 0.0000e+00, -3.0648e-04, -9.1409e-06,  ...,  1.3118e-03,
          0.0000e+00, -1.4811e-04],
        [ 0.0000e+00, -3.0648e-04, -9.1409e-06,  ...,  1.3118e-03,
          0.0000e+00, -1.4811e-04],
        ...,
        [ 0.0000e+00, -3.0648e-04, -9.1409e-06,  ...,  1.3118e-03,
          0.0000e+00, -1.4811e-04],
        [ 0.0000e+00, -3.0648e-04, -9.1409e-06,  ...,  1.3118e-03,
          0.0000e+00, -1.4811e-04],
        [ 0.0000e+00, -3.0648e-04, -9.1409e-06,  ...,  1.3118e-03,
          0.0000e+00, -1.4811e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3439.8494, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.9122, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.4799, device='cuda:0')



h[100].sum tensor(-19.2021, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-7.9088, device='cuda:0')



h[200].sum tensor(-4.7561, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-41.9980, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0026, 0.0068,  ..., 0.0217, 0.0000, 0.0030],
        [0.0000, 0.0011, 0.0031,  ..., 0.0127, 0.0000, 0.0014],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(80798.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0861, 0.0140, 0.0665,  ..., 0.0782, 0.0000, 0.0000],
        [0.0468, 0.0271, 0.0467,  ..., 0.0516, 0.0000, 0.0000],
        [0.0200, 0.0433, 0.0328,  ..., 0.0327, 0.0000, 0.0000],
        ...,
        [0.0037, 0.0546, 0.0254,  ..., 0.0212, 0.0000, 0.0000],
        [0.0082, 0.0516, 0.0282,  ..., 0.0247, 0.0000, 0.0000],
        [0.0259, 0.0417, 0.0376,  ..., 0.0362, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(821304.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(14697.9238, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(9.9788, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6214.2197, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-814.9734, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12391.3242, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(789.0807, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1983],
        [-0.7195],
        [-1.2824],
        ...,
        [-2.6486],
        [-2.4464],
        [-2.0811]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-296592.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0137],
        [1.0158],
        [1.0236],
        ...,
        [0.9997],
        [0.9984],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368959.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(213.9800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0138],
        [1.0159],
        [1.0236],
        ...,
        [0.9996],
        [0.9984],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368965.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(213.9800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.1993e-04, -2.5436e-05,  ...,  1.2941e-03,
          0.0000e+00, -1.4967e-04],
        [ 0.0000e+00, -3.1993e-04, -2.5436e-05,  ...,  1.2941e-03,
          0.0000e+00, -1.4967e-04],
        [ 0.0000e+00, -3.1993e-04, -2.5436e-05,  ...,  1.2941e-03,
          0.0000e+00, -1.4967e-04],
        ...,
        [ 0.0000e+00, -3.1993e-04, -2.5436e-05,  ...,  1.2941e-03,
          0.0000e+00, -1.4967e-04],
        [ 0.0000e+00, -3.1993e-04, -2.5436e-05,  ...,  1.2941e-03,
          0.0000e+00, -1.4967e-04],
        [ 0.0000e+00, -3.1993e-04, -2.5436e-05,  ...,  1.2941e-03,
          0.0000e+00, -1.4967e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2929.1973, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.7377, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.0295, device='cuda:0')



h[100].sum tensor(-10.8223, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.4187, device='cuda:0')



h[200].sum tensor(-10.1682, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.4649, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60953.4453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0031, 0.0536, 0.0241,  ..., 0.0198, 0.0000, 0.0000],
        [0.0032, 0.0538, 0.0243,  ..., 0.0199, 0.0000, 0.0000],
        [0.0034, 0.0539, 0.0245,  ..., 0.0202, 0.0000, 0.0000],
        ...,
        [0.0037, 0.0550, 0.0255,  ..., 0.0210, 0.0000, 0.0000],
        [0.0037, 0.0550, 0.0255,  ..., 0.0210, 0.0000, 0.0000],
        [0.0037, 0.0549, 0.0255,  ..., 0.0210, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(721701.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11060.8623, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-70.3333, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6703.7969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-694.9053, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10576.0205, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(584.7086, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.9144],
        [-2.9488],
        [-2.9651],
        ...,
        [-2.7710],
        [-2.7640],
        [-2.7610]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-334221.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0138],
        [1.0159],
        [1.0236],
        ...,
        [0.9996],
        [0.9984],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368965.5625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 360.0 event: 1800 loss: tensor(483.6108, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.8734, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0138],
        [1.0159],
        [1.0237],
        ...,
        [0.9996],
        [0.9984],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368972.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.8734, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.3235e-04, -3.9302e-05,  ...,  1.2836e-03,
          0.0000e+00, -1.4601e-04],
        [-4.0356e-03,  7.9115e-04,  2.3475e-03,  ...,  7.0442e-03,
         -6.6751e-04,  1.0291e-03],
        [-4.0356e-03,  7.9115e-04,  2.3475e-03,  ...,  7.0442e-03,
         -6.6751e-04,  1.0291e-03],
        ...,
        [ 0.0000e+00, -3.3235e-04, -3.9302e-05,  ...,  1.2836e-03,
          0.0000e+00, -1.4601e-04],
        [ 0.0000e+00, -3.3235e-04, -3.9302e-05,  ...,  1.2836e-03,
          0.0000e+00, -1.4601e-04],
        [ 0.0000e+00, -3.3235e-04, -3.9302e-05,  ...,  1.2836e-03,
          0.0000e+00, -1.4601e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2916.5303, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.5043, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.8619, device='cuda:0')



h[100].sum tensor(-10.6574, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.3752, device='cuda:0')



h[200].sum tensor(-10.5127, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.2339, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0014, 0.0043,  ..., 0.0157, 0.0000, 0.0019],
        [0.0000, 0.0014, 0.0043,  ..., 0.0158, 0.0000, 0.0019],
        [0.0000, 0.0014, 0.0043,  ..., 0.0159, 0.0000, 0.0019],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61313.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0539, 0.0232, 0.0500,  ..., 0.0569, 0.0000, 0.0000],
        [0.0460, 0.0277, 0.0455,  ..., 0.0522, 0.0000, 0.0000],
        [0.0439, 0.0291, 0.0445,  ..., 0.0508, 0.0000, 0.0000],
        ...,
        [0.0037, 0.0552, 0.0255,  ..., 0.0210, 0.0000, 0.0000],
        [0.0037, 0.0552, 0.0255,  ..., 0.0210, 0.0000, 0.0000],
        [0.0037, 0.0552, 0.0255,  ..., 0.0210, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(730654., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11206.2637, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-70.3482, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6952.0459, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-696.4057, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10731.7930, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(589.1216, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0189],
        [-1.4575],
        [-1.8503],
        ...,
        [-2.7903],
        [-2.7827],
        [-2.7794]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-363513.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0138],
        [1.0159],
        [1.0237],
        ...,
        [0.9996],
        [0.9984],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368972.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.3535, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0139],
        [1.0160],
        [1.0237],
        ...,
        [0.9995],
        [0.9983],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368979.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.3535, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.5586e-04, -4.5274e-05,  ...,  1.3129e-03,
          0.0000e+00, -1.3193e-04],
        [ 0.0000e+00, -3.5586e-04, -4.5274e-05,  ...,  1.3129e-03,
          0.0000e+00, -1.3193e-04],
        [ 0.0000e+00, -3.5586e-04, -4.5274e-05,  ...,  1.3129e-03,
          0.0000e+00, -1.3193e-04],
        ...,
        [ 0.0000e+00, -3.5586e-04, -4.5274e-05,  ...,  1.3129e-03,
          0.0000e+00, -1.3193e-04],
        [ 0.0000e+00, -3.5586e-04, -4.5274e-05,  ...,  1.3129e-03,
          0.0000e+00, -1.3193e-04],
        [ 0.0000e+00, -3.5586e-04, -4.5274e-05,  ...,  1.3129e-03,
          0.0000e+00, -1.3193e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3126.2437, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.1129, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.6752, device='cuda:0')



h[100].sum tensor(-13.8197, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.6242, device='cuda:0')



h[200].sum tensor(-9.3007, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-29.8661, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68479.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0528, 0.0243, 0.0513,  ..., 0.0543, 0.0000, 0.0000],
        [0.0490, 0.0272, 0.0489,  ..., 0.0511, 0.0000, 0.0000],
        [0.0409, 0.0320, 0.0449,  ..., 0.0457, 0.0000, 0.0000],
        ...,
        [0.0038, 0.0548, 0.0255,  ..., 0.0215, 0.0000, 0.0000],
        [0.0038, 0.0548, 0.0255,  ..., 0.0214, 0.0000, 0.0000],
        [0.0038, 0.0548, 0.0255,  ..., 0.0214, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(760597.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12460.6201, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-41.3788, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6560.2871, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-738.6260, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11276.4785, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(663.9158, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0334],
        [-0.0733],
        [-0.1588],
        ...,
        [-2.4667],
        [-2.4364],
        [-2.3324]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-325382.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0139],
        [1.0160],
        [1.0237],
        ...,
        [0.9995],
        [0.9983],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368979.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3020],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(320.2350, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0140],
        [1.0160],
        [1.0238],
        ...,
        [0.9995],
        [0.9982],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368986.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3020],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(320.2350, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.8316e-03,  6.8718e-04,  2.2240e-03,  ...,  6.8348e-03,
         -6.2990e-04,  1.0031e-03],
        [-5.6800e-03,  1.2023e-03,  3.3195e-03,  ...,  9.4803e-03,
         -9.3376e-04,  1.5429e-03],
        [ 0.0000e+00, -3.8069e-04, -4.6990e-05,  ...,  1.3507e-03,
          0.0000e+00, -1.1581e-04],
        ...,
        [ 0.0000e+00, -3.8069e-04, -4.6990e-05,  ...,  1.3507e-03,
          0.0000e+00, -1.1581e-04],
        [ 0.0000e+00, -3.8069e-04, -4.6990e-05,  ...,  1.3507e-03,
          0.0000e+00, -1.1581e-04],
        [ 0.0000e+00, -3.8069e-04, -4.6990e-05,  ...,  1.3507e-03,
          0.0000e+00, -1.1581e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3301.9678, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.8306, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.4858, device='cuda:0')



h[100].sum tensor(-16.3674, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.6129, device='cuda:0')



h[200].sum tensor(-8.4854, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-35.1167, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0073, 0.0186,  ..., 0.0509, 0.0000, 0.0088],
        [0.0000, 0.0025, 0.0069,  ..., 0.0223, 0.0000, 0.0032],
        [0.0000, 0.0012, 0.0034,  ..., 0.0138, 0.0000, 0.0016],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73864.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1608, 0.0000, 0.1058,  ..., 0.1268, 0.0000, 0.0000],
        [0.1041, 0.0117, 0.0766,  ..., 0.0894, 0.0000, 0.0000],
        [0.0527, 0.0252, 0.0505,  ..., 0.0552, 0.0000, 0.0000],
        ...,
        [0.0084, 0.0517, 0.0277,  ..., 0.0251, 0.0000, 0.0000],
        [0.0107, 0.0503, 0.0288,  ..., 0.0267, 0.0000, 0.0000],
        [0.0084, 0.0517, 0.0277,  ..., 0.0251, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(778571.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(13286.5977, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-17.8238, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6167.9136, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-769.7142, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11509.1318, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(720.8882, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1516],
        [-0.1245],
        [-0.6327],
        ...,
        [-2.4203],
        [-2.3629],
        [-2.4111]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-287564.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0140],
        [1.0160],
        [1.0238],
        ...,
        [0.9995],
        [0.9982],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368986.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.9907, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0141],
        [1.0161],
        [1.0239],
        ...,
        [0.9994],
        [0.9982],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368993.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.9907, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -4.0443e-04, -5.2503e-05,  ...,  1.3771e-03,
          0.0000e+00, -9.7182e-05],
        [-7.0217e-03,  1.5533e-03,  4.1136e-03,  ...,  1.1441e-02,
         -1.1508e-03,  1.9566e-03],
        [-1.2383e-02,  3.0482e-03,  7.2949e-03,  ...,  1.9125e-02,
         -2.0295e-03,  3.5249e-03],
        ...,
        [ 0.0000e+00, -4.0443e-04, -5.2503e-05,  ...,  1.3771e-03,
          0.0000e+00, -9.7182e-05],
        [ 0.0000e+00, -4.0443e-04, -5.2503e-05,  ...,  1.3771e-03,
          0.0000e+00, -9.7182e-05],
        [ 0.0000e+00, -4.0443e-04, -5.2503e-05,  ...,  1.3771e-03,
          0.0000e+00, -9.7182e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3060.1155, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.5916, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.0996, device='cuda:0')



h[100].sum tensor(-12.0774, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.9559, device='cuda:0')



h[200].sum tensor(-11.8474, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.3172, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0028, 0.0075,  ..., 0.0240, 0.0000, 0.0036],
        [0.0000, 0.0051, 0.0133,  ..., 0.0382, 0.0000, 0.0064],
        [0.0000, 0.0093, 0.0232,  ..., 0.0621, 0.0000, 0.0111],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64076.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1024, 0.0158, 0.0761,  ..., 0.0877, 0.0000, 0.0000],
        [0.1807, 0.0046, 0.1169,  ..., 0.1388, 0.0000, 0.0000],
        [0.2636, 0.0000, 0.1606,  ..., 0.1925, 0.0000, 0.0000],
        ...,
        [0.0040, 0.0541, 0.0255,  ..., 0.0224, 0.0000, 0.0000],
        [0.0040, 0.0541, 0.0255,  ..., 0.0224, 0.0000, 0.0000],
        [0.0040, 0.0541, 0.0255,  ..., 0.0223, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(732496.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11353.5615, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-63.8457, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6510.0410, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-710.7877, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10531.4688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(617.0497, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4036],
        [ 0.0261],
        [ 0.2325],
        ...,
        [-2.7466],
        [-2.7399],
        [-2.7370]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-310580.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0141],
        [1.0161],
        [1.0239],
        ...,
        [0.9994],
        [0.9982],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368993.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(175.1064, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0142],
        [1.0162],
        [1.0240],
        ...,
        [0.9993],
        [0.9981],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368999.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(175.1064, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0043e-02,  2.3775e-03,  5.9031e-03,  ...,  1.5795e-02,
         -1.6409e-03,  2.8608e-03],
        [ 0.0000e+00, -4.2336e-04, -6.1677e-05,  ...,  1.3831e-03,
          0.0000e+00, -8.1347e-05],
        [-7.8331e-03,  1.7612e-03,  4.5906e-03,  ...,  1.2624e-02,
         -1.2798e-03,  2.2134e-03],
        ...,
        [ 0.0000e+00, -4.2336e-04, -6.1677e-05,  ...,  1.3831e-03,
          0.0000e+00, -8.1347e-05],
        [ 0.0000e+00, -4.2336e-04, -6.1677e-05,  ...,  1.3831e-03,
          0.0000e+00, -8.1347e-05],
        [ 0.0000e+00, -4.2336e-04, -6.1677e-05,  ...,  1.3831e-03,
          0.0000e+00, -8.1347e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2876.1946, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.9387, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.9358, device='cuda:0')



h[100].sum tensor(-8.8793, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.6160, device='cuda:0')



h[200].sum tensor(-14.3046, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.2020, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0032, 0.0093,  ..., 0.0285, 0.0000, 0.0044],
        [0.0000, 0.0080, 0.0204,  ..., 0.0555, 0.0000, 0.0099],
        [0.0000, 0.0124, 0.0289,  ..., 0.0760, 0.0000, 0.0141],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59288.6484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1961, 0.0000, 0.1230,  ..., 0.1495, 0.0000, 0.0000],
        [0.3031, 0.0000, 0.1813,  ..., 0.2170, 0.0000, 0.0000],
        [0.4275, 0.0000, 0.2503,  ..., 0.2945, 0.0000, 0.0000],
        ...,
        [0.0041, 0.0543, 0.0256,  ..., 0.0226, 0.0000, 0.0000],
        [0.0041, 0.0543, 0.0256,  ..., 0.0225, 0.0000, 0.0000],
        [0.0041, 0.0542, 0.0256,  ..., 0.0225, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(713868.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10832.4355, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-82.9038, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6469.9614, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-680.5226, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10205.7998, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(568.9044, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2122],
        [ 0.1851],
        [ 0.1527],
        ...,
        [-2.7436],
        [-2.7370],
        [-2.7342]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-294700.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0142],
        [1.0162],
        [1.0240],
        ...,
        [0.9993],
        [0.9981],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368999.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.2598, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0143],
        [1.0162],
        [1.0241],
        ...,
        [0.9993],
        [0.9980],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369005.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.2598, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -4.2656e-04, -8.0219e-05,  ...,  1.3618e-03,
          0.0000e+00, -6.9057e-05],
        [-7.2169e-03,  1.5877e-03,  4.2099e-03,  ...,  1.1731e-02,
         -1.1755e-03,  2.0484e-03],
        [-4.8784e-03,  9.3500e-04,  2.8198e-03,  ...,  8.3707e-03,
         -7.9460e-04,  1.3623e-03],
        ...,
        [ 0.0000e+00, -4.2656e-04, -8.0219e-05,  ...,  1.3618e-03,
          0.0000e+00, -6.9057e-05],
        [ 0.0000e+00, -4.2656e-04, -8.0219e-05,  ...,  1.3618e-03,
          0.0000e+00, -6.9057e-05],
        [ 0.0000e+00, -4.2656e-04, -8.0219e-05,  ...,  1.3618e-03,
          0.0000e+00, -6.9057e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2961.8821, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.9197, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.2559, device='cuda:0')



h[100].sum tensor(-10.2346, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.2180, device='cuda:0')



h[200].sum tensor(-13.6590, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.3990, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0043,  ..., 0.0160, 0.0000, 0.0021],
        [0.0000, 0.0022, 0.0063,  ..., 0.0212, 0.0000, 0.0031],
        [0.0000, 0.0103, 0.0253,  ..., 0.0674, 0.0000, 0.0123],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60071.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0701, 0.0193, 0.0593,  ..., 0.0666, 0.0000, 0.0000],
        [0.1258, 0.0094, 0.0879,  ..., 0.1032, 0.0000, 0.0000],
        [0.2237, 0.0000, 0.1395,  ..., 0.1664, 0.0000, 0.0000],
        ...,
        [0.0043, 0.0548, 0.0262,  ..., 0.0228, 0.0000, 0.0000],
        [0.0043, 0.0548, 0.0262,  ..., 0.0228, 0.0000, 0.0000],
        [0.0043, 0.0548, 0.0262,  ..., 0.0228, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(712251.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10934.2852, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-78.3341, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6465.7705, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-684.5829, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10205.7627, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(581.7399, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0495],
        [ 0.1493],
        [ 0.1899],
        ...,
        [-2.7521],
        [-2.7454],
        [-2.7398]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-286070.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0143],
        [1.0162],
        [1.0241],
        ...,
        [0.9993],
        [0.9980],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369005.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(197.2151, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0144],
        [1.0162],
        [1.0241],
        ...,
        [0.9992],
        [0.9980],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369011., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(197.2151, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -4.2644e-04, -9.8355e-05,  ...,  1.3269e-03,
          0.0000e+00, -6.3596e-05],
        [ 0.0000e+00, -4.2644e-04, -9.8355e-05,  ...,  1.3269e-03,
          0.0000e+00, -6.3596e-05],
        [ 0.0000e+00, -4.2644e-04, -9.8355e-05,  ...,  1.3269e-03,
          0.0000e+00, -6.3596e-05],
        ...,
        [ 0.0000e+00, -4.2644e-04, -9.8355e-05,  ...,  1.3269e-03,
          0.0000e+00, -6.3596e-05],
        [ 0.0000e+00, -4.2644e-04, -9.8355e-05,  ...,  1.3269e-03,
          0.0000e+00, -6.3596e-05],
        [ 0.0000e+00, -4.2644e-04, -9.8355e-05,  ...,  1.3269e-03,
          0.0000e+00, -6.3596e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2938.5427, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.6013, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.6953, device='cuda:0')



h[100].sum tensor(-10.0121, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.0725, device='cuda:0')



h[200].sum tensor(-13.8770, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.6265, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62845.5078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0039, 0.0543, 0.0253,  ..., 0.0215, 0.0000, 0.0000],
        [0.0057, 0.0530, 0.0267,  ..., 0.0233, 0.0000, 0.0000],
        [0.0246, 0.0427, 0.0366,  ..., 0.0356, 0.0000, 0.0000],
        ...,
        [0.0045, 0.0556, 0.0267,  ..., 0.0228, 0.0000, 0.0000],
        [0.0045, 0.0556, 0.0266,  ..., 0.0228, 0.0000, 0.0000],
        [0.0045, 0.0556, 0.0266,  ..., 0.0227, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(730794.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11973.7129, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-61.1962, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6310.5879, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-700.2709, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10699.7324, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(617.2248, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6481],
        [-1.3460],
        [-0.8334],
        ...,
        [-2.7743],
        [-2.7683],
        [-2.7657]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-270714.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0144],
        [1.0162],
        [1.0241],
        ...,
        [0.9992],
        [0.9980],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369011., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.0786, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0145],
        [1.0163],
        [1.0242],
        ...,
        [0.9992],
        [0.9979],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369016.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.0786, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -4.2316e-04, -1.1101e-04,  ...,  1.2945e-03,
          0.0000e+00, -6.3522e-05],
        [ 0.0000e+00, -4.2316e-04, -1.1101e-04,  ...,  1.2945e-03,
          0.0000e+00, -6.3522e-05],
        [ 0.0000e+00, -4.2316e-04, -1.1101e-04,  ...,  1.2945e-03,
          0.0000e+00, -6.3522e-05],
        ...,
        [ 0.0000e+00, -4.2316e-04, -1.1101e-04,  ...,  1.2945e-03,
          0.0000e+00, -6.3522e-05],
        [ 0.0000e+00, -4.2316e-04, -1.1101e-04,  ...,  1.2945e-03,
          0.0000e+00, -6.3522e-05],
        [ 0.0000e+00, -4.2316e-04, -1.1101e-04,  ...,  1.2945e-03,
          0.0000e+00, -6.3522e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3035.5581, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.2505, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.0270, device='cuda:0')



h[100].sum tensor(-11.8237, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.9370, device='cuda:0')



h[200].sum tensor(-12.7301, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.2172, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64754.1953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0039, 0.0550, 0.0254,  ..., 0.0212, 0.0000, 0.0000],
        [0.0040, 0.0552, 0.0255,  ..., 0.0213, 0.0000, 0.0000],
        [0.0042, 0.0553, 0.0258,  ..., 0.0216, 0.0000, 0.0000],
        ...,
        [0.0045, 0.0564, 0.0268,  ..., 0.0225, 0.0000, 0.0000],
        [0.0045, 0.0564, 0.0268,  ..., 0.0225, 0.0000, 0.0000],
        [0.0045, 0.0564, 0.0268,  ..., 0.0225, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(738117.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12146.9268, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.5103, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6573.1543, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-711.2169, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10944.3135, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(638.1690, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7627],
        [-2.7228],
        [-2.6356],
        ...,
        [-2.8057],
        [-2.7985],
        [-2.7954]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-297022.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0145],
        [1.0163],
        [1.0242],
        ...,
        [0.9992],
        [0.9979],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369016.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.8557, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0146],
        [1.0163],
        [1.0242],
        ...,
        [0.9991],
        [0.9979],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369022.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.8557, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -4.1929e-04, -1.1792e-04,  ...,  1.2700e-03,
          0.0000e+00, -6.1221e-05],
        [ 0.0000e+00, -4.1929e-04, -1.1792e-04,  ...,  1.2700e-03,
          0.0000e+00, -6.1221e-05],
        [ 0.0000e+00, -4.1929e-04, -1.1792e-04,  ...,  1.2700e-03,
          0.0000e+00, -6.1221e-05],
        ...,
        [ 0.0000e+00, -4.1929e-04, -1.1792e-04,  ...,  1.2700e-03,
          0.0000e+00, -6.1221e-05],
        [ 0.0000e+00, -4.1929e-04, -1.1792e-04,  ...,  1.2700e-03,
          0.0000e+00, -6.1221e-05],
        [ 0.0000e+00, -4.1929e-04, -1.1792e-04,  ...,  1.2700e-03,
          0.0000e+00, -6.1221e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2925.8352, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.0235, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.3034, device='cuda:0')



h[100].sum tensor(-10.2930, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.2303, device='cuda:0')



h[200].sum tensor(-13.6832, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.4643, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61628.4727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0039, 0.0557, 0.0254,  ..., 0.0209, 0.0000, 0.0000],
        [0.0039, 0.0558, 0.0256,  ..., 0.0210, 0.0000, 0.0000],
        [0.0042, 0.0560, 0.0258,  ..., 0.0213, 0.0000, 0.0000],
        ...,
        [0.0045, 0.0571, 0.0268,  ..., 0.0222, 0.0000, 0.0000],
        [0.0045, 0.0570, 0.0268,  ..., 0.0222, 0.0000, 0.0000],
        [0.0045, 0.0570, 0.0268,  ..., 0.0222, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(723466.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11738.3652, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-62.1904, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6626.6045, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-690.9360, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10727.4932, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(606.7390, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6111],
        [-2.7551],
        [-2.8815],
        ...,
        [-2.8294],
        [-2.8218],
        [-2.8183]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-293175.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0146],
        [1.0163],
        [1.0242],
        ...,
        [0.9991],
        [0.9979],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369022.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.3931, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0148],
        [1.0164],
        [1.0243],
        ...,
        [0.9991],
        [0.9978],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369027.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.3931, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -4.1810e-04, -1.1581e-04,  ...,  1.2669e-03,
          0.0000e+00, -5.8858e-05],
        [ 0.0000e+00, -4.1810e-04, -1.1581e-04,  ...,  1.2669e-03,
          0.0000e+00, -5.8858e-05],
        [ 0.0000e+00, -4.1810e-04, -1.1581e-04,  ...,  1.2669e-03,
          0.0000e+00, -5.8858e-05],
        ...,
        [ 0.0000e+00, -4.1810e-04, -1.1581e-04,  ...,  1.2669e-03,
          0.0000e+00, -5.8858e-05],
        [ 0.0000e+00, -4.1810e-04, -1.1581e-04,  ...,  1.2669e-03,
          0.0000e+00, -5.8858e-05],
        [ 0.0000e+00, -4.1810e-04, -1.1581e-04,  ...,  1.2669e-03,
          0.0000e+00, -5.8858e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2960.0498, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.0213, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.4603, device='cuda:0')



h[100].sum tensor(-10.9722, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.5305, device='cuda:0')



h[200].sum tensor(-13.2575, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.0585, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63521.8555, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0037, 0.0561, 0.0253,  ..., 0.0207, 0.0000, 0.0000],
        [0.0037, 0.0562, 0.0254,  ..., 0.0208, 0.0000, 0.0000],
        [0.0040, 0.0563, 0.0257,  ..., 0.0210, 0.0000, 0.0000],
        ...,
        [0.0043, 0.0575, 0.0267,  ..., 0.0219, 0.0000, 0.0000],
        [0.0043, 0.0574, 0.0267,  ..., 0.0219, 0.0000, 0.0000],
        [0.0043, 0.0574, 0.0266,  ..., 0.0219, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(740318.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12005.4629, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-56.3286, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6908.8950, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-701.2896, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10963.1582, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(620.5974, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6760],
        [-2.8462],
        [-2.9560],
        ...,
        [-2.8558],
        [-2.8485],
        [-2.8453]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-314891.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0148],
        [1.0164],
        [1.0243],
        ...,
        [0.9991],
        [0.9978],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369027.6562, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 370.0 event: 1850 loss: tensor(466.4203, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2864],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.0369, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0149],
        [1.0165],
        [1.0243],
        ...,
        [0.9990],
        [0.9978],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369033.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2864],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.0369, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -4.2215e-04, -1.0384e-04,  ...,  1.2855e-03,
          0.0000e+00, -6.0645e-05],
        [-5.3380e-03,  1.0747e-03,  3.0862e-03,  ...,  9.0016e-03,
         -8.5610e-04,  1.5159e-03],
        [-3.9758e-03,  6.9269e-04,  2.2721e-03,  ...,  7.0324e-03,
         -6.3762e-04,  1.1136e-03],
        ...,
        [ 0.0000e+00, -4.2215e-04, -1.0384e-04,  ...,  1.2855e-03,
          0.0000e+00, -6.0645e-05],
        [ 0.0000e+00, -4.2215e-04, -1.0384e-04,  ...,  1.2855e-03,
          0.0000e+00, -6.0645e-05],
        [ 0.0000e+00, -4.2215e-04, -1.0384e-04,  ...,  1.2855e-03,
          0.0000e+00, -6.0645e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2833.2627, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.2292, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.2486, device='cuda:0')



h[100].sum tensor(-9.0563, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.6972, device='cuda:0')



h[200].sum tensor(-14.6175, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.6330, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0011, 0.0031,  ..., 0.0130, 0.0000, 0.0015],
        [0.0000, 0.0024, 0.0068,  ..., 0.0222, 0.0000, 0.0033],
        [0.0000, 0.0096, 0.0238,  ..., 0.0638, 0.0000, 0.0117],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60284.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0584, 0.0258, 0.0534,  ..., 0.0586, 0.0000, 0.0000],
        [0.1195, 0.0131, 0.0850,  ..., 0.0985, 0.0000, 0.0000],
        [0.2240, 0.0000, 0.1404,  ..., 0.1658, 0.0000, 0.0000],
        ...,
        [0.0039, 0.0575, 0.0261,  ..., 0.0216, 0.0000, 0.0000],
        [0.0039, 0.0575, 0.0261,  ..., 0.0216, 0.0000, 0.0000],
        [0.0039, 0.0575, 0.0261,  ..., 0.0216, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(732225.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11222.3047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-73.6687, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7317.3555, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-681.1161, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10777.0703, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(575.6224, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0812],
        [ 0.0825],
        [ 0.2130],
        ...,
        [-2.8779],
        [-2.8703],
        [-2.8671]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-361491.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0149],
        [1.0165],
        [1.0243],
        ...,
        [0.9990],
        [0.9978],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369033.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.9563, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0150],
        [1.0166],
        [1.0244],
        ...,
        [0.9990],
        [0.9978],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369039.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.9563, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.1729e-03,  1.0391e-03,  3.0023e-03,  ...,  8.7918e-03,
         -8.2704e-04,  1.4700e-03],
        [ 0.0000e+00, -4.1325e-04, -9.2872e-05,  ...,  1.3045e-03,
          0.0000e+00, -5.9756e-05],
        [ 0.0000e+00, -4.1325e-04, -9.2872e-05,  ...,  1.3045e-03,
          0.0000e+00, -5.9756e-05],
        ...,
        [ 0.0000e+00, -4.1325e-04, -9.2872e-05,  ...,  1.3045e-03,
          0.0000e+00, -5.9756e-05],
        [ 0.0000e+00, -4.1325e-04, -9.2872e-05,  ...,  1.3045e-03,
          0.0000e+00, -5.9756e-05],
        [ 0.0000e+00, -4.1325e-04, -9.2872e-05,  ...,  1.3045e-03,
          0.0000e+00, -5.9756e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3166.3408, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.9586, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.0761, device='cuda:0')



h[100].sum tensor(-14.3416, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.9877, device='cuda:0')



h[200].sum tensor(-11.3984, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-31.7964, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0036, 0.0092,  ..., 0.0280, 0.0000, 0.0045],
        [0.0000, 0.0067, 0.0166,  ..., 0.0461, 0.0000, 0.0082],
        [0.0000, 0.0058, 0.0139,  ..., 0.0393, 0.0000, 0.0068],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68529.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1646, 0.0000, 0.1082,  ..., 0.1281, 0.0000, 0.0000],
        [0.2100, 0.0000, 0.1333,  ..., 0.1564, 0.0000, 0.0000],
        [0.2197, 0.0000, 0.1393,  ..., 0.1620, 0.0000, 0.0000],
        ...,
        [0.0036, 0.0576, 0.0259,  ..., 0.0215, 0.0000, 0.0000],
        [0.0036, 0.0576, 0.0259,  ..., 0.0215, 0.0000, 0.0000],
        [0.0036, 0.0575, 0.0259,  ..., 0.0215, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(763595.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12460.0566, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-38.2814, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6895.3628, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-729.9460, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11349.1836, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(654.6860, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2519],
        [ 0.2418],
        [ 0.2319],
        ...,
        [-2.8941],
        [-2.8865],
        [-2.8833]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-315500.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0150],
        [1.0166],
        [1.0244],
        ...,
        [0.9990],
        [0.9978],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369039.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3254],
        [0.3054],
        [0.3022],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.8219, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0151],
        [1.0167],
        [1.0245],
        ...,
        [0.9990],
        [0.9977],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369045.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3254],
        [0.3054],
        [0.3022],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.8219, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.5023e-02,  3.8277e-03,  8.9172e-03,  ...,  2.3103e-02,
         -2.3943e-03,  4.3892e-03],
        [-2.1424e-02,  5.6279e-03,  1.2752e-02,  ...,  3.2381e-02,
         -3.4146e-03,  6.2845e-03],
        [-1.5408e-02,  3.9361e-03,  9.1482e-03,  ...,  2.3662e-02,
         -2.4558e-03,  4.5034e-03],
        ...,
        [ 0.0000e+00, -3.9711e-04, -8.1682e-05,  ...,  1.3316e-03,
          0.0000e+00, -5.8603e-05],
        [ 0.0000e+00, -3.9711e-04, -8.1682e-05,  ...,  1.3316e-03,
          0.0000e+00, -5.8603e-05],
        [ 0.0000e+00, -3.9711e-04, -8.1682e-05,  ...,  1.3316e-03,
          0.0000e+00, -5.8603e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3193.8391, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.4230, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.3041, device='cuda:0')



h[100].sum tensor(-14.6533, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.0469, device='cuda:0')



h[200].sum tensor(-11.3961, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-32.1106, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0210, 0.0479,  ..., 0.1220, 0.0000, 0.0236],
        [0.0000, 0.0181, 0.0418,  ..., 0.1072, 0.0000, 0.0206],
        [0.0000, 0.0148, 0.0347,  ..., 0.0902, 0.0000, 0.0171],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70563.3828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.4917, 0.0000, 0.2859,  ..., 0.3346, 0.0000, 0.0000],
        [0.4614, 0.0000, 0.2691,  ..., 0.3158, 0.0000, 0.0000],
        [0.4162, 0.0000, 0.2442,  ..., 0.2878, 0.0000, 0.0000],
        ...,
        [0.0034, 0.0573, 0.0258,  ..., 0.0216, 0.0000, 0.0000],
        [0.0034, 0.0573, 0.0257,  ..., 0.0215, 0.0000, 0.0000],
        [0.0034, 0.0573, 0.0257,  ..., 0.0215, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(768865.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12592.7207, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-32.2880, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6762.9648, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-742.6700, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11409.5059, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(669.0740, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1567],
        [ 0.1561],
        [ 0.1579],
        ...,
        [-2.6574],
        [-2.5778],
        [-2.5748]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-304864.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0151],
        [1.0167],
        [1.0245],
        ...,
        [0.9990],
        [0.9977],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369045.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.1749, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0151],
        [1.0167],
        [1.0245],
        ...,
        [0.9989],
        [0.9977],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369051.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.1749, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.6777e-04, -7.2179e-05,  ...,  1.3523e-03,
          0.0000e+00, -5.6868e-05],
        [ 0.0000e+00, -3.6777e-04, -7.2179e-05,  ...,  1.3523e-03,
          0.0000e+00, -5.6868e-05],
        [ 0.0000e+00, -3.6777e-04, -7.2179e-05,  ...,  1.3523e-03,
          0.0000e+00, -5.6868e-05],
        ...,
        [ 0.0000e+00, -3.6777e-04, -7.2179e-05,  ...,  1.3523e-03,
          0.0000e+00, -5.6868e-05],
        [ 0.0000e+00, -3.6777e-04, -7.2179e-05,  ...,  1.3523e-03,
          0.0000e+00, -5.6868e-05],
        [ 0.0000e+00, -3.6777e-04, -7.2179e-05,  ...,  1.3523e-03,
          0.0000e+00, -5.6868e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3159.2681, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.3811, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.1385, device='cuda:0')



h[100].sum tensor(-13.9349, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.7444, device='cuda:0')



h[200].sum tensor(-11.8050, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.5045, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68519.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0649, 0.0206, 0.0576,  ..., 0.0636, 0.0000, 0.0000],
        [0.0693, 0.0200, 0.0601,  ..., 0.0664, 0.0000, 0.0000],
        [0.0779, 0.0176, 0.0647,  ..., 0.0720, 0.0000, 0.0000],
        ...,
        [0.0033, 0.0572, 0.0259,  ..., 0.0219, 0.0000, 0.0000],
        [0.0033, 0.0572, 0.0259,  ..., 0.0218, 0.0000, 0.0000],
        [0.0033, 0.0572, 0.0259,  ..., 0.0218, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(753855.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12248.1787, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-40.7098, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6428.2700, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-732.9130, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11146.7178, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(646.0370, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1375],
        [ 0.1419],
        [ 0.1513],
        ...,
        [-2.9080],
        [-2.9006],
        [-2.8974]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-262921.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0151],
        [1.0167],
        [1.0245],
        ...,
        [0.9989],
        [0.9977],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369051.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(171.7983, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0152],
        [1.0168],
        [1.0245],
        ...,
        [0.9989],
        [0.9977],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369057.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(171.7983, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.1972e-04, -7.1445e-05,  ...,  1.3404e-03,
          0.0000e+00, -5.8276e-05],
        [ 0.0000e+00, -3.1972e-04, -7.1445e-05,  ...,  1.3404e-03,
          0.0000e+00, -5.8276e-05],
        [ 0.0000e+00, -3.1972e-04, -7.1445e-05,  ...,  1.3404e-03,
          0.0000e+00, -5.8276e-05],
        ...,
        [ 0.0000e+00, -3.1972e-04, -7.1445e-05,  ...,  1.3404e-03,
          0.0000e+00, -5.8276e-05],
        [ 0.0000e+00, -3.1972e-04, -7.1445e-05,  ...,  1.3404e-03,
          0.0000e+00, -5.8276e-05],
        [ 0.0000e+00, -3.1972e-04, -7.1445e-05,  ...,  1.3404e-03,
          0.0000e+00, -5.8276e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2811.3281, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.4961, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.6725, device='cuda:0')



h[100].sum tensor(-8.5402, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.5477, device='cuda:0')



h[200].sum tensor(-14.8216, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.8393, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0014, 0.0036,  ..., 0.0144, 0.0000, 0.0018],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57367.2227, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0037, 0.0550, 0.0262,  ..., 0.0224, 0.0000, 0.0000],
        [0.0169, 0.0479, 0.0329,  ..., 0.0313, 0.0000, 0.0000],
        [0.0544, 0.0270, 0.0528,  ..., 0.0575, 0.0000, 0.0000],
        ...,
        [0.0033, 0.0576, 0.0264,  ..., 0.0221, 0.0000, 0.0000],
        [0.0033, 0.0576, 0.0264,  ..., 0.0221, 0.0000, 0.0000],
        [0.0033, 0.0576, 0.0264,  ..., 0.0221, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(720425.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10204.4375, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-100.6732, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7480.0288, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-672.5241, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10441.1367, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(527.6779, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2191],
        [-1.6869],
        [-1.0302],
        ...,
        [-2.9300],
        [-2.9224],
        [-2.9192]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-370785.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0152],
        [1.0168],
        [1.0245],
        ...,
        [0.9989],
        [0.9977],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369057.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.6700, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0152],
        [1.0168],
        [1.0245],
        ...,
        [0.9989],
        [0.9977],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369057.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.6700, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.1972e-04, -7.1445e-05,  ...,  1.3404e-03,
          0.0000e+00, -5.8276e-05],
        [ 0.0000e+00, -3.1972e-04, -7.1445e-05,  ...,  1.3404e-03,
          0.0000e+00, -5.8276e-05],
        [-1.4434e-02,  3.7620e-03,  8.5915e-03,  ...,  2.2304e-02,
         -2.2861e-03,  4.2240e-03],
        ...,
        [ 0.0000e+00, -3.1972e-04, -7.1445e-05,  ...,  1.3404e-03,
          0.0000e+00, -5.8276e-05],
        [ 0.0000e+00, -3.1972e-04, -7.1445e-05,  ...,  1.3404e-03,
          0.0000e+00, -5.8276e-05],
        [ 0.0000e+00, -3.1972e-04, -7.1445e-05,  ...,  1.3404e-03,
          0.0000e+00, -5.8276e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2991.9858, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.6355, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.3578, device='cuda:0')



h[100].sum tensor(-11.3692, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.7634, device='cuda:0')



h[200].sum tensor(-13.0021, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.2951, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0038, 0.0087,  ..., 0.0267, 0.0000, 0.0043],
        [0.0000, 0.0031, 0.0071,  ..., 0.0229, 0.0000, 0.0035],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63013.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0241, 0.0439, 0.0371,  ..., 0.0356, 0.0000, 0.0000],
        [0.0738, 0.0236, 0.0642,  ..., 0.0690, 0.0000, 0.0000],
        [0.0962, 0.0134, 0.0757,  ..., 0.0838, 0.0000, 0.0000],
        ...,
        [0.0033, 0.0576, 0.0264,  ..., 0.0221, 0.0000, 0.0000],
        [0.0033, 0.0576, 0.0264,  ..., 0.0221, 0.0000, 0.0000],
        [0.0033, 0.0576, 0.0264,  ..., 0.0221, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(739904.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11250.4092, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-72.4081, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7051.5483, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-705.6339, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10860.8047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(589.1852, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0606],
        [-1.4131],
        [-0.7969],
        ...,
        [-2.9250],
        [-2.9177],
        [-2.9151]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-333639.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0152],
        [1.0168],
        [1.0245],
        ...,
        [0.9989],
        [0.9977],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369057.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2664],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.7170, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0153],
        [1.0169],
        [1.0245],
        ...,
        [0.9988],
        [0.9976],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369064.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2664],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.7170, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.6177e-04, -7.1522e-05,  ...,  1.3255e-03,
          0.0000e+00, -5.2092e-05],
        [-4.9370e-03,  1.1392e-03,  2.8942e-03,  ...,  8.5030e-03,
         -7.7951e-04,  1.4143e-03],
        [-4.4438e-03,  9.9923e-04,  2.5979e-03,  ...,  7.7859e-03,
         -7.0163e-04,  1.2678e-03],
        ...,
        [ 0.0000e+00, -2.6177e-04, -7.1522e-05,  ...,  1.3255e-03,
          0.0000e+00, -5.2092e-05],
        [ 0.0000e+00, -2.6177e-04, -7.1522e-05,  ...,  1.3255e-03,
          0.0000e+00, -5.2092e-05],
        [ 0.0000e+00, -2.6177e-04, -7.1522e-05,  ...,  1.3255e-03,
          0.0000e+00, -5.2092e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2913.0808, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.6795, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.2128, device='cuda:0')



h[100].sum tensor(-10.0282, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.2068, device='cuda:0')



h[200].sum tensor(-13.2795, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.3395, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0012, 0.0029,  ..., 0.0126, 0.0000, 0.0014],
        [0.0000, 0.0019, 0.0050,  ..., 0.0179, 0.0000, 0.0024],
        [0.0000, 0.0052, 0.0129,  ..., 0.0373, 0.0000, 0.0063],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61480.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0709, 0.0186, 0.0617,  ..., 0.0703, 0.0000, 0.0000],
        [0.1051, 0.0087, 0.0789,  ..., 0.0944, 0.0000, 0.0000],
        [0.1408, 0.0006, 0.0972,  ..., 0.1194, 0.0000, 0.0000],
        ...,
        [0.0036, 0.0580, 0.0273,  ..., 0.0228, 0.0000, 0.0000],
        [0.0036, 0.0579, 0.0273,  ..., 0.0228, 0.0000, 0.0000],
        [0.0036, 0.0579, 0.0273,  ..., 0.0228, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(738331.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11189.7139, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-83.5828, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7168.5439, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-703.6952, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10864.5459, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(579.3632, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0531],
        [ 0.1130],
        [ 0.1831],
        ...,
        [-2.9436],
        [-2.9359],
        [-2.9328]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-351468.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0153],
        [1.0169],
        [1.0245],
        ...,
        [0.9988],
        [0.9976],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369064.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4924],
        [0.5430],
        [0.6982],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.2874, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0154],
        [1.0170],
        [1.0246],
        ...,
        [0.9988],
        [0.9976],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369071.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4924],
        [0.5430],
        [0.6982],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.2874, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.0054e-02,  5.4893e-03,  1.1991e-02,  ...,  3.0510e-02,
         -3.1565e-03,  5.9184e-03],
        [-2.7071e-02,  7.4867e-03,  1.6211e-02,  ...,  4.0722e-02,
         -4.2609e-03,  8.0049e-03],
        [-2.0847e-02,  5.7148e-03,  1.2468e-02,  ...,  3.1663e-02,
         -3.2812e-03,  6.1540e-03],
        ...,
        [ 0.0000e+00, -2.1925e-04, -6.6824e-05,  ...,  1.3247e-03,
          0.0000e+00, -4.5177e-05],
        [ 0.0000e+00, -2.1925e-04, -6.6824e-05,  ...,  1.3247e-03,
          0.0000e+00, -4.5177e-05],
        [ 0.0000e+00, -2.1925e-04, -6.6824e-05,  ...,  1.3247e-03,
          0.0000e+00, -4.5177e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3128.4673, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.3069, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.1924, device='cuda:0')



h[100].sum tensor(-13.1840, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.4989, device='cuda:0')



h[200].sum tensor(-10.8555, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-29.2009, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0253, 0.0550,  ..., 0.1391, 0.0000, 0.0271],
        [0.0000, 0.0286, 0.0620,  ..., 0.1560, 0.0000, 0.0306],
        [0.0000, 0.0281, 0.0610,  ..., 0.1538, 0.0000, 0.0301],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68024.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.5391, 0.0000, 0.3224,  ..., 0.3773, 0.0000, 0.0000],
        [0.6007, 0.0000, 0.3573,  ..., 0.4170, 0.0000, 0.0000],
        [0.5937, 0.0000, 0.3534,  ..., 0.4127, 0.0000, 0.0000],
        ...,
        [0.0037, 0.0581, 0.0279,  ..., 0.0234, 0.0000, 0.0000],
        [0.0037, 0.0581, 0.0279,  ..., 0.0234, 0.0000, 0.0000],
        [0.0037, 0.0581, 0.0279,  ..., 0.0234, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(766945.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12413.1016, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-59.4388, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6858.4531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-748.7114, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11455.2988, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(651.6204, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0211],
        [-0.0346],
        [-0.0502],
        ...,
        [-2.9517],
        [-2.9440],
        [-2.9406]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-326797.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0154],
        [1.0170],
        [1.0246],
        ...,
        [0.9988],
        [0.9976],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369071.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.2728, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0154],
        [1.0171],
        [1.0246],
        ...,
        [0.9988],
        [0.9975],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369077.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.2728, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.0312e-04, -5.4707e-05,  ...,  1.3390e-03,
          0.0000e+00, -3.7625e-05],
        [ 0.0000e+00, -2.0312e-04, -5.4707e-05,  ...,  1.3390e-03,
          0.0000e+00, -3.7625e-05],
        [ 0.0000e+00, -2.0312e-04, -5.4707e-05,  ...,  1.3390e-03,
          0.0000e+00, -3.7625e-05],
        ...,
        [ 0.0000e+00, -2.0312e-04, -5.4707e-05,  ...,  1.3390e-03,
          0.0000e+00, -3.7625e-05],
        [ 0.0000e+00, -2.0312e-04, -5.4707e-05,  ...,  1.3390e-03,
          0.0000e+00, -3.7625e-05],
        [ 0.0000e+00, -2.0312e-04, -5.4707e-05,  ...,  1.3390e-03,
          0.0000e+00, -3.7625e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2974.8911, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.4236, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.1324, device='cuda:0')



h[100].sum tensor(-10.5278, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.4454, device='cuda:0')



h[200].sum tensor(-12.4162, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.6067, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0041,  ..., 0.0156, 0.0000, 0.0020],
        [0.0000, 0.0008, 0.0020,  ..., 0.0105, 0.0000, 0.0010],
        [0.0000, 0.0013, 0.0030,  ..., 0.0129, 0.0000, 0.0015],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62377.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0496, 0.0293, 0.0503,  ..., 0.0592, 0.0000, 0.0000],
        [0.0425, 0.0335, 0.0469,  ..., 0.0539, 0.0000, 0.0000],
        [0.0511, 0.0290, 0.0513,  ..., 0.0601, 0.0000, 0.0000],
        ...,
        [0.0038, 0.0581, 0.0283,  ..., 0.0240, 0.0000, 0.0000],
        [0.0038, 0.0581, 0.0282,  ..., 0.0240, 0.0000, 0.0000],
        [0.0038, 0.0581, 0.0282,  ..., 0.0240, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(739745.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11432.5684, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-85.6171, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6852.2344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-720.1572, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10909.3398, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(593.6180, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9053],
        [-0.7450],
        [-0.5595],
        ...,
        [-2.9520],
        [-2.9446],
        [-2.9416]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-331617., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0154],
        [1.0171],
        [1.0246],
        ...,
        [0.9988],
        [0.9975],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369077.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3291],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.5637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0155],
        [1.0173],
        [1.0247],
        ...,
        [0.9987],
        [0.9975],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369084.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3291],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.5637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.6681e-03,  2.2709e-03,  5.1875e-03,  ...,  1.4010e-02,
         -1.3558e-03,  2.5519e-03],
        [-6.0794e-03,  1.5311e-03,  3.6276e-03,  ...,  1.0234e-02,
         -9.5087e-04,  1.7803e-03],
        [ 0.0000e+00, -2.0620e-04, -3.5854e-05,  ...,  1.3655e-03,
          0.0000e+00, -3.1810e-05],
        ...,
        [ 0.0000e+00, -2.0620e-04, -3.5854e-05,  ...,  1.3655e-03,
          0.0000e+00, -3.1810e-05],
        [ 0.0000e+00, -2.0620e-04, -3.5854e-05,  ...,  1.3655e-03,
          0.0000e+00, -3.1810e-05],
        [ 0.0000e+00, -2.0620e-04, -3.5854e-05,  ...,  1.3655e-03,
          0.0000e+00, -3.1810e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3064.2407, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.9724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.6677, device='cuda:0')



h[100].sum tensor(-11.5802, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.8438, device='cuda:0')



h[200].sum tensor(-11.7054, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.7221, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0090, 0.0207,  ..., 0.0559, 0.0000, 0.0102],
        [0.0000, 0.0035, 0.0083,  ..., 0.0257, 0.0000, 0.0041],
        [0.0000, 0.0016, 0.0037,  ..., 0.0146, 0.0000, 0.0018],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63897.9922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1749, 0.0000, 0.1196,  ..., 0.1428, 0.0000, 0.0000],
        [0.1201, 0.0068, 0.0896,  ..., 0.1063, 0.0000, 0.0000],
        [0.0778, 0.0169, 0.0668,  ..., 0.0777, 0.0000, 0.0000],
        ...,
        [0.0038, 0.0580, 0.0283,  ..., 0.0245, 0.0000, 0.0000],
        [0.0038, 0.0579, 0.0283,  ..., 0.0245, 0.0000, 0.0000],
        [0.0038, 0.0579, 0.0283,  ..., 0.0245, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(742289.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11567.0059, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-83.5619, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6633.5342, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-734.0687, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10916.2734, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(605.8116, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1513],
        [ 0.0903],
        [-0.0744],
        ...,
        [-2.9516],
        [-2.9442],
        [-2.9410]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-307155.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0155],
        [1.0173],
        [1.0247],
        ...,
        [0.9987],
        [0.9975],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369084.4375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 380.0 event: 1900 loss: tensor(444.7501, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.6884, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0155],
        [1.0174],
        [1.0248],
        ...,
        [0.9987],
        [0.9975],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369090.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.6884, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.1448e-04, -1.5869e-05,  ...,  1.3889e-03,
          0.0000e+00, -2.9681e-05],
        [ 0.0000e+00, -2.1448e-04, -1.5869e-05,  ...,  1.3889e-03,
          0.0000e+00, -2.9681e-05],
        [ 0.0000e+00, -2.1448e-04, -1.5869e-05,  ...,  1.3889e-03,
          0.0000e+00, -2.9681e-05],
        ...,
        [ 0.0000e+00, -2.1448e-04, -1.5869e-05,  ...,  1.3889e-03,
          0.0000e+00, -2.9681e-05],
        [ 0.0000e+00, -2.1448e-04, -1.5869e-05,  ...,  1.3889e-03,
          0.0000e+00, -2.9681e-05],
        [ 0.0000e+00, -2.1448e-04, -1.5869e-05,  ...,  1.3889e-03,
          0.0000e+00, -2.9681e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2985.5220, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.9022, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.2901, device='cuda:0')



h[100].sum tensor(-10.1635, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.2269, device='cuda:0')



h[200].sum tensor(-12.5185, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.4460, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61724.6523, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0031, 0.0564, 0.0267,  ..., 0.0234, 0.0000, 0.0000],
        [0.0032, 0.0566, 0.0268,  ..., 0.0235, 0.0000, 0.0000],
        [0.0048, 0.0553, 0.0283,  ..., 0.0256, 0.0000, 0.0000],
        ...,
        [0.0037, 0.0578, 0.0281,  ..., 0.0248, 0.0000, 0.0000],
        [0.0037, 0.0578, 0.0281,  ..., 0.0248, 0.0000, 0.0000],
        [0.0037, 0.0578, 0.0281,  ..., 0.0248, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(737798.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11144.2715, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-97.1804, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6741.6899, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-724.4367, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10731.4004, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(578.0506, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.8236],
        [-2.6303],
        [-2.3125],
        ...,
        [-2.6395],
        [-2.8568],
        [-2.9205]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-320948.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0155],
        [1.0174],
        [1.0248],
        ...,
        [0.9987],
        [0.9975],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369090.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.9402, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0156],
        [1.0175],
        [1.0250],
        ...,
        [0.9987],
        [0.9974],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369096.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.9402, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.2512e-04,  3.9901e-06,  ...,  1.4073e-03,
          0.0000e+00, -2.6132e-05],
        [ 0.0000e+00, -2.2512e-04,  3.9901e-06,  ...,  1.4073e-03,
          0.0000e+00, -2.6132e-05],
        [ 0.0000e+00, -2.2512e-04,  3.9901e-06,  ...,  1.4073e-03,
          0.0000e+00, -2.6132e-05],
        ...,
        [ 0.0000e+00, -2.2512e-04,  3.9901e-06,  ...,  1.4073e-03,
          0.0000e+00, -2.6132e-05],
        [ 0.0000e+00, -2.2512e-04,  3.9901e-06,  ...,  1.4073e-03,
          0.0000e+00, -2.6132e-05],
        [ 0.0000e+00, -2.2512e-04,  3.9901e-06,  ...,  1.4073e-03,
          0.0000e+00, -2.6132e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3276.3474, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.2074, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.5523, device='cuda:0')



h[100].sum tensor(-14.4576, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.1112, device='cuda:0')



h[200].sum tensor(-9.6398, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-32.4526, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 1.6151e-05,  ..., 5.6967e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.6202e-05,  ..., 5.7145e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.6272e-05,  ..., 5.7391e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 1.6660e-05,  ..., 5.8760e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.6655e-05,  ..., 5.8742e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.6649e-05,  ..., 5.8723e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70547.7422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0402, 0.0344, 0.0463,  ..., 0.0523, 0.0000, 0.0000],
        [0.0217, 0.0448, 0.0365,  ..., 0.0394, 0.0000, 0.0000],
        [0.0159, 0.0486, 0.0333,  ..., 0.0348, 0.0000, 0.0000],
        ...,
        [0.0036, 0.0577, 0.0279,  ..., 0.0249, 0.0000, 0.0000],
        [0.0036, 0.0577, 0.0279,  ..., 0.0249, 0.0000, 0.0000],
        [0.0035, 0.0577, 0.0279,  ..., 0.0249, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(772435.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12657.1855, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-59.0016, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6184.3291, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-779.5295, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11384.3096, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(666.7762, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2925],
        [-0.7046],
        [-1.0279],
        ...,
        [-2.9670],
        [-2.9527],
        [-2.9059]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-272440.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0156],
        [1.0175],
        [1.0250],
        ...,
        [0.9987],
        [0.9974],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369096.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(318.9587, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0157],
        [1.0176],
        [1.0251],
        ...,
        [0.9986],
        [0.9974],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369101.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(318.9587, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.9632e-03,  2.6298e-03,  6.0085e-03,  ...,  1.5985e-02,
         -1.5436e-03,  2.9537e-03],
        [-5.9636e-03,  1.4837e-03,  3.5899e-03,  ...,  1.0128e-02,
         -9.2396e-04,  1.7574e-03],
        [-1.3276e-02,  3.5792e-03,  8.0120e-03,  ...,  2.0837e-02,
         -2.0570e-03,  3.9447e-03],
        ...,
        [ 0.0000e+00, -2.2510e-04, -1.6213e-05,  ...,  1.3954e-03,
          0.0000e+00, -2.6414e-05],
        [ 0.0000e+00, -2.2510e-04, -1.6213e-05,  ...,  1.3954e-03,
          0.0000e+00, -2.6414e-05],
        [ 0.0000e+00, -2.2510e-04, -1.6213e-05,  ...,  1.3954e-03,
          0.0000e+00, -2.6414e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3354.9160, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.2559, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.3842, device='cuda:0')



h[100].sum tensor(-15.8475, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.5866, device='cuda:0')



h[200].sum tensor(-8.4913, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-34.9768, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0061, 0.0146,  ..., 0.0413, 0.0000, 0.0072],
        [0.0000, 0.0087, 0.0198,  ..., 0.0536, 0.0000, 0.0097],
        [0.0000, 0.0044, 0.0103,  ..., 0.0308, 0.0000, 0.0051],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72118.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1627, 0.0000, 0.1110,  ..., 0.1369, 0.0000, 0.0000],
        [0.1811, 0.0000, 0.1223,  ..., 0.1479, 0.0000, 0.0000],
        [0.1534, 0.0000, 0.1079,  ..., 0.1288, 0.0000, 0.0000],
        ...,
        [0.0032, 0.0581, 0.0277,  ..., 0.0244, 0.0000, 0.0000],
        [0.0032, 0.0581, 0.0277,  ..., 0.0244, 0.0000, 0.0000],
        [0.0032, 0.0581, 0.0277,  ..., 0.0244, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(781807.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12589.4180, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-51.9891, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6521.4199, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-787.6365, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11428.1455, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(680.3215, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2597],
        [ 0.2853],
        [ 0.2808],
        ...,
        [-2.9798],
        [-2.9601],
        [-2.9515]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-321456.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0157],
        [1.0176],
        [1.0251],
        ...,
        [0.9986],
        [0.9974],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369101.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.4448, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0158],
        [1.0177],
        [1.0252],
        ...,
        [0.9986],
        [0.9974],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369105.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.4448, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.7096e-03,  1.1247e-03,  2.8148e-03,  ...,  8.2832e-03,
         -7.2736e-04,  1.3853e-03],
        [ 0.0000e+00, -2.2488e-04, -3.6081e-05,  ...,  1.3777e-03,
          0.0000e+00, -2.5299e-05],
        [ 0.0000e+00, -2.2488e-04, -3.6081e-05,  ...,  1.3777e-03,
          0.0000e+00, -2.5299e-05],
        ...,
        [ 0.0000e+00, -2.2488e-04, -3.6081e-05,  ...,  1.3777e-03,
          0.0000e+00, -2.5299e-05],
        [ 0.0000e+00, -2.2488e-04, -3.6081e-05,  ...,  1.3777e-03,
          0.0000e+00, -2.5299e-05],
        [ 0.0000e+00, -2.2488e-04, -3.6081e-05,  ...,  1.3777e-03,
          0.0000e+00, -2.5299e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2921.0273, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.9438, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.3952, device='cuda:0')



h[100].sum tensor(-9.4979, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.9947, device='cuda:0')



h[200].sum tensor(-12.2912, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.2130, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0042, 0.0097,  ..., 0.0292, 0.0000, 0.0048],
        [0.0000, 0.0011, 0.0029,  ..., 0.0126, 0.0000, 0.0014],
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60938.0547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1429, 0.0082, 0.1022,  ..., 0.1207, 0.0000, 0.0000],
        [0.0762, 0.0207, 0.0664,  ..., 0.0758, 0.0000, 0.0000],
        [0.0314, 0.0410, 0.0420,  ..., 0.0442, 0.0000, 0.0000],
        ...,
        [0.0030, 0.0585, 0.0276,  ..., 0.0238, 0.0000, 0.0000],
        [0.0030, 0.0585, 0.0275,  ..., 0.0238, 0.0000, 0.0000],
        [0.0030, 0.0585, 0.0275,  ..., 0.0238, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(739141.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10964.8516, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-93.2456, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6757.5273, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-717.9454, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10562.9912, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(563.1918, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1171],
        [-0.0863],
        [-0.4668],
        ...,
        [-3.0492],
        [-3.0412],
        [-3.0378]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-322397., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0158],
        [1.0177],
        [1.0252],
        ...,
        [0.9986],
        [0.9974],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369105.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.5778, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0159],
        [1.0178],
        [1.0253],
        ...,
        [0.9985],
        [0.9973],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369110.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.5778, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.3690e-04, -5.4313e-05,  ...,  1.3709e-03,
          0.0000e+00, -1.3803e-05],
        [ 0.0000e+00, -2.3690e-04, -5.4313e-05,  ...,  1.3709e-03,
          0.0000e+00, -1.3803e-05],
        [-9.1628e-03,  2.3883e-03,  5.4982e-03,  ...,  1.4824e-02,
         -1.4106e-03,  2.7347e-03],
        ...,
        [ 0.0000e+00, -2.3690e-04, -5.4313e-05,  ...,  1.3709e-03,
          0.0000e+00, -1.3803e-05],
        [ 0.0000e+00, -2.3690e-04, -5.4313e-05,  ...,  1.3709e-03,
          0.0000e+00, -1.3803e-05],
        [ 0.0000e+00, -2.3690e-04, -5.4313e-05,  ...,  1.3709e-03,
          0.0000e+00, -1.3803e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2863.9539, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.8296, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.2121, device='cuda:0')



h[100].sum tensor(-8.7353, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.6877, device='cuda:0')



h[200].sum tensor(-12.6618, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.5827, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0033, 0.0078,  ..., 0.0248, 0.0000, 0.0039],
        [0.0000, 0.0028, 0.0073,  ..., 0.0237, 0.0000, 0.0037],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59133.9102, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0458, 0.0323, 0.0503,  ..., 0.0543, 0.0000, 0.0000],
        [0.0767, 0.0183, 0.0660,  ..., 0.0764, 0.0000, 0.0000],
        [0.1015, 0.0104, 0.0786,  ..., 0.0945, 0.0000, 0.0000],
        ...,
        [0.0029, 0.0588, 0.0275,  ..., 0.0237, 0.0000, 0.0000],
        [0.0029, 0.0588, 0.0275,  ..., 0.0236, 0.0000, 0.0000],
        [0.0029, 0.0588, 0.0275,  ..., 0.0236, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(740737.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10502.3379, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-103.9091, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7359.2485, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-706.6964, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10478.4414, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(542.2753, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2499],
        [-0.1596],
        [ 0.0336],
        ...,
        [-3.0543],
        [-3.0564],
        [-3.0546]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-397376.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0159],
        [1.0178],
        [1.0253],
        ...,
        [0.9985],
        [0.9973],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369110.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.8168, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0160],
        [1.0179],
        [1.0255],
        ...,
        [0.9985],
        [0.9973],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369115.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.8168, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.5183e-04, -7.3684e-05,  ...,  1.3742e-03,
          0.0000e+00,  9.0477e-06],
        [ 0.0000e+00, -2.5183e-04, -7.3684e-05,  ...,  1.3742e-03,
          0.0000e+00,  9.0477e-06],
        [ 0.0000e+00, -2.5183e-04, -7.3684e-05,  ...,  1.3742e-03,
          0.0000e+00,  9.0477e-06],
        ...,
        [ 0.0000e+00, -2.5183e-04, -7.3684e-05,  ...,  1.3742e-03,
          0.0000e+00,  9.0477e-06],
        [ 0.0000e+00, -2.5183e-04, -7.3684e-05,  ...,  1.3742e-03,
          0.0000e+00,  9.0477e-06],
        [ 0.0000e+00, -2.5183e-04, -7.3684e-05,  ...,  1.3742e-03,
          0.0000e+00,  9.0477e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3014.3127, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.9961, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.8920, device='cuda:0')



h[100].sum tensor(-10.8867, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.6425, device='cuda:0')



h[200].sum tensor(-11.5023, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.6532, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.5645e-03, 0.0000e+00,
         3.6636e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.5824e-03, 0.0000e+00,
         3.6753e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.6069e-03, 0.0000e+00,
         3.6915e-05],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.7397e-03, 0.0000e+00,
         3.7789e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.7380e-03, 0.0000e+00,
         3.7778e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.7360e-03, 0.0000e+00,
         3.7765e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62550.5039, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0065, 0.0546, 0.0290,  ..., 0.0266, 0.0000, 0.0000],
        [0.0083, 0.0546, 0.0291,  ..., 0.0271, 0.0000, 0.0000],
        [0.0119, 0.0529, 0.0311,  ..., 0.0298, 0.0000, 0.0000],
        ...,
        [0.0056, 0.0570, 0.0298,  ..., 0.0266, 0.0000, 0.0000],
        [0.0030, 0.0590, 0.0278,  ..., 0.0240, 0.0000, 0.0000],
        [0.0030, 0.0590, 0.0277,  ..., 0.0240, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(752923.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11034.1973, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-89.9421, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7256.9194, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-729.0142, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10685.2373, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(580.5306, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6845],
        [-1.6308],
        [-1.3714],
        ...,
        [-2.8379],
        [-2.9925],
        [-3.0485]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-389460.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0160],
        [1.0179],
        [1.0255],
        ...,
        [0.9985],
        [0.9973],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369115.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.9495, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0161],
        [1.0181],
        [1.0256],
        ...,
        [0.9985],
        [0.9973],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369121.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.9495, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.9296e-04, -8.8339e-05,  ...,  1.3885e-03,
          0.0000e+00,  5.1096e-05],
        [ 0.0000e+00, -2.9296e-04, -8.8339e-05,  ...,  1.3885e-03,
          0.0000e+00,  5.1096e-05],
        [ 0.0000e+00, -2.9296e-04, -8.8339e-05,  ...,  1.3885e-03,
          0.0000e+00,  5.1096e-05],
        ...,
        [ 0.0000e+00, -2.9296e-04, -8.8339e-05,  ...,  1.3885e-03,
          0.0000e+00,  5.1096e-05],
        [ 0.0000e+00, -2.9296e-04, -8.8339e-05,  ...,  1.3885e-03,
          0.0000e+00,  5.1096e-05],
        [ 0.0000e+00, -2.9296e-04, -8.8339e-05,  ...,  1.3885e-03,
          0.0000e+00,  5.1096e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3110.6995, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.8567, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.8126, device='cuda:0')



h[100].sum tensor(-12.1479, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.1409, device='cuda:0')



h[200].sum tensor(-11.3093, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.2996, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0002],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65741.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0059, 0.0559, 0.0276,  ..., 0.0254, 0.0000, 0.0000],
        [0.0038, 0.0573, 0.0269,  ..., 0.0239, 0.0000, 0.0000],
        [0.0030, 0.0580, 0.0268,  ..., 0.0234, 0.0000, 0.0000],
        ...,
        [0.0033, 0.0591, 0.0278,  ..., 0.0244, 0.0000, 0.0000],
        [0.0033, 0.0591, 0.0278,  ..., 0.0244, 0.0000, 0.0000],
        [0.0033, 0.0591, 0.0278,  ..., 0.0244, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(759075.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11638.7578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-72.6689, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6881.5049, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-749.7499, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10746.2363, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(616.1763, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7971],
        [-2.2457],
        [-2.5626],
        ...,
        [-3.0665],
        [-3.0587],
        [-3.0555]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-347783.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0161],
        [1.0181],
        [1.0256],
        ...,
        [0.9985],
        [0.9973],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369121.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.5206, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0163],
        [1.0182],
        [1.0258],
        ...,
        [0.9985],
        [0.9972],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369126.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.5206, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.2773e-04, -1.0540e-04,  ...,  1.3977e-03,
          0.0000e+00,  9.0657e-05],
        [ 0.0000e+00, -3.2773e-04, -1.0540e-04,  ...,  1.3977e-03,
          0.0000e+00,  9.0657e-05],
        [ 0.0000e+00, -3.2773e-04, -1.0540e-04,  ...,  1.3977e-03,
          0.0000e+00,  9.0657e-05],
        ...,
        [ 0.0000e+00, -3.2773e-04, -1.0540e-04,  ...,  1.3977e-03,
          0.0000e+00,  9.0657e-05],
        [ 0.0000e+00, -3.2773e-04, -1.0540e-04,  ...,  1.3977e-03,
          0.0000e+00,  9.0657e-05],
        [ 0.0000e+00, -3.2773e-04, -1.0540e-04,  ...,  1.3977e-03,
          0.0000e+00,  9.0657e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2998.1340, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.0845, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.8338, device='cuda:0')



h[100].sum tensor(-10.2577, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.3680, device='cuda:0')



h[200].sum tensor(-13.0032, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.1952, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0004],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61986.5352, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0030, 0.0579, 0.0265,  ..., 0.0235, 0.0000, 0.0000],
        [0.0030, 0.0581, 0.0266,  ..., 0.0236, 0.0000, 0.0000],
        [0.0033, 0.0582, 0.0269,  ..., 0.0239, 0.0000, 0.0000],
        ...,
        [0.0036, 0.0593, 0.0280,  ..., 0.0249, 0.0000, 0.0000],
        [0.0036, 0.0593, 0.0279,  ..., 0.0249, 0.0000, 0.0000],
        [0.0036, 0.0593, 0.0279,  ..., 0.0249, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(742298.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11305.0049, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-86.6714, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6772.6787, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-729.3747, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10402.4609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(580.7736, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.0802],
        [-2.9676],
        [-2.7737],
        ...,
        [-3.0559],
        [-3.0482],
        [-3.0451]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-327090.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0163],
        [1.0182],
        [1.0258],
        ...,
        [0.9985],
        [0.9972],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369126.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5249],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(253.9485, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0164],
        [1.0184],
        [1.0259],
        ...,
        [0.9984],
        [0.9972],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369131.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5249],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(253.9485, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0003, -0.0001,  ...,  0.0014,  0.0000,  0.0001],
        [-0.0096,  0.0024,  0.0057,  ...,  0.0156, -0.0015,  0.0030],
        [-0.0102,  0.0026,  0.0061,  ...,  0.0164, -0.0015,  0.0032],
        ...,
        [ 0.0000, -0.0003, -0.0001,  ...,  0.0014,  0.0000,  0.0001],
        [ 0.0000, -0.0003, -0.0001,  ...,  0.0014,  0.0000,  0.0001],
        [ 0.0000, -0.0003, -0.0001,  ...,  0.0014,  0.0000,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3138.3384, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.0786, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.2104, device='cuda:0')



h[100].sum tensor(-12.2885, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.2441, device='cuda:0')



h[200].sum tensor(-11.9493, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.8478, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0024, 0.0058,  ..., 0.0200, 0.0000, 0.0034],
        [0.0000, 0.0045, 0.0109,  ..., 0.0327, 0.0000, 0.0060],
        [0.0000, 0.0142, 0.0328,  ..., 0.0865, 0.0000, 0.0170],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0005],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0005],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0005]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66319.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0711, 0.0270, 0.0632,  ..., 0.0696, 0.0000, 0.0000],
        [0.1350, 0.0123, 0.0968,  ..., 0.1112, 0.0000, 0.0000],
        [0.2463, 0.0000, 0.1560,  ..., 0.1827, 0.0000, 0.0000],
        ...,
        [0.0039, 0.0596, 0.0282,  ..., 0.0253, 0.0000, 0.0000],
        [0.0039, 0.0595, 0.0282,  ..., 0.0253, 0.0000, 0.0000],
        [0.0039, 0.0595, 0.0282,  ..., 0.0253, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(762336.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12292.2354, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-68.1886, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6575.3091, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-756.8646, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10802.1855, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(630.1671, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0625],
        [-0.6184],
        [-0.1429],
        ...,
        [-3.0570],
        [-3.0494],
        [-3.0463]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-307939.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0164],
        [1.0184],
        [1.0259],
        ...,
        [0.9984],
        [0.9972],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369131.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.1355, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0165],
        [1.0185],
        [1.0261],
        ...,
        [0.9984],
        [0.9972],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369137.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.1355, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0004, -0.0001,  ...,  0.0014,  0.0000,  0.0001],
        [ 0.0000, -0.0004, -0.0001,  ...,  0.0014,  0.0000,  0.0001],
        [ 0.0000, -0.0004, -0.0001,  ...,  0.0014,  0.0000,  0.0001],
        ...,
        [ 0.0000, -0.0004, -0.0001,  ...,  0.0014,  0.0000,  0.0001],
        [ 0.0000, -0.0004, -0.0001,  ...,  0.0014,  0.0000,  0.0001],
        [ 0.0000, -0.0004, -0.0001,  ...,  0.0014,  0.0000,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3335.8403, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.4297, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.0800, device='cuda:0')



h[100].sum tensor(-15.2397, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.5076, device='cuda:0')



h[200].sum tensor(-10.0607, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-34.5575, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0005],
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0005],
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0005],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73684.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0035, 0.0585, 0.0271,  ..., 0.0243, 0.0000, 0.0000],
        [0.0048, 0.0575, 0.0283,  ..., 0.0259, 0.0000, 0.0000],
        [0.0142, 0.0526, 0.0327,  ..., 0.0326, 0.0000, 0.0000],
        ...,
        [0.0042, 0.0599, 0.0287,  ..., 0.0257, 0.0000, 0.0000],
        [0.0042, 0.0599, 0.0286,  ..., 0.0257, 0.0000, 0.0000],
        [0.0042, 0.0599, 0.0286,  ..., 0.0257, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(789880.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(13666.4990, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-33.1217, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6261.4668, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-801.7856, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11293.1055, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(714.5205, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7693],
        [-1.6026],
        [-1.2050],
        ...,
        [-3.0536],
        [-3.0510],
        [-3.0497]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-277636.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0165],
        [1.0185],
        [1.0261],
        ...,
        [0.9984],
        [0.9972],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369137.5000, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 390.0 event: 1950 loss: tensor(496.2259, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(240.6774, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0167],
        [1.0186],
        [1.0261],
        ...,
        [0.9984],
        [0.9972],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369143.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(240.6774, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0004, -0.0002,  ...,  0.0013,  0.0000,  0.0001],
        [ 0.0000, -0.0004, -0.0002,  ...,  0.0013,  0.0000,  0.0001],
        [ 0.0000, -0.0004, -0.0002,  ...,  0.0013,  0.0000,  0.0001],
        ...,
        [ 0.0000, -0.0004, -0.0002,  ...,  0.0013,  0.0000,  0.0001],
        [ 0.0000, -0.0004, -0.0002,  ...,  0.0013,  0.0000,  0.0001],
        [ 0.0000, -0.0004, -0.0002,  ...,  0.0013,  0.0000,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3079.3772, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.1054, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.1542, device='cuda:0')



h[100].sum tensor(-11.6172, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.9701, device='cuda:0')



h[200].sum tensor(-12.1522, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.3925, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0005],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0005],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0005],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0005],
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0005],
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0005]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67141.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0038, 0.0590, 0.0276,  ..., 0.0244, 0.0000, 0.0000],
        [0.0038, 0.0591, 0.0277,  ..., 0.0246, 0.0000, 0.0000],
        [0.0041, 0.0593, 0.0280,  ..., 0.0249, 0.0000, 0.0000],
        ...,
        [0.0044, 0.0604, 0.0291,  ..., 0.0259, 0.0000, 0.0000],
        [0.0044, 0.0604, 0.0291,  ..., 0.0259, 0.0000, 0.0000],
        [0.0044, 0.0604, 0.0291,  ..., 0.0259, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(779768.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12775.1328, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-68.4356, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7037.7114, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-764.4510, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11154.5234, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(649.2579, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.4168],
        [-2.5911],
        [-2.7018],
        ...,
        [-3.0842],
        [-3.0765],
        [-3.0733]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-361463.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0167],
        [1.0186],
        [1.0261],
        ...,
        [0.9984],
        [0.9972],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369143.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(244.8800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0168],
        [1.0188],
        [1.0262],
        ...,
        [0.9984],
        [0.9971],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369148.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(244.8800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0003, -0.0002,  ...,  0.0013,  0.0000,  0.0001],
        [ 0.0000, -0.0003, -0.0002,  ...,  0.0013,  0.0000,  0.0001],
        [ 0.0000, -0.0003, -0.0002,  ...,  0.0013,  0.0000,  0.0001],
        ...,
        [ 0.0000, -0.0003, -0.0002,  ...,  0.0013,  0.0000,  0.0001],
        [ 0.0000, -0.0003, -0.0002,  ...,  0.0013,  0.0000,  0.0001],
        [ 0.0000, -0.0003, -0.0002,  ...,  0.0013,  0.0000,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3091.8462, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.5771, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.4887, device='cuda:0')



h[100].sum tensor(-11.9325, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.0568, device='cuda:0')



h[200].sum tensor(-11.6566, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.8534, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0005],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0005],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0005],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0005],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0005],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0005]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65367.0234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0053, 0.0585, 0.0283,  ..., 0.0256, 0.0000, 0.0000],
        [0.0116, 0.0548, 0.0316,  ..., 0.0308, 0.0000, 0.0000],
        [0.0305, 0.0449, 0.0410,  ..., 0.0436, 0.0000, 0.0000],
        ...,
        [0.0046, 0.0608, 0.0295,  ..., 0.0261, 0.0000, 0.0000],
        [0.0046, 0.0607, 0.0294,  ..., 0.0261, 0.0000, 0.0000],
        [0.0046, 0.0607, 0.0294,  ..., 0.0261, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(752339.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12493.5518, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-66.8365, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6555.5703, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-752.6403, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10826.8066, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(640.2885, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1220],
        [-1.6836],
        [-1.1570],
        ...,
        [-3.1009],
        [-3.0931],
        [-3.0899]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-307069.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0168],
        [1.0188],
        [1.0262],
        ...,
        [0.9984],
        [0.9971],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369148.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3306],
        [0.5620],
        [0.3574],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.7720, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0170],
        [1.0189],
        [1.0262],
        ...,
        [0.9983],
        [0.9971],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369153.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3306],
        [0.5620],
        [0.3574],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.7720, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0230e-02,  2.5843e-03,  6.0568e-03,  ...,  1.6467e-02,
         -1.5352e-03,  3.2018e-03],
        [-1.8790e-02,  5.0416e-03,  1.1281e-02,  ...,  2.9160e-02,
         -2.8197e-03,  5.8047e-03],
        [-2.2727e-02,  6.1719e-03,  1.3683e-02,  ...,  3.4998e-02,
         -3.4105e-03,  7.0020e-03],
        ...,
        [ 0.0000e+00, -3.5257e-04, -1.8664e-04,  ...,  1.2964e-03,
          0.0000e+00,  9.0773e-05],
        [ 0.0000e+00, -3.5257e-04, -1.8664e-04,  ...,  1.2964e-03,
          0.0000e+00,  9.0773e-05],
        [ 0.0000e+00, -3.5257e-04, -1.8664e-04,  ...,  1.2964e-03,
          0.0000e+00,  9.0773e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2985.0310, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.4370, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.2517, device='cuda:0')



h[100].sum tensor(-10.4753, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.4764, device='cuda:0')



h[200].sum tensor(-12.4652, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.7711, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0177, 0.0399,  ..., 0.1040, 0.0000, 0.0206],
        [0.0000, 0.0240, 0.0532,  ..., 0.1365, 0.0000, 0.0273],
        [0.0000, 0.0239, 0.0532,  ..., 0.1364, 0.0000, 0.0273],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63425.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.4616, 0.0000, 0.2709,  ..., 0.3200, 0.0000, 0.0000],
        [0.5320, 0.0000, 0.3096,  ..., 0.3640, 0.0000, 0.0000],
        [0.5585, 0.0000, 0.3242,  ..., 0.3807, 0.0000, 0.0000],
        ...,
        [0.0046, 0.0609, 0.0293,  ..., 0.0260, 0.0000, 0.0000],
        [0.0046, 0.0609, 0.0293,  ..., 0.0259, 0.0000, 0.0000],
        [0.0046, 0.0608, 0.0293,  ..., 0.0259, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(752584.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12296.5059, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-77.7906, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6813.1206, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-739.7350, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10845.9766, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(618.1165, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2175],
        [ 0.2110],
        [ 0.2097],
        ...,
        [-3.1159],
        [-3.1076],
        [-3.1041]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-327038.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0170],
        [1.0189],
        [1.0262],
        ...,
        [0.9983],
        [0.9971],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369153.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4875],
        [0.3958],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(142.4794, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0171],
        [1.0190],
        [1.0263],
        ...,
        [0.9983],
        [0.9971],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369158.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4875],
        [0.3958],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(142.4794, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.8647e-03,  2.1776e-03,  5.2351e-03,  ...,  1.4474e-02,
         -1.3260e-03,  2.7651e-03],
        [-7.1956e-03,  1.6980e-03,  4.2154e-03,  ...,  1.1996e-02,
         -1.0763e-03,  2.2569e-03],
        [-1.6353e-02,  4.3293e-03,  9.8103e-03,  ...,  2.5593e-02,
         -2.4462e-03,  5.0448e-03],
        ...,
        [ 0.0000e+00, -3.6949e-04, -1.8078e-04,  ...,  1.3125e-03,
          0.0000e+00,  6.6418e-05],
        [ 0.0000e+00, -3.6949e-04, -1.8078e-04,  ...,  1.3125e-03,
          0.0000e+00,  6.6418e-05],
        [ 0.0000e+00, -3.6949e-04, -1.8078e-04,  ...,  1.3125e-03,
          0.0000e+00,  6.6418e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2736.4229, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.0605, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(11.3392, device='cuda:0')



h[100].sum tensor(-6.8240, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-2.9422, device='cuda:0')



h[200].sum tensor(-14.9035, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.6242, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0034, 0.0086,  ..., 0.0270, 0.0000, 0.0047],
        [0.0000, 0.0118, 0.0276,  ..., 0.0741, 0.0000, 0.0144],
        [0.0000, 0.0124, 0.0287,  ..., 0.0770, 0.0000, 0.0150],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55567.8516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1143, 0.0153, 0.0852,  ..., 0.0975, 0.0000, 0.0000],
        [0.2089, 0.0000, 0.1350,  ..., 0.1582, 0.0000, 0.0000],
        [0.2463, 0.0000, 0.1548,  ..., 0.1823, 0.0000, 0.0000],
        ...,
        [0.0045, 0.0608, 0.0289,  ..., 0.0257, 0.0000, 0.0000],
        [0.0045, 0.0607, 0.0289,  ..., 0.0257, 0.0000, 0.0000],
        [0.0045, 0.0607, 0.0289,  ..., 0.0257, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(725176.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10659.4463, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-117.6812, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7437.7417, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-692.1852, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10215.7344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(528.3669, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9011],
        [-0.2341],
        [ 0.1656],
        ...,
        [-3.1251],
        [-3.1173],
        [-3.1141]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-386219.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0171],
        [1.0190],
        [1.0263],
        ...,
        [0.9983],
        [0.9971],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369158.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2546],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.2601, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0172],
        [1.0192],
        [1.0263],
        ...,
        [0.9983],
        [0.9971],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369164.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2546],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.2601, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.6246e-03,  9.3774e-04,  2.6597e-03,  ...,  8.2129e-03,
         -6.8953e-04,  1.4474e-03],
        [ 0.0000e+00, -3.9177e-04, -1.6910e-04,  ...,  1.3374e-03,
          0.0000e+00,  3.8007e-05],
        [-4.6246e-03,  9.3774e-04,  2.6597e-03,  ...,  8.2129e-03,
         -6.8953e-04,  1.4474e-03],
        ...,
        [ 0.0000e+00, -3.9177e-04, -1.6910e-04,  ...,  1.3374e-03,
          0.0000e+00,  3.8007e-05],
        [ 0.0000e+00, -3.9177e-04, -1.6910e-04,  ...,  1.3374e-03,
          0.0000e+00,  3.8007e-05],
        [ 0.0000e+00, -3.9177e-04, -1.6910e-04,  ...,  1.3374e-03,
          0.0000e+00,  3.8007e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2937.5835, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.3167, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.0968, device='cuda:0')



h[100].sum tensor(-9.7068, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.1767, device='cuda:0')



h[200].sum tensor(-13.0792, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.1797, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0007, 0.0022,  ..., 0.0111, 0.0000, 0.0013],
        [0.0000, 0.0033, 0.0098,  ..., 0.0308, 0.0000, 0.0054],
        [0.0000, 0.0007, 0.0022,  ..., 0.0112, 0.0000, 0.0013],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60894.2148, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0329, 0.0419, 0.0414,  ..., 0.0454, 0.0000, 0.0000],
        [0.0557, 0.0296, 0.0522,  ..., 0.0612, 0.0000, 0.0000],
        [0.0337, 0.0421, 0.0419,  ..., 0.0460, 0.0000, 0.0000],
        ...,
        [0.0044, 0.0605, 0.0283,  ..., 0.0253, 0.0000, 0.0000],
        [0.0044, 0.0605, 0.0283,  ..., 0.0253, 0.0000, 0.0000],
        [0.0044, 0.0605, 0.0283,  ..., 0.0252, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(742167.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11385.0381, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-95.0387, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7093.6543, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-722.0897, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10530.7021, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(578.1597, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0193],
        [-2.0939],
        [-2.3327],
        ...,
        [-3.1325],
        [-3.1247],
        [-3.1215]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-358315., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0172],
        [1.0192],
        [1.0263],
        ...,
        [0.9983],
        [0.9971],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369164.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(501.3649, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0174],
        [1.0193],
        [1.0263],
        ...,
        [0.9983],
        [0.9971],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369169.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(501.3649, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -4.0165e-04, -1.5829e-04,  ...,  1.3631e-03,
          0.0000e+00,  9.3583e-06],
        [-6.3537e-03,  1.4272e-03,  3.7324e-03,  ...,  1.0821e-02,
         -9.4427e-04,  1.9474e-03],
        [-2.3669e-02,  6.4113e-03,  1.4336e-02,  ...,  3.6596e-02,
         -3.5177e-03,  7.2292e-03],
        ...,
        [ 0.0000e+00, -4.0165e-04, -1.5829e-04,  ...,  1.3631e-03,
          0.0000e+00,  9.3583e-06],
        [ 0.0000e+00, -4.0165e-04, -1.5829e-04,  ...,  1.3631e-03,
          0.0000e+00,  9.3583e-06],
        [ 0.0000e+00, -4.0165e-04, -1.5829e-04,  ...,  1.3631e-03,
          0.0000e+00,  9.3583e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3920.5967, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-35.1303, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(39.9010, device='cuda:0')



h[100].sum tensor(-23.8085, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.3533, device='cuda:0')



h[200].sum tensor(-3.9354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-54.9793, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 4.0789e-03, 1.0086e-02,  ..., 3.0821e-02, 0.0000e+00,
         5.2216e-03],
        [0.0000e+00, 1.3201e-02, 3.0207e-02,  ..., 8.0146e-02, 0.0000e+00,
         1.5326e-02],
        [0.0000e+00, 1.7724e-02, 4.0550e-02,  ..., 1.0571e-01, 0.0000e+00,
         2.0559e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.6974e-03, 0.0000e+00,
         3.9116e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.6958e-03, 0.0000e+00,
         3.9105e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.6937e-03, 0.0000e+00,
         3.9090e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(91399.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2261, 0.0086, 0.1431,  ..., 0.1671, 0.0000, 0.0000],
        [0.4150, 0.0000, 0.2437,  ..., 0.2860, 0.0000, 0.0000],
        [0.5703, 0.0000, 0.3267,  ..., 0.3838, 0.0000, 0.0000],
        ...,
        [0.0044, 0.0602, 0.0279,  ..., 0.0250, 0.0000, 0.0000],
        [0.0044, 0.0602, 0.0279,  ..., 0.0250, 0.0000, 0.0000],
        [0.0044, 0.0601, 0.0279,  ..., 0.0250, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(882893.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(16610.5156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(28.6290, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6395.8940, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-903.2247, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13080.1191, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(890.2422, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2040],
        [ 0.2263],
        [ 0.2292],
        ...,
        [-3.1344],
        [-3.1266],
        [-3.1235]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-312988.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0174],
        [1.0193],
        [1.0263],
        ...,
        [0.9983],
        [0.9971],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369169.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(161.7711, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0175],
        [1.0194],
        [1.0263],
        ...,
        [0.9983],
        [0.9970],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369174.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(161.7711, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -4.0559e-04, -1.4655e-04,  ...,  1.3840e-03,
          0.0000e+00, -1.8681e-05],
        [ 0.0000e+00, -4.0559e-04, -1.4655e-04,  ...,  1.3840e-03,
          0.0000e+00, -1.8681e-05],
        [ 0.0000e+00, -4.0559e-04, -1.4655e-04,  ...,  1.3840e-03,
          0.0000e+00, -1.8681e-05],
        ...,
        [ 0.0000e+00, -4.0559e-04, -1.4655e-04,  ...,  1.3840e-03,
          0.0000e+00, -1.8681e-05],
        [ 0.0000e+00, -4.0559e-04, -1.4655e-04,  ...,  1.3840e-03,
          0.0000e+00, -1.8681e-05],
        [ 0.0000e+00, -4.0559e-04, -1.4655e-04,  ...,  1.3840e-03,
          0.0000e+00, -1.8681e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2819.9905, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.4108, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.8745, device='cuda:0')



h[100].sum tensor(-7.7301, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.3406, device='cuda:0')



h[200].sum tensor(-14.6247, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-17.7397, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57935.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0037, 0.0586, 0.0261,  ..., 0.0234, 0.0000, 0.0000],
        [0.0038, 0.0588, 0.0262,  ..., 0.0236, 0.0000, 0.0000],
        [0.0070, 0.0568, 0.0285,  ..., 0.0264, 0.0000, 0.0000],
        ...,
        [0.0044, 0.0600, 0.0275,  ..., 0.0249, 0.0000, 0.0000],
        [0.0044, 0.0600, 0.0275,  ..., 0.0249, 0.0000, 0.0000],
        [0.0043, 0.0599, 0.0275,  ..., 0.0248, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(737752.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10746.2383, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-119.5839, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7322.8647, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-704.8525, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10408.1484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(532.4490, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.9342],
        [-2.6956],
        [-2.2976],
        ...,
        [-3.1362],
        [-3.1286],
        [-3.1255]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-384041.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0175],
        [1.0194],
        [1.0263],
        ...,
        [0.9983],
        [0.9970],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369174.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.3286, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0176],
        [1.0194],
        [1.0263],
        ...,
        [0.9983],
        [0.9970],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369180.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.3286, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -4.0012e-04, -1.3920e-04,  ...,  1.3906e-03,
          0.0000e+00, -4.2547e-05],
        [ 0.0000e+00, -4.0012e-04, -1.3920e-04,  ...,  1.3906e-03,
          0.0000e+00, -4.2547e-05],
        [ 0.0000e+00, -4.0012e-04, -1.3920e-04,  ...,  1.3906e-03,
          0.0000e+00, -4.2547e-05],
        ...,
        [ 0.0000e+00, -4.0012e-04, -1.3920e-04,  ...,  1.3906e-03,
          0.0000e+00, -4.2547e-05],
        [ 0.0000e+00, -4.0012e-04, -1.3920e-04,  ...,  1.3906e-03,
          0.0000e+00, -4.2547e-05],
        [ 0.0000e+00, -4.0012e-04, -1.3920e-04,  ...,  1.3906e-03,
          0.0000e+00, -4.2547e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3037.2400, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.8624, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.0123, device='cuda:0')



h[100].sum tensor(-10.7411, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.6738, device='cuda:0')



h[200].sum tensor(-12.5217, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.8190, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63528.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0037, 0.0586, 0.0260,  ..., 0.0235, 0.0000, 0.0000],
        [0.0038, 0.0588, 0.0262,  ..., 0.0236, 0.0000, 0.0000],
        [0.0041, 0.0589, 0.0264,  ..., 0.0239, 0.0000, 0.0000],
        ...,
        [0.0044, 0.0600, 0.0275,  ..., 0.0249, 0.0000, 0.0000],
        [0.0044, 0.0600, 0.0275,  ..., 0.0249, 0.0000, 0.0000],
        [0.0044, 0.0600, 0.0274,  ..., 0.0249, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(751760.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11787.4658, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-92.8175, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6659.4541, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-738.0497, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10791.8711, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(590.8876, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.9475],
        [-3.0049],
        [-2.9971],
        ...,
        [-3.1411],
        [-3.1335],
        [-3.1305]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-314308.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0176],
        [1.0194],
        [1.0263],
        ...,
        [0.9983],
        [0.9970],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369180.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4541],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(336.2469, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0177],
        [1.0195],
        [1.0263],
        ...,
        [0.9982],
        [0.9970],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369185.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4541],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(336.2469, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.4630e-02,  6.7524e-03,  1.4992e-02,  ...,  3.8160e-02,
         -3.6250e-03,  7.4659e-03],
        [-1.6268e-02,  4.3302e-03,  9.8562e-03,  ...,  2.5674e-02,
         -2.3943e-03,  4.9097e-03],
        [-8.4701e-03,  2.0714e-03,  5.0667e-03,  ...,  1.4030e-02,
         -1.2466e-03,  2.5259e-03],
        ...,
        [ 0.0000e+00, -3.8219e-04, -1.3561e-04,  ...,  1.3814e-03,
          0.0000e+00, -6.3353e-05],
        [ 0.0000e+00, -3.8219e-04, -1.3561e-04,  ...,  1.3814e-03,
          0.0000e+00, -6.3353e-05],
        [ 0.0000e+00, -3.8219e-04, -1.3561e-04,  ...,  1.3814e-03,
          0.0000e+00, -6.3353e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3427.5781, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.0039, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.7601, device='cuda:0')



h[100].sum tensor(-16.2472, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.9436, device='cuda:0')



h[200].sum tensor(-8.5514, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-36.8726, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0199, 0.0449,  ..., 0.1160, 0.0000, 0.0224],
        [0.0000, 0.0141, 0.0327,  ..., 0.0865, 0.0000, 0.0163],
        [0.0000, 0.0076, 0.0182,  ..., 0.0510, 0.0000, 0.0091],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75721.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.4364, 0.0000, 0.2549,  ..., 0.3032, 0.0000, 0.0000],
        [0.3746, 0.0000, 0.2215,  ..., 0.2645, 0.0000, 0.0000],
        [0.2954, 0.0000, 0.1787,  ..., 0.2148, 0.0000, 0.0000],
        ...,
        [0.0044, 0.0603, 0.0278,  ..., 0.0252, 0.0000, 0.0000],
        [0.0044, 0.0603, 0.0277,  ..., 0.0252, 0.0000, 0.0000],
        [0.0044, 0.0603, 0.0277,  ..., 0.0251, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(804751.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(14109.4902, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-41.2217, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6170.6646, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-811.1479, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11918.5576, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(720.1700, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2622],
        [ 0.2720],
        [ 0.2839],
        ...,
        [-3.1523],
        [-3.1447],
        [-3.1416]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-275415.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0177],
        [1.0195],
        [1.0263],
        ...,
        [0.9982],
        [0.9970],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369185.2500, device='cuda:0', grad_fn=<SumBackward0>)
time passed so far:
 0:00:35.556010
evaluation loss: 489.01910400390625
epoch: 0 mean loss: 496.5912780761719
=> saveing checkpoint at epoch 0
checkpoint is saved at: /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/./TrainingBha.py", line 138, in <module>
    writer.add_hparams({'Lr': LrVal, 'weight_decay': weight_decay_val, 'BatchSize':BatchSize},\
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/utils/tensorboard/writer.py", line 313, in add_hparams
    w_hp.add_scalar(k, v)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/utils/tensorboard/writer.py", line 354, in add_scalar
    summary = scalar(
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/utils/tensorboard/summary.py", line 250, in scalar
    assert scalar.squeeze().ndim == 0, "scalar should be 0D"
AssertionError: scalar should be 0D

real	1m10.828s
user	0m46.607s
sys	0m18.415s
