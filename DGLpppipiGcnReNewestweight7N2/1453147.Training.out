0: gpu032.ihep.ac.cn
GPU 0: Tesla V100-SXM2-32GB (UUID: GPU-7c6767fd-6329-5119-6fa4-37b51a04c0e7)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1127.8.2.el7.x86_64/extra/nvidia.ko.xz
alias:          char-major-195-*
version:        450.36.06
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.8
srcversion:     BB5CB243542347D4EB0C79C
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        
vermagic:       3.10.0-1127.8.2.el7.x86_64 SMP mod_unload modversions 
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_MapRegistersEarly:int
parm:           NVreg_RegisterForACPIEvents:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_EnableBacklightHandler:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_AssignGpus:charp

nvidia-smi:
Wed Jul  6 00:11:39 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:3B:00.0 Off |                    0 |
| N/A   46C    P0    45W / 300W |      0MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: Tesla V100-SXM2-32GB

 CUDA Device Total Memory [GB]: 34.089730048

 Device capability: (7, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2ab10c8fbfa0> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m30.456s
user	0m2.724s
sys	0m2.020s
[00:12:09] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch




 Training ... 






 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[ 0.0327],
        [ 0.0095],
        [-0.0531],
        ...,
        [-0.8722],
        [-0.7391],
        [ 0.8190]], device='cuda:0', requires_grad=True) 
node features sum: tensor(-89.9332, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14





 Loading data ... 


shape (2000, 6796) (2000, 6796)
sum 189931 265535
shape torch.Size([2000, 6796]) torch.Size([2000, 6796])
Model name: DGLpppipiGcnReNewestweight7N2
net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[-0.0485, -0.0018, -0.0916,  0.0745, -0.1503, -0.0639,  0.1470,  0.0768,
         -0.0309, -0.0188,  0.1365, -0.1363, -0.0970, -0.1204, -0.1517,  0.0591,
          0.0120, -0.0629, -0.0771, -0.0086, -0.0840, -0.1004, -0.0379,  0.0504,
         -0.0221,  0.1361,  0.1179,  0.0118,  0.1075, -0.1465,  0.0342,  0.1483,
         -0.0834,  0.1324,  0.1002, -0.1173,  0.0571, -0.0315, -0.1366, -0.0625,
         -0.1156, -0.0883,  0.0194,  0.1274,  0.1441,  0.0703,  0.0699,  0.0392,
         -0.0555, -0.0454,  0.1477, -0.0528,  0.0380,  0.0725, -0.0969, -0.1147,
         -0.1261, -0.1375,  0.0925, -0.0257, -0.1133, -0.0506,  0.0998,  0.0161,
          0.0929,  0.1307,  0.0171,  0.1337,  0.0440, -0.0829, -0.1150, -0.0753,
         -0.1206, -0.0868, -0.0946, -0.0619,  0.0607,  0.0996,  0.1019,  0.1363,
         -0.1308, -0.0752, -0.0186, -0.0295, -0.0274, -0.1515, -0.0555, -0.1055,
         -0.1370, -0.1436,  0.1348, -0.0848, -0.1138, -0.1131,  0.0116,  0.0561,
          0.0327,  0.1199,  0.0830,  0.1139,  0.0850, -0.0067, -0.1475, -0.1211,
          0.0021, -0.0136, -0.0808,  0.0142,  0.0384, -0.1432, -0.1408, -0.0472,
          0.1427,  0.0412, -0.1353,  0.0722,  0.0620,  0.1398,  0.1336, -0.1041,
         -0.0569, -0.1165,  0.0385, -0.0637,  0.0782, -0.1267,  0.0334, -0.0427,
         -0.1068, -0.0114,  0.0367, -0.0954,  0.0806,  0.1096, -0.1178,  0.1296,
          0.1460, -0.0230, -0.0435, -0.0879,  0.0287, -0.0812, -0.1203,  0.1449,
          0.0325,  0.0934,  0.0093,  0.0142, -0.0723,  0.1466,  0.0711,  0.1040,
         -0.1521, -0.0050, -0.0774,  0.1028, -0.0440, -0.1223,  0.1297, -0.0948,
         -0.0045, -0.0195, -0.0041,  0.0121,  0.0529,  0.1102,  0.0611, -0.0099,
          0.0541,  0.1300, -0.0685, -0.0654, -0.0841,  0.1041, -0.0077, -0.0340,
         -0.0740, -0.1317,  0.0515, -0.0293,  0.0431,  0.0891,  0.0500,  0.0110,
          0.1382, -0.0420,  0.1312, -0.0409, -0.0705,  0.1188,  0.0660,  0.1265,
          0.1380, -0.1174, -0.0953, -0.1196,  0.0130,  0.0947, -0.0484,  0.0700,
         -0.1052,  0.0392,  0.0475,  0.1183,  0.0621,  0.0674,  0.0676,  0.0394,
         -0.0793, -0.0322, -0.0627,  0.0295,  0.1090, -0.1487, -0.1207,  0.0301,
          0.0862, -0.0886, -0.0287,  0.0777,  0.0664, -0.1207,  0.0188, -0.1320,
         -0.1060, -0.1268,  0.0600, -0.0248,  0.0474,  0.1180, -0.1293, -0.0822,
         -0.0933,  0.1452,  0.1399,  0.1065, -0.1130,  0.0597,  0.1095,  0.0448,
          0.0111,  0.1436, -0.0716,  0.0140,  0.1352, -0.1339, -0.0693, -0.1136,
         -0.0378, -0.0488,  0.0389,  0.1350, -0.0753, -0.1492, -0.0434, -0.0321]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0485, -0.0018, -0.0916,  0.0745, -0.1503, -0.0639,  0.1470,  0.0768,
         -0.0309, -0.0188,  0.1365, -0.1363, -0.0970, -0.1204, -0.1517,  0.0591,
          0.0120, -0.0629, -0.0771, -0.0086, -0.0840, -0.1004, -0.0379,  0.0504,
         -0.0221,  0.1361,  0.1179,  0.0118,  0.1075, -0.1465,  0.0342,  0.1483,
         -0.0834,  0.1324,  0.1002, -0.1173,  0.0571, -0.0315, -0.1366, -0.0625,
         -0.1156, -0.0883,  0.0194,  0.1274,  0.1441,  0.0703,  0.0699,  0.0392,
         -0.0555, -0.0454,  0.1477, -0.0528,  0.0380,  0.0725, -0.0969, -0.1147,
         -0.1261, -0.1375,  0.0925, -0.0257, -0.1133, -0.0506,  0.0998,  0.0161,
          0.0929,  0.1307,  0.0171,  0.1337,  0.0440, -0.0829, -0.1150, -0.0753,
         -0.1206, -0.0868, -0.0946, -0.0619,  0.0607,  0.0996,  0.1019,  0.1363,
         -0.1308, -0.0752, -0.0186, -0.0295, -0.0274, -0.1515, -0.0555, -0.1055,
         -0.1370, -0.1436,  0.1348, -0.0848, -0.1138, -0.1131,  0.0116,  0.0561,
          0.0327,  0.1199,  0.0830,  0.1139,  0.0850, -0.0067, -0.1475, -0.1211,
          0.0021, -0.0136, -0.0808,  0.0142,  0.0384, -0.1432, -0.1408, -0.0472,
          0.1427,  0.0412, -0.1353,  0.0722,  0.0620,  0.1398,  0.1336, -0.1041,
         -0.0569, -0.1165,  0.0385, -0.0637,  0.0782, -0.1267,  0.0334, -0.0427,
         -0.1068, -0.0114,  0.0367, -0.0954,  0.0806,  0.1096, -0.1178,  0.1296,
          0.1460, -0.0230, -0.0435, -0.0879,  0.0287, -0.0812, -0.1203,  0.1449,
          0.0325,  0.0934,  0.0093,  0.0142, -0.0723,  0.1466,  0.0711,  0.1040,
         -0.1521, -0.0050, -0.0774,  0.1028, -0.0440, -0.1223,  0.1297, -0.0948,
         -0.0045, -0.0195, -0.0041,  0.0121,  0.0529,  0.1102,  0.0611, -0.0099,
          0.0541,  0.1300, -0.0685, -0.0654, -0.0841,  0.1041, -0.0077, -0.0340,
         -0.0740, -0.1317,  0.0515, -0.0293,  0.0431,  0.0891,  0.0500,  0.0110,
          0.1382, -0.0420,  0.1312, -0.0409, -0.0705,  0.1188,  0.0660,  0.1265,
          0.1380, -0.1174, -0.0953, -0.1196,  0.0130,  0.0947, -0.0484,  0.0700,
         -0.1052,  0.0392,  0.0475,  0.1183,  0.0621,  0.0674,  0.0676,  0.0394,
         -0.0793, -0.0322, -0.0627,  0.0295,  0.1090, -0.1487, -0.1207,  0.0301,
          0.0862, -0.0886, -0.0287,  0.0777,  0.0664, -0.1207,  0.0188, -0.1320,
         -0.1060, -0.1268,  0.0600, -0.0248,  0.0474,  0.1180, -0.1293, -0.0822,
         -0.0933,  0.1452,  0.1399,  0.1065, -0.1130,  0.0597,  0.1095,  0.0448,
          0.0111,  0.1436, -0.0716,  0.0140,  0.1352, -0.1339, -0.0693, -0.1136,
         -0.0378, -0.0488,  0.0389,  0.1350, -0.0753, -0.1492, -0.0434, -0.0321]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[ 0.0064, -0.0732,  0.1144,  ..., -0.0770, -0.0026, -0.0395],
        [-0.0515, -0.0521, -0.0937,  ...,  0.0705,  0.1129,  0.0952],
        [ 0.0410, -0.0390,  0.0461,  ..., -0.1090,  0.0317,  0.0821],
        ...,
        [-0.0123,  0.0492, -0.0038,  ..., -0.0652, -0.0654,  0.0516],
        [-0.0469, -0.0989,  0.0762,  ...,  0.0677,  0.0481,  0.0957],
        [-0.0692, -0.0622, -0.0805,  ...,  0.0148,  0.1141,  0.0870]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0064, -0.0732,  0.1144,  ..., -0.0770, -0.0026, -0.0395],
        [-0.0515, -0.0521, -0.0937,  ...,  0.0705,  0.1129,  0.0952],
        [ 0.0410, -0.0390,  0.0461,  ..., -0.1090,  0.0317,  0.0821],
        ...,
        [-0.0123,  0.0492, -0.0038,  ..., -0.0652, -0.0654,  0.0516],
        [-0.0469, -0.0989,  0.0762,  ...,  0.0677,  0.0481,  0.0957],
        [-0.0692, -0.0622, -0.0805,  ...,  0.0148,  0.1141,  0.0870]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[ 0.1089, -0.1219, -0.0757,  ...,  0.1326,  0.1036, -0.1217],
        [-0.1226,  0.0048, -0.0358,  ...,  0.1179,  0.0652,  0.1362],
        [ 0.1695,  0.0549,  0.0308,  ..., -0.1629,  0.1431, -0.0517],
        ...,
        [ 0.1112,  0.1562,  0.0749,  ..., -0.0144, -0.1681, -0.0611],
        [ 0.0101,  0.0085, -0.1622,  ...,  0.1372,  0.0372,  0.1409],
        [-0.1526,  0.0244,  0.0555,  ...,  0.0660,  0.1213, -0.0031]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.1089, -0.1219, -0.0757,  ...,  0.1326,  0.1036, -0.1217],
        [-0.1226,  0.0048, -0.0358,  ...,  0.1179,  0.0652,  0.1362],
        [ 0.1695,  0.0549,  0.0308,  ..., -0.1629,  0.1431, -0.0517],
        ...,
        [ 0.1112,  0.1562,  0.0749,  ..., -0.0144, -0.1681, -0.0611],
        [ 0.0101,  0.0085, -0.1622,  ...,  0.1372,  0.0372,  0.1409],
        [-0.1526,  0.0244,  0.0555,  ...,  0.0660,  0.1213, -0.0031]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[-0.2380,  0.1985, -0.2029,  ...,  0.1800,  0.2301,  0.0183],
        [ 0.1003,  0.1421, -0.1690,  ...,  0.2342, -0.1879,  0.1005],
        [-0.0333, -0.0152,  0.0012,  ..., -0.1839,  0.2332,  0.2106],
        ...,
        [-0.2480,  0.0046,  0.0045,  ..., -0.0585,  0.2190, -0.0348],
        [-0.2279, -0.1765, -0.0641,  ..., -0.0417, -0.1456, -0.0480],
        [ 0.0668, -0.0971, -0.2429,  ...,  0.1217,  0.0156,  0.0817]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.2380,  0.1985, -0.2029,  ...,  0.1800,  0.2301,  0.0183],
        [ 0.1003,  0.1421, -0.1690,  ...,  0.2342, -0.1879,  0.1005],
        [-0.0333, -0.0152,  0.0012,  ..., -0.1839,  0.2332,  0.2106],
        ...,
        [-0.2480,  0.0046,  0.0045,  ..., -0.0585,  0.2190, -0.0348],
        [-0.2279, -0.1765, -0.0641,  ..., -0.0417, -0.1456, -0.0480],
        [ 0.0668, -0.0971, -0.2429,  ...,  0.1217,  0.0156,  0.0817]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[ 0.0197],
        [-0.0863],
        [ 0.3381],
        [ 0.3024],
        [-0.1693],
        [-0.1520],
        [-0.3181],
        [-0.3355],
        [-0.3854],
        [ 0.1922],
        [ 0.2651],
        [-0.2781],
        [-0.1483],
        [ 0.3734],
        [-0.1811],
        [ 0.1677],
        [-0.0105],
        [-0.1470],
        [-0.1778],
        [-0.3562],
        [-0.1610],
        [ 0.4161],
        [ 0.1152],
        [ 0.1505],
        [ 0.1324],
        [ 0.1360],
        [-0.0541],
        [-0.2198],
        [ 0.2876],
        [ 0.1635],
        [-0.0072],
        [-0.3661]], device='cuda:0') 
 Parameter containing:
tensor([[ 0.0197],
        [-0.0863],
        [ 0.3381],
        [ 0.3024],
        [-0.1693],
        [-0.1520],
        [-0.3181],
        [-0.3355],
        [-0.3854],
        [ 0.1922],
        [ 0.2651],
        [-0.2781],
        [-0.1483],
        [ 0.3734],
        [-0.1811],
        [ 0.1677],
        [-0.0105],
        [-0.1470],
        [-0.1778],
        [-0.3562],
        [-0.1610],
        [ 0.4161],
        [ 0.1152],
        [ 0.1505],
        [ 0.1324],
        [ 0.1360],
        [-0.0541],
        [-0.2198],
        [ 0.2876],
        [ 0.1635],
        [-0.0072],
        [-0.3661]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[ 0.0961,  0.0777, -0.1484,  0.1119, -0.1034, -0.0513,  0.0760, -0.0483,
          0.0945, -0.0043,  0.0979,  0.0916,  0.0964,  0.1132, -0.0256, -0.0795,
          0.0374,  0.1451, -0.0055, -0.0162,  0.0370, -0.1115, -0.0541,  0.1317,
          0.0416, -0.1503,  0.0072,  0.0257,  0.0439, -0.1264,  0.0830, -0.0956,
          0.0782,  0.0505, -0.0218, -0.1320, -0.0111,  0.1381,  0.1271, -0.0600,
         -0.1068,  0.0649, -0.1524, -0.0540,  0.0847,  0.1173, -0.0093, -0.0517,
          0.0729,  0.0016, -0.0847,  0.0276, -0.0311,  0.1217,  0.1300,  0.1462,
         -0.0101, -0.1180,  0.0020,  0.1434, -0.0767,  0.0859,  0.0404,  0.1322,
         -0.0786, -0.0137, -0.0619, -0.0232, -0.0361, -0.1260,  0.0257,  0.1158,
         -0.1321, -0.0572, -0.0900,  0.0137,  0.1136,  0.1365, -0.0756, -0.1351,
         -0.0649,  0.0308,  0.1052, -0.0228, -0.0841,  0.0668, -0.1417, -0.0089,
          0.0547,  0.1181,  0.0017, -0.0826, -0.0848, -0.0866, -0.1499, -0.1339,
         -0.0399, -0.0362, -0.1131, -0.1513,  0.1067,  0.0076, -0.0663,  0.1177,
          0.0545,  0.1258,  0.0795,  0.0308,  0.0300,  0.1444,  0.0850, -0.1510,
          0.0281,  0.1329,  0.0865, -0.1027, -0.0338, -0.0610, -0.1478, -0.0119,
          0.0318,  0.0269,  0.0225, -0.0097, -0.0803,  0.0421, -0.0734, -0.1003,
          0.0144, -0.0932, -0.0565,  0.0885, -0.0909, -0.0490,  0.0787,  0.0912,
          0.0751, -0.1474,  0.0047, -0.0223, -0.0719,  0.0371, -0.0440, -0.0287,
          0.0225, -0.0940, -0.0481,  0.0322,  0.0446,  0.1375, -0.0200,  0.1027,
          0.0188, -0.0965, -0.0820,  0.0264, -0.0284, -0.1432,  0.1072, -0.1028,
         -0.0756, -0.0075,  0.0865, -0.1140, -0.1396, -0.0469,  0.1143, -0.0294,
          0.0780,  0.0947,  0.1508, -0.0641,  0.0094, -0.1389, -0.1451, -0.0864,
          0.0808,  0.0094, -0.0743, -0.1234,  0.0883,  0.0326,  0.0886, -0.0705,
         -0.0867, -0.0412, -0.1479, -0.1408,  0.0533, -0.1156,  0.0152,  0.1413,
          0.0728,  0.1175, -0.1102, -0.0673,  0.0854,  0.0736,  0.0589,  0.0262,
          0.1044,  0.0569, -0.1108, -0.0623, -0.1443, -0.1346, -0.0484,  0.0108,
          0.1347, -0.1291, -0.1478, -0.0675, -0.0978,  0.0649, -0.1363, -0.0945,
          0.0051, -0.0628, -0.0630, -0.0549,  0.0172,  0.0114,  0.0179, -0.0032,
          0.1310, -0.1431, -0.1458,  0.0697, -0.1258,  0.0340,  0.0481,  0.1096,
         -0.0517, -0.1269, -0.1495,  0.0390, -0.0692,  0.0592,  0.0654, -0.1524,
         -0.0356,  0.0061,  0.1479, -0.0380, -0.0901,  0.0113, -0.0583,  0.0148,
          0.0608, -0.0189,  0.1095,  0.0483, -0.1196, -0.0029,  0.0831, -0.0524]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0961,  0.0777, -0.1484,  0.1119, -0.1034, -0.0513,  0.0760, -0.0483,
          0.0945, -0.0043,  0.0979,  0.0916,  0.0964,  0.1132, -0.0256, -0.0795,
          0.0374,  0.1451, -0.0055, -0.0162,  0.0370, -0.1115, -0.0541,  0.1317,
          0.0416, -0.1503,  0.0072,  0.0257,  0.0439, -0.1264,  0.0830, -0.0956,
          0.0782,  0.0505, -0.0218, -0.1320, -0.0111,  0.1381,  0.1271, -0.0600,
         -0.1068,  0.0649, -0.1524, -0.0540,  0.0847,  0.1173, -0.0093, -0.0517,
          0.0729,  0.0016, -0.0847,  0.0276, -0.0311,  0.1217,  0.1300,  0.1462,
         -0.0101, -0.1180,  0.0020,  0.1434, -0.0767,  0.0859,  0.0404,  0.1322,
         -0.0786, -0.0137, -0.0619, -0.0232, -0.0361, -0.1260,  0.0257,  0.1158,
         -0.1321, -0.0572, -0.0900,  0.0137,  0.1136,  0.1365, -0.0756, -0.1351,
         -0.0649,  0.0308,  0.1052, -0.0228, -0.0841,  0.0668, -0.1417, -0.0089,
          0.0547,  0.1181,  0.0017, -0.0826, -0.0848, -0.0866, -0.1499, -0.1339,
         -0.0399, -0.0362, -0.1131, -0.1513,  0.1067,  0.0076, -0.0663,  0.1177,
          0.0545,  0.1258,  0.0795,  0.0308,  0.0300,  0.1444,  0.0850, -0.1510,
          0.0281,  0.1329,  0.0865, -0.1027, -0.0338, -0.0610, -0.1478, -0.0119,
          0.0318,  0.0269,  0.0225, -0.0097, -0.0803,  0.0421, -0.0734, -0.1003,
          0.0144, -0.0932, -0.0565,  0.0885, -0.0909, -0.0490,  0.0787,  0.0912,
          0.0751, -0.1474,  0.0047, -0.0223, -0.0719,  0.0371, -0.0440, -0.0287,
          0.0225, -0.0940, -0.0481,  0.0322,  0.0446,  0.1375, -0.0200,  0.1027,
          0.0188, -0.0965, -0.0820,  0.0264, -0.0284, -0.1432,  0.1072, -0.1028,
         -0.0756, -0.0075,  0.0865, -0.1140, -0.1396, -0.0469,  0.1143, -0.0294,
          0.0780,  0.0947,  0.1508, -0.0641,  0.0094, -0.1389, -0.1451, -0.0864,
          0.0808,  0.0094, -0.0743, -0.1234,  0.0883,  0.0326,  0.0886, -0.0705,
         -0.0867, -0.0412, -0.1479, -0.1408,  0.0533, -0.1156,  0.0152,  0.1413,
          0.0728,  0.1175, -0.1102, -0.0673,  0.0854,  0.0736,  0.0589,  0.0262,
          0.1044,  0.0569, -0.1108, -0.0623, -0.1443, -0.1346, -0.0484,  0.0108,
          0.1347, -0.1291, -0.1478, -0.0675, -0.0978,  0.0649, -0.1363, -0.0945,
          0.0051, -0.0628, -0.0630, -0.0549,  0.0172,  0.0114,  0.0179, -0.0032,
          0.1310, -0.1431, -0.1458,  0.0697, -0.1258,  0.0340,  0.0481,  0.1096,
         -0.0517, -0.1269, -0.1495,  0.0390, -0.0692,  0.0592,  0.0654, -0.1524,
         -0.0356,  0.0061,  0.1479, -0.0380, -0.0901,  0.0113, -0.0583,  0.0148,
          0.0608, -0.0189,  0.1095,  0.0483, -0.1196, -0.0029,  0.0831, -0.0524]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[ 0.0035,  0.1192,  0.0544,  ...,  0.1222, -0.0859, -0.0965],
        [ 0.0140, -0.0304,  0.1131,  ...,  0.0875,  0.0996,  0.1025],
        [-0.1086, -0.1135,  0.0946,  ...,  0.0981, -0.0023,  0.1155],
        ...,
        [-0.1198,  0.1012,  0.0939,  ...,  0.1064, -0.0499,  0.0196],
        [-0.0841,  0.1033, -0.0654,  ...,  0.1009,  0.0602, -0.0890],
        [ 0.0204, -0.0133,  0.1169,  ..., -0.0592,  0.0250, -0.0109]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0035,  0.1192,  0.0544,  ...,  0.1222, -0.0859, -0.0965],
        [ 0.0140, -0.0304,  0.1131,  ...,  0.0875,  0.0996,  0.1025],
        [-0.1086, -0.1135,  0.0946,  ...,  0.0981, -0.0023,  0.1155],
        ...,
        [-0.1198,  0.1012,  0.0939,  ...,  0.1064, -0.0499,  0.0196],
        [-0.0841,  0.1033, -0.0654,  ...,  0.1009,  0.0602, -0.0890],
        [ 0.0204, -0.0133,  0.1169,  ..., -0.0592,  0.0250, -0.0109]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[ 0.1212, -0.1686,  0.1642,  ..., -0.1234, -0.0516,  0.1652],
        [ 0.1082,  0.1685, -0.0024,  ..., -0.0914,  0.1247,  0.0603],
        [-0.1056,  0.1626, -0.0071,  ..., -0.1711,  0.0271,  0.1598],
        ...,
        [ 0.1626, -0.0645,  0.1386,  ...,  0.0636, -0.0307, -0.1418],
        [ 0.1058, -0.1767, -0.1392,  ..., -0.0782, -0.0502, -0.0179],
        [-0.0846,  0.1334,  0.0764,  ..., -0.1706,  0.1632,  0.0064]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.1212, -0.1686,  0.1642,  ..., -0.1234, -0.0516,  0.1652],
        [ 0.1082,  0.1685, -0.0024,  ..., -0.0914,  0.1247,  0.0603],
        [-0.1056,  0.1626, -0.0071,  ..., -0.1711,  0.0271,  0.1598],
        ...,
        [ 0.1626, -0.0645,  0.1386,  ...,  0.0636, -0.0307, -0.1418],
        [ 0.1058, -0.1767, -0.1392,  ..., -0.0782, -0.0502, -0.0179],
        [-0.0846,  0.1334,  0.0764,  ..., -0.1706,  0.1632,  0.0064]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[-0.1269,  0.0789,  0.1794,  ...,  0.0155,  0.1138, -0.0313],
        [-0.2053,  0.1603, -0.0352,  ...,  0.1422,  0.0491, -0.1024],
        [ 0.1440, -0.1905, -0.2160,  ..., -0.2098,  0.0627, -0.1624],
        ...,
        [-0.2034,  0.2200,  0.2253,  ..., -0.2148, -0.2144, -0.2150],
        [-0.1196, -0.0204,  0.0628,  ..., -0.0377, -0.2340, -0.1684],
        [ 0.1418,  0.0793,  0.1991,  ..., -0.0874, -0.1978, -0.2376]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.1269,  0.0789,  0.1794,  ...,  0.0155,  0.1138, -0.0313],
        [-0.2053,  0.1603, -0.0352,  ...,  0.1422,  0.0491, -0.1024],
        [ 0.1440, -0.1905, -0.2160,  ..., -0.2098,  0.0627, -0.1624],
        ...,
        [-0.2034,  0.2200,  0.2253,  ..., -0.2148, -0.2144, -0.2150],
        [-0.1196, -0.0204,  0.0628,  ..., -0.0377, -0.2340, -0.1684],
        [ 0.1418,  0.0793,  0.1991,  ..., -0.0874, -0.1978, -0.2376]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[ 0.0026],
        [ 0.1139],
        [ 0.3767],
        [ 0.1905],
        [ 0.2140],
        [ 0.1320],
        [ 0.4228],
        [ 0.3925],
        [ 0.1803],
        [-0.2230],
        [-0.3602],
        [ 0.3634],
        [ 0.0812],
        [-0.3042],
        [ 0.4049],
        [ 0.2517],
        [-0.1522],
        [-0.2028],
        [-0.0958],
        [-0.2909],
        [ 0.2654],
        [-0.2595],
        [ 0.2081],
        [ 0.2121],
        [ 0.1405],
        [-0.3923],
        [-0.2138],
        [-0.0979],
        [-0.2492],
        [-0.1447],
        [ 0.3369],
        [-0.1106]], device='cuda:0') 
 Parameter containing:
tensor([[ 0.0026],
        [ 0.1139],
        [ 0.3767],
        [ 0.1905],
        [ 0.2140],
        [ 0.1320],
        [ 0.4228],
        [ 0.3925],
        [ 0.1803],
        [-0.2230],
        [-0.3602],
        [ 0.3634],
        [ 0.0812],
        [-0.3042],
        [ 0.4049],
        [ 0.2517],
        [-0.1522],
        [-0.2028],
        [-0.0958],
        [-0.2909],
        [ 0.2654],
        [-0.2595],
        [ 0.2081],
        [ 0.2121],
        [ 0.1405],
        [-0.3923],
        [-0.2138],
        [-0.0979],
        [-0.2492],
        [-0.1447],
        [ 0.3369],
        [-0.1106]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2988],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(173.1525, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2988],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(173.1525, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0045, -0.0049,  0.0045,  ..., -0.0055, -0.0062,  0.0061],
        [-0.0148, -0.0163,  0.0151,  ..., -0.0183, -0.0207,  0.0204],
        [-0.0098, -0.0108,  0.0100,  ..., -0.0122, -0.0137,  0.0135],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(33.6848, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.3977, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.8803, device='cuda:0')



h[100].sum tensor(-9.8187, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.2008, device='cuda:0')



h[200].sum tensor(-16.0462, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-16.6708, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0346,  ..., 0.0000, 0.0000, 0.0467],
        [0.0000, 0.0000, 0.0304,  ..., 0.0000, 0.0000, 0.0411],
        [0.0000, 0.0000, 0.0418,  ..., 0.0000, 0.0000, 0.0565],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(19879.3086, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1485, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1456, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1463, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(94554.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-38.0956, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2324.2090, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(148.2812, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-55.6502, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4082],
        [-0.3930],
        [-0.3529],
        ...,
        [-0.0030],
        [-0.0031],
        [-0.0032]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(-20991.8145, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 20 from the network before training 
result1: tensor([[-0.4082],
        [-0.3930],
        [-0.3529],
        ...,
        [-0.0030],
        [-0.0031],
        [-0.0032]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0.     0.     0.2988 ... 0.     0.     0.    ]



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2988],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet].sum tensor(273.7637, device='cuda:0')



input graph: 
g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2988],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(273.7637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0083, -0.0004,  0.0034,  ...,  0.0050, -0.0064, -0.0060],
        [-0.0275, -0.0013,  0.0114,  ...,  0.0167, -0.0213, -0.0198],
        [-0.0182, -0.0008,  0.0075,  ...,  0.0111, -0.0141, -0.0132],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(-220.0722, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-36.5104, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-37.8770, device='cuda:0')



h[100].sum tensor(37.5067, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(38.9106, device='cuda:0')



h[200].sum tensor(-7.7636, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.0542, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0261,  ..., 0.0383, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0230,  ..., 0.0337, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0316,  ..., 0.0464, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(28727.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0104, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([13592, 128]) 
h2.sum tensor(139296.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-205.8227, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-186.3312, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1835.3779, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(123.0834, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-8.3528e-01],
        [-8.0407e-01],
        [-7.2208e-01],
        ...,
        [-2.8018e-05],
        [-4.6264e-04],
        [-3.1007e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([13592, 1]) 
h5.sum tensor(-58569.1289, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[-0.4082],
        [-0.3930],
        [-0.3529],
        ...,
        [-0.0030],
        [-0.0031],
        [-0.0032]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0.     0.     0.2988 ... 0.     0.     0.    ]



load_model False 
TraEvN 2000 
BatchSize 15 
EpochNum 10 
epoch_save 5 
LrVal 0.0001 
weight_decay 5e-05 






optimizer.param_groups [{'params': [Parameter containing:
tensor([[-0.0318, -0.0514,  0.1149,  0.0253,  0.1344, -0.0146, -0.0616,  0.1109,
          0.1205, -0.1387, -0.0105, -0.0960, -0.0932, -0.0598, -0.0520,  0.0273,
         -0.0007, -0.0163, -0.0733,  0.0180,  0.0188,  0.0552, -0.1209,  0.0115,
          0.0983, -0.1151, -0.0530, -0.0044, -0.1070,  0.0282, -0.1008,  0.0961,
          0.0366, -0.1104, -0.0172, -0.0100,  0.0736, -0.0649,  0.1150,  0.0207,
         -0.0961, -0.0924, -0.1381,  0.1038,  0.1266, -0.0088,  0.1321,  0.0237,
          0.1258, -0.0497, -0.0956, -0.0124, -0.1388,  0.0494, -0.0349,  0.0123,
         -0.0713,  0.0307, -0.0239, -0.0201,  0.1081,  0.1210,  0.0493, -0.1051,
         -0.0275,  0.1367, -0.1073,  0.1119, -0.0002, -0.0843, -0.0853,  0.0865,
         -0.0037,  0.0072,  0.0730,  0.0322,  0.0294, -0.1431, -0.0970,  0.1173,
          0.0047, -0.0048,  0.1207, -0.0830, -0.1065,  0.0720,  0.0610,  0.0770,
          0.0926,  0.0818, -0.0118, -0.0452, -0.0970,  0.0725,  0.1172,  0.0737,
         -0.0022, -0.0297, -0.1081,  0.0596, -0.1066, -0.1213, -0.0133,  0.1258,
         -0.0273, -0.1361, -0.1142, -0.0865,  0.1500,  0.1240,  0.0286, -0.1178,
          0.0827, -0.1152, -0.1031,  0.0097,  0.0562, -0.0451, -0.1515, -0.0815,
         -0.1254, -0.0986, -0.0465,  0.0732,  0.1223, -0.0384, -0.0982,  0.0405,
          0.1308,  0.1154, -0.1451, -0.0196,  0.0838, -0.0677, -0.0142, -0.0352,
          0.0461,  0.0731,  0.0633,  0.1212, -0.1232, -0.0005, -0.1376,  0.1283,
         -0.0318, -0.0379, -0.0251, -0.0629,  0.1052,  0.0920, -0.1134, -0.0116,
         -0.0104,  0.1095,  0.0794, -0.0090, -0.0959, -0.1469,  0.1472, -0.1521,
          0.1282,  0.1319,  0.0412,  0.0186, -0.1489,  0.0389, -0.0987, -0.0610,
          0.1325, -0.1099, -0.1240,  0.0965,  0.1050, -0.0760,  0.0038, -0.0521,
         -0.0948,  0.0024, -0.0321, -0.0523,  0.1159, -0.1088,  0.0750, -0.0768,
         -0.0452,  0.0260, -0.0838, -0.0978,  0.0320,  0.1452,  0.0108,  0.0751,
          0.0642, -0.0233, -0.0556, -0.0821, -0.0418,  0.1064, -0.0884,  0.0343,
         -0.1401, -0.0085,  0.1509, -0.0040, -0.1309, -0.1169, -0.0271,  0.1410,
         -0.0221,  0.0281, -0.1074, -0.0887, -0.0938, -0.0920, -0.0685, -0.0997,
          0.1147,  0.0146, -0.1200, -0.1435,  0.1262,  0.1453,  0.0245, -0.0949,
          0.1121, -0.0586,  0.1388, -0.0286, -0.1224, -0.0887, -0.0460,  0.1097,
          0.1427,  0.1140,  0.1119, -0.1320,  0.1074, -0.1455,  0.0420, -0.1340,
         -0.0906, -0.0732, -0.0923, -0.0304,  0.0073,  0.0948,  0.0277, -0.1285,
          0.0824, -0.1286, -0.1096,  0.0006,  0.1180,  0.0120, -0.1508, -0.0691]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0624,  0.0092,  0.1138,  ..., -0.0209,  0.0842,  0.0322],
        [ 0.1222,  0.0234, -0.0355,  ...,  0.0152, -0.0558,  0.1021],
        [-0.0469, -0.0463, -0.0569,  ...,  0.0198,  0.0853, -0.0584],
        ...,
        [-0.0286,  0.0846,  0.0957,  ..., -0.1170, -0.0065,  0.0055],
        [ 0.0866, -0.0522,  0.0843,  ..., -0.0891,  0.0661,  0.0089],
        [ 0.0061, -0.0408,  0.0029,  ..., -0.0948,  0.0882, -0.0613]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0435,  0.1060,  0.0972,  ..., -0.0083, -0.1726,  0.1744],
        [ 0.1093, -0.0264, -0.1464,  ..., -0.1265,  0.0925,  0.1550],
        [ 0.0014, -0.0747, -0.0142,  ..., -0.0521, -0.0771, -0.0424],
        ...,
        [ 0.1315,  0.1728,  0.0226,  ...,  0.1386,  0.0939,  0.0127],
        [-0.0883, -0.0437,  0.0503,  ..., -0.0982, -0.0729, -0.1069],
        [-0.1511, -0.0398,  0.0196,  ..., -0.1190,  0.1670, -0.1147]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1864, -0.2079,  0.2350,  ...,  0.1207,  0.0735, -0.2321],
        [-0.2341,  0.1711,  0.1027,  ..., -0.1925, -0.1649, -0.0452],
        [ 0.0583, -0.1073,  0.0798,  ...,  0.2342, -0.2283,  0.2444],
        ...,
        [ 0.0165,  0.0609,  0.0731,  ...,  0.1718,  0.2232, -0.2285],
        [ 0.2406,  0.2297,  0.2410,  ..., -0.0471,  0.0562,  0.1142],
        [ 0.1908, -0.2081, -0.0642,  ...,  0.0162,  0.1031,  0.0602]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.4152],
        [-0.3243],
        [-0.1915],
        [ 0.3361],
        [-0.1823],
        [ 0.3592],
        [ 0.2361],
        [-0.0860],
        [ 0.0241],
        [ 0.2517],
        [ 0.1799],
        [-0.3354],
        [-0.2744],
        [-0.1197],
        [-0.2760],
        [-0.2032],
        [ 0.2594],
        [-0.2844],
        [ 0.1069],
        [-0.3338],
        [-0.1757],
        [-0.3045],
        [ 0.3782],
        [ 0.1185],
        [ 0.2257],
        [-0.3711],
        [-0.3933],
        [-0.3071],
        [-0.3447],
        [ 0.0113],
        [-0.0294],
        [-0.2846]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



optimizer.param_groups [{'params': [Parameter containing:
tensor([[-0.0318, -0.0514,  0.1149,  0.0253,  0.1344, -0.0146, -0.0616,  0.1109,
          0.1205, -0.1387, -0.0105, -0.0960, -0.0932, -0.0598, -0.0520,  0.0273,
         -0.0007, -0.0163, -0.0733,  0.0180,  0.0188,  0.0552, -0.1209,  0.0115,
          0.0983, -0.1151, -0.0530, -0.0044, -0.1070,  0.0282, -0.1008,  0.0961,
          0.0366, -0.1104, -0.0172, -0.0100,  0.0736, -0.0649,  0.1150,  0.0207,
         -0.0961, -0.0924, -0.1381,  0.1038,  0.1266, -0.0088,  0.1321,  0.0237,
          0.1258, -0.0497, -0.0956, -0.0124, -0.1388,  0.0494, -0.0349,  0.0123,
         -0.0713,  0.0307, -0.0239, -0.0201,  0.1081,  0.1210,  0.0493, -0.1051,
         -0.0275,  0.1367, -0.1073,  0.1119, -0.0002, -0.0843, -0.0853,  0.0865,
         -0.0037,  0.0072,  0.0730,  0.0322,  0.0294, -0.1431, -0.0970,  0.1173,
          0.0047, -0.0048,  0.1207, -0.0830, -0.1065,  0.0720,  0.0610,  0.0770,
          0.0926,  0.0818, -0.0118, -0.0452, -0.0970,  0.0725,  0.1172,  0.0737,
         -0.0022, -0.0297, -0.1081,  0.0596, -0.1066, -0.1213, -0.0133,  0.1258,
         -0.0273, -0.1361, -0.1142, -0.0865,  0.1500,  0.1240,  0.0286, -0.1178,
          0.0827, -0.1152, -0.1031,  0.0097,  0.0562, -0.0451, -0.1515, -0.0815,
         -0.1254, -0.0986, -0.0465,  0.0732,  0.1223, -0.0384, -0.0982,  0.0405,
          0.1308,  0.1154, -0.1451, -0.0196,  0.0838, -0.0677, -0.0142, -0.0352,
          0.0461,  0.0731,  0.0633,  0.1212, -0.1232, -0.0005, -0.1376,  0.1283,
         -0.0318, -0.0379, -0.0251, -0.0629,  0.1052,  0.0920, -0.1134, -0.0116,
         -0.0104,  0.1095,  0.0794, -0.0090, -0.0959, -0.1469,  0.1472, -0.1521,
          0.1282,  0.1319,  0.0412,  0.0186, -0.1489,  0.0389, -0.0987, -0.0610,
          0.1325, -0.1099, -0.1240,  0.0965,  0.1050, -0.0760,  0.0038, -0.0521,
         -0.0948,  0.0024, -0.0321, -0.0523,  0.1159, -0.1088,  0.0750, -0.0768,
         -0.0452,  0.0260, -0.0838, -0.0978,  0.0320,  0.1452,  0.0108,  0.0751,
          0.0642, -0.0233, -0.0556, -0.0821, -0.0418,  0.1064, -0.0884,  0.0343,
         -0.1401, -0.0085,  0.1509, -0.0040, -0.1309, -0.1169, -0.0271,  0.1410,
         -0.0221,  0.0281, -0.1074, -0.0887, -0.0938, -0.0920, -0.0685, -0.0997,
          0.1147,  0.0146, -0.1200, -0.1435,  0.1262,  0.1453,  0.0245, -0.0949,
          0.1121, -0.0586,  0.1388, -0.0286, -0.1224, -0.0887, -0.0460,  0.1097,
          0.1427,  0.1140,  0.1119, -0.1320,  0.1074, -0.1455,  0.0420, -0.1340,
         -0.0906, -0.0732, -0.0923, -0.0304,  0.0073,  0.0948,  0.0277, -0.1285,
          0.0824, -0.1286, -0.1096,  0.0006,  0.1180,  0.0120, -0.1508, -0.0691]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0624,  0.0092,  0.1138,  ..., -0.0209,  0.0842,  0.0322],
        [ 0.1222,  0.0234, -0.0355,  ...,  0.0152, -0.0558,  0.1021],
        [-0.0469, -0.0463, -0.0569,  ...,  0.0198,  0.0853, -0.0584],
        ...,
        [-0.0286,  0.0846,  0.0957,  ..., -0.1170, -0.0065,  0.0055],
        [ 0.0866, -0.0522,  0.0843,  ..., -0.0891,  0.0661,  0.0089],
        [ 0.0061, -0.0408,  0.0029,  ..., -0.0948,  0.0882, -0.0613]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0435,  0.1060,  0.0972,  ..., -0.0083, -0.1726,  0.1744],
        [ 0.1093, -0.0264, -0.1464,  ..., -0.1265,  0.0925,  0.1550],
        [ 0.0014, -0.0747, -0.0142,  ..., -0.0521, -0.0771, -0.0424],
        ...,
        [ 0.1315,  0.1728,  0.0226,  ...,  0.1386,  0.0939,  0.0127],
        [-0.0883, -0.0437,  0.0503,  ..., -0.0982, -0.0729, -0.1069],
        [-0.1511, -0.0398,  0.0196,  ..., -0.1190,  0.1670, -0.1147]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1864, -0.2079,  0.2350,  ...,  0.1207,  0.0735, -0.2321],
        [-0.2341,  0.1711,  0.1027,  ..., -0.1925, -0.1649, -0.0452],
        [ 0.0583, -0.1073,  0.0798,  ...,  0.2342, -0.2283,  0.2444],
        ...,
        [ 0.0165,  0.0609,  0.0731,  ...,  0.1718,  0.2232, -0.2285],
        [ 0.2406,  0.2297,  0.2410,  ..., -0.0471,  0.0562,  0.1142],
        [ 0.1908, -0.2081, -0.0642,  ...,  0.0162,  0.1031,  0.0602]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.4152],
        [-0.3243],
        [-0.1915],
        [ 0.3361],
        [-0.1823],
        [ 0.3592],
        [ 0.2361],
        [-0.0860],
        [ 0.0241],
        [ 0.2517],
        [ 0.1799],
        [-0.3354],
        [-0.2744],
        [-0.1197],
        [-0.2760],
        [-0.2032],
        [ 0.2594],
        [-0.2844],
        [ 0.1069],
        [-0.3338],
        [-0.1757],
        [-0.3045],
        [ 0.3782],
        [ 0.1185],
        [ 0.2257],
        [-0.3711],
        [-0.3933],
        [-0.3071],
        [-0.3447],
        [ 0.0113],
        [-0.0294],
        [-0.2846]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}, {'params': [tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1550.9999, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1550.9999, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2808.8958, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-47.9356, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-214.5912, device='cuda:0')



h[100].sum tensor(-160.9502, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(220.4469, device='cuda:0')



h[200].sum tensor(-211.5524, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-45.6309, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0201,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(146466.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0208, 0.0179,  ..., 0.0000, 0.0102, 0.0000],
        [0.0000, 0.0088, 0.0076,  ..., 0.0000, 0.0043, 0.0000],
        [0.0000, 0.0060, 0.0052,  ..., 0.0000, 0.0030, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(987054.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-308.4853, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17024.4277, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-417.4338, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(936.0522, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0694e+00],
        [-9.3884e-01],
        [-9.2076e-01],
        ...,
        [-4.4979e-06],
        [-5.9096e-07],
        [ 0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-395227.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 0.0 event: 0 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1622.8817, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9999],
        [0.9999],
        [0.9999],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097680.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1622.8817, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2942.6626, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-50.1395, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-224.5365, device='cuda:0')



h[100].sum tensor(-168.7207, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(230.6636, device='cuda:0')



h[200].sum tensor(-221.8154, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-47.7457, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0106,  ..., 0.0011, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(150398.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0113, 0.0097,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0031, 0.0026,  ..., 0.0000, 0.0015, 0.0000],
        [0.0000, 0.0012, 0.0011,  ..., 0.0000, 0.0006, 0.0000],
        ...,
        [0.0000, 0.0049, 0.0042,  ..., 0.0000, 0.0024, 0.0000],
        [0.0000, 0.0082, 0.0071,  ..., 0.0000, 0.0040, 0.0000],
        [0.0000, 0.0121, 0.0104,  ..., 0.0000, 0.0059, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1003470.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-316.3588, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17316.4805, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-428.3942, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(960.9825, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4968],
        [-0.3989],
        [-0.4208],
        ...,
        [-0.3523],
        [-0.4572],
        [-0.5614]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-401539.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [0.9999],
        [0.9999],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097680.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5166],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1691.9197, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9998],
        [0.9998],
        [0.9998],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097570.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5166],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1691.9197, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0033, -0.0054,  0.0121,  ...,  0.0012, -0.0159, -0.0073],
        [-0.0074, -0.0120,  0.0269,  ...,  0.0028, -0.0354, -0.0162],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-3057.4338, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-52.0127, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-234.0883, device='cuda:0')



h[100].sum tensor(-175.4107, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(240.4761, device='cuda:0')



h[200].sum tensor(-230.6624, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-49.7768, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1077,  ..., 0.0110, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0542,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0609,  ..., 0.0062, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(158794.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0773, 0.0664,  ..., 0.0000, 0.0377, 0.0000],
        [0.0000, 0.0591, 0.0507,  ..., 0.0000, 0.0288, 0.0000],
        [0.0000, 0.0544, 0.0467,  ..., 0.0000, 0.0265, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1057301., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-333.5898, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18254.8633, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-452.0524, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1014.4216, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.0380],
        [-2.6276],
        [-2.2924],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-416197.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [0.9998],
        [0.9998],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097570.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1482.7794, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9997],
        [0.9997],
        [0.9997],
        ...,
        [0.9997],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097461.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1482.7794, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -6.3881e-05,  ..., -6.3881e-05,
          0.0000e+00,  0.0000e+00],
        [-5.3752e-03, -8.7271e-03,  1.9535e-02,  ...,  1.9353e-03,
         -2.5719e-02, -1.1752e-02],
        [ 0.0000e+00,  0.0000e+00, -6.3881e-05,  ..., -6.3881e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -6.3881e-05,  ..., -6.3881e-05,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -6.3881e-05,  ..., -6.3881e-05,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -6.3881e-05,  ..., -6.3881e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2688.9619, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-45.6109, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-205.1524, device='cuda:0')



h[100].sum tensor(-154.1619, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(210.7505, device='cuda:0')



h[200].sum tensor(-202.7662, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-43.6238, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0799,  ..., 0.0079, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0159,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0195,  ..., 0.0019, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0073,  ..., 0.0007, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(143462.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0366, 0.0303,  ..., 0.0000, 0.0158, 0.0000],
        [0.0000, 0.0181, 0.0145,  ..., 0.0000, 0.0073, 0.0000],
        [0.0000, 0.0117, 0.0091,  ..., 0.0000, 0.0044, 0.0000],
        ...,
        [0.0000, 0.0001, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0033, 0.0024,  ..., 0.0000, 0.0011, 0.0000],
        [0.0000, 0.0103, 0.0080,  ..., 0.0000, 0.0039, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(950587., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-336.3861, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17071.0527, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-396.8969, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(154.6698, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(892.3574, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9905],
        [-0.7510],
        [-0.5235],
        ...,
        [-0.0669],
        [-0.1733],
        [-0.3287]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-308597.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [0.9997],
        [0.9997],
        ...,
        [0.9997],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097461.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2637],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1714.7555, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9996],
        [0.9996],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097352.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2637],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1714.7555, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0001,  ..., -0.0001,  0.0000,  0.0000],
        [-0.0021, -0.0034,  0.0074,  ...,  0.0007, -0.0099, -0.0045],
        [ 0.0000,  0.0000, -0.0001,  ..., -0.0001,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ..., -0.0001,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ..., -0.0001,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ..., -0.0001,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-3098.9146, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-52.5744, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-237.2478, device='cuda:0')



h[100].sum tensor(-178.0940, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(243.7218, device='cuda:0')



h[200].sum tensor(-234.2962, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-50.4487, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0378,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0115,  ..., 0.0011, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0074,  ..., 0.0007, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(166740.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0154, 0.0114,  ..., 0.0000, 0.0045, 0.0000],
        [0.0000, 0.0120, 0.0085,  ..., 0.0000, 0.0032, 0.0000],
        [0.0000, 0.0152, 0.0112,  ..., 0.0000, 0.0047, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1088284.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-413.9818, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20034.3242, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-452.7683, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(269.0453, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1020.3219, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4250],
        [-0.5887],
        [-0.8576],
        ...,
        [ 0.0032],
        [ 0.0032],
        [ 0.0032]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-300096.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9996],
        [0.9996],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097352.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1718.7347, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9995],
        [0.9995],
        ...,
        [0.9995],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097243.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1718.7347, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ..., -0.0002,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ..., -0.0002,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-3086.1230, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-52.3570, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-237.7984, device='cuda:0')



h[100].sum tensor(-177.7537, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(244.2874, device='cuda:0')



h[200].sum tensor(-233.9012, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-50.5657, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(169668.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1093666.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-443.6822, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20547.7539, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-453.0319, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(425.8519, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1022.5554, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0045],
        [-0.0098],
        [-0.0384],
        ...,
        [ 0.0038],
        [ 0.0038],
        [ 0.0038]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-257898.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9995],
        [0.9995],
        ...,
        [0.9995],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097243.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1779.0912, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9994],
        [0.9994],
        ...,
        [0.9994],
        [0.9994],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097134.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1779.0912, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ..., -0.0002,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ..., -0.0002,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-3195.9790, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-54.2955, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-246.1491, device='cuda:0')



h[100].sum tensor(-184.7483, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(252.8660, device='cuda:0')



h[200].sum tensor(-243.1601, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-52.3414, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(178751.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0012, 0.0001,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1159723.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-483.8617, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22140.6836, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-471.0611, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(488.4416, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1065.3927, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1109],
        [-0.0579],
        [-0.0349],
        ...,
        [ 0.0048],
        [ 0.0048],
        [ 0.0047]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-245392.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9994],
        [0.9994],
        ...,
        [0.9994],
        [0.9994],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097134.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5615],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1370.9707, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9993],
        [0.9993],
        ...,
        [0.9993],
        [0.9993],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097026., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5615],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1370.9707, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ..., -0.0002,  0.0000,  0.0000],
        [-0.0078, -0.0128,  0.0286,  ...,  0.0027, -0.0378, -0.0172],
        [-0.0035, -0.0057,  0.0125,  ...,  0.0011, -0.0167, -0.0076],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ..., -0.0002,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2460.4666, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-41.5718, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-189.6829, device='cuda:0')



h[100].sum tensor(-141.7726, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(194.8589, device='cuda:0')



h[200].sum tensor(-186.6389, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-40.3344, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0705,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0687,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.1010,  ..., 0.0093, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(140051.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0476, 0.0363,  ..., 0.0000, 0.0143, 0.0000],
        [0.0000, 0.0538, 0.0414,  ..., 0.0000, 0.0165, 0.0000],
        [0.0000, 0.0582, 0.0450,  ..., 0.0000, 0.0179, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(888194.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-418.5419, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17532.0723, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-357.3471, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(667.9811, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(809.5671, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8551],
        [-1.8959],
        [-1.8355],
        ...,
        [ 0.0060],
        [ 0.0059],
        [ 0.0058]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-139396.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9993],
        [0.9993],
        ...,
        [0.9993],
        [0.9993],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097026., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2593],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1649.0205, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9992],
        [0.9992],
        ...,
        [0.9992],
        [0.9992],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096917.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2593],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1649.0205, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0003,  ..., -0.0003,  0.0000,  0.0000],
        [-0.0020, -0.0033,  0.0072,  ...,  0.0005, -0.0097, -0.0044],
        [ 0.0000,  0.0000, -0.0003,  ..., -0.0003,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ..., -0.0003,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ..., -0.0003,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ..., -0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2942.4019, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-50.0310, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-228.1530, device='cuda:0')



h[100].sum tensor(-171.0065, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(234.3787, device='cuda:0')



h[200].sum tensor(-225.1753, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-48.5147, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0259,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0058,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0072,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(167496.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0061, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0026, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0032, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1058410.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-494.0972, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20919.2715, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-428.0139, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(703.2532, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(971.6108, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3595],
        [-0.3825],
        [-0.4289],
        ...,
        [ 0.0065],
        [ 0.0065],
        [ 0.0066]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-158876.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9992],
        [0.9992],
        ...,
        [0.9992],
        [0.9992],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096917.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3391],
        [0.3093],
        [0.3354],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1398.6345, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9991],
        ...,
        [0.9991],
        [0.9991],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096808.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3391],
        [0.3093],
        [0.3354],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1398.6345, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0043, -0.0070,  0.0156,  ...,  0.0013, -0.0207, -0.0094],
        [-0.0091, -0.0149,  0.0336,  ...,  0.0031, -0.0444, -0.0202],
        [-0.0086, -0.0141,  0.0317,  ...,  0.0029, -0.0420, -0.0191],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ..., -0.0003,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ..., -0.0003,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ..., -0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2481.5356, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-42.0327, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-193.5104, device='cuda:0')



h[100].sum tensor(-143.9940, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(198.7908, device='cuda:0')



h[200].sum tensor(-189.6492, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-41.1483, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1054,  ..., 0.0095, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.1181,  ..., 0.0108, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.1359,  ..., 0.0126, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(146519.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0629, 0.0476,  ..., 0.0000, 0.0169, 0.0000],
        [0.0000, 0.0747, 0.0573,  ..., 0.0000, 0.0215, 0.0000],
        [0.0000, 0.0788, 0.0607,  ..., 0.0000, 0.0232, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(936639.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-462.5538, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18845.2109, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-366.6800, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(918.1555, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(832.6368, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3049],
        [-1.4438],
        [-1.4430],
        ...,
        [ 0.0088],
        [ 0.0080],
        [ 0.0073]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-122570.2734, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9991],
        ...,
        [0.9991],
        [0.9991],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096808.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1335.2832, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9990],
        [0.9990],
        ...,
        [0.9990],
        [0.9990],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096700., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1335.2832, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0078, -0.0127,  0.0286,  ...,  0.0026, -0.0378, -0.0172],
        [-0.0080, -0.0132,  0.0296,  ...,  0.0027, -0.0392, -0.0178],
        [-0.0077, -0.0126,  0.0284,  ...,  0.0026, -0.0376, -0.0171],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ..., -0.0003,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ..., -0.0003,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ..., -0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2370.3015, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-40.1435, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-184.7453, device='cuda:0')



h[100].sum tensor(-137.8350, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(189.7866, device='cuda:0')



h[200].sum tensor(-181.5786, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-39.2845, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1225,  ..., 0.0111, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.1033,  ..., 0.0092, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.1060,  ..., 0.0095, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(139210.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0705, 0.0533,  ..., 0.0000, 0.0189, 0.0000],
        [0.0000, 0.0665, 0.0500,  ..., 0.0000, 0.0173, 0.0000],
        [0.0000, 0.0619, 0.0462,  ..., 0.0000, 0.0156, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(867790.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-459.3006, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17770.4727, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-342.2637, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1048.2303, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(778.5930, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9974],
        [-0.9789],
        [-0.8721],
        ...,
        [ 0.0075],
        [ 0.0075],
        [ 0.0075]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-86549.8047, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9990],
        [0.9990],
        ...,
        [0.9990],
        [0.9990],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096700., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 10.0 event: 150 loss: tensor(0.0908, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3284],
        [0.2822],
        [0.2483]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1456.2028, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9989],
        [0.9989],
        ...,
        [0.9989],
        [0.9989],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096591.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3284],
        [0.2822],
        [0.2483]], device='cuda:0') 
g.ndata[nfet].sum tensor(1456.2028, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0003,  ..., -0.0003,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ..., -0.0003,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ..., -0.0003,  0.0000,  0.0000],
        ...,
        [-0.0049, -0.0081,  0.0181,  ...,  0.0015, -0.0241, -0.0110],
        [-0.0044, -0.0072,  0.0162,  ...,  0.0013, -0.0216, -0.0098],
        [-0.0044, -0.0072,  0.0160,  ...,  0.0013, -0.0214, -0.0097]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2558.7500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-43.5628, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-201.4753, device='cuda:0')



h[100].sum tensor(-149.9168, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(206.9731, device='cuda:0')



h[200].sum tensor(-197.5396, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-42.8419, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0270,  ..., 0.0024, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0549,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0601,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0575,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(150746.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 4.7460e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.0435e-03, 6.1134e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.2030e-02, 6.5665e-03,  ..., 0.0000e+00, 1.2745e-03,
         0.0000e+00],
        ...,
        [0.0000e+00, 2.2935e-02, 1.3926e-02,  ..., 0.0000e+00, 1.0019e-03,
         0.0000e+00],
        [0.0000e+00, 2.6984e-02, 1.7217e-02,  ..., 0.0000e+00, 2.1785e-03,
         0.0000e+00],
        [0.0000e+00, 2.7986e-02, 1.8023e-02,  ..., 0.0000e+00, 2.4397e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(936010.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-496.0192, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19221.8477, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-370.0397, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1090.7363, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(843.4789, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0095],
        [-0.0695],
        [-0.1539],
        ...,
        [-0.2818],
        [-0.3406],
        [-0.3759]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-86171.4531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9989],
        [0.9989],
        ...,
        [0.9989],
        [0.9989],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096591.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1593.0643, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9988],
        [0.9988],
        [0.9988],
        ...,
        [0.9988],
        [0.9988],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096482.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1593.0643, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0040, -0.0066,  0.0147,  ...,  0.0012, -0.0197, -0.0089],
        [-0.0016, -0.0026,  0.0057,  ...,  0.0002, -0.0079, -0.0036],
        [-0.0024, -0.0040,  0.0087,  ...,  0.0006, -0.0119, -0.0054],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ..., -0.0004,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ..., -0.0004,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ..., -0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2780.1807, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-47.6149, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-220.4111, device='cuda:0')



h[100].sum tensor(-164.2370, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(226.4256, device='cuda:0')



h[200].sum tensor(-216.4580, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-46.8685, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0239,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0557,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0242,  ..., 0.0018, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(164246.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0133, 0.0058,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0192, 0.0105,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0118, 0.0051,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1025320.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-536.4789, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21084.6250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-403.8215, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1025.6350, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(922.1768, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3379],
        [-0.2893],
        [-0.2216],
        ...,
        [ 0.0085],
        [ 0.0085],
        [ 0.0085]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-95889.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9988],
        [0.9988],
        [0.9988],
        ...,
        [0.9988],
        [0.9988],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096482.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3066],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1456.1750, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9987],
        [0.9987],
        [0.9987],
        ...,
        [0.9987],
        [0.9987],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096373.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3066],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1456.1750, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ..., -0.0004,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ..., -0.0004,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ..., -0.0004,  0.0000,  0.0000],
        ...,
        [-0.0025, -0.0041,  0.0090,  ...,  0.0006, -0.0122, -0.0055],
        [-0.0023, -0.0038,  0.0084,  ...,  0.0005, -0.0115, -0.0052],
        [-0.0046, -0.0076,  0.0170,  ...,  0.0014, -0.0228, -0.0103]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2542.4609, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-43.4990, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-201.4715, device='cuda:0')



h[100].sum tensor(-150.3851, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(206.9692, device='cuda:0')



h[200].sum tensor(-198.2469, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-42.8411, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0079,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0507,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0398,  ..., 0.0026, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0354,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(154757.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.0241e-03, 6.4462e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 5.7879e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 2.4805e-02, 1.4686e-02,  ..., 0.0000e+00, 2.5144e-04,
         0.0000e+00],
        [0.0000e+00, 2.3893e-02, 1.3936e-02,  ..., 0.0000e+00, 1.5009e-04,
         0.0000e+00],
        [0.0000e+00, 2.3220e-02, 1.3385e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(960081.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-524.9789, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20002.2656, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-374.5645, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1149.6582, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(856.7253, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0654],
        [-0.0130],
        [ 0.0189],
        ...,
        [-0.4469],
        [-0.4317],
        [-0.4123]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-66993.5469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9987],
        [0.9987],
        [0.9987],
        ...,
        [0.9987],
        [0.9987],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096373.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2764],
        [0.3259],
        [0.4558],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1543.3264, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9986],
        [0.9986],
        [0.9986],
        ...,
        [0.9986],
        [0.9986],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096265.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2764],
        [0.3259],
        [0.4558],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1543.3264, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0047, -0.0078,  0.0174,  ...,  0.0014, -0.0233, -0.0105],
        [-0.0056, -0.0091,  0.0206,  ...,  0.0017, -0.0273, -0.0124],
        [-0.0025, -0.0041,  0.0089,  ...,  0.0005, -0.0122, -0.0055],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ..., -0.0004,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ..., -0.0004,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ..., -0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2666.6953, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-45.8632, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-213.5295, device='cuda:0')



h[100].sum tensor(-158.9243, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(219.3562, device='cuda:0')



h[200].sum tensor(-209.5515, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-45.4052, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0657,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0578,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0612,  ..., 0.0047, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(160840.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0313, 0.0196,  ..., 0.0000, 0.0014, 0.0000],
        [0.0000, 0.0257, 0.0151,  ..., 0.0000, 0.0005, 0.0000],
        [0.0000, 0.0231, 0.0130,  ..., 0.0000, 0.0003, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(998523.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-545.7329, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20756.5000, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-390.3650, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1315.2188, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(891.6702, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4796],
        [-0.4020],
        [-0.3360],
        ...,
        [ 0.0093],
        [ 0.0093],
        [ 0.0093]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-76480.7656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9986],
        [0.9986],
        [0.9986],
        ...,
        [0.9986],
        [0.9986],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096265.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2942],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.2856]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1889.9858, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9985],
        [0.9985],
        [0.9985],
        ...,
        [0.9985],
        [0.9985],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096156.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2942],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.2856]], device='cuda:0') 
g.ndata[nfet].sum tensor(1889.9858, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0092, -0.0151,  0.0342,  ...,  0.0030, -0.0452, -0.0205],
        [-0.0017, -0.0027,  0.0059,  ...,  0.0002, -0.0082, -0.0037],
        [-0.0022, -0.0037,  0.0080,  ...,  0.0004, -0.0110, -0.0050],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ..., -0.0004,  0.0000,  0.0000],
        [-0.0039, -0.0064,  0.0143,  ...,  0.0011, -0.0192, -0.0087],
        [-0.0017, -0.0028,  0.0061,  ...,  0.0003, -0.0085, -0.0038]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-3215.8142, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-55.9401, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-261.4921, device='cuda:0')



h[100].sum tensor(-194.2908, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(268.6276, device='cuda:0')



h[200].sum tensor(-256.2430, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-55.6040, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0652,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0854,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0236,  ..., 0.0016, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0243,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0276,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0450,  ..., 0.0030, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(195922.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0453, 0.0306,  ..., 0.0000, 0.0053, 0.0000],
        [0.0000, 0.0398, 0.0262,  ..., 0.0000, 0.0040, 0.0000],
        [0.0000, 0.0202, 0.0113,  ..., 0.0000, 0.0008, 0.0000],
        ...,
        [0.0000, 0.0110, 0.0042,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0143, 0.0058,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0154, 0.0066,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1229027.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-631.0447, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(25315.6680, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-481.6058, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1123.2920, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1103.3320, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0047],
        [-0.8888],
        [-0.7197],
        ...,
        [-0.1858],
        [-0.2130],
        [-0.1911]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-103576.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9985],
        [0.9985],
        [0.9985],
        ...,
        [0.9985],
        [0.9985],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096156.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1488.8586, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9984],
        [0.9984],
        [0.9984],
        ...,
        [0.9984],
        [0.9984],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096047.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1488.8586, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ..., -0.0004,  0.0000,  0.0000],
        [-0.0017, -0.0027,  0.0059,  ...,  0.0002, -0.0082, -0.0037],
        [-0.0017, -0.0027,  0.0059,  ...,  0.0002, -0.0082, -0.0037],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ..., -0.0004,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ..., -0.0004,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ..., -0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2541.9355, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-43.9084, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-205.9935, device='cuda:0')



h[100].sum tensor(-152.8563, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(211.6146, device='cuda:0')



h[200].sum tensor(-201.6426, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-43.8027, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0241,  ..., 0.0009, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0106,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0106,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(157879.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0007,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(966364.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-554.1821, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20374.5000, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-377.4723, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1420.1852, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(864.3640, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1143],
        [-0.1741],
        [-0.2864],
        ...,
        [ 0.0103],
        [ 0.0102],
        [ 0.0104]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-52658.2422, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9984],
        [0.9984],
        [0.9984],
        ...,
        [0.9984],
        [0.9984],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096047.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3247],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1485.5027, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9984],
        [0.9984],
        [0.9984],
        ...,
        [0.9984],
        [0.9984],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096047.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3247],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1485.5027, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0100, -0.0164,  0.0373,  ...,  0.0033, -0.0492, -0.0223],
        [-0.0033, -0.0055,  0.0122,  ...,  0.0008, -0.0164, -0.0074],
        [-0.0124, -0.0205,  0.0467,  ...,  0.0043, -0.0615, -0.0278],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ..., -0.0004,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ..., -0.0004,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ..., -0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2528.1987, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-43.6588, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-205.5292, device='cuda:0')



h[100].sum tensor(-151.9875, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(211.1376, device='cuda:0')



h[200].sum tensor(-200.4966, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-43.7040, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0726,  ..., 0.0057, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.1581,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.1468,  ..., 0.0131, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(154950.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0565, 0.0394,  ..., 0.0000, 0.0080, 0.0000],
        [0.0000, 0.0855, 0.0628,  ..., 0.0000, 0.0179, 0.0000],
        [0.0000, 0.0956, 0.0709,  ..., 0.0000, 0.0213, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(952936.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-548.6066, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20177.9492, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-367.8015, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1383.2542, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(844.1787, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1367],
        [-1.4104],
        [-1.5920],
        ...,
        [ 0.0100],
        [ 0.0099],
        [ 0.0099]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-46082.0391, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9984],
        [0.9984],
        [0.9984],
        ...,
        [0.9984],
        [0.9984],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096047.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1342.4327, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9983],
        [0.9983],
        [0.9983],
        ...,
        [0.9983],
        [0.9983],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095939.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1342.4327, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ..., -0.0004,  0.0000,  0.0000],
        [-0.0015, -0.0026,  0.0055,  ...,  0.0002, -0.0077, -0.0035],
        [-0.0053, -0.0087,  0.0196,  ...,  0.0016, -0.0261, -0.0118],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ..., -0.0004,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ..., -0.0004,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ..., -0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2282.2349, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-39.3251, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-185.7345, device='cuda:0')



h[100].sum tensor(-137.2193, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(190.8028, device='cuda:0')



h[200].sum tensor(-181.0563, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-39.4948, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0098,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0342,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0520,  ..., 0.0036, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(145808.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0065, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0167, 0.0079,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0252, 0.0139,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(902219.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-533.6306, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19259.9336, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-342.2972, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1512.2881, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(784.6706, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1827],
        [-0.2887],
        [-0.3651],
        ...,
        [ 0.0103],
        [ 0.0102],
        [ 0.0102]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34504.4570, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9983],
        [0.9983],
        [0.9983],
        ...,
        [0.9983],
        [0.9983],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095939.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1387.2144, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9982],
        [0.9982],
        [0.9982],
        ...,
        [0.9982],
        [0.9982],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095830.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1387.2144, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ..., -0.0004,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ..., -0.0004,  0.0000,  0.0000],
        [-0.0018, -0.0030,  0.0064,  ...,  0.0002, -0.0089, -0.0040],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ..., -0.0004,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ..., -0.0004,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ..., -0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2345.4688, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-40.6282, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-191.9303, device='cuda:0')



h[100].sum tensor(-142.0974, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(197.1677, device='cuda:0')



h[200].sum tensor(-187.5358, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-40.8123, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0116,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0160,  ..., 0.0004, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(150248.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0001,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(926505.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-549.7635, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19825.1953, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-352.2257, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1492.8909, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(808.8229, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0162],
        [-0.0711],
        [-0.1315],
        ...,
        [ 0.0138],
        [ 0.0004],
        [-0.0456]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-31909.6035, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9982],
        [0.9982],
        [0.9982],
        ...,
        [0.9982],
        [0.9982],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095830.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6367],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1321.8168, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9981],
        [0.9981],
        [0.9981],
        ...,
        [0.9981],
        [0.9981],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095721.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6367],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1321.8168, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0020, -0.0033,  0.0072,  ...,  0.0003, -0.0100, -0.0045],
        [-0.0084, -0.0140,  0.0319,  ...,  0.0028, -0.0421, -0.0190],
        [-0.0034, -0.0056,  0.0125,  ...,  0.0008, -0.0169, -0.0076],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ..., -0.0004,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ..., -0.0004,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ..., -0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2224.4709, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-38.5527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-182.8821, device='cuda:0')



h[100].sum tensor(-135.1541, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(187.8726, device='cuda:0')



h[200].sum tensor(-178.4132, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-38.8883, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1082,  ..., 0.0091, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0732,  ..., 0.0057, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0866,  ..., 0.0070, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(146101.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0494, 0.0329,  ..., 0.0000, 0.0046, 0.0000],
        [0.0000, 0.0525, 0.0354,  ..., 0.0000, 0.0050, 0.0000],
        [0.0000, 0.0566, 0.0386,  ..., 0.0000, 0.0061, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(908868.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-546.5624, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19596.8496, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-338.4596, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1501.8904, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(779.1030, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2269],
        [-1.4383],
        [-1.5680],
        ...,
        [ 0.0115],
        [ 0.0114],
        [ 0.0113]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-22455.3828, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9981],
        [0.9981],
        [0.9981],
        ...,
        [0.9981],
        [0.9981],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095721.7500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 20.0 event: 300 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4812],
        [0.3899],
        [0.2646],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1453.2106, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9980],
        [0.9980],
        [0.9980],
        ...,
        [0.9980],
        [0.9980],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095613.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4812],
        [0.3899],
        [0.2646],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1453.2106, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0110, -0.0182,  0.0418,  ...,  0.0037, -0.0549, -0.0248],
        [-0.0101, -0.0168,  0.0384,  ...,  0.0034, -0.0506, -0.0228],
        [-0.0081, -0.0134,  0.0306,  ...,  0.0026, -0.0405, -0.0182],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ..., -0.0004,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ..., -0.0004,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ..., -0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2419.3022, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-42.3175, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-201.0614, device='cuda:0')



h[100].sum tensor(-148.7008, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(206.5479, device='cuda:0')



h[200].sum tensor(-196.3409, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-42.7539, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1651,  ..., 0.0147, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.1536,  ..., 0.0136, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.1297,  ..., 0.0112, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(155405.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1017, 0.0746,  ..., 0.0000, 0.0205, 0.0000],
        [0.0000, 0.0963, 0.0702,  ..., 0.0000, 0.0187, 0.0000],
        [0.0000, 0.0867, 0.0625,  ..., 0.0000, 0.0155, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(945069.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-571.3653, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20307.2578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-362.7199, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1600.6804, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(834.5403, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.2615],
        [-3.2198],
        [-3.1065],
        ...,
        [ 0.0110],
        [ 0.0110],
        [ 0.0109]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24274.5508, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9980],
        [0.9980],
        [0.9980],
        ...,
        [0.9980],
        [0.9980],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095613.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1412.7876, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9979],
        [0.9979],
        [0.9979],
        ...,
        [0.9979],
        [0.9979],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095504.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1412.7876, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [-0.0042, -0.0069,  0.0156,  ...,  0.0011, -0.0209, -0.0094],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2341.3960, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-41.0437, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-195.4686, device='cuda:0')



h[100].sum tensor(-144.5642, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(200.8024, device='cuda:0')



h[200].sum tensor(-190.9229, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-41.5647, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0213,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0193,  ..., 0.0007, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(156519.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0049, 0.0010,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0006,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(980894.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-577.9379, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21086.5703, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-363.4806, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1582.7832, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(837.4142, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0223],
        [-0.0889],
        [-0.1589],
        ...,
        [ 0.0112],
        [ 0.0112],
        [ 0.0111]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-27347.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9979],
        [0.9979],
        [0.9979],
        ...,
        [0.9979],
        [0.9979],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095504.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1312.7942, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9978],
        [0.9978],
        [0.9978],
        ...,
        [0.9978],
        [0.9978],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095395.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1312.7942, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2170.0112, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-38.0097, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-181.6338, device='cuda:0')



h[100].sum tensor(-134.1943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(186.5902, device='cuda:0')



h[200].sum tensor(-177.2684, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-38.6228, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(146096.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(897181.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-560.5374, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19577.6152, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-333.8629, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1571.4347, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(771.2263, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0395],
        [0.0347],
        [0.0131],
        ...,
        [0.0114],
        [0.0113],
        [0.0113]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-6205.8115, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9978],
        [0.9978],
        [0.9978],
        ...,
        [0.9978],
        [0.9978],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095395.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1619.2588, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9977],
        [0.9977],
        [0.9977],
        ...,
        [0.9977],
        [0.9977],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095287.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1619.2588, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2612.8264, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-46.5189, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-224.0352, device='cuda:0')



h[100].sum tensor(-164.6251, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(230.1486, device='cuda:0')



h[200].sum tensor(-217.5171, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-47.6391, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(171397.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.8725e-03, 1.5979e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1053470.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-619.0091, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22596.6875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-401.4901, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1541.0178, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(925.9688, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0357],
        [-0.0113],
        [-0.0459],
        ...,
        [ 0.0134],
        [ 0.0135],
        [ 0.0136]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-30811.3398, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9977],
        [0.9977],
        [0.9977],
        ...,
        [0.9977],
        [0.9977],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095287.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2998],
        [0.4436],
        [0.4502],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1800.2104, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9976],
        [0.9976],
        [0.9976],
        ...,
        [0.9976],
        [0.9976],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095178.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2998],
        [0.4436],
        [0.4502],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1800.2104, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0053, -0.0088,  0.0200,  ...,  0.0016, -0.0266, -0.0120],
        [-0.0107, -0.0178,  0.0410,  ...,  0.0036, -0.0538, -0.0242],
        [-0.0135, -0.0225,  0.0520,  ...,  0.0047, -0.0682, -0.0306],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2880.0312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-51.7984, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-249.0711, device='cuda:0')



h[100].sum tensor(-183.7443, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(255.8677, device='cuda:0')



h[200].sum tensor(-242.8352, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-52.9628, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0982,  ..., 0.0080, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.1379,  ..., 0.0119, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.1745,  ..., 0.0155, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(190552.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0642, 0.0438,  ..., 0.0000, 0.0065, 0.0000],
        [0.0000, 0.0843, 0.0599,  ..., 0.0000, 0.0129, 0.0000],
        [0.0000, 0.1012, 0.0733,  ..., 0.0000, 0.0182, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1183587., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-665.1210, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(25182., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-451.1983, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1429.7510, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1041.7279, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2389],
        [-2.5911],
        [-2.8787],
        ...,
        [ 0.0118],
        [ 0.0117],
        [ 0.0116]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-45170.9336, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9976],
        [0.9976],
        [0.9976],
        ...,
        [0.9976],
        [0.9976],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095178.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2812],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1397.5367, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9975],
        [0.9975],
        [0.9975],
        ...,
        [0.9975],
        [0.9975],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095069.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2812],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1397.5367, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [-0.0021, -0.0034,  0.0076,  ...,  0.0003, -0.0104, -0.0047],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2249.3069, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-40.0392, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-193.3585, device='cuda:0')



h[100].sum tensor(-142.3692, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(198.6348, device='cuda:0')



h[200].sum tensor(-188.1976, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-41.1160, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0331,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0061,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0188,  ..., 0.0006, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(154964.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 6.7834e-03, 5.8344e-06,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.8420e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 6.4320e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(961014.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-589.6578, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20932.1367, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-353.9669, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1638.2860, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(819.1276, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1697],
        [-0.1792],
        [-0.1807],
        ...,
        [ 0.0119],
        [ 0.0118],
        [ 0.0118]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-9074.4297, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9975],
        [0.9975],
        [0.9975],
        ...,
        [0.9975],
        [0.9975],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095069.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1149.7751, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9975],
        [0.9975],
        [0.9975],
        ...,
        [0.9975],
        [0.9975],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095069.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1149.7751, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [-0.0016, -0.0027,  0.0059,  ...,  0.0002, -0.0083, -0.0037],
        [-0.0016, -0.0027,  0.0059,  ...,  0.0002, -0.0083, -0.0037],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1872.9673, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-32.8718, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-159.0790, device='cuda:0')



h[100].sum tensor(-116.8839, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(163.4199, device='cuda:0')



h[200].sum tensor(-154.5086, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-33.8268, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0162,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0213,  ..., 0.0008, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0157,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(130202.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0035, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(788255.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-532.9110, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17490.5781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-290.1538, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1922.1193, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(668.7237, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0290],
        [-0.0490],
        [-0.0486],
        ...,
        [ 0.0119],
        [ 0.0118],
        [ 0.0118]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(10030.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9975],
        [0.9975],
        [0.9975],
        ...,
        [0.9975],
        [0.9975],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095069.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4390],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1494.5598, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9974],
        [0.9974],
        [0.9974],
        ...,
        [0.9974],
        [0.9974],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1094961., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4390],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1494.5598, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [-0.0032, -0.0054,  0.0121,  ...,  0.0008, -0.0163, -0.0073],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2375.0703, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-42.6411, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-206.7823, device='cuda:0')



h[100].sum tensor(-151.9829, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(212.4249, device='cuda:0')



h[200].sum tensor(-200.9524, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-43.9704, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0196,  ..., 0.0011, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0097,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0435,  ..., 0.0025, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(162643.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0032, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(996927.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-607.6119, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21558.1738, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-376.5494, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1771.2715, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(868.0063, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0314],
        [-0.0304],
        [-0.0397],
        ...,
        [ 0.0121],
        [ 0.0120],
        [ 0.0120]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-17749.4648, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9974],
        [0.9974],
        [0.9974],
        ...,
        [0.9974],
        [0.9974],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1094961., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1396.1572, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9973],
        [0.9973],
        [0.9973],
        ...,
        [0.9973],
        [0.9973],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1094852.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1396.1572, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0016, -0.0027,  0.0059,  ...,  0.0001, -0.0083, -0.0037],
        [-0.0016, -0.0027,  0.0059,  ...,  0.0001, -0.0083, -0.0037],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2206.0154, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-39.5983, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-193.1676, device='cuda:0')



h[100].sum tensor(-141.4756, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(198.4387, device='cuda:0')



h[200].sum tensor(-187.1029, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-41.0754, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0106,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0106,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0259,  ..., 0.0012, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(150507.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(905705.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-586.0251, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19963.2891, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-340.4146, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1662.9543, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(789.2640, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0374],
        [ 0.0192],
        [-0.0191],
        ...,
        [ 0.0121],
        [ 0.0121],
        [ 0.0120]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(8320.1553, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9973],
        [0.9973],
        [0.9973],
        ...,
        [0.9973],
        [0.9973],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1094852.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2974],
        [0.2874],
        [0.4065],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1443.0061, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9972],
        [0.9972],
        [0.9972],
        ...,
        [0.9972],
        [0.9972],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1094743.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2974],
        [0.2874],
        [0.4065],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1443.0061, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0036, -0.0060,  0.0136,  ...,  0.0009, -0.0182, -0.0082],
        [-0.0070, -0.0117,  0.0270,  ...,  0.0022, -0.0356, -0.0160],
        [-0.0093, -0.0156,  0.0363,  ...,  0.0031, -0.0476, -0.0213],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2257.6665, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-40.7990, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-199.6495, device='cuda:0')



h[100].sum tensor(-146.1155, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(205.0975, device='cuda:0')



h[200].sum tensor(-193.2840, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-42.4537, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1018,  ..., 0.0083, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0970,  ..., 0.0078, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.1130,  ..., 0.0094, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(156789.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0519, 0.0335,  ..., 0.0000, 0.0016, 0.0000],
        [0.0000, 0.0513, 0.0330,  ..., 0.0000, 0.0013, 0.0000],
        [0.0000, 0.0502, 0.0322,  ..., 0.0000, 0.0010, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(953289.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-599.2103, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20754.5352, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-359.9002, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1884.0525, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(829.7250, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7999],
        [-0.7993],
        [-0.7273],
        ...,
        [ 0.0341],
        [ 0.0302],
        [ 0.0151]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-9260.9980, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9972],
        [0.9972],
        [0.9972],
        ...,
        [0.9972],
        [0.9972],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1094743.6250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 30.0 event: 450 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6274],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1336.2196, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9971],
        [0.9971],
        [0.9971],
        ...,
        [0.9971],
        [0.9971],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1094635., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6274],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1336.2196, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0036, -0.0060,  0.0137,  ...,  0.0009, -0.0184, -0.0082],
        [-0.0062, -0.0104,  0.0240,  ...,  0.0019, -0.0317, -0.0142],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2098.3689, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-37.9080, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-184.8749, device='cuda:0')



h[100].sum tensor(-136.0886, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(189.9197, device='cuda:0')



h[200].sum tensor(-180.0621, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-39.3120, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0943,  ..., 0.0075, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0499,  ..., 0.0036, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0343,  ..., 0.0025, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(147743.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0312, 0.0171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0236, 0.0112,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0140, 0.0062,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(892898.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-580.5519, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19632.4102, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-334.9526, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1924.1216, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(772.0892, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3893],
        [-0.3749],
        [-0.3128],
        ...,
        [ 0.0123],
        [ 0.0123],
        [ 0.0122]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(1896.3218, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9971],
        [0.9971],
        [0.9971],
        ...,
        [0.9971],
        [0.9971],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1094635., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2769],
        [0.3801],
        [0.2729],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1503.1597, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9971],
        [0.9971],
        [0.9971],
        ...,
        [0.9971],
        [0.9971],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1094635., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2769],
        [0.3801],
        [0.2729],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1503.1597, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0047, -0.0080,  0.0182,  ...,  0.0014, -0.0243, -0.0109],
        [-0.0082, -0.0137,  0.0318,  ...,  0.0027, -0.0419, -0.0187],
        [-0.0089, -0.0150,  0.0349,  ...,  0.0030, -0.0458, -0.0205],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2330.8218, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-42.4356, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-207.9721, device='cuda:0')



h[100].sum tensor(-152.3428, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(213.6472, device='cuda:0')



h[200].sum tensor(-201.5684, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-44.2234, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0734,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.1013,  ..., 0.0082, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.1159,  ..., 0.0096, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(167500.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0310, 0.0170,  ..., 0.0000, 0.0003, 0.0000],
        [0.0000, 0.0420, 0.0257,  ..., 0.0000, 0.0006, 0.0000],
        [0.0000, 0.0475, 0.0300,  ..., 0.0000, 0.0005, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1054705.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-626.2036, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22771.5781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-386.8844, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1768.6753, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(893.8095, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3661],
        [-0.4285],
        [-0.4388],
        ...,
        [ 0.0123],
        [ 0.0123],
        [ 0.0122]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-26800.2461, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9971],
        [0.9971],
        [0.9971],
        ...,
        [0.9971],
        [0.9971],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1094635., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4739],
        [0.5479],
        [0.5757],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1301.2494, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9970],
        [0.9970],
        [0.9970],
        ...,
        [0.9970],
        [0.9970],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1094526.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4739],
        [0.5479],
        [0.5757],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1301.2494, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0112, -0.0188,  0.0439,  ...,  0.0039, -0.0574, -0.0257],
        [-0.0127, -0.0213,  0.0498,  ...,  0.0045, -0.0650, -0.0291],
        [-0.0090, -0.0151,  0.0352,  ...,  0.0030, -0.0462, -0.0206],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [-0.0036, -0.0060,  0.0138,  ...,  0.0009, -0.0184, -0.0082]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2029.2939, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-36.7542, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-180.0365, device='cuda:0')



h[100].sum tensor(-132.2652, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(184.9493, device='cuda:0')



h[200].sum tensor(-175.0439, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-38.2832, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1248,  ..., 0.0105, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.1442,  ..., 0.0124, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.1509,  ..., 0.0131, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0255,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0442,  ..., 0.0030, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(148638.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0538, 0.0351,  ..., 0.0000, 0.0034, 0.0000],
        [0.0000, 0.0638, 0.0429,  ..., 0.0000, 0.0054, 0.0000],
        [0.0000, 0.0656, 0.0443,  ..., 0.0000, 0.0057, 0.0000],
        ...,
        [0.0000, 0.0041, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0142, 0.0061,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0236, 0.0112,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(919175.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-584.4462, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20170.4648, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-336.5271, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1901.9285, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(776.2184, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6099],
        [-0.7237],
        [-0.7487],
        ...,
        [-0.1407],
        [-0.2615],
        [-0.3474]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-2594.7439, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9970],
        [0.9970],
        [0.9970],
        ...,
        [0.9970],
        [0.9970],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1094526.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1323.7964, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9969],
        [0.9969],
        [0.9969],
        ...,
        [0.9969],
        [0.9969],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1094417.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1323.7964, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [-0.0040, -0.0067,  0.0154,  ...,  0.0011, -0.0205, -0.0092],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2037.9028, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-37.1207, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-183.1560, device='cuda:0')



h[100].sum tensor(-133.9074, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(188.1539, device='cuda:0')



h[200].sum tensor(-177.2584, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-38.9465, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0210,  ..., 0.0012, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0239,  ..., 0.0010, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(150261.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0100, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(927999.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-589.4974, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20329.7148, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-341.1678, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1958.3643, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(786.3868, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0736],
        [-0.1827],
        [-0.3122],
        ...,
        [ 0.0126],
        [ 0.0125],
        [ 0.0125]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-3496.6199, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9969],
        [0.9969],
        [0.9969],
        ...,
        [0.9969],
        [0.9969],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1094417.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1213.2301, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9968],
        [0.9968],
        [0.9968],
        ...,
        [0.9968],
        [0.9968],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1094308.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1213.2301, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [-0.0024, -0.0040,  0.0089,  ...,  0.0004, -0.0122, -0.0054],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1870.5436, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-34.0064, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-167.8585, device='cuda:0')



h[100].sum tensor(-122.9704, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(172.4389, device='cuda:0')



h[200].sum tensor(-162.8185, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-35.6936, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0365,  ..., 0.0027, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0318,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0305,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0107,  ..., 0.0006, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(137860.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0266, 0.0134,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0237, 0.0111,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0204, 0.0088,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0050, 0.0010,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(832538.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-563.3864, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18494.3281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-307.3575, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2060.0461, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(708.8617, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6690],
        [-0.6808],
        [-0.6497],
        ...,
        [ 0.0119],
        [-0.0191],
        [-0.0893]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(12318.9297, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9968],
        [0.9968],
        [0.9968],
        ...,
        [0.9968],
        [0.9968],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1094308.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1765.4762, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9967],
        [0.9967],
        [0.9967],
        ...,
        [0.9967],
        [0.9967],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1094200.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1765.4762, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [-0.0036, -0.0060,  0.0138,  ...,  0.0009, -0.0185, -0.0083],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2619.0977, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-49.1213, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-244.2654, device='cuda:0')



h[100].sum tensor(-178.0595, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(250.9308, device='cuda:0')



h[200].sum tensor(-235.8140, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-51.9409, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0138,  ..., 0.0009, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0112,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0499,  ..., 0.0031, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(186602.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0018, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1143412., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-674.8027, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(24681.6934, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-434.9019, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1615.1621, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1006.4128, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0264],
        [-0.0086],
        [-0.0007],
        ...,
        [ 0.0127],
        [ 0.0126],
        [ 0.0126]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-19829.0645, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9967],
        [0.9967],
        [0.9967],
        ...,
        [0.9967],
        [0.9967],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1094200.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6191],
        [0.3860],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1227.8320, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9966],
        [0.9966],
        [0.9966],
        ...,
        [0.9966],
        [0.9966],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1094091.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6191],
        [0.3860],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1227.8320, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0044, -0.0074,  0.0172,  ...,  0.0012, -0.0228, -0.0102],
        [-0.0064, -0.0108,  0.0252,  ...,  0.0020, -0.0332, -0.0148],
        [-0.0160, -0.0271,  0.0640,  ...,  0.0058, -0.0832, -0.0371],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1857.0979, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-34.1179, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-169.8787, device='cuda:0')



h[100].sum tensor(-123.9752, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(174.5143, device='cuda:0')



h[200].sum tensor(-164.2255, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-36.1232, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0581,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.1414,  ..., 0.0121, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.1635,  ..., 0.0143, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(137492.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0448, 0.0276,  ..., 0.0000, 0.0013, 0.0000],
        [0.0000, 0.0682, 0.0461,  ..., 0.0000, 0.0060, 0.0000],
        [0.0000, 0.0846, 0.0590,  ..., 0.0000, 0.0105, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(826784.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-567.3210, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18472.9668, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-303.3749, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2039.4673, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(703.3520, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3629],
        [-1.5605],
        [-1.7222],
        ...,
        [ 0.0127],
        [ 0.0126],
        [ 0.0126]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(20670.9922, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9966],
        [0.9966],
        [0.9966],
        ...,
        [0.9966],
        [0.9966],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1094091.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1411.9010, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9965],
        [0.9965],
        [0.9965],
        ...,
        [0.9965],
        [0.9965],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093982.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1411.9010, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0024, -0.0041,  0.0092,  ...,  0.0004, -0.0125, -0.0056],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2089.3799, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-39.0024, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-195.3459, device='cuda:0')



h[100].sum tensor(-142.0708, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(200.6764, device='cuda:0')



h[200].sum tensor(-188.2400, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-41.5386, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0074,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0091,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(156426.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(962984.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-612.1475, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21206.1758, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-352.5824, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1766.4066, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(819.2352, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0127],
        [-0.0271],
        [-0.0554],
        ...,
        [ 0.0132],
        [ 0.0131],
        [ 0.0130]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(3567.9893, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9965],
        [0.9965],
        [0.9965],
        ...,
        [0.9965],
        [0.9965],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093982.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3721],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1474.9250, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9964],
        [0.9964],
        [0.9964],
        ...,
        [0.9964],
        [0.9964],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093874., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3721],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1474.9250, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0018, -0.0030,  0.0067,  ...,  0.0002, -0.0093, -0.0041],
        [-0.0045, -0.0076,  0.0176,  ...,  0.0013, -0.0233, -0.0104],
        [-0.0062, -0.0106,  0.0248,  ...,  0.0020, -0.0326, -0.0145],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2152.8777, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-40.5219, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-204.0657, device='cuda:0')



h[100].sum tensor(-147.9674, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(209.6342, device='cuda:0')



h[200].sum tensor(-196.0986, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-43.3928, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0463,  ..., 0.0027, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0562,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0991,  ..., 0.0079, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(161590.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0243, 0.0112,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0347, 0.0194,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0503, 0.0317,  ..., 0.0000, 0.0014, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(988283.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-623.6511, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21696.5742, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-366.7784, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1796.7954, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(850.8270, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6701],
        [-0.9059],
        [-1.1443],
        ...,
        [ 0.0134],
        [ 0.0133],
        [ 0.0132]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(2236.2852, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9964],
        [0.9964],
        [0.9964],
        ...,
        [0.9964],
        [0.9964],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093874., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1190.6757, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9963],
        [0.9963],
        [0.9963],
        ...,
        [0.9963],
        [0.9963],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093765.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1190.6757, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0046, -0.0078,  0.0182,  ...,  0.0013, -0.0241, -0.0107],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [-0.0036, -0.0062,  0.0143,  ...,  0.0009, -0.0191, -0.0085],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1753.0250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-32.5934, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-164.7379, device='cuda:0')



h[100].sum tensor(-119.3082, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(169.2332, device='cuda:0')



h[200].sum tensor(-158.1540, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-35.0301, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0147,  ..., 0.0010, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0438,  ..., 0.0029, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0430,  ..., 0.0028, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(134342., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0128, 0.0028,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0234, 0.0106,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0298, 0.0156,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(808295.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-559.2428, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17960.7109, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-298.6895, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2338.9961, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(686.4038, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8466],
        [-0.8689],
        [-0.8902],
        ...,
        [ 0.0129],
        [ 0.0128],
        [ 0.0128]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(9170.6016, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9963],
        [0.9963],
        [0.9963],
        ...,
        [0.9963],
        [0.9963],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093765.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 40.0 event: 600 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1496.4321, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9962],
        [0.9962],
        [0.9962],
        ...,
        [0.9962],
        [0.9962],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093656.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1496.4321, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.5278e-03, -2.5956e-03,  5.7087e-03,  ...,  9.4101e-05,
         -8.0148e-03, -3.5599e-03],
        [ 0.0000e+00,  0.0000e+00, -5.1886e-04,  ..., -5.1886e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -5.1886e-04,  ..., -5.1886e-04,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -5.1886e-04,  ..., -5.1886e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -5.1886e-04,  ..., -5.1886e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -5.1886e-04,  ..., -5.1886e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2146.6094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-40.8847, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-207.0414, device='cuda:0')



h[100].sum tensor(-150.0272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(212.6910, device='cuda:0')



h[200].sum tensor(-198.9215, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-44.0255, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0277,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0160,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(168154.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 9.4437e-03, 8.1849e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 5.2202e-03, 8.1848e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 7.5387e-04, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1053408.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-637.7284, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22815.1934, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-386.8395, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1988.8038, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(893.4224, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0826],
        [-0.0570],
        [-0.0229],
        ...,
        [ 0.0129],
        [ 0.0128],
        [ 0.0128]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-23065.5664, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9962],
        [0.9962],
        [0.9962],
        ...,
        [0.9962],
        [0.9962],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093656.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.7329],
        [0.6099],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1474.6163, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9961],
        [0.9961],
        [0.9961],
        ...,
        [0.9961],
        [0.9961],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093547.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.7329],
        [0.6099],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1474.6163, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0104, -0.0178,  0.0422,  ...,  0.0037, -0.0549, -0.0244],
        [-0.0115, -0.0196,  0.0466,  ...,  0.0041, -0.0605, -0.0269],
        [-0.0126, -0.0215,  0.0512,  ...,  0.0046, -0.0665, -0.0295],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2099.7842, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-40.1687, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-204.0230, device='cuda:0')



h[100].sum tensor(-147.7636, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(209.5903, device='cuda:0')



h[200].sum tensor(-195.9660, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-43.3837, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1189,  ..., 0.0098, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.1643,  ..., 0.0143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.1444,  ..., 0.0123, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(157335.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0653, 0.0435,  ..., 0.0000, 0.0039, 0.0000],
        [0.0000, 0.0732, 0.0497,  ..., 0.0000, 0.0065, 0.0000],
        [0.0000, 0.0670, 0.0449,  ..., 0.0000, 0.0052, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(944314.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-617.9404, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20916.7305, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-353.2447, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1811.1091, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(822.0743, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0382],
        [-1.0540],
        [-0.9955],
        ...,
        [ 0.0129],
        [ 0.0129],
        [ 0.0128]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(14199.6699, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9961],
        [0.9961],
        [0.9961],
        ...,
        [0.9961],
        [0.9961],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093547.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1224.2810, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9960],
        [0.9960],
        [0.9960],
        ...,
        [0.9960],
        [0.9960],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093439.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1224.2810, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1760.0021, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-33.3424, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-169.3874, device='cuda:0')



h[100].sum tensor(-122.9559, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(174.0096, device='cuda:0')



h[200].sum tensor(-163.1040, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-36.0187, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0151,  ..., 0.0010, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0469,  ..., 0.0037, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0218,  ..., 0.0012, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(139784.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0035, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0123, 0.0050,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0285, 0.0147,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0106, 0.0032,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0028, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(846323.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-576.8923, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18907.8555, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-308.3699, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2079.3293, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(714.9125, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2036],
        [-0.3740],
        [-0.5578],
        ...,
        [-0.1970],
        [-0.1021],
        [-0.0245]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(20327.1211, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9960],
        [0.9960],
        [0.9960],
        ...,
        [0.9960],
        [0.9960],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093439.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1338.7021, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9959],
        [0.9959],
        [0.9959],
        ...,
        [0.9959],
        [0.9959],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093330.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1338.7021, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0065, -0.0111,  0.0262,  ...,  0.0021, -0.0343, -0.0152],
        [-0.0034, -0.0057,  0.0133,  ...,  0.0008, -0.0177, -0.0079],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1890.1143, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-36.2821, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-185.2184, device='cuda:0')



h[100].sum tensor(-134.1283, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(190.2725, device='cuda:0')



h[200].sum tensor(-177.9662, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-39.3850, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1116,  ..., 0.0091, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0465,  ..., 0.0036, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0423,  ..., 0.0028, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(148397.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0495, 0.0311,  ..., 0.0000, 0.0019, 0.0000],
        [0.0000, 0.0312, 0.0167,  ..., 0.0000, 0.0003, 0.0000],
        [0.0000, 0.0249, 0.0117,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(896485.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-596.8475, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19891.3086, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-331.4449, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2028.2031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(768.1805, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0907],
        [-0.8642],
        [-0.6864],
        ...,
        [ 0.0130],
        [ 0.0129],
        [ 0.0129]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(12228.4609, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9959],
        [0.9959],
        [0.9959],
        ...,
        [0.9959],
        [0.9959],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093330.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4824],
        [0.3977],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1508.2367, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9958],
        [0.9958],
        [0.9958],
        ...,
        [0.9958],
        [0.9958],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093221.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4824],
        [0.3977],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1508.2367, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0076, -0.0129,  0.0308,  ...,  0.0026, -0.0402, -0.0178],
        [-0.0071, -0.0121,  0.0287,  ...,  0.0024, -0.0375, -0.0166],
        [-0.0104, -0.0178,  0.0426,  ...,  0.0037, -0.0553, -0.0245],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2088.1548, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-40.6965, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-208.6746, device='cuda:0')



h[100].sum tensor(-150.8214, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(214.3688, device='cuda:0')



h[200].sum tensor(-200.1622, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-44.3728, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0758,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.1543,  ..., 0.0133, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.1473,  ..., 0.0126, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(163849.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0473, 0.0292,  ..., 0.0000, 0.0016, 0.0000],
        [0.0000, 0.0685, 0.0459,  ..., 0.0000, 0.0047, 0.0000],
        [0.0000, 0.0718, 0.0485,  ..., 0.0000, 0.0056, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(990457.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-630.9058, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21672.5469, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-374.0764, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2011.0413, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(864.8564, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3561],
        [-1.5161],
        [-1.6210],
        ...,
        [ 0.0152],
        [ 0.0188],
        [ 0.0176]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-2751.7588, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9958],
        [0.9958],
        [0.9958],
        ...,
        [0.9958],
        [0.9958],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093221.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1222.4298, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9957],
        [0.9957],
        [0.9957],
        ...,
        [0.9957],
        [0.9957],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093113., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1222.4298, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0005,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1709.1703, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-32.9099, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-169.1313, device='cuda:0')



h[100].sum tensor(-122.2681, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(173.7465, device='cuda:0')



h[200].sum tensor(-162.3060, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-35.9643, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(140724.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(857785.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-581.5137, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19193.0703, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-309.8515, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2016.7286, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(719.4551, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0047],
        [ 0.0009],
        [ 0.0105],
        ...,
        [ 0.0130],
        [ 0.0130],
        [ 0.0130]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(22357.1133, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9957],
        [0.9957],
        [0.9957],
        ...,
        [0.9957],
        [0.9957],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093113., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1565.6714, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9956],
        [0.9956],
        [0.9956],
        ...,
        [0.9956],
        [0.9956],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093004.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1565.6714, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -5.2579e-04,  ..., -5.2579e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -5.2579e-04,  ..., -5.2579e-04,
          0.0000e+00,  0.0000e+00],
        [-1.4227e-03, -2.4372e-03,  5.3941e-03,  ...,  5.6558e-05,
         -7.5884e-03, -3.3537e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -5.2579e-04,  ..., -5.2579e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -5.2579e-04,  ..., -5.2579e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -5.2579e-04,  ..., -5.2579e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2114.0286, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-41.7900, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-216.6211, device='cuda:0')



h[100].sum tensor(-155.6472, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(222.5322, device='cuda:0')



h[200].sum tensor(-206.6640, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-46.0625, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 9.6591e-03,  ..., 5.6310e-05, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.1096e-02,  ..., 7.5752e-04, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(168275.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0026, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0007,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1022348.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-642.3167, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22398.0684, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-384.1357, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1870.7333, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(889.8513, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1515],
        [-0.2165],
        [-0.3080],
        ...,
        [ 0.0132],
        [ 0.0131],
        [ 0.0130]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(429.8042, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9956],
        [0.9956],
        [0.9956],
        ...,
        [0.9956],
        [0.9956],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093004.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1270.6741, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9955],
        [0.9955],
        [0.9955],
        ...,
        [0.9955],
        [0.9955],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092895.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1270.6741, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0006,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0006,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0006,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [-0.0025, -0.0043,  0.0100,  ...,  0.0005, -0.0135, -0.0060],
        [ 0.0000,  0.0000, -0.0006,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0006,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1767.8862, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-33.8765, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-175.8062, device='cuda:0')



h[100].sum tensor(-126.4889, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(180.6036, device='cuda:0')



h[200].sum tensor(-167.9880, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-37.3836, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0193,  ..., 0.0010, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0099,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(144833.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 7.9605e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 9.1499e-04, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 4.7422e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.8822e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(879978.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-554.7419, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19409.4766, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-340.2280, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1786.6965, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(633.5083, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0270],
        [0.0322],
        [0.0357],
        ...,
        [0.0018],
        [0.0151],
        [0.0204]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(29706.8047, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9955],
        [0.9955],
        [0.9955],
        ...,
        [0.9955],
        [0.9955],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092895.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1392.1897, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9954],
        [0.9954],
        [0.9954],
        ...,
        [0.9954],
        [0.9954],
        [0.9954]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092786.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1392.1897, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0006,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0006,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0006,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0006,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0006,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0006,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1930.1392, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-36.9486, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-192.6187, device='cuda:0')



h[100].sum tensor(-138.3057, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(197.8748, device='cuda:0')



h[200].sum tensor(-183.7251, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-40.9587, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0309,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0421,  ..., 0.0032, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0307,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(155610.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0290, 0.0097,  ..., 0.0000, 0.0003, 0.0000],
        [0.0000, 0.0310, 0.0099,  ..., 0.0000, 0.0003, 0.0000],
        [0.0000, 0.0247, 0.0066,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(949374.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-545.8619, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20557.0781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-386.1608, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1529.3398, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(597.1289, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5301],
        [-1.3666],
        [-1.1685],
        ...,
        [ 0.0075],
        [ 0.0075],
        [ 0.0075]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(29337.8301, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9954],
        [0.9954],
        [0.9954],
        ...,
        [0.9954],
        [0.9954],
        [0.9954]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092786.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1225.6698, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9953],
        [0.9953],
        [0.9953],
        ...,
        [0.9953],
        [0.9953],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092677.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1225.6698, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0007,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0007,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0007,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0007,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0007,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0007,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1744.7068, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-32.5372, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-169.5796, device='cuda:0')



h[100].sum tensor(-122.0990, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(174.2070, device='cuda:0')



h[200].sum tensor(-162.2344, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-36.0596, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(139087.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(818815.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-481.4903, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17697.6055, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-356.8188, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1447.7872, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(406.6062, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0566],
        [0.0673],
        [0.0762],
        ...,
        [0.0060],
        [0.0061],
        [0.0064]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(43005.0898, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9953],
        [0.9953],
        [0.9953],
        ...,
        [0.9953],
        [0.9953],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092677.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 50.0 event: 750 loss: tensor(0.0737, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4792],
        [0.0000],
        [0.7246],
        ...,
        [0.0000],
        [0.2690],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1421.2837, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9952],
        [0.9952],
        [0.9952],
        ...,
        [0.9952],
        [0.9952],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092568.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4792],
        [0.0000],
        [0.7246],
        ...,
        [0.0000],
        [0.2690],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1421.2837, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0064, -0.0111,  0.0264,  ...,  0.0021, -0.0347, -0.0153],
        [-0.0122, -0.0210,  0.0506,  ...,  0.0045, -0.0656, -0.0289],
        [-0.0090, -0.0155,  0.0372,  ...,  0.0032, -0.0486, -0.0214],
        ...,
        [-0.0055, -0.0094,  0.0223,  ...,  0.0017, -0.0295, -0.0130],
        [-0.0019, -0.0033,  0.0073,  ...,  0.0003, -0.0103, -0.0045],
        [-0.0018, -0.0031,  0.0070,  ...,  0.0002, -0.0098, -0.0043]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1991.7631, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-37.6280, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-196.6441, device='cuda:0')



h[100].sum tensor(-141.5586, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(202.0100, device='cuda:0')



h[200].sum tensor(-188.1349, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-41.8146, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1675,  ..., 0.0147, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.1407,  ..., 0.0121, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.1608,  ..., 0.0140, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0369,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0527,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0194,  ..., 0.0010, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(159266.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0921, 0.0442,  ..., 0.0000, 0.0147, 0.0000],
        [0.0000, 0.0887, 0.0419,  ..., 0.0000, 0.0139, 0.0000],
        [0.0000, 0.0877, 0.0414,  ..., 0.0000, 0.0138, 0.0000],
        ...,
        [0.0000, 0.0251, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0206, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(953834.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(45.2980, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-496.2558, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20230.0273, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-426.6997, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1172.0193, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(443.7161, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0124],
        [-1.0565],
        [-1.0642],
        ...,
        [-0.1716],
        [-0.0944],
        [-0.0252]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(44473.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9952],
        [0.9952],
        [0.9952],
        ...,
        [0.9952],
        [0.9952],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092568.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6323],
        [0.4954],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1704.0856, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9951],
        [0.9951],
        [0.9951],
        ...,
        [0.9951],
        [0.9951],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092459.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6323],
        [0.4954],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1704.0856, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0062, -0.0107,  0.0254,  ...,  0.0021, -0.0335, -0.0147],
        [-0.0071, -0.0123,  0.0293,  ...,  0.0024, -0.0385, -0.0169],
        [-0.0033, -0.0058,  0.0134,  ...,  0.0009, -0.0181, -0.0080],
        ...,
        [ 0.0000,  0.0000, -0.0007,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0007,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0007,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2333.1733, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-44.9315, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-235.7716, device='cuda:0')



h[100].sum tensor(-169.4614, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(242.2052, device='cuda:0')



h[200].sum tensor(-225.2718, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-50.1347, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1271,  ..., 0.0107, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.1132,  ..., 0.0094, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.1121,  ..., 0.0093, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(195869.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0623, 0.0214,  ..., 0.0000, 0.0073, 0.0000],
        [0.0000, 0.0640, 0.0223,  ..., 0.0000, 0.0074, 0.0000],
        [0.0000, 0.0634, 0.0217,  ..., 0.0000, 0.0069, 0.0000],
        ...,
        [0.0005, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1243126., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(90.4808, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-546.4017, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(25738.7930, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-542.9611, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(907.2578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(591.3969, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4951],
        [-0.5130],
        [-0.4790],
        ...,
        [ 0.0032],
        [ 0.0032],
        [ 0.0032]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(22692.2656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9951],
        [0.9951],
        [0.9951],
        ...,
        [0.9951],
        [0.9951],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092459.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1432.2850, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9950],
        [0.9950],
        [0.9950],
        ...,
        [0.9950],
        [0.9950],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092350.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1432.2850, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0008,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0008,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0008,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0008,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0008,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0008,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2006.5594, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-37.4998, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-198.1662, device='cuda:0')



h[100].sum tensor(-141.7903, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(203.5737, device='cuda:0')



h[200].sum tensor(-188.5320, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-42.1383, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(163127.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0007, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0007, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0007, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0007, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0007, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0007, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(996509.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(168.9872, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-456.8513, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20650.0273, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-462.2455, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(910.4797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(323.3327, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0034],
        [0.0051],
        [0.0081],
        ...,
        [0.0012],
        [0.0012],
        [0.0012]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(45321.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9950],
        [0.9950],
        [0.9950],
        ...,
        [0.9950],
        [0.9950],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092350.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1364.7399, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9949],
        [0.9949],
        [0.9949],
        ...,
        [0.9949],
        [0.9949],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092242.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1364.7399, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0016, -0.0029,  0.0062,  ...,  0.0002, -0.0090, -0.0039],
        [ 0.0000,  0.0000, -0.0008,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0008,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0008,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0008,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0008,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1933.2239, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-35.6923, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-188.8208, device='cuda:0')



h[100].sum tensor(-135.2984, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(193.9733, device='cuda:0')



h[200].sum tensor(-179.9427, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-40.1511, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0228,  ..., 0.0009, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0111,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(156815.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0121, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0009, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0009, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0009, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(949934.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(194.4454, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-423.9556, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19643.1895, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-456.6019, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(705.2009, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(219.2585, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0581],
        [ 0.0786],
        [ 0.0742],
        ...,
        [-0.0003],
        [-0.0004],
        [-0.0004]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(54027.0078, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9949],
        [0.9949],
        [0.9949],
        ...,
        [0.9949],
        [0.9949],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092242.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3013],
        [0.7891],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1263.7090, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9948],
        [0.9948],
        [0.9948],
        ...,
        [0.9948],
        [0.9948],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092133.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3013],
        [0.7891],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1263.7090, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0053, -0.0091,  0.0217,  ...,  0.0017, -0.0287, -0.0126],
        [-0.0020, -0.0035,  0.0078,  ...,  0.0003, -0.0110, -0.0048],
        [-0.0053, -0.0091,  0.0217,  ...,  0.0017, -0.0287, -0.0126],
        ...,
        [ 0.0000,  0.0000, -0.0008,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0008,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0008,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1822.1135, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-33.0461, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-174.8425, device='cuda:0')



h[100].sum tensor(-125.5858, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(179.6136, device='cuda:0')



h[200].sum tensor(-167.0647, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-37.1787, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0460,  ..., 0.0028, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0849,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0251,  ..., 0.0016, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(149979.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 2.3592e-02, 0.0000e+00,  ..., 0.0000e+00, 8.3758e-05,
         0.0000e+00],
        [0.0000e+00, 2.9417e-02, 0.0000e+00,  ..., 0.0000e+00, 1.3773e-04,
         0.0000e+00],
        [0.0000e+00, 1.8058e-02, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.0909e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.0909e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.0909e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(914296.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(299.1440, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-394.1639, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18574.7109, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-444.9143, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(733.0076, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(126.7521, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0810],
        [ 0.0751],
        [ 0.0916],
        ...,
        [-0.0020],
        [-0.0019],
        [-0.0019]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(45538.4922, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9948],
        [0.9948],
        [0.9948],
        ...,
        [0.9948],
        [0.9948],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092133.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3115],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1597.2953, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9948],
        [0.9948],
        [0.9948],
        ...,
        [0.9948],
        [0.9948],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092133.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3115],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1597.2953, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0008,  ..., -0.0005,  0.0000,  0.0000],
        [-0.0021, -0.0036,  0.0080,  ...,  0.0004, -0.0113, -0.0050],
        [ 0.0000,  0.0000, -0.0008,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0008,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0008,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0008,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2205.7356, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-41.7203, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-220.9964, device='cuda:0')



h[100].sum tensor(-158.5506, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(227.0269, device='cuda:0')



h[200].sum tensor(-210.9174, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-46.9929, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0344,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0168,  ..., 0.0008, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0128,  ..., 0.0004, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(179544.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0109, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0011, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0011, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0011, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1090662., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(254.0484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-452.3073, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22147.8281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-530.9847, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(643.2150, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(303.0837, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1359],
        [ 0.1308],
        [ 0.1214],
        ...,
        [-0.0022],
        [-0.0022],
        [-0.0022]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(37772.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9948],
        [0.9948],
        [0.9948],
        ...,
        [0.9948],
        [0.9948],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092133.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1696.7344, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9947],
        [0.9947],
        [0.9947],
        ...,
        [0.9947],
        [0.9947],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092024.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1696.7344, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0024, -0.0042,  0.0096,  ...,  0.0005, -0.0133, -0.0058],
        [-0.0017, -0.0030,  0.0066,  ...,  0.0002, -0.0095, -0.0042],
        [-0.0017, -0.0030,  0.0066,  ...,  0.0002, -0.0095, -0.0042],
        ...,
        [ 0.0000,  0.0000, -0.0009,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0009,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0009,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2310.8296, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-44.0624, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-234.7545, device='cuda:0')



h[100].sum tensor(-167.8779, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(241.1604, device='cuda:0')



h[200].sum tensor(-223.3783, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-49.9185, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0541,  ..., 0.0036, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0449,  ..., 0.0027, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0202,  ..., 0.0007, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(193534.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.3508e-02, 0.0000e+00,  ..., 0.0000e+00, 8.7839e-04,
         0.0000e+00],
        [0.0000e+00, 2.5780e-02, 0.0000e+00,  ..., 0.0000e+00, 9.9488e-05,
         0.0000e+00],
        [0.0000e+00, 1.4758e-02, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.2621e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.2621e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.2621e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1216914.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(284.6994, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-461.2341, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(24576.3750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-580.1499, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(521.7343, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(332.8226, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0453],
        [ 0.0825],
        [ 0.1020],
        ...,
        [-0.0040],
        [-0.0040],
        [-0.0040]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(42263.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9947],
        [0.9947],
        [0.9947],
        ...,
        [0.9947],
        [0.9947],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092024.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1337.9180, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9947],
        [0.9947],
        [0.9947],
        ...,
        [0.9947],
        [0.9947],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092024.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1337.9180, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0009,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0009,  ..., -0.0005,  0.0000,  0.0000],
        [-0.0018, -0.0031,  0.0068,  ...,  0.0002, -0.0098, -0.0043],
        ...,
        [ 0.0000,  0.0000, -0.0009,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0009,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0009,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1904.8500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-34.8145, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-185.1099, device='cuda:0')



h[100].sum tensor(-132.6436, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(190.1611, device='cuda:0')



h[200].sum tensor(-176.4955, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-39.3620, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0068,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0353,  ..., 0.0026, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(159737.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0213, 0.0000,  ..., 0.0000, 0.0004, 0.0000],
        ...,
        [0.0013, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0013, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0013, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(988490.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(321.6834, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-398.2998, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19842.0781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-482.7413, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(593.3870, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(134.3672, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1253],
        [ 0.1127],
        [ 0.0979],
        ...,
        [-0.0040],
        [-0.0040],
        [-0.0040]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(35432.7031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9947],
        [0.9947],
        [0.9947],
        ...,
        [0.9947],
        [0.9947],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092024.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2612],
        [0.3628],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1356.6160, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9946],
        [0.9946],
        [0.9946],
        ...,
        [0.9946],
        [0.9946],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091916.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2612],
        [0.3628],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1356.6160, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0035, -0.0062,  0.0144,  ...,  0.0010, -0.0195, -0.0085],
        [-0.0037, -0.0065,  0.0152,  ...,  0.0011, -0.0206, -0.0090],
        [-0.0045, -0.0078,  0.0185,  ...,  0.0014, -0.0248, -0.0109],
        ...,
        [ 0.0000,  0.0000, -0.0009,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0009,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0009,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1918.5992, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-35.0614, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-187.6969, device='cuda:0')



h[100].sum tensor(-133.9254, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(192.8187, device='cuda:0')



h[200].sum tensor(-178.2434, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-39.9121, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0305,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0612,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0688,  ..., 0.0051, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(158985.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 2.1068e-02, 0.0000e+00,  ..., 0.0000e+00, 6.6963e-05,
         0.0000e+00],
        [0.0000e+00, 2.7844e-02, 0.0000e+00,  ..., 0.0000e+00, 4.6679e-04,
         0.0000e+00],
        [0.0000e+00, 2.9077e-02, 0.0000e+00,  ..., 0.0000e+00, 5.8680e-04,
         0.0000e+00],
        ...,
        [1.4172e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.4172e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.4172e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(972571.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(372.9482, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-381.0951, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19391.5938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-488.5946, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(516.8105, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(81.7827, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1160],
        [ 0.1166],
        [ 0.1218],
        ...,
        [-0.0056],
        [-0.0056],
        [-0.0056]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(39359.0117, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9946],
        [0.9946],
        [0.9946],
        ...,
        [0.9946],
        [0.9946],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091916.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1362.3661, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9945],
        [0.9945],
        [0.9945],
        ...,
        [0.9945],
        [0.9945],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091807.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1362.3661, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0009,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0009,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0009,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0009,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0009,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0009,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1925.7118, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-35.1911, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-188.4924, device='cuda:0')



h[100].sum tensor(-134.7644, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(193.6359, device='cuda:0')



h[200].sum tensor(-179.4027, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-40.0812, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(161208., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.4365e-04, 4.5761e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [6.5608e-04, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [6.5608e-04, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.5574e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5574e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5574e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(996747.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(417.7012, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-370.2539, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19842.5293, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-501.5555, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(447.1302, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(51.1131, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0850],
        [ 0.0861],
        [ 0.0898],
        ...,
        [-0.0069],
        [-0.0068],
        [-0.0068]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(46516.3633, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9945],
        [0.9945],
        [0.9945],
        ...,
        [0.9945],
        [0.9945],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091807.7500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 60.0 event: 900 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1417.5375, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9944],
        [0.9944],
        [0.9944],
        ...,
        [0.9944],
        [0.9944],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091699.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1417.5375, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0009,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0009,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0009,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0009,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0009,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0009,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1980.7388, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-36.4692, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-196.1257, device='cuda:0')



h[100].sum tensor(-140.0171, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(201.4776, device='cuda:0')



h[200].sum tensor(-186.4395, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-41.7044, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(166136.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0011, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0015, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0016, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0008, 0.0005, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0017, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1029876.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(482.6348, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-372.0366, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20181.8066, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-521.0284, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(411.1660, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(48.0141, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0176],
        [0.0097],
        [0.0089],
        ...,
        [0.0379],
        [0.0219],
        [0.0040]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(30385.0430, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9944],
        [0.9944],
        [0.9944],
        ...,
        [0.9944],
        [0.9944],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091699.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1331.2137, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9943],
        [0.9943],
        [0.9943],
        ...,
        [0.9943],
        [0.9943],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091590.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1331.2137, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0009,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0009,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0009,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0009,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0009,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0009,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1875.4121, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-34.0285, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-184.1823, device='cuda:0')



h[100].sum tensor(-130.9823, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(189.2082, device='cuda:0')



h[200].sum tensor(-174.4508, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-39.1647, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(157119.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0009, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0017, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0018, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0018, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0018, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(969580.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(493.1489, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-339.5503, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19014.9453, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-502.7764, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(333.0961, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-47.0100, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0826],
        [ 0.0636],
        [ 0.0594],
        ...,
        [-0.0091],
        [-0.0091],
        [-0.0090]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(44475.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9943],
        [0.9943],
        [0.9943],
        ...,
        [0.9943],
        [0.9943],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091590.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5181],
        [0.5444],
        [0.5391],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1375.4944, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9942],
        [0.9942],
        [0.9942],
        ...,
        [0.9942],
        [0.9942],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091482.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5181],
        [0.5444],
        [0.5391],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1375.4944, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0064, -0.0111,  0.0268,  ...,  0.0022, -0.0353, -0.0154],
        [-0.0125, -0.0218,  0.0534,  ...,  0.0048, -0.0692, -0.0302],
        [-0.0125, -0.0219,  0.0537,  ...,  0.0049, -0.0696, -0.0304],
        ...,
        [-0.0033, -0.0058,  0.0134,  ...,  0.0009, -0.0183, -0.0080],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1920.4821, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-35.1350, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-190.3088, device='cuda:0')



h[100].sum tensor(-135.5897, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(195.5019, device='cuda:0')



h[200].sum tensor(-180.6303, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-40.4675, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1211,  ..., 0.0103, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.1572,  ..., 0.0138, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2038,  ..., 0.0184, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0271,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0234,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(163200.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0705, 0.0106,  ..., 0.0000, 0.0135, 0.0000],
        [0.0000, 0.0942, 0.0209,  ..., 0.0000, 0.0199, 0.0000],
        [0.0000, 0.1128, 0.0304,  ..., 0.0000, 0.0250, 0.0000],
        ...,
        [0.0000, 0.0270, 0.0000,  ..., 0.0000, 0.0019, 0.0000],
        [0.0000, 0.0222, 0.0000,  ..., 0.0000, 0.0013, 0.0000],
        [0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.0003, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1015245., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(525.4013, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-342.1311, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19741.1152, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-524.0231, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(296.8174, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-38.8165, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2725],
        [-0.3380],
        [-0.3583],
        ...,
        [-0.0164],
        [-0.0035],
        [ 0.0177]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(39413.1641, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9942],
        [0.9942],
        [0.9942],
        ...,
        [0.9942],
        [0.9942],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091482.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1206.6663, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9941],
        [0.9941],
        [0.9941],
        ...,
        [0.9941],
        [0.9941],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091373.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1206.6663, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [-0.0022, -0.0038,  0.0085,  ...,  0.0004, -0.0120, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1733.9885, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.7560, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-166.9503, device='cuda:0')



h[100].sum tensor(-118.9973, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(171.5060, device='cuda:0')



h[200].sum tensor(-158.5640, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-35.5005, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0084,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0150,  ..., 0.0006, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(147872.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0007, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0016, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0010, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(920697.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(625.0327, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-305.4185, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17542.1680, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-481.4944, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(320.2156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-152.6163, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0460],
        [0.0284],
        [0.0135],
        ...,
        [0.0034],
        [0.0204],
        [0.0431]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(31293.2148, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9941],
        [0.9941],
        [0.9941],
        ...,
        [0.9941],
        [0.9941],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091373.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5796],
        [0.6226],
        [0.0000],
        ...,
        [0.3176],
        [0.3633],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1523.2872, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9940],
        [0.9940],
        [0.9940],
        ...,
        [0.9940],
        [0.9940],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091265.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5796],
        [0.6226],
        [0.0000],
        ...,
        [0.3176],
        [0.3633],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1523.2872, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0105, -0.0183,  0.0449,  ...,  0.0040, -0.0583, -0.0254],
        [-0.0099, -0.0174,  0.0426,  ...,  0.0038, -0.0555, -0.0242],
        [-0.0142, -0.0248,  0.0611,  ...,  0.0056, -0.0790, -0.0344],
        ...,
        [-0.0041, -0.0072,  0.0171,  ...,  0.0013, -0.0230, -0.0100],
        [-0.0021, -0.0036,  0.0081,  ...,  0.0004, -0.0115, -0.0050],
        [-0.0024, -0.0041,  0.0094,  ...,  0.0005, -0.0132, -0.0057]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2055.6992, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-38.5555, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-210.7569, device='cuda:0')



h[100].sum tensor(-149.5600, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(216.5080, device='cuda:0')



h[200].sum tensor(-199.3363, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-44.8156, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1760,  ..., 0.0157, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2221,  ..., 0.0202, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.1254,  ..., 0.0107, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0509,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0485,  ..., 0.0031, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0215,  ..., 0.0009, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(175757.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1065, 0.0248,  ..., 0.0000, 0.0240, 0.0000],
        [0.0000, 0.1184, 0.0306,  ..., 0.0000, 0.0272, 0.0000],
        [0.0000, 0.0909, 0.0168,  ..., 0.0000, 0.0195, 0.0000],
        ...,
        [0.0000, 0.0350, 0.0000,  ..., 0.0000, 0.0032, 0.0000],
        [0.0000, 0.0330, 0.0000,  ..., 0.0000, 0.0027, 0.0000],
        [0.0000, 0.0269, 0.0000,  ..., 0.0000, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1093621.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(526.9720, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-343.6980, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21248.2578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-571.4830, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(235.7551, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-17.4249, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4074],
        [-0.4022],
        [-0.3004],
        ...,
        [ 0.0523],
        [ 0.0622],
        [ 0.0719]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(48686.0547, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9940],
        [0.9940],
        [0.9940],
        ...,
        [0.9940],
        [0.9940],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091265.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5317],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(935.2174, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9940],
        [0.9940],
        [0.9940],
        ...,
        [0.9940],
        [0.9940],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091265.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5317],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(935.2174, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3049e-03, -2.2872e-03,  4.7503e-03,  ...,  4.9212e-05,
         -7.2828e-03, -3.1755e-03],
        [-4.7546e-03, -8.3338e-03,  1.9889e-02,  ...,  1.5430e-03,
         -2.6536e-02, -1.1571e-02],
        [-1.9070e-03, -3.3426e-03,  7.3927e-03,  ...,  3.0993e-04,
         -1.0643e-02, -4.6408e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -9.7617e-04,  ..., -5.1581e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -9.7617e-04,  ..., -5.1581e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -9.7617e-04,  ..., -5.1581e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1445.9054, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.8822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-129.3936, device='cuda:0')



h[100].sum tensor(-92.6412, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(132.9244, device='cuda:0')



h[200].sum tensor(-123.4739, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.5144, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0727,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0329,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0346,  ..., 0.0022, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(125975.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0301, 0.0000,  ..., 0.0000, 0.0026, 0.0000],
        [0.0000, 0.0254, 0.0000,  ..., 0.0000, 0.0010, 0.0000],
        [0.0000, 0.0238, 0.0000,  ..., 0.0000, 0.0010, 0.0000],
        ...,
        [0.0021, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0021, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0021, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(798791.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(973.0044, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-265.5997, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14520.0039, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-411.5744, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(483.3118, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-290.2390, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1833],
        [ 0.1875],
        [ 0.1875],
        ...,
        [-0.0109],
        [-0.0109],
        [-0.0108]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(19773.5605, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9940],
        [0.9940],
        [0.9940],
        ...,
        [0.9940],
        [0.9940],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091265.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9939],
        [0.9939],
        [0.9939],
        ...,
        [0.9939],
        [0.9939],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091157., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-460.7272, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(46237.8516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0021, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0021, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0021, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0021, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0021, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(344779.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1763.4182, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-133.0302, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4040.0461, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-158.9330, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(858.3701, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-748.2974, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0170],
        [-0.0172],
        [-0.0175],
        ...,
        [-0.0113],
        [-0.0112],
        [-0.0112]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-21799.3398, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9939],
        [0.9939],
        [0.9939],
        ...,
        [0.9939],
        [0.9939],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091157., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9938],
        [0.9938],
        [0.9938],
        ...,
        [0.9938],
        [0.9938],
        [0.9938]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091048.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-467.3719, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(46685.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0022, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0022, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0022, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0022, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0022, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0022, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(348886.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1805.9110, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-128.0969, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4017.2046, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-161.8534, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(810.3049, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-760.0004, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0177],
        [-0.0178],
        [-0.0181],
        ...,
        [-0.0117],
        [-0.0117],
        [-0.0117]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-22246.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9938],
        [0.9938],
        [0.9938],
        ...,
        [0.9938],
        [0.9938],
        [0.9938]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091048.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9937],
        [0.9937],
        [0.9937],
        ...,
        [0.9937],
        [0.9937],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1090940.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-473.4016, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(47091.4258, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0022, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0022, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0022, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0022, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0022, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0022, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(352606.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1844.2576, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-123.6203, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3996.6377, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-164.5010, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(766.6230, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-770.6096, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0182],
        [-0.0184],
        [-0.0187],
        ...,
        [-0.0121],
        [-0.0120],
        [-0.0120]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-22657., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9937],
        [0.9937],
        [0.9937],
        ...,
        [0.9937],
        [0.9937],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1090940.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9936],
        [0.9936],
        [0.9936],
        ...,
        [0.9936],
        [0.9936],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1090831.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-478.8713, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(47459.0547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0023, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0023, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0023, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0023, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0023, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0023, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(355976.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1878.8435, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-119.5588, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3978.1187, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-166.9009, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(727.2006, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-780.2258, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0188],
        [-0.0189],
        [-0.0192],
        ...,
        [-0.0124],
        [-0.0124],
        [-0.0124]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-23025.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9936],
        [0.9936],
        [0.9936],
        ...,
        [0.9936],
        [0.9936],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1090831.7500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 70.0 event: 1050 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9935],
        [0.9935],
        [0.9935],
        ...,
        [0.9935],
        [0.9935],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1090723.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-483.8324, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(47792.0352, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0023, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0023, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0023, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0023, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0023, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0023, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(359029.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1910.0205, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-115.8741, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3961.4438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-169.0761, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(693.0707, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-788.9399, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0192],
        [-0.0194],
        [-0.0197],
        ...,
        [-0.0127],
        [-0.0126],
        [-0.0126]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-23355.6797, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9935],
        [0.9935],
        [0.9935],
        ...,
        [0.9935],
        [0.9935],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1090723.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9934],
        [0.9934],
        [0.9934],
        ...,
        [0.9934],
        [0.9934],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1090615., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-488.3338, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(48093.5547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0023, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0023, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0024, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0023, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0023, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0023, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(361801.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1938.1055, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-112.5316, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3946.4297, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-171.0470, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(663.4320, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-796.8351, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0196],
        [-0.0198],
        [-0.0201],
        ...,
        [-0.0130],
        [-0.0129],
        [-0.0129]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-23648.7246, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9934],
        [0.9934],
        [0.9934],
        ...,
        [0.9934],
        [0.9934],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1090615., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9933],
        [0.9933],
        [0.9933],
        ...,
        [0.9933],
        [0.9933],
        [0.9933]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1090506.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-492.4152, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(48366.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0024, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0024, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0024, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0024, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0024, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0024, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(364313., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1963.3879, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-109.4998, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3932.9146, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-172.8327, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(637.3573, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-803.9865, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0200],
        [-0.0202],
        [-0.0205],
        ...,
        [-0.0132],
        [-0.0132],
        [-0.0132]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-23910.9766, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9933],
        [0.9933],
        [0.9933],
        ...,
        [0.9933],
        [0.9933],
        [0.9933]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1090506.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9932],
        [0.9932],
        [0.9932],
        ...,
        [0.9932],
        [0.9932],
        [0.9932]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1090398.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-496.1167, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(48613.5547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0024, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0024, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0024, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0024, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0024, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0024, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(366586.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1986.1316, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-106.7497, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3920.7495, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-174.4499, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(613.6714, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-810.4628, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0203],
        [-0.0205],
        [-0.0208],
        ...,
        [-0.0135],
        [-0.0134],
        [-0.0134]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24132.5195, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9932],
        [0.9932],
        [0.9932],
        ...,
        [0.9932],
        [0.9932],
        [0.9932]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1090398.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9931],
        [0.9931],
        [0.9931],
        ...,
        [0.9931],
        [0.9931],
        [0.9931]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1090289.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0010,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-499.4735, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(48837.0547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0024, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0024, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0024, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0024, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0024, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0024, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(368645.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2006.5740, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-104.2558, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3909.8032, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-175.9145, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(592.1469, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-816.3268, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0206],
        [-0.0208],
        [-0.0211],
        ...,
        [-0.0137],
        [-0.0136],
        [-0.0136]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24260.2656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9931],
        [0.9931],
        [0.9931],
        ...,
        [0.9931],
        [0.9931],
        [0.9931]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1090289.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9930],
        [0.9930],
        [0.9930],
        ...,
        [0.9930],
        [0.9930],
        [0.9930]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1090181.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-502.5162, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(49039.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0024, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0024, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0024, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(370507.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2024.9316, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-101.9940, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3899.9558, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-177.2406, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(572.5896, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-821.6348, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0209],
        [-0.0211],
        [-0.0214],
        ...,
        [-0.0139],
        [-0.0138],
        [-0.0138]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24333.1172, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9930],
        [0.9930],
        [0.9930],
        ...,
        [0.9930],
        [0.9930],
        [0.9930]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1090181.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9929],
        [0.9929],
        [0.9929],
        ...,
        [0.9929],
        [0.9929],
        [0.9929]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1090073.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-505.2756, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(49221.9727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(372190.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2041.4014, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-99.9431, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3891.1008, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-178.4408, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(554.8223, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-826.4384, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0211],
        [-0.0213],
        [-0.0217],
        ...,
        [-0.0141],
        [-0.0140],
        [-0.0140]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24406.7656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9929],
        [0.9929],
        [0.9929],
        ...,
        [0.9929],
        [0.9929],
        [0.9929]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1090073.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9928],
        [0.9928],
        [0.9928],
        ...,
        [0.9928],
        [0.9928],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1089964.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-507.7763, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(49387.1680, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(373708.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2056.1614, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-98.0834, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3883.1411, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-179.5270, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(538.6838, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-830.7841, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0213],
        [-0.0215],
        [-0.0218],
        ...,
        [-0.0143],
        [-0.0142],
        [-0.0142]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24471.3477, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9928],
        [0.9928],
        [0.9928],
        ...,
        [0.9928],
        [0.9928],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1089964.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9927],
        [0.9927],
        [0.9927],
        ...,
        [0.9927],
        [0.9927],
        [0.9927]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1089856.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-510.0430, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(49536.4023, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(375076.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2069.3735, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-96.3971, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3875.9907, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-180.5095, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(524.0257, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-834.7146, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0215],
        [-0.0217],
        [-0.0220],
        ...,
        [-0.0144],
        [-0.0143],
        [-0.0143]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24525.7441, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9927],
        [0.9927],
        [0.9927],
        ...,
        [0.9927],
        [0.9927],
        [0.9927]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1089856.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9926],
        [0.9926],
        [0.9926],
        ...,
        [0.9926],
        [0.9926],
        [0.9926]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1089748.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-512.0970, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(49671.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(376309.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2081.1848, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-94.8681, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3869.5708, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-181.3982, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(510.7130, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-838.2686, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0216],
        [-0.0218],
        [-0.0221],
        ...,
        [-0.0145],
        [-0.0145],
        [-0.0145]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24574.2617, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9926],
        [0.9926],
        [0.9926],
        ...,
        [0.9926],
        [0.9926],
        [0.9926]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1089748.1250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 80.0 event: 1200 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9925],
        [0.9925],
        [0.9925],
        ...,
        [0.9925],
        [0.9925],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1089639.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-513.9592, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(49792.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(377419.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2091.7285, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-93.4820, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3863.8108, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-182.2017, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(498.6236, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-841.4808, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0217],
        [-0.0219],
        [-0.0222],
        ...,
        [-0.0146],
        [-0.0146],
        [-0.0146]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24618.7402, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9925],
        [0.9925],
        [0.9925],
        ...,
        [0.9925],
        [0.9925],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1089639.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9924],
        [0.9924],
        [0.9924],
        ...,
        [0.9924],
        [0.9924],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1089531.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-515.6462, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(49902.5742, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(378416.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2101.1255, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-92.2251, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3858.6470, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-182.9281, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(487.6443, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-844.3832, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0218],
        [-0.0220],
        [-0.0223],
        ...,
        [-0.0148],
        [-0.0147],
        [-0.0147]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24656.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9924],
        [0.9924],
        [0.9924],
        ...,
        [0.9924],
        [0.9924],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1089531.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9923],
        [0.9923],
        [0.9923],
        ...,
        [0.9923],
        [0.9923],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1089423.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-517.1750, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50001.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(379312.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2109.4851, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-91.0856, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3854.0210, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-183.5844, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(477.6733, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-847.0048, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0219],
        [-0.0221],
        [-0.0224],
        ...,
        [-0.0148],
        [-0.0148],
        [-0.0148]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24689.5703, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9923],
        [0.9923],
        [0.9923],
        ...,
        [0.9923],
        [0.9923],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1089423.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9922],
        [0.9922],
        [0.9922],
        ...,
        [0.9922],
        [0.9922],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1089314.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-518.5597, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50090.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(380116.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2116.9072, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-90.0528, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3849.8823, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-184.1772, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(468.6179, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-849.3714, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0219],
        [-0.0221],
        [-0.0225],
        ...,
        [-0.0149],
        [-0.0149],
        [-0.0148]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24716.3359, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9922],
        [0.9922],
        [0.9922],
        ...,
        [0.9922],
        [0.9922],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1089314.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9921],
        [0.9921],
        [0.9921],
        ...,
        [0.9921],
        [0.9921],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1089206.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-519.8149, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50171.0039, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(380840.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2123.4812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-89.1163, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3846.1829, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-184.7123, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(460.3930, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-851.5071, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0220],
        [-0.0222],
        [-0.0225],
        ...,
        [-0.0150],
        [-0.0149],
        [-0.0149]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24739.4707, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9921],
        [0.9921],
        [0.9921],
        ...,
        [0.9921],
        [0.9921],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1089206.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9920],
        [0.9920],
        [0.9920],
        ...,
        [0.9920],
        [0.9920],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1089098.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-520.9519, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50243.2617, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(381488.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2129.2891, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-88.2672, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3842.8809, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-185.1953, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(452.9217, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-853.4337, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0220],
        [-0.0222],
        [-0.0226],
        ...,
        [-0.0151],
        [-0.0150],
        [-0.0150]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24758.1289, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9920],
        [0.9920],
        [0.9920],
        ...,
        [0.9920],
        [0.9920],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1089098.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9919],
        [0.9919],
        [0.9919],
        ...,
        [0.9919],
        [0.9919],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1088989.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-521.9816, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50309.3359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(382079.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2133.5608, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-87.4982, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3839.4121, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-185.5080, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(447.3445, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-855.2990, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0221],
        [-0.0223],
        [-0.0227],
        ...,
        [-0.0151],
        [-0.0151],
        [-0.0150]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24802.6328, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9919],
        [0.9919],
        [0.9919],
        ...,
        [0.9919],
        [0.9919],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1088989.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9918],
        [0.9918],
        [0.9918],
        ...,
        [0.9918],
        [0.9918],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1088881.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-522.9138, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50369.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(382611.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2136.9275, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-86.8015, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3836.0911, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-185.7368, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(442.7932, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-857.0352, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0222],
        [-0.0224],
        [-0.0227],
        ...,
        [-0.0152],
        [-0.0151],
        [-0.0151]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24852.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9918],
        [0.9918],
        [0.9918],
        ...,
        [0.9918],
        [0.9918],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1088881.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9917],
        [0.9917],
        [0.9917],
        ...,
        [0.9917],
        [0.9917],
        [0.9917]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1088773.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-523.7585, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50422.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(383085.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2139.8401, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-86.1696, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3833.1304, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-185.9420, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(438.6515, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-858.5996, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0223],
        [-0.0225],
        [-0.0228],
        ...,
        [-0.0153],
        [-0.0152],
        [-0.0152]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24893.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9917],
        [0.9917],
        [0.9917],
        ...,
        [0.9917],
        [0.9917],
        [0.9917]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1088773.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9916],
        [0.9916],
        [0.9916],
        ...,
        [0.9916],
        [0.9916],
        [0.9916]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1088664.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-524.5233, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50471.0508, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(383507.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2142.3420, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-85.5967, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3830.4949, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-186.1262, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(434.8807, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-860.0079, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0223],
        [-0.0225],
        [-0.0229],
        ...,
        [-0.0153],
        [-0.0152],
        [-0.0152]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24927.5547, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9916],
        [0.9916],
        [0.9916],
        ...,
        [0.9916],
        [0.9916],
        [0.9916]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1088664.7500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 90.0 event: 1350 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9915],
        [0.9915],
        [0.9915],
        ...,
        [0.9915],
        [0.9915],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1088556.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-525.2162, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50514.1758, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(383880.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2144.4739, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-85.0773, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3828.1536, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-186.2911, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(431.4462, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-861.2748, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0224],
        [-0.0226],
        [-0.0229],
        ...,
        [-0.0153],
        [-0.0153],
        [-0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24954.1074, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9915],
        [0.9915],
        [0.9915],
        ...,
        [0.9915],
        [0.9915],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1088556.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9914],
        [0.9914],
        [0.9914],
        ...,
        [0.9914],
        [0.9914],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1088448.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-525.8436, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50552.7461, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(384210.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2146.2722, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-84.6063, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3826.0776, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-186.4388, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(428.3170, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-862.4138, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0224],
        [-0.0226],
        [-0.0229],
        ...,
        [-0.0154],
        [-0.0153],
        [-0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24974.4336, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9914],
        [0.9914],
        [0.9914],
        ...,
        [0.9914],
        [0.9914],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1088448.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9913],
        [0.9913],
        [0.9913],
        ...,
        [0.9913],
        [0.9913],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1088339.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-526.4113, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50587.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(384501.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2147.7686, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-84.1791, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3824.2429, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-186.5706, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(425.4642, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-863.4366, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0224],
        [-0.0226],
        [-0.0229],
        ...,
        [-0.0154],
        [-0.0154],
        [-0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24988.9922, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9913],
        [0.9913],
        [0.9913],
        ...,
        [0.9913],
        [0.9913],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1088339.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9912],
        [0.9912],
        [0.9912],
        ...,
        [0.9912],
        [0.9912],
        [0.9912]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1088231.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-526.9260, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50617.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(384757.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2148.9941, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-83.7915, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3822.6248, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-186.6882, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(422.8622, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-864.3543, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0224],
        [-0.0226],
        [-0.0230],
        ...,
        [-0.0154],
        [-0.0154],
        [-0.0154]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24998.2930, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9912],
        [0.9912],
        [0.9912],
        ...,
        [0.9912],
        [0.9912],
        [0.9912]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1088231.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9911],
        [0.9911],
        [0.9911],
        ...,
        [0.9911],
        [0.9911],
        [0.9911]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1088123.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-527.3907, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50645.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(384980.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2149.9741, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-83.4398, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3821.2029, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-186.7928, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(420.4875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-865.1768, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0224],
        [-0.0226],
        [-0.0230],
        ...,
        [-0.0155],
        [-0.0154],
        [-0.0154]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-25002.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9911],
        [0.9911],
        [0.9911],
        ...,
        [0.9911],
        [0.9911],
        [0.9911]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1088123.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9910],
        [0.9910],
        [0.9910],
        ...,
        [0.9910],
        [0.9910],
        [0.9910]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1088014.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-527.8125, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50669.4141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(385174.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2150.7339, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-83.1208, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3819.9585, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-186.8859, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(418.3174, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-865.9131, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0225],
        [-0.0227],
        [-0.0230],
        ...,
        [-0.0155],
        [-0.0154],
        [-0.0154]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-25003.0352, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9910],
        [0.9910],
        [0.9910],
        ...,
        [0.9910],
        [0.9910],
        [0.9910]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1088014.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9909],
        [0.9909],
        [0.9909],
        ...,
        [0.9909],
        [0.9909],
        [0.9909]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1087906.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-528.1935, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50690.8672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(385342.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2151.2939, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-82.8315, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3818.8745, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-186.9682, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(416.3342, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-866.5712, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0225],
        [-0.0227],
        [-0.0230],
        ...,
        [-0.0155],
        [-0.0154],
        [-0.0154]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24999.3379, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9909],
        [0.9909],
        [0.9909],
        ...,
        [0.9909],
        [0.9909],
        [0.9909]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1087906.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9908],
        [0.9908],
        [0.9908],
        ...,
        [0.9908],
        [0.9908],
        [0.9908]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1087798.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-528.5389, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50709.8086, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(385485.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2151.6746, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-82.5686, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3817.9351, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-187.0409, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(414.5197, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-867.1586, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0225],
        [-0.0227],
        [-0.0230],
        ...,
        [-0.0155],
        [-0.0154],
        [-0.0154]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24992.0820, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9908],
        [0.9908],
        [0.9908],
        ...,
        [0.9908],
        [0.9908],
        [0.9908]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1087798.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9907],
        [0.9907],
        [0.9907],
        ...,
        [0.9907],
        [0.9907],
        [0.9907]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1087689.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-528.8517, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50726.4609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(385608., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2151.8928, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-82.3301, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3817.1270, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-187.1051, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(412.8582, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-867.6818, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0225],
        [-0.0227],
        [-0.0230],
        ...,
        [-0.0155],
        [-0.0154],
        [-0.0154]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24981.6289, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9907],
        [0.9907],
        [0.9907],
        ...,
        [0.9907],
        [0.9907],
        [0.9907]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1087689.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9906],
        [0.9906],
        [0.9906],
        ...,
        [0.9906],
        [0.9906],
        [0.9906]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1087581.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-529.1343, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50741.0547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(385710.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2151.9658, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-82.1134, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3816.4365, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-187.1614, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(411.3353, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-868.1471, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0225],
        [-0.0227],
        [-0.0230],
        ...,
        [-0.0155],
        [-0.0155],
        [-0.0154]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24968.2676, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9906],
        [0.9906],
        [0.9906],
        ...,
        [0.9906],
        [0.9906],
        [0.9906]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1087581.5000, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 100.0 event: 1500 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9905],
        [0.9905],
        [0.9905],
        ...,
        [0.9905],
        [0.9905],
        [0.9905]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1087473.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-529.3899, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50753.7695, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(385795., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2151.9058, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-81.9165, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3815.8533, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-187.2105, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(409.9380, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-868.5602, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0224],
        [-0.0226],
        [-0.0230],
        ...,
        [-0.0155],
        [-0.0155],
        [-0.0154]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24952.2734, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9905],
        [0.9905],
        [0.9905],
        ...,
        [0.9905],
        [0.9905],
        [0.9905]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1087473.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9904],
        [0.9904],
        [0.9904],
        ...,
        [0.9904],
        [0.9904],
        [0.9904]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1087364.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-529.6228, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50764.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(385863.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2151.7271, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-81.7378, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3815.3662, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-187.2530, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(408.6539, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-868.9253, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0224],
        [-0.0226],
        [-0.0230],
        ...,
        [-0.0155],
        [-0.0155],
        [-0.0154]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24933.8984, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9904],
        [0.9904],
        [0.9904],
        ...,
        [0.9904],
        [0.9904],
        [0.9904]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1087364.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9903],
        [0.9903],
        [0.9903],
        ...,
        [0.9903],
        [0.9903],
        [0.9903]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1087256.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-529.8314, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50774.2930, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(385917.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2151.4414, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-81.5752, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3814.9658, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-187.2899, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(407.4734, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-869.2474, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0224],
        [-0.0226],
        [-0.0230],
        ...,
        [-0.0155],
        [-0.0154],
        [-0.0154]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24913.3711, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9903],
        [0.9903],
        [0.9903],
        ...,
        [0.9903],
        [0.9903],
        [0.9903]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1087256.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9902],
        [0.9902],
        [0.9902],
        ...,
        [0.9902],
        [0.9902],
        [0.9902]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1087148.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-530.0210, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50782.3984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(385958.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2151.0576, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-81.4272, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3814.6445, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-187.3214, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(406.3862, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-869.5308, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0224],
        [-0.0226],
        [-0.0230],
        ...,
        [-0.0155],
        [-0.0154],
        [-0.0154]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24890.8965, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9902],
        [0.9902],
        [0.9902],
        ...,
        [0.9902],
        [0.9902],
        [0.9902]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1087148.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9901],
        [0.9901],
        [0.9901],
        ...,
        [0.9901],
        [0.9901],
        [0.9901]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1087039.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-530.1925, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50789.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(385987.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2150.5864, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-81.2927, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3814.3938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-187.3481, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(405.3835, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-869.7788, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0224],
        [-0.0226],
        [-0.0229],
        ...,
        [-0.0155],
        [-0.0154],
        [-0.0154]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24866.6523, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9901],
        [0.9901],
        [0.9901],
        ...,
        [0.9901],
        [0.9901],
        [0.9901]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1087039.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9900],
        [0.9900],
        [0.9900],
        ...,
        [0.9900],
        [0.9900],
        [0.9900]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1086931.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-530.3480, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50794.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(386005.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2150.0361, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-81.1702, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3814.2073, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-187.3705, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(404.4571, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-869.9949, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0224],
        [-0.0226],
        [-0.0229],
        ...,
        [-0.0155],
        [-0.0154],
        [-0.0154]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24840.8145, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9900],
        [0.9900],
        [0.9900],
        ...,
        [0.9900],
        [0.9900],
        [0.9900]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1086931.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9899],
        [0.9899],
        [0.9899],
        ...,
        [0.9899],
        [0.9899],
        [0.9899]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1086823.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-530.4894, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50799.6484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(386013.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2149.4141, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-81.0588, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3814.0789, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-187.3889, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(403.6006, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-870.1821, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0224],
        [-0.0226],
        [-0.0229],
        ...,
        [-0.0155],
        [-0.0154],
        [-0.0154]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24813.5430, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9899],
        [0.9899],
        [0.9899],
        ...,
        [0.9899],
        [0.9899],
        [0.9899]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1086823.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9898],
        [0.9898],
        [0.9898],
        ...,
        [0.9898],
        [0.9898],
        [0.9898]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1086714.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-530.6158, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50803.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(386012.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2148.7266, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.9571, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3814.0022, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-187.4039, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(402.8065, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-870.3430, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0223],
        [-0.0225],
        [-0.0229],
        ...,
        [-0.0155],
        [-0.0154],
        [-0.0154]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24784.9609, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9898],
        [0.9898],
        [0.9898],
        ...,
        [0.9898],
        [0.9898],
        [0.9898]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1086714.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9897],
        [0.9897],
        [0.9897],
        ...,
        [0.9897],
        [0.9897],
        [0.9897]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1086606.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-530.7305, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50806.3203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(386004.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2147.9800, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.8644, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3813.9727, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-187.4156, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(402.0689, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-870.4803, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0223],
        [-0.0225],
        [-0.0229],
        ...,
        [-0.0155],
        [-0.0154],
        [-0.0154]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24755.2012, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9897],
        [0.9897],
        [0.9897],
        ...,
        [0.9897],
        [0.9897],
        [0.9897]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1086606.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9896],
        [0.9896],
        [0.9896],
        ...,
        [0.9896],
        [0.9896],
        [0.9896]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1086498.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-530.8352, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50808.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(385988.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2147.1809, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.7798, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3813.9849, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-187.4244, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(401.3833, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-870.5962, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0223],
        [-0.0225],
        [-0.0228],
        ...,
        [-0.0154],
        [-0.0154],
        [-0.0154]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24724.3789, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9896],
        [0.9896],
        [0.9896],
        ...,
        [0.9896],
        [0.9896],
        [0.9896]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1086498.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 110.0 event: 1650 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9895],
        [0.9895],
        [0.9895],
        ...,
        [0.9895],
        [0.9895],
        [0.9895]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1086390., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-530.9286, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50809.9297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(385965.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2146.3323, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.7024, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3814.0349, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-187.4306, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(400.7440, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-870.6926, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0223],
        [-0.0225],
        [-0.0228],
        ...,
        [-0.0154],
        [-0.0154],
        [-0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24692.6133, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9895],
        [0.9895],
        [0.9895],
        ...,
        [0.9895],
        [0.9895],
        [0.9895]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1086390., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9894],
        [0.9894],
        [0.9894],
        ...,
        [0.9894],
        [0.9894],
        [0.9894]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1086281.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-531.0137, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50810.7617, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(385937.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2145.4397, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.6318, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3814.1191, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-187.4344, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(400.1474, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-870.7715, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0223],
        [-0.0225],
        [-0.0228],
        ...,
        [-0.0154],
        [-0.0154],
        [-0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24659.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9894],
        [0.9894],
        [0.9894],
        ...,
        [0.9894],
        [0.9894],
        [0.9894]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1086281.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9893],
        [0.9893],
        [0.9893],
        ...,
        [0.9893],
        [0.9893],
        [0.9893]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1086173.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-531.0906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50811.0352, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(385903.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2144.5059, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.5671, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3814.2344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-187.4360, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(399.5888, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-870.8347, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0222],
        [-0.0224],
        [-0.0228],
        ...,
        [-0.0154],
        [-0.0153],
        [-0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24626.6680, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9893],
        [0.9893],
        [0.9893],
        ...,
        [0.9893],
        [0.9893],
        [0.9893]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1086173.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9892],
        [0.9892],
        [0.9892],
        ...,
        [0.9892],
        [0.9892],
        [0.9892]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1086065., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-531.1600, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50810.7930, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(385864.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2143.5356, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.5078, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3814.3777, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-187.4358, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(399.0648, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-870.8834, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0222],
        [-0.0224],
        [-0.0227],
        ...,
        [-0.0154],
        [-0.0153],
        [-0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24592.9492, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9892],
        [0.9892],
        [0.9892],
        ...,
        [0.9892],
        [0.9892],
        [0.9892]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1086065., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9891],
        [0.9891],
        [0.9891],
        ...,
        [0.9891],
        [0.9891],
        [0.9891]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085956.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-531.2235, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50810.0859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(385821.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2142.5322, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.4535, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3814.5454, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-187.4337, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(398.5724, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-870.9193, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0222],
        [-0.0224],
        [-0.0227],
        ...,
        [-0.0154],
        [-0.0153],
        [-0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24558.6289, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9891],
        [0.9891],
        [0.9891],
        ...,
        [0.9891],
        [0.9891],
        [0.9891]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085956.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9890],
        [0.9890],
        [0.9890],
        ...,
        [0.9890],
        [0.9890],
        [0.9890]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085848.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-531.2805, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50808.9648, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(385774.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2141.4980, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.4035, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3814.7356, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-187.4300, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(398.1090, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-870.9432, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0222],
        [-0.0224],
        [-0.0227],
        ...,
        [-0.0153],
        [-0.0153],
        [-0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24523.7617, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9890],
        [0.9890],
        [0.9890],
        ...,
        [0.9890],
        [0.9890],
        [0.9890]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085848.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9889],
        [0.9889],
        [0.9889],
        ...,
        [0.9889],
        [0.9889],
        [0.9889]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085740., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-531.3320, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50807.4648, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(385723.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2140.4355, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.3576, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3814.9458, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-187.4249, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(397.6713, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-870.9565, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0221],
        [-0.0223],
        [-0.0227],
        ...,
        [-0.0153],
        [-0.0153],
        [-0.0152]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24488.3535, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9889],
        [0.9889],
        [0.9889],
        ...,
        [0.9889],
        [0.9889],
        [0.9889]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085740., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9888],
        [0.9888],
        [0.9888],
        ...,
        [0.9888],
        [0.9888],
        [0.9888]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085631.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-531.3784, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50805.6211, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(385669.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2139.3472, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.3153, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3815.1741, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-187.4185, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(397.2567, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-870.9603, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0221],
        [-0.0223],
        [-0.0226],
        ...,
        [-0.0153],
        [-0.0152],
        [-0.0152]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24452.5039, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9888],
        [0.9888],
        [0.9888],
        ...,
        [0.9888],
        [0.9888],
        [0.9888]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085631.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9887],
        [0.9887],
        [0.9887],
        ...,
        [0.9887],
        [0.9887],
        [0.9887]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085523.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-531.4203, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50803.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(385612.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2138.2341, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.2764, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3815.4182, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-187.4109, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(396.8633, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-870.9555, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0221],
        [-0.0223],
        [-0.0226],
        ...,
        [-0.0153],
        [-0.0152],
        [-0.0152]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24416.2324, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9887],
        [0.9887],
        [0.9887],
        ...,
        [0.9887],
        [0.9887],
        [0.9887]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085523.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9886],
        [0.9886],
        [0.9886],
        ...,
        [0.9886],
        [0.9886],
        [0.9886]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085415., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-531.4584, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50801.0391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(385553.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2137.0996, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.2403, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3815.6765, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-187.4022, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(396.4891, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-870.9426, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0220],
        [-0.0222],
        [-0.0226],
        ...,
        [-0.0153],
        [-0.0152],
        [-0.0152]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24379.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9886],
        [0.9886],
        [0.9886],
        ...,
        [0.9886],
        [0.9886],
        [0.9886]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085415., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 120.0 event: 1800 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9885],
        [0.9885],
        [0.9885],
        ...,
        [0.9885],
        [0.9885],
        [0.9885]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085306.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-531.4926, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50798.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(385491.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2135.9448, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.2069, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3815.9480, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-187.3926, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(396.1328, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-870.9229, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0220],
        [-0.0222],
        [-0.0225],
        ...,
        [-0.0152],
        [-0.0152],
        [-0.0152]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24342.6484, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9885],
        [0.9885],
        [0.9885],
        ...,
        [0.9885],
        [0.9885],
        [0.9885]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085306.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9885],
        [0.9885],
        [0.9885],
        ...,
        [0.9885],
        [0.9885],
        [0.9885]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085306.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-531.4926, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50798.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(385491.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2135.9448, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.2069, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3815.9480, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-187.3926, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(396.1328, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-870.9229, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0220],
        [-0.0222],
        [-0.0225],
        ...,
        [-0.0152],
        [-0.0152],
        [-0.0152]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24342.6484, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9885],
        [0.9885],
        [0.9885],
        ...,
        [0.9885],
        [0.9885],
        [0.9885]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085306.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9884],
        [0.9884],
        [0.9884],
        ...,
        [0.9884],
        [0.9884],
        [0.9884]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085198.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-531.5243, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50795.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(385426.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2134.7717, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.1761, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3816.2307, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-187.3821, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(395.7921, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-870.8964, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0220],
        [-0.0222],
        [-0.0225],
        ...,
        [-0.0152],
        [-0.0152],
        [-0.0151]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24305.4180, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9884],
        [0.9884],
        [0.9884],
        ...,
        [0.9884],
        [0.9884],
        [0.9884]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085198.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9883],
        [0.9883],
        [0.9883],
        ...,
        [0.9883],
        [0.9883],
        [0.9883]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085090.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-531.5520, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50792.3164, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(385360.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2133.5811, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.1473, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3816.5237, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-187.3708, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(395.4659, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-870.8642, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0220],
        [-0.0222],
        [-0.0225],
        ...,
        [-0.0152],
        [-0.0151],
        [-0.0151]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24267.9355, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9883],
        [0.9883],
        [0.9883],
        ...,
        [0.9883],
        [0.9883],
        [0.9883]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085090.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9883],
        [0.9883],
        [0.9883],
        ...,
        [0.9883],
        [0.9883],
        [0.9883]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085090.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-531.5520, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50792.3164, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(385360.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2133.5811, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.1473, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3816.5237, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-187.3708, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(395.4659, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-870.8642, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0220],
        [-0.0222],
        [-0.0225],
        ...,
        [-0.0152],
        [-0.0151],
        [-0.0151]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24267.9355, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9883],
        [0.9883],
        [0.9883],
        ...,
        [0.9883],
        [0.9883],
        [0.9883]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085090.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9882],
        [0.9882],
        [0.9882],
        ...,
        [0.9882],
        [0.9882],
        [0.9882]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1084981.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-531.5778, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50789.0078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(385292.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2132.3745, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.1206, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3816.8257, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-187.3589, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(395.1525, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-870.8267, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0219],
        [-0.0221],
        [-0.0225],
        ...,
        [-0.0152],
        [-0.0151],
        [-0.0151]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24230.2090, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9882],
        [0.9882],
        [0.9882],
        ...,
        [0.9882],
        [0.9882],
        [0.9882]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1084981.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9881],
        [0.9881],
        [0.9881],
        ...,
        [0.9881],
        [0.9881],
        [0.9881]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1084873.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-531.6006, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50785.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(385222.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2131.1528, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.0956, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3817.1357, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-187.3462, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(394.8509, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-870.7844, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0219],
        [-0.0221],
        [-0.0224],
        ...,
        [-0.0152],
        [-0.0151],
        [-0.0151]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24192.2773, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9881],
        [0.9881],
        [0.9881],
        ...,
        [0.9881],
        [0.9881],
        [0.9881]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1084873.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9880],
        [0.9880],
        [0.9880],
        ...,
        [0.9880],
        [0.9880],
        [0.9880]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1084765.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-531.6208, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50781.8945, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(385151.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2129.9167, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.0724, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3817.4531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-187.3330, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(394.5604, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-870.7380, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0219],
        [-0.0221],
        [-0.0224],
        ...,
        [-0.0151],
        [-0.0151],
        [-0.0151]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24154.2891, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9880],
        [0.9880],
        [0.9880],
        ...,
        [0.9880],
        [0.9880],
        [0.9880]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1084765.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9879],
        [0.9879],
        [0.9879],
        ...,
        [0.9879],
        [0.9879],
        [0.9879]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1084656.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-531.6396, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50778.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(385079.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2128.6682, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.0506, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3817.7773, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-187.3193, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(394.2794, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-870.6876, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0218],
        [-0.0220],
        [-0.0224],
        ...,
        [-0.0151],
        [-0.0151],
        [-0.0150]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24116.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9879],
        [0.9879],
        [0.9879],
        ...,
        [0.9879],
        [0.9879],
        [0.9879]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1084656.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9878],
        [0.9878],
        [0.9878],
        ...,
        [0.9878],
        [0.9878],
        [0.9878]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1084548.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-531.6570, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50774.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(385005.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2127.4067, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.0300, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3818.1060, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-187.3050, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(394.0069, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-870.6339, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0218],
        [-0.0220],
        [-0.0223],
        ...,
        [-0.0151],
        [-0.0150],
        [-0.0150]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24078.2051, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9878],
        [0.9878],
        [0.9878],
        ...,
        [0.9878],
        [0.9878],
        [0.9878]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1084548.5000, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 130.0 event: 1950 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9877],
        [0.9877],
        [0.9877],
        ...,
        [0.9877],
        [0.9877],
        [0.9877]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1084440.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-531.6720, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50770.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(384930.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2126.1343, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.0108, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3818.4404, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-187.2903, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(393.7427, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-870.5768, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0218],
        [-0.0220],
        [-0.0223],
        ...,
        [-0.0151],
        [-0.0150],
        [-0.0150]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24039.9863, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9877],
        [0.9877],
        [0.9877],
        ...,
        [0.9877],
        [0.9877],
        [0.9877]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1084440.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9876],
        [0.9876],
        [0.9876],
        ...,
        [0.9876],
        [0.9876],
        [0.9876]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1084332., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0011,  ..., -0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-531.6859, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(50766.1133, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(384854.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2124.8516, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-79.9926, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3818.7783, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-187.2753, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(393.4859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-870.5168, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0218],
        [-0.0219],
        [-0.0223],
        ...,
        [-0.0150],
        [-0.0150],
        [-0.0150]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-24001.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9876],
        [0.9876],
        [0.9876],
        ...,
        [0.9876],
        [0.9876],
        [0.9876]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1084332., device='cuda:0', grad_fn=<SumBackward0>)
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/./Training.py", line 76, in <module>
    featbatch = TraTen[i : i + BatchSize].reshape(BatchSize * 6796, 1)
RuntimeError: shape '[101940, 1]' is invalid for input of size 33980

real	0m43.353s
user	0m12.169s
sys	0m9.046s
